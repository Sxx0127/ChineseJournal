nohup: ignoring input
/home/suxiaoxin/.conda/envs/sxx/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at ./data/glue/glue.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at ./model/models--distilbert-base-multilingual-cased/ and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 294,912 || all params: 135,621,122 || trainable%: 0.2175
ROUND:0
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:00<00:09,  1.51it/s]                                              {'loss': 0.7191, 'grad_norm': 0.11958245187997818, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:09,  1.51it/s]                                              {'loss': 0.7148, 'grad_norm': 0.07216303050518036, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:08,  1.51it/s] 20%|██        | 3/15 [00:00<00:02,  4.67it/s]                                              {'loss': 0.745, 'grad_norm': 0.18137137591838837, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:02,  4.67it/s]                                              {'loss': 0.7305, 'grad_norm': 0.13202957808971405, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:02,  4.67it/s] 33%|███▎      | 5/15 [00:00<00:01,  7.12it/s]                                              {'loss': 0.7068, 'grad_norm': 0.08639265596866608, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:01,  7.12it/s]                                              {'loss': 0.6997, 'grad_norm': 0.10070497542619705, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:01,  7.12it/s] 47%|████▋     | 7/15 [00:01<00:00,  9.49it/s]                                              {'loss': 0.7036, 'grad_norm': 0.08851736038923264, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:01<00:00,  9.49it/s]                                              {'loss': 0.728, 'grad_norm': 0.14325712621212006, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:01<00:00,  9.49it/s] 60%|██████    | 9/15 [00:01<00:00, 11.60it/s]                                              {'loss': 0.7163, 'grad_norm': 0.09841883927583694, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:01<00:00, 11.60it/s]                                              {'loss': 0.7109, 'grad_norm': 0.10163452476263046, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:01<00:00, 11.60it/s] 73%|███████▎  | 11/15 [00:01<00:00, 12.30it/s]                                               {'loss': 0.7104, 'grad_norm': 0.11543317884206772, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:01<00:00, 12.30it/s]                                               {'loss': 0.7251, 'grad_norm': 0.13826048374176025, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:01<00:00, 12.30it/s] 87%|████████▋ | 13/15 [00:01<00:00, 13.48it/s]                                               {'loss': 0.7219, 'grad_norm': 0.12573787569999695, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:01<00:00, 13.48it/s]                                               {'loss': 0.7221, 'grad_norm': 0.10681171715259552, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:01<00:00, 13.48it/s]100%|██████████| 15/15 [00:01<00:00, 14.75it/s]                                               {'loss': 0.7195, 'grad_norm': 0.12306642532348633, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.75it/s]                                               {'train_runtime': 1.5924, 'train_samples_per_second': 266.889, 'train_steps_per_second': 9.42, 'train_loss': 0.718246332804362, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.75it/s]100%|██████████| 15/15 [00:01<00:00,  9.42it/s]
CLIENT:64
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7118, 'grad_norm': 0.0946613997220993, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.72it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.7459, 'grad_norm': 0.1785169243812561, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.7192, 'grad_norm': 0.09966537356376648, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.29it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.99it/s]                                              {'loss': 0.6969, 'grad_norm': 0.05659407004714012, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.99it/s]                                              {'loss': 0.7431, 'grad_norm': 0.17028391361236572, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.99it/s] 40%|████      | 6/15 [00:00<00:00, 16.98it/s]                                              {'loss': 0.726, 'grad_norm': 0.1298728734254837, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.98it/s]                                              {'loss': 0.7157, 'grad_norm': 0.07839111983776093, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.98it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.54it/s]                                              {'loss': 0.7312, 'grad_norm': 0.13430579006671906, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.54it/s]                                              {'loss': 0.7427, 'grad_norm': 0.18192510306835175, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.54it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.82it/s]                                               {'loss': 0.7201, 'grad_norm': 0.09080897271633148, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.82it/s]                                               {'loss': 0.7223, 'grad_norm': 0.1732570379972458, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.82it/s] 80%|████████  | 12/15 [00:00<00:00, 16.56it/s]                                               {'loss': 0.7093, 'grad_norm': 0.11112162470817566, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.56it/s]                                               {'loss': 0.7261, 'grad_norm': 0.10174143314361572, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.56it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.7262, 'grad_norm': 0.1296411156654358, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.7401, 'grad_norm': 0.1713544875383377, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.57it/s]                                               {'train_runtime': 1.0005, 'train_samples_per_second': 424.794, 'train_steps_per_second': 14.993, 'train_loss': 0.7251157641410828, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.57it/s]100%|██████████| 15/15 [00:01<00:00, 15.00it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7341, 'grad_norm': 0.14404992759227753, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.95it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.52it/s]                                              {'loss': 0.7426, 'grad_norm': 0.1543341726064682, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.52it/s]                                              {'loss': 0.7105, 'grad_norm': 0.11125723272562027, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.52it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.7209, 'grad_norm': 0.09071685373783112, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.7533, 'grad_norm': 0.18154755234718323, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.33it/s] 40%|████      | 6/15 [00:00<00:00, 16.60it/s]                                              {'loss': 0.7354, 'grad_norm': 0.14020152390003204, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.60it/s]                                              {'loss': 0.7181, 'grad_norm': 0.12520861625671387, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.60it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.7188, 'grad_norm': 0.12087809294462204, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.7463, 'grad_norm': 0.19042420387268066, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.36it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.20it/s]                                               {'loss': 0.7365, 'grad_norm': 0.1496313214302063, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.20it/s]                                               {'loss': 0.7103, 'grad_norm': 0.08957493305206299, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.20it/s] 80%|████████  | 12/15 [00:00<00:00, 17.19it/s]                                               {'loss': 0.7455, 'grad_norm': 0.2009790539741516, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 17.19it/s]                                               {'loss': 0.7328, 'grad_norm': 0.16770674288272858, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 17.19it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.96it/s]                                               {'loss': 0.7207, 'grad_norm': 0.1130080446600914, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.96it/s]                                               {'loss': 0.7428, 'grad_norm': 0.1315545290708542, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.96it/s]                                               {'train_runtime': 0.9931, 'train_samples_per_second': 427.953, 'train_steps_per_second': 15.104, 'train_loss': 0.7312243580818176, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.96it/s]100%|██████████| 15/15 [00:00<00:00, 15.11it/s]
CLIENT:80
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7158, 'grad_norm': 0.06345166265964508, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.63it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.72it/s]                                              {'loss': 0.7092, 'grad_norm': 0.08479905128479004, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.72it/s]                                              {'loss': 0.7462, 'grad_norm': 0.17151404917240143, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.72it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.7162, 'grad_norm': 0.08608434349298477, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.7245, 'grad_norm': 0.12920014560222626, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.52it/s] 40%|████      | 6/15 [00:00<00:00, 16.70it/s]                                              {'loss': 0.7039, 'grad_norm': 0.06704971939325333, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.70it/s]                                              {'loss': 0.7088, 'grad_norm': 0.08596432954072952, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.70it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.7006, 'grad_norm': 0.0685030072927475, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.7427, 'grad_norm': 0.14122967422008514, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.46it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.29it/s]                                               {'loss': 0.7205, 'grad_norm': 0.11160759627819061, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.29it/s]                                               {'loss': 0.6993, 'grad_norm': 0.06929998099803925, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.29it/s] 80%|████████  | 12/15 [00:00<00:00, 16.71it/s]                                               {'loss': 0.7521, 'grad_norm': 0.13091693818569183, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.71it/s]                                               {'loss': 0.728, 'grad_norm': 0.14201508462429047, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.71it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.6932, 'grad_norm': 0.047678425908088684, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.7316, 'grad_norm': 0.14379553496837616, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.56it/s]                                               {'train_runtime': 0.9979, 'train_samples_per_second': 425.884, 'train_steps_per_second': 15.031, 'train_loss': 0.7194949587186178, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.56it/s]100%|██████████| 15/15 [00:00<00:00, 15.04it/s]
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7167, 'grad_norm': 0.10332008451223373, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.58it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.7353, 'grad_norm': 0.13156865537166595, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.7405, 'grad_norm': 0.15159659087657928, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.27it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.04it/s]                                              {'loss': 0.7244, 'grad_norm': 0.13449355959892273, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.04it/s]                                              {'loss': 0.7244, 'grad_norm': 0.09853758662939072, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.04it/s] 40%|████      | 6/15 [00:00<00:00, 17.04it/s]                                              {'loss': 0.7332, 'grad_norm': 0.15245921909809113, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 17.04it/s]                                              {'loss': 0.7283, 'grad_norm': 0.1247231662273407, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 17.04it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.73it/s]                                              {'loss': 0.7267, 'grad_norm': 0.11759144067764282, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.73it/s]                                              {'loss': 0.7234, 'grad_norm': 0.14156939089298248, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.73it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.24it/s]                                               {'loss': 0.7297, 'grad_norm': 0.13168758153915405, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.24it/s]                                               {'loss': 0.7119, 'grad_norm': 0.10933154076337814, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.24it/s] 80%|████████  | 12/15 [00:00<00:00, 16.72it/s]                                               {'loss': 0.7422, 'grad_norm': 0.15630455315113068, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.72it/s]                                               {'loss': 0.7275, 'grad_norm': 0.15003935992717743, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.72it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.705, 'grad_norm': 0.10330343246459961, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.7301, 'grad_norm': 0.15012672543525696, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.57it/s]                                               {'train_runtime': 0.993, 'train_samples_per_second': 428.01, 'train_steps_per_second': 15.106, 'train_loss': 0.7266236384709676, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.57it/s]100%|██████████| 15/15 [00:00<00:00, 15.11it/s]
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7363, 'grad_norm': 0.13605909049510956, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.66it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7352, 'grad_norm': 0.12865322828292847, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7334, 'grad_norm': 0.11336804181337357, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.25it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.25it/s]                                              {'loss': 0.721, 'grad_norm': 0.11560626327991486, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.25it/s]                                              {'loss': 0.7366, 'grad_norm': 0.14037244021892548, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.25it/s] 40%|████      | 6/15 [00:00<00:00, 17.58it/s]                                              {'loss': 0.7329, 'grad_norm': 0.1250438541173935, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 17.58it/s]                                              {'loss': 0.7178, 'grad_norm': 0.08740853518247604, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 17.58it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.71it/s]                                              {'loss': 0.7262, 'grad_norm': 0.16038423776626587, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.71it/s]                                              {'loss': 0.7311, 'grad_norm': 0.14520716667175293, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.71it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.19it/s]                                               {'loss': 0.718, 'grad_norm': 0.10980381816625595, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.19it/s]                                               {'loss': 0.7321, 'grad_norm': 0.13196316361427307, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.19it/s] 80%|████████  | 12/15 [00:00<00:00, 16.67it/s]                                               {'loss': 0.7381, 'grad_norm': 0.17380291223526, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.67it/s]                                               {'loss': 0.7278, 'grad_norm': 0.1548619419336319, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.67it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.7158, 'grad_norm': 0.11502545326948166, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.7368, 'grad_norm': 0.1332099586725235, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.49it/s]                                               {'train_runtime': 0.9883, 'train_samples_per_second': 430.035, 'train_steps_per_second': 15.178, 'train_loss': 0.7292764584223429, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.49it/s]100%|██████████| 15/15 [00:00<00:00, 15.18it/s]
CLIENT:43
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7412, 'grad_norm': 0.13124507665634155, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.55it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.65it/s]                                              {'loss': 0.7059, 'grad_norm': 0.05745895579457283, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.65it/s]                                              {'loss': 0.7451, 'grad_norm': 0.16623935103416443, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.65it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.54it/s]                                              {'loss': 0.7245, 'grad_norm': 0.11027498543262482, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.54it/s]                                              {'loss': 0.7292, 'grad_norm': 0.11176593601703644, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.54it/s] 40%|████      | 6/15 [00:00<00:00, 16.65it/s]                                              {'loss': 0.7187, 'grad_norm': 0.11508330702781677, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.65it/s]                                              {'loss': 0.7167, 'grad_norm': 0.12013330310583115, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.65it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.7376, 'grad_norm': 0.1300123780965805, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.6979, 'grad_norm': 0.07697964459657669, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.32it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.7524, 'grad_norm': 0.21085484325885773, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.7182, 'grad_norm': 0.07964487373828888, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.63it/s] 80%|████████  | 12/15 [00:00<00:00, 16.33it/s]                                               {'loss': 0.6926, 'grad_norm': 0.05849754437804222, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.33it/s]                                               {'loss': 0.7446, 'grad_norm': 0.1743379682302475, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.33it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.7073, 'grad_norm': 0.06822942942380905, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.7261, 'grad_norm': 0.10414468497037888, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.22it/s]                                               {'train_runtime': 1.0199, 'train_samples_per_second': 416.725, 'train_steps_per_second': 14.708, 'train_loss': 0.7238654573758443, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.22it/s]100%|██████████| 15/15 [00:01<00:00, 14.71it/s]
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.724, 'grad_norm': 0.13537174463272095, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.85it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.7476, 'grad_norm': 0.17219460010528564, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.7073, 'grad_norm': 0.10237773507833481, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.34it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.62it/s]                                              {'loss': 0.7342, 'grad_norm': 0.16996590793132782, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.62it/s]                                              {'loss': 0.7361, 'grad_norm': 0.13386647403240204, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.62it/s] 40%|████      | 6/15 [00:00<00:00, 16.76it/s]                                              {'loss': 0.7227, 'grad_norm': 0.10372848063707352, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.76it/s]                                              {'loss': 0.7355, 'grad_norm': 0.14444997906684875, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.76it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.7437, 'grad_norm': 0.16557931900024414, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.7153, 'grad_norm': 0.10850995033979416, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.40it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.95it/s]                                               {'loss': 0.7246, 'grad_norm': 0.12920835614204407, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.95it/s]                                               {'loss': 0.723, 'grad_norm': 0.11543728411197662, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.95it/s] 80%|████████  | 12/15 [00:00<00:00, 16.47it/s]                                               {'loss': 0.7515, 'grad_norm': 0.22311387956142426, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.47it/s]                                               {'loss': 0.7413, 'grad_norm': 0.1877547949552536, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.47it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.7392, 'grad_norm': 0.13947618007659912, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.722, 'grad_norm': 0.11008039116859436, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.30it/s]                                               {'train_runtime': 1.0104, 'train_samples_per_second': 420.61, 'train_steps_per_second': 14.845, 'train_loss': 0.731199053923289, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.30it/s]100%|██████████| 15/15 [00:01<00:00, 14.85it/s]
CLIENT:7
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7258, 'grad_norm': 0.1117490753531456, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.68it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.7212, 'grad_norm': 0.07693099230527878, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.7426, 'grad_norm': 0.1483880877494812, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.20it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.81it/s]                                              {'loss': 0.7182, 'grad_norm': 0.12360552698373795, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.81it/s]                                              {'loss': 0.7388, 'grad_norm': 0.12075667083263397, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.81it/s] 40%|████      | 6/15 [00:00<00:00, 16.69it/s]                                              {'loss': 0.696, 'grad_norm': 0.07633217424154282, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.69it/s]                                              {'loss': 0.7289, 'grad_norm': 0.13023290038108826, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.69it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.7198, 'grad_norm': 0.09585554897785187, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.7244, 'grad_norm': 0.11124475300312042, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.30it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.92it/s]                                               {'loss': 0.7168, 'grad_norm': 0.10796580463647842, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.92it/s]                                               {'loss': 0.7198, 'grad_norm': 0.10738280415534973, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.92it/s] 80%|████████  | 12/15 [00:00<00:00, 16.58it/s]                                               {'loss': 0.7287, 'grad_norm': 0.11916039884090424, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.58it/s]                                               {'loss': 0.715, 'grad_norm': 0.08273302763700485, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.58it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.7275, 'grad_norm': 0.14226503670215607, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.7129, 'grad_norm': 0.12090043723583221, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.37it/s]                                               {'train_runtime': 1.0092, 'train_samples_per_second': 421.146, 'train_steps_per_second': 14.864, 'train_loss': 0.7224392453829448, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.37it/s]100%|██████████| 15/15 [00:01<00:00, 14.87it/s]
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7272, 'grad_norm': 0.11839815229177475, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.74it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.79it/s]                                              {'loss': 0.702, 'grad_norm': 0.07507026195526123, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.79it/s]                                              {'loss': 0.7374, 'grad_norm': 0.12001556158065796, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.79it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.7075, 'grad_norm': 0.053707197308540344, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.7329, 'grad_norm': 0.15346013009548187, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.89it/s] 40%|████      | 6/15 [00:00<00:00, 17.06it/s]                                              {'loss': 0.7082, 'grad_norm': 0.11076019704341888, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 17.06it/s]                                              {'loss': 0.72, 'grad_norm': 0.07861783355474472, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 17.06it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.65it/s]                                              {'loss': 0.7212, 'grad_norm': 0.1267932504415512, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.65it/s]                                              {'loss': 0.7185, 'grad_norm': 0.10481920838356018, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.65it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.18it/s]                                               {'loss': 0.7176, 'grad_norm': 0.06905985623598099, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.18it/s]                                               {'loss': 0.7277, 'grad_norm': 0.12414951622486115, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.18it/s] 80%|████████  | 12/15 [00:00<00:00, 16.67it/s]                                               {'loss': 0.7342, 'grad_norm': 0.1398441642522812, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.67it/s]                                               {'loss': 0.7436, 'grad_norm': 0.17355623841285706, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.67it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.7095, 'grad_norm': 0.07452630251646042, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.6958, 'grad_norm': 0.05931331217288971, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.47it/s]                                               {'train_runtime': 1.0, 'train_samples_per_second': 425.003, 'train_steps_per_second': 15.0, 'train_loss': 0.720210858186086, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.47it/s]100%|██████████| 15/15 [00:00<00:00, 15.01it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 35.51it/s] 24%|██▍       | 8/33 [00:00<00:00, 30.16it/s] 36%|███▋      | 12/33 [00:00<00:00, 28.03it/s] 45%|████▌     | 15/33 [00:00<00:00, 27.32it/s] 55%|█████▍    | 18/33 [00:00<00:00, 27.37it/s] 64%|██████▎   | 21/33 [00:00<00:00, 26.81it/s] 73%|███████▎  | 24/33 [00:00<00:00, 26.40it/s] 82%|████████▏ | 27/33 [00:00<00:00, 26.46it/s] 91%|█████████ | 30/33 [00:01<00:00, 26.59it/s]100%|██████████| 33/33 [00:01<00:00, 27.55it/s]
{'eval_loss': 0.7231781482696533, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3096836049856184, 'eval_runtime': 1.2385, 'eval_samples_per_second': 842.126, 'eval_steps_per_second': 26.644}
ROUND:1
CLIENT:26
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7425, 'grad_norm': 0.15755949914455414, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.59it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.61it/s]                                              {'loss': 0.7174, 'grad_norm': 0.12704047560691833, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.61it/s]                                              {'loss': 0.7041, 'grad_norm': 0.0620567761361599, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.61it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.7277, 'grad_norm': 0.10567781329154968, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.7198, 'grad_norm': 0.09959517419338226, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.47it/s] 40%|████      | 6/15 [00:00<00:00, 16.63it/s]                                              {'loss': 0.7213, 'grad_norm': 0.16399912536144257, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.63it/s]                                              {'loss': 0.7249, 'grad_norm': 0.1509530246257782, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.63it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.7107, 'grad_norm': 0.07722892612218857, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.7188, 'grad_norm': 0.1385270208120346, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.89it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.42it/s]                                               {'loss': 0.7114, 'grad_norm': 0.10214169323444366, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.42it/s]                                               {'loss': 0.7186, 'grad_norm': 0.1178463026881218, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.42it/s] 80%|████████  | 12/15 [00:00<00:00, 17.01it/s]                                               {'loss': 0.7449, 'grad_norm': 0.1689939796924591, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 17.01it/s]                                               {'loss': 0.7183, 'grad_norm': 0.12201020866632462, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 17.01it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.86it/s]                                               {'loss': 0.731, 'grad_norm': 0.13567887246608734, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.86it/s]                                               {'loss': 0.7273, 'grad_norm': 0.11429900676012039, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.86it/s]                                               {'train_runtime': 0.9866, 'train_samples_per_second': 430.762, 'train_steps_per_second': 15.203, 'train_loss': 0.7225743770599365, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.86it/s]100%|██████████| 15/15 [00:00<00:00, 15.21it/s]
CLIENT:80
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7155, 'grad_norm': 0.06418684870004654, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.14it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.7087, 'grad_norm': 0.08580541610717773, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.7451, 'grad_norm': 0.17449209094047546, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.15it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.7157, 'grad_norm': 0.0874100849032402, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.7237, 'grad_norm': 0.1315910369157791, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.36it/s] 40%|████      | 6/15 [00:00<00:00, 16.70it/s]                                              {'loss': 0.7037, 'grad_norm': 0.06802108883857727, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.70it/s]                                              {'loss': 0.7082, 'grad_norm': 0.08802218735218048, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.70it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.10it/s]                                              {'loss': 0.7002, 'grad_norm': 0.07000609487295151, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.10it/s]                                              {'loss': 0.7417, 'grad_norm': 0.14512833952903748, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.10it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.7197, 'grad_norm': 0.11455070972442627, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.6989, 'grad_norm': 0.07073153555393219, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.69it/s] 80%|████████  | 12/15 [00:00<00:00, 16.25it/s]                                               {'loss': 0.7512, 'grad_norm': 0.13502411544322968, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.25it/s]                                               {'loss': 0.727, 'grad_norm': 0.14641672372817993, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.25it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.6932, 'grad_norm': 0.04842095077037811, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.7306, 'grad_norm': 0.14814774692058563, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.05it/s]                                               {'train_runtime': 1.0365, 'train_samples_per_second': 410.041, 'train_steps_per_second': 14.472, 'train_loss': 0.7188914895057679, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.05it/s]100%|██████████| 15/15 [00:01<00:00, 14.48it/s]
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7114, 'grad_norm': 0.09568081051111221, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.47it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.50it/s]                                              {'loss': 0.7174, 'grad_norm': 0.09083021432161331, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.50it/s]                                              {'loss': 0.7578, 'grad_norm': 0.20849311351776123, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.50it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.7458, 'grad_norm': 0.15749876201152802, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.7174, 'grad_norm': 0.12461449205875397, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.26it/s] 40%|████      | 6/15 [00:00<00:00, 16.23it/s]                                              {'loss': 0.7093, 'grad_norm': 0.09423227608203888, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.23it/s]                                              {'loss': 0.6991, 'grad_norm': 0.0984635055065155, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.23it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.68it/s]                                              {'loss': 0.7316, 'grad_norm': 0.13678859174251556, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.68it/s]                                              {'loss': 0.7245, 'grad_norm': 0.1546126902103424, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.68it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.7288, 'grad_norm': 0.11641734838485718, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.6961, 'grad_norm': 0.11716783046722412, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.85it/s] 80%|████████  | 12/15 [00:00<00:00, 16.38it/s]                                               {'loss': 0.7361, 'grad_norm': 0.14486363530158997, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.38it/s]                                               {'loss': 0.7195, 'grad_norm': 0.10957224667072296, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.38it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.7225, 'grad_norm': 0.1548549234867096, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.7149, 'grad_norm': 0.1204775869846344, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.24it/s]                                               {'train_runtime': 1.0195, 'train_samples_per_second': 416.889, 'train_steps_per_second': 14.714, 'train_loss': 0.7221307873725891, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.24it/s]100%|██████████| 15/15 [00:01<00:00, 14.72it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7448, 'grad_norm': 0.14756469428539276, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.76it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.77it/s]                                              {'loss': 0.7246, 'grad_norm': 0.07577081024646759, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.77it/s]                                              {'loss': 0.6781, 'grad_norm': 0.06127191334962845, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.77it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.7124, 'grad_norm': 0.08358945697546005, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.719, 'grad_norm': 0.06962032616138458, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.47it/s] 40%|████      | 6/15 [00:00<00:00, 17.03it/s]                                              {'loss': 0.7157, 'grad_norm': 0.09751724451780319, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 17.03it/s]                                              {'loss': 0.7162, 'grad_norm': 0.09450120478868484, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 17.03it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.7123, 'grad_norm': 0.08281803131103516, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.7184, 'grad_norm': 0.08834346383810043, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.35it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.65it/s]                                               {'loss': 0.7049, 'grad_norm': 0.055740781128406525, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.65it/s]                                               {'loss': 0.708, 'grad_norm': 0.08181663602590561, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.65it/s] 80%|████████  | 12/15 [00:00<00:00, 16.36it/s]                                               {'loss': 0.7215, 'grad_norm': 0.14438828825950623, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.36it/s]                                               {'loss': 0.7226, 'grad_norm': 0.10480383783578873, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.36it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.7114, 'grad_norm': 0.07981236279010773, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.7119, 'grad_norm': 0.08013571053743362, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.22it/s]                                               {'train_runtime': 1.019, 'train_samples_per_second': 417.088, 'train_steps_per_second': 14.721, 'train_loss': 0.7147915045420329, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.22it/s]100%|██████████| 15/15 [00:01<00:00, 14.73it/s]
CLIENT:77
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7242, 'grad_norm': 0.1109040379524231, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.43it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.7204, 'grad_norm': 0.08186649531126022, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.7169, 'grad_norm': 0.101619653403759, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.52it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.19it/s]                                              {'loss': 0.7084, 'grad_norm': 0.06243753433227539, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.19it/s]                                              {'loss': 0.7248, 'grad_norm': 0.0967736765742302, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.19it/s] 40%|████      | 6/15 [00:00<00:00, 16.90it/s]                                              {'loss': 0.7279, 'grad_norm': 0.1472240388393402, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.90it/s]                                              {'loss': 0.7279, 'grad_norm': 0.11171934008598328, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.90it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.7101, 'grad_norm': 0.07893741875886917, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.7242, 'grad_norm': 0.10735291242599487, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.33it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.87it/s]                                               {'loss': 0.7229, 'grad_norm': 0.08830947428941727, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.87it/s]                                               {'loss': 0.7252, 'grad_norm': 0.13329516351222992, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.87it/s] 80%|████████  | 12/15 [00:00<00:00, 16.53it/s]                                               {'loss': 0.7187, 'grad_norm': 0.08088310807943344, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.53it/s]                                               {'loss': 0.7297, 'grad_norm': 0.13069801032543182, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.53it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.7151, 'grad_norm': 0.05886048823595047, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.7519, 'grad_norm': 0.14166375994682312, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.29it/s]                                               {'train_runtime': 1.0059, 'train_samples_per_second': 422.514, 'train_steps_per_second': 14.912, 'train_loss': 0.7232158462206523, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.29it/s]100%|██████████| 15/15 [00:01<00:00, 14.92it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7137, 'grad_norm': 0.09744071215391159, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.84it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.40it/s]                                              {'loss': 0.725, 'grad_norm': 0.15959201753139496, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.40it/s]                                              {'loss': 0.7445, 'grad_norm': 0.19099324941635132, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.40it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.74it/s]                                              {'loss': 0.7185, 'grad_norm': 0.1027790829539299, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.74it/s]                                              {'loss': 0.7441, 'grad_norm': 0.1845981776714325, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.74it/s] 40%|████      | 6/15 [00:00<00:00, 17.18it/s]                                              {'loss': 0.7323, 'grad_norm': 0.13822825253009796, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 17.18it/s]                                              {'loss': 0.7503, 'grad_norm': 0.17400820553302765, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 17.18it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.713, 'grad_norm': 0.11942928284406662, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.7149, 'grad_norm': 0.13840268552303314, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.53it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.09it/s]                                               {'loss': 0.7365, 'grad_norm': 0.1503540724515915, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.09it/s]                                               {'loss': 0.7168, 'grad_norm': 0.14179874956607819, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.09it/s] 80%|████████  | 12/15 [00:00<00:00, 16.64it/s]                                               {'loss': 0.7182, 'grad_norm': 0.15158632397651672, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.64it/s]                                               {'loss': 0.7113, 'grad_norm': 0.1131276860833168, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.64it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.98it/s]                                               {'loss': 0.7271, 'grad_norm': 0.18070611357688904, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.98it/s]                                               {'loss': 0.7406, 'grad_norm': 0.2067517191171646, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.98it/s]                                               {'train_runtime': 0.9874, 'train_samples_per_second': 430.435, 'train_steps_per_second': 15.192, 'train_loss': 0.7271237730979919, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.98it/s]100%|██████████| 15/15 [00:00<00:00, 15.20it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7332, 'grad_norm': 0.1752542108297348, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.60it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.63it/s]                                              {'loss': 0.7497, 'grad_norm': 0.18612980842590332, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.63it/s]                                              {'loss': 0.7192, 'grad_norm': 0.1266096830368042, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.63it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.7332, 'grad_norm': 0.15047124028205872, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.7395, 'grad_norm': 0.18962304294109344, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.41it/s] 40%|████      | 6/15 [00:00<00:00, 16.55it/s]                                              {'loss': 0.7325, 'grad_norm': 0.17008522152900696, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.55it/s]                                              {'loss': 0.757, 'grad_norm': 0.21011385321617126, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.55it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.7363, 'grad_norm': 0.1957019418478012, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.7242, 'grad_norm': 0.10632850229740143, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.28it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.60it/s]                                               {'loss': 0.7321, 'grad_norm': 0.15721097588539124, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.60it/s]                                               {'loss': 0.7221, 'grad_norm': 0.1994299441576004, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.60it/s] 80%|████████  | 12/15 [00:00<00:00, 16.26it/s]                                               {'loss': 0.7454, 'grad_norm': 0.19318711757659912, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.26it/s]                                               {'loss': 0.7271, 'grad_norm': 0.17785689234733582, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.26it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.7312, 'grad_norm': 0.18640439212322235, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.7412, 'grad_norm': 0.215926393866539, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.24it/s]                                               {'train_runtime': 1.019, 'train_samples_per_second': 417.06, 'train_steps_per_second': 14.72, 'train_loss': 0.7349262078603108, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.24it/s]100%|██████████| 15/15 [00:01<00:00, 14.73it/s]
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.729, 'grad_norm': 0.1605280041694641, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.34it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.7033, 'grad_norm': 0.06712177395820618, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.7137, 'grad_norm': 0.09204357862472534, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.64it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.17it/s]                                              {'loss': 0.7224, 'grad_norm': 0.07769457995891571, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.17it/s]                                              {'loss': 0.7344, 'grad_norm': 0.13533781468868256, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.17it/s] 40%|████      | 6/15 [00:00<00:00, 16.98it/s]                                              {'loss': 0.7002, 'grad_norm': 0.09293895214796066, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.98it/s]                                              {'loss': 0.7202, 'grad_norm': 0.12533655762672424, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.98it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.7116, 'grad_norm': 0.09081517904996872, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.7125, 'grad_norm': 0.1078513041138649, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.33it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.15it/s]                                               {'loss': 0.7502, 'grad_norm': 0.20505549013614655, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.15it/s]                                               {'loss': 0.6952, 'grad_norm': 0.04680633917450905, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.15it/s] 80%|████████  | 12/15 [00:00<00:00, 16.91it/s]                                               {'loss': 0.7179, 'grad_norm': 0.09994085878133774, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.91it/s]                                               {'loss': 0.7255, 'grad_norm': 0.13942544162273407, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.91it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.7086, 'grad_norm': 0.10093814879655838, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.7002, 'grad_norm': 0.09665224701166153, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.50it/s]                                               {'train_runtime': 0.998, 'train_samples_per_second': 425.852, 'train_steps_per_second': 15.03, 'train_loss': 0.7163344105084737, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.50it/s]100%|██████████| 15/15 [00:00<00:00, 15.04it/s]
CLIENT:20
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7321, 'grad_norm': 0.13957415521144867, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.24it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.24it/s]                                              {'loss': 0.7055, 'grad_norm': 0.06700011342763901, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.24it/s]                                              {'loss': 0.7345, 'grad_norm': 0.12271994352340698, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.24it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.55it/s]                                              {'loss': 0.705, 'grad_norm': 0.05500506982207298, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.55it/s]                                              {'loss': 0.724, 'grad_norm': 0.15267011523246765, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.55it/s] 40%|████      | 6/15 [00:00<00:00, 16.73it/s]                                              {'loss': 0.7046, 'grad_norm': 0.08923064172267914, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.73it/s]                                              {'loss': 0.7153, 'grad_norm': 0.12103331834077835, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.73it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.7122, 'grad_norm': 0.07012367993593216, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.7238, 'grad_norm': 0.1392376869916916, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.21it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.76it/s]                                               {'loss': 0.7317, 'grad_norm': 0.11133630573749542, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.76it/s]                                               {'loss': 0.7099, 'grad_norm': 0.0955599844455719, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.76it/s] 80%|████████  | 12/15 [00:00<00:00, 16.49it/s]                                               {'loss': 0.7225, 'grad_norm': 0.11830019950866699, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.49it/s]                                               {'loss': 0.7452, 'grad_norm': 0.17438288033008575, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.49it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.7071, 'grad_norm': 0.09221282601356506, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.6815, 'grad_norm': 0.05878056585788727, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.35it/s]                                               {'train_runtime': 1.0152, 'train_samples_per_second': 418.628, 'train_steps_per_second': 14.775, 'train_loss': 0.7169885357220968, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.35it/s]100%|██████████| 15/15 [00:01<00:00, 14.78it/s]
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7232, 'grad_norm': 0.13745923340320587, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.73it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.46it/s]                                              {'loss': 0.7465, 'grad_norm': 0.17493905127048492, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.46it/s]                                              {'loss': 0.7068, 'grad_norm': 0.10354600101709366, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.46it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.82it/s]                                              {'loss': 0.7331, 'grad_norm': 0.1729031354188919, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.82it/s]                                              {'loss': 0.7353, 'grad_norm': 0.13676032423973083, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.82it/s] 40%|████      | 6/15 [00:00<00:00, 17.07it/s]                                              {'loss': 0.7221, 'grad_norm': 0.10622867941856384, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 17.07it/s]                                              {'loss': 0.7346, 'grad_norm': 0.14837287366390228, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 17.07it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.7426, 'grad_norm': 0.17033836245536804, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.7146, 'grad_norm': 0.1121966689825058, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.34it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.10it/s]                                               {'loss': 0.7237, 'grad_norm': 0.1338588148355484, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.10it/s]                                               {'loss': 0.7222, 'grad_norm': 0.11949197202920914, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.10it/s] 80%|████████  | 12/15 [00:00<00:00, 16.82it/s]                                               {'loss': 0.7499, 'grad_norm': 0.23147711157798767, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.82it/s]                                               {'loss': 0.7398, 'grad_norm': 0.1948690116405487, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.82it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.7382, 'grad_norm': 0.14438636600971222, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.7213, 'grad_norm': 0.1141078844666481, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.52it/s]                                               {'train_runtime': 0.9993, 'train_samples_per_second': 425.314, 'train_steps_per_second': 15.011, 'train_loss': 0.7302536686261495, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.52it/s]100%|██████████| 15/15 [00:00<00:00, 15.02it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 37.00it/s] 24%|██▍       | 8/33 [00:00<00:00, 29.85it/s] 36%|███▋      | 12/33 [00:00<00:00, 28.49it/s] 45%|████▌     | 15/33 [00:00<00:00, 27.57it/s] 55%|█████▍    | 18/33 [00:00<00:00, 26.78it/s] 64%|██████▎   | 21/33 [00:00<00:00, 26.31it/s] 73%|███████▎  | 24/33 [00:00<00:00, 25.94it/s] 82%|████████▏ | 27/33 [00:00<00:00, 25.87it/s] 91%|█████████ | 30/33 [00:01<00:00, 26.29it/s]100%|██████████| 33/33 [00:01<00:00, 27.23it/s]100%|██████████| 33/33 [00:01<00:00, 27.29it/s]
{'eval_loss': 0.7224888801574707, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3096836049856184, 'eval_runtime': 1.25, 'eval_samples_per_second': 834.375, 'eval_steps_per_second': 26.399}
ROUND:2
CLIENT:75
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7022, 'grad_norm': 0.07047992199659348, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.48it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.7399, 'grad_norm': 0.16167083382606506, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.7256, 'grad_norm': 0.1212025061249733, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.53it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.39it/s]                                              {'loss': 0.7176, 'grad_norm': 0.08485838770866394, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.39it/s]                                              {'loss': 0.7463, 'grad_norm': 0.16561752557754517, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.39it/s] 40%|████      | 6/15 [00:00<00:00, 17.02it/s]                                              {'loss': 0.7121, 'grad_norm': 0.10505794733762741, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 17.02it/s]                                              {'loss': 0.7332, 'grad_norm': 0.15926027297973633, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 17.02it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.77it/s]                                              {'loss': 0.6992, 'grad_norm': 0.10177631676197052, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.77it/s]                                              {'loss': 0.7111, 'grad_norm': 0.0919574424624443, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.77it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.94it/s]                                               {'loss': 0.7113, 'grad_norm': 0.09868144243955612, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.94it/s]                                               {'loss': 0.7081, 'grad_norm': 0.10823547095060349, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.94it/s] 80%|████████  | 12/15 [00:00<00:00, 16.62it/s]                                               {'loss': 0.7203, 'grad_norm': 0.18058821558952332, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.62it/s]                                               {'loss': 0.7185, 'grad_norm': 0.09582004696130753, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.62it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.7253, 'grad_norm': 0.14817671477794647, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.7212, 'grad_norm': 0.11789825558662415, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.56it/s]                                               {'train_runtime': 0.9918, 'train_samples_per_second': 428.512, 'train_steps_per_second': 15.124, 'train_loss': 0.7194748163223267, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.56it/s]100%|██████████| 15/15 [00:00<00:00, 15.13it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7025, 'grad_norm': 0.11371950805187225, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.31it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.46it/s]                                              {'loss': 0.7435, 'grad_norm': 0.17675217986106873, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.46it/s]                                              {'loss': 0.6896, 'grad_norm': 0.054681431502103806, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.46it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.69it/s]                                              {'loss': 0.6999, 'grad_norm': 0.10030274093151093, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.69it/s]                                              {'loss': 0.7245, 'grad_norm': 0.1276078224182129, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.69it/s] 40%|████      | 6/15 [00:00<00:00, 16.66it/s]                                              {'loss': 0.731, 'grad_norm': 0.15210016071796417, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.66it/s]                                              {'loss': 0.7238, 'grad_norm': 0.14425243437290192, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.66it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.7157, 'grad_norm': 0.12907534837722778, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.7065, 'grad_norm': 0.09060446172952652, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.13it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.71it/s]                                               {'loss': 0.7215, 'grad_norm': 0.10269930213689804, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.71it/s]                                               {'loss': 0.7139, 'grad_norm': 0.14144529402256012, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.71it/s] 80%|████████  | 12/15 [00:00<00:00, 16.42it/s]                                               {'loss': 0.7225, 'grad_norm': 0.1283005028963089, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.42it/s]                                               {'loss': 0.707, 'grad_norm': 0.12007898092269897, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.42it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.7299, 'grad_norm': 0.14887508749961853, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.7021, 'grad_norm': 0.12450859695672989, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.26it/s]                                               {'train_runtime': 1.009, 'train_samples_per_second': 421.191, 'train_steps_per_second': 14.866, 'train_loss': 0.7155821879704793, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.26it/s]100%|██████████| 15/15 [00:01<00:00, 14.87it/s]
CLIENT:46
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7107, 'grad_norm': 0.09741226583719254, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.95it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.40it/s]                                              {'loss': 0.7132, 'grad_norm': 0.09312316030263901, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.40it/s]                                              {'loss': 0.7582, 'grad_norm': 0.17499656975269318, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.40it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.7096, 'grad_norm': 0.060154180973768234, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.7337, 'grad_norm': 0.17189504206180573, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.13it/s] 40%|████      | 6/15 [00:00<00:00, 16.27it/s]                                              {'loss': 0.7223, 'grad_norm': 0.1257583498954773, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.27it/s]                                              {'loss': 0.7314, 'grad_norm': 0.13755503296852112, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.27it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.7172, 'grad_norm': 0.12775500118732452, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.7066, 'grad_norm': 0.07204373180866241, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.46it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.68it/s]                                               {'loss': 0.7433, 'grad_norm': 0.12183388322591782, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.68it/s]                                               {'loss': 0.699, 'grad_norm': 0.08280616253614426, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.68it/s] 80%|████████  | 12/15 [00:00<00:00, 16.43it/s]                                               {'loss': 0.7149, 'grad_norm': 0.1605210155248642, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.43it/s]                                               {'loss': 0.7164, 'grad_norm': 0.1221369057893753, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.43it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.44it/s]                                               {'loss': 0.7353, 'grad_norm': 0.16904884576797485, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.44it/s]                                               {'loss': 0.7024, 'grad_norm': 0.0616019070148468, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.44it/s]                                               {'train_runtime': 1.0108, 'train_samples_per_second': 420.469, 'train_steps_per_second': 14.84, 'train_loss': 0.7209420601526896, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.44it/s]100%|██████████| 15/15 [00:01<00:00, 14.85it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7439, 'grad_norm': 0.15167446434497833, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.59it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.75it/s]                                              {'loss': 0.7242, 'grad_norm': 0.07763200998306274, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.75it/s]                                              {'loss': 0.6784, 'grad_norm': 0.06279826164245605, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.75it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.74it/s]                                              {'loss': 0.7119, 'grad_norm': 0.08617605268955231, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.74it/s]                                              {'loss': 0.7186, 'grad_norm': 0.07134819775819778, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.74it/s] 40%|████      | 6/15 [00:00<00:00, 16.56it/s]                                              {'loss': 0.7152, 'grad_norm': 0.10020775347948074, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.56it/s]                                              {'loss': 0.7156, 'grad_norm': 0.09772039204835892, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.56it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.7118, 'grad_norm': 0.08467627316713333, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.718, 'grad_norm': 0.09090211987495422, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.36it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.13it/s]                                               {'loss': 0.7047, 'grad_norm': 0.05721859633922577, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.13it/s]                                               {'loss': 0.7075, 'grad_norm': 0.0844530239701271, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.13it/s] 80%|████████  | 12/15 [00:00<00:00, 16.99it/s]                                               {'loss': 0.7205, 'grad_norm': 0.1492929607629776, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.99it/s]                                               {'loss': 0.7219, 'grad_norm': 0.10894523561000824, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.99it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.711, 'grad_norm': 0.08210138976573944, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.7115, 'grad_norm': 0.0828096866607666, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.56it/s]                                               {'train_runtime': 0.9964, 'train_samples_per_second': 426.532, 'train_steps_per_second': 15.054, 'train_loss': 0.7143015027046203, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.56it/s]100%|██████████| 15/15 [00:00<00:00, 15.06it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7321, 'grad_norm': 0.1800122708082199, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.65it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.54it/s]                                              {'loss': 0.7486, 'grad_norm': 0.19178561866283417, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.54it/s]                                              {'loss': 0.7184, 'grad_norm': 0.13062798976898193, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.54it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.85it/s]                                              {'loss': 0.7322, 'grad_norm': 0.15534347295761108, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.85it/s]                                              {'loss': 0.7382, 'grad_norm': 0.196115642786026, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.85it/s] 40%|████      | 6/15 [00:00<00:00, 16.97it/s]                                              {'loss': 0.7313, 'grad_norm': 0.17610372602939606, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.97it/s]                                              {'loss': 0.7553, 'grad_norm': 0.21893317997455597, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.97it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.7347, 'grad_norm': 0.2039504200220108, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.7235, 'grad_norm': 0.11117823421955109, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.47it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.97it/s]                                               {'loss': 0.7308, 'grad_norm': 0.16425664722919464, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.97it/s]                                               {'loss': 0.7203, 'grad_norm': 0.2092200070619583, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.97it/s] 80%|████████  | 12/15 [00:00<00:00, 16.39it/s]                                               {'loss': 0.7436, 'grad_norm': 0.2038227915763855, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.39it/s]                                               {'loss': 0.7254, 'grad_norm': 0.18627387285232544, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.39it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.7295, 'grad_norm': 0.19650454819202423, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.7391, 'grad_norm': 0.22714507579803467, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.31it/s]                                               {'train_runtime': 1.0098, 'train_samples_per_second': 420.882, 'train_steps_per_second': 14.855, 'train_loss': 0.7335348725318909, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.31it/s]100%|██████████| 15/15 [00:01<00:00, 14.86it/s]
CLIENT:39
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.754, 'grad_norm': 0.17288900911808014, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.78it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.42it/s]                                              {'loss': 0.707, 'grad_norm': 0.06977250427007675, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.42it/s]                                              {'loss': 0.7273, 'grad_norm': 0.12351497262716293, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.42it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.7226, 'grad_norm': 0.09481816738843918, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.7215, 'grad_norm': 0.1272197961807251, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.35it/s] 40%|████      | 6/15 [00:00<00:00, 16.45it/s]                                              {'loss': 0.7038, 'grad_norm': 0.137034073472023, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.45it/s]                                              {'loss': 0.7101, 'grad_norm': 0.07849550247192383, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.45it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.7324, 'grad_norm': 0.14248542487621307, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.7242, 'grad_norm': 0.15596891939640045, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.30it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.7196, 'grad_norm': 0.12181462347507477, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.7208, 'grad_norm': 0.1351865828037262, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.55it/s] 80%|████████  | 12/15 [00:00<00:00, 16.19it/s]                                               {'loss': 0.7089, 'grad_norm': 0.13142129778862, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.19it/s]                                               {'loss': 0.7406, 'grad_norm': 0.1772366166114807, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.19it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.7194, 'grad_norm': 0.10796286165714264, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.7117, 'grad_norm': 0.08918335288763046, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.16it/s]                                               {'train_runtime': 1.0241, 'train_samples_per_second': 415.001, 'train_steps_per_second': 14.647, 'train_loss': 0.7215948621431987, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.16it/s]100%|██████████| 15/15 [00:01<00:00, 14.65it/s]
CLIENT:23
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7476, 'grad_norm': 0.1683068722486496, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.83it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.72it/s]                                              {'loss': 0.7216, 'grad_norm': 0.1038089245557785, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.72it/s]                                              {'loss': 0.7366, 'grad_norm': 0.12749284505844116, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.72it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.7144, 'grad_norm': 0.10179264843463898, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.7431, 'grad_norm': 0.17074820399284363, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.52it/s] 40%|████      | 6/15 [00:00<00:00, 16.44it/s]                                              {'loss': 0.71, 'grad_norm': 0.1280619353055954, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.44it/s]                                              {'loss': 0.7149, 'grad_norm': 0.09708024561405182, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.44it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.10it/s]                                              {'loss': 0.7246, 'grad_norm': 0.1336863487958908, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.10it/s]                                              {'loss': 0.7367, 'grad_norm': 0.20617108047008514, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.10it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.7267, 'grad_norm': 0.0981416329741478, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.7265, 'grad_norm': 0.16921332478523254, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.85it/s] 80%|████████  | 12/15 [00:00<00:00, 16.53it/s]                                               {'loss': 0.7056, 'grad_norm': 0.14357183873653412, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.53it/s]                                               {'loss': 0.7301, 'grad_norm': 0.1986413449048996, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.53it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.7102, 'grad_norm': 0.08059989660978317, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.7233, 'grad_norm': 0.15014158189296722, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.28it/s]                                               {'train_runtime': 1.0181, 'train_samples_per_second': 417.461, 'train_steps_per_second': 14.734, 'train_loss': 0.7247894048690796, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.28it/s]100%|██████████| 15/15 [00:01<00:00, 14.74it/s]
CLIENT:20
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7313, 'grad_norm': 0.14396917819976807, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.78it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.35it/s]                                              {'loss': 0.7052, 'grad_norm': 0.06852378696203232, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.35it/s]                                              {'loss': 0.7338, 'grad_norm': 0.12647120654582977, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.35it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.7047, 'grad_norm': 0.056233420968055725, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.723, 'grad_norm': 0.15664036571979523, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.40it/s] 40%|████      | 6/15 [00:00<00:00, 16.45it/s]                                              {'loss': 0.7041, 'grad_norm': 0.09193999320268631, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.45it/s]                                              {'loss': 0.7145, 'grad_norm': 0.12553192675113678, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.45it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.7118, 'grad_norm': 0.07209639251232147, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.7228, 'grad_norm': 0.14464342594146729, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.18it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.7309, 'grad_norm': 0.11547292023897171, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.7093, 'grad_norm': 0.0993754118680954, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.51it/s] 80%|████████  | 12/15 [00:00<00:00, 16.40it/s]                                               {'loss': 0.7217, 'grad_norm': 0.12285450100898743, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.40it/s]                                               {'loss': 0.7438, 'grad_norm': 0.18197323381900787, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.40it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.58it/s]                                               {'loss': 0.7064, 'grad_norm': 0.09602150321006775, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.58it/s]                                               {'loss': 0.6816, 'grad_norm': 0.0609389953315258, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.58it/s]                                               {'train_runtime': 1.0147, 'train_samples_per_second': 418.847, 'train_steps_per_second': 14.783, 'train_loss': 0.7163215041160583, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.58it/s]100%|██████████| 15/15 [00:01<00:00, 14.79it/s]
CLIENT:70
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7266, 'grad_norm': 0.1625259816646576, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.71it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.55it/s]                                              {'loss': 0.7059, 'grad_norm': 0.09723731130361557, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.55it/s]                                              {'loss': 0.7272, 'grad_norm': 0.1455320417881012, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.55it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.7276, 'grad_norm': 0.118242546916008, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.7193, 'grad_norm': 0.1350972205400467, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.31it/s] 40%|████      | 6/15 [00:00<00:00, 16.41it/s]                                              {'loss': 0.7356, 'grad_norm': 0.15332071483135223, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.41it/s]                                              {'loss': 0.7334, 'grad_norm': 0.2041512429714203, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.41it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.7186, 'grad_norm': 0.09602848440408707, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.704, 'grad_norm': 0.1122565045952797, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.51it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.87it/s]                                               {'loss': 0.7317, 'grad_norm': 0.19587872922420502, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.87it/s]                                               {'loss': 0.7025, 'grad_norm': 0.09577340632677078, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.87it/s] 80%|████████  | 12/15 [00:00<00:00, 16.54it/s]                                               {'loss': 0.719, 'grad_norm': 0.11206338554620743, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.54it/s]                                               {'loss': 0.7398, 'grad_norm': 0.2081034928560257, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.54it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.6945, 'grad_norm': 0.07289553433656693, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.7168, 'grad_norm': 0.1675594002008438, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.52it/s]                                               {'train_runtime': 1.0051, 'train_samples_per_second': 422.83, 'train_steps_per_second': 14.923, 'train_loss': 0.7201635479927063, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.52it/s]100%|██████████| 15/15 [00:01<00:00, 14.93it/s]
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7228, 'grad_norm': 0.09791723638772964, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.91it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.7146, 'grad_norm': 0.08999551832675934, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.6594, 'grad_norm': 0.09120538830757141, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.43it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.75it/s]                                              {'loss': 0.7036, 'grad_norm': 0.046033281832933426, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.75it/s]                                              {'loss': 0.7084, 'grad_norm': 0.07253947108983994, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.75it/s] 40%|████      | 6/15 [00:00<00:00, 16.37it/s]                                              {'loss': 0.7275, 'grad_norm': 0.10745909810066223, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.37it/s]                                              {'loss': 0.7138, 'grad_norm': 0.07758487015962601, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.37it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.692, 'grad_norm': 0.05524148792028427, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.7088, 'grad_norm': 0.10613515973091125, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.17it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.02it/s]                                               {'loss': 0.6877, 'grad_norm': 0.0491216666996479, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.02it/s]                                               {'loss': 0.6952, 'grad_norm': 0.06535667926073074, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.02it/s] 80%|████████  | 12/15 [00:00<00:00, 16.47it/s]                                               {'loss': 0.7037, 'grad_norm': 0.11610682308673859, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.47it/s]                                               {'loss': 0.7129, 'grad_norm': 0.08211958408355713, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.47it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.6897, 'grad_norm': 0.04405130818486214, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.7292, 'grad_norm': 0.09845492243766785, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.20it/s]                                               {'train_runtime': 1.0151, 'train_samples_per_second': 418.675, 'train_steps_per_second': 14.777, 'train_loss': 0.7046192447344463, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.20it/s]100%|██████████| 15/15 [00:01<00:00, 14.78it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 36.76it/s] 24%|██▍       | 8/33 [00:00<00:00, 29.51it/s] 36%|███▋      | 12/33 [00:00<00:00, 28.17it/s] 45%|████▌     | 15/33 [00:00<00:00, 27.26it/s] 55%|█████▍    | 18/33 [00:00<00:00, 26.57it/s] 64%|██████▎   | 21/33 [00:00<00:00, 26.17it/s] 73%|███████▎  | 24/33 [00:00<00:00, 25.82it/s] 82%|████████▏ | 27/33 [00:01<00:00, 25.63it/s] 91%|█████████ | 30/33 [00:01<00:00, 25.45it/s]100%|██████████| 33/33 [00:01<00:00, 26.96it/s]
{'eval_loss': 0.7217524647712708, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3096836049856184, 'eval_runtime': 1.2646, 'eval_samples_per_second': 824.751, 'eval_steps_per_second': 26.095}
ROUND:3
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7214, 'grad_norm': 0.14660225808620453, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.52it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.65it/s]                                              {'loss': 0.7442, 'grad_norm': 0.18652808666229248, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.65it/s]                                              {'loss': 0.7056, 'grad_norm': 0.1086764708161354, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.65it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.76it/s]                                              {'loss': 0.7307, 'grad_norm': 0.185838520526886, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.76it/s]                                              {'loss': 0.7334, 'grad_norm': 0.14761324226856232, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.76it/s] 40%|████      | 6/15 [00:00<00:00, 16.62it/s]                                              {'loss': 0.7207, 'grad_norm': 0.11440166085958481, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.62it/s]                                              {'loss': 0.7323, 'grad_norm': 0.16188181936740875, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.62it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.7398, 'grad_norm': 0.18556401133537292, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.7129, 'grad_norm': 0.12226133793592453, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.20it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.08it/s]                                               {'loss': 0.7215, 'grad_norm': 0.14606069028377533, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.08it/s]                                               {'loss': 0.7201, 'grad_norm': 0.13214071094989777, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.08it/s] 80%|████████  | 12/15 [00:00<00:00, 16.76it/s]                                               {'loss': 0.7458, 'grad_norm': 0.25567251443862915, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.76it/s]                                               {'loss': 0.7362, 'grad_norm': 0.21586614847183228, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.76it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.7356, 'grad_norm': 0.15968148410320282, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.7193, 'grad_norm': 0.1249665841460228, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.52it/s]                                               {'train_runtime': 1.0034, 'train_samples_per_second': 423.576, 'train_steps_per_second': 14.95, 'train_loss': 0.727970012029012, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.52it/s]100%|██████████| 15/15 [00:01<00:00, 14.96it/s]
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7001, 'grad_norm': 0.08347144722938538, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.66it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.41it/s]                                              {'loss': 0.7219, 'grad_norm': 0.17401857674121857, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.41it/s]                                              {'loss': 0.7325, 'grad_norm': 0.17276522517204285, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.41it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.72it/s]                                              {'loss': 0.7253, 'grad_norm': 0.11560925096273422, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.72it/s]                                              {'loss': 0.743, 'grad_norm': 0.1963852196931839, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.72it/s] 40%|████      | 6/15 [00:00<00:00, 16.81it/s]                                              {'loss': 0.7218, 'grad_norm': 0.12544937431812286, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.81it/s]                                              {'loss': 0.741, 'grad_norm': 0.20560641586780548, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.81it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.714, 'grad_norm': 0.11685270816087723, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.7074, 'grad_norm': 0.08159804344177246, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.35it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.02it/s]                                               {'loss': 0.7214, 'grad_norm': 0.10976815968751907, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.02it/s]                                               {'loss': 0.7083, 'grad_norm': 0.16672009229660034, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.02it/s] 80%|████████  | 12/15 [00:00<00:00, 16.42it/s]                                               {'loss': 0.7418, 'grad_norm': 0.18104645609855652, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.42it/s]                                               {'loss': 0.7223, 'grad_norm': 0.1294797956943512, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.42it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.89it/s]                                               {'loss': 0.7257, 'grad_norm': 0.16123132407665253, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.89it/s]                                               {'loss': 0.721, 'grad_norm': 0.18119694292545319, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.89it/s]                                               {'train_runtime': 0.9985, 'train_samples_per_second': 425.65, 'train_steps_per_second': 15.023, 'train_loss': 0.7231706937154134, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.89it/s]100%|██████████| 15/15 [00:00<00:00, 15.03it/s]
CLIENT:76
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7163, 'grad_norm': 0.10067059099674225, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.40it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.78it/s]                                              {'loss': 0.722, 'grad_norm': 0.10103419423103333, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.78it/s]                                              {'loss': 0.7225, 'grad_norm': 0.18994668126106262, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.78it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.08it/s]                                              {'loss': 0.7187, 'grad_norm': 0.10077010095119476, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.08it/s]                                              {'loss': 0.6973, 'grad_norm': 0.0725199282169342, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.08it/s] 40%|████      | 6/15 [00:00<00:00, 16.27it/s]                                              {'loss': 0.7525, 'grad_norm': 0.23677758872509003, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.27it/s]                                              {'loss': 0.7121, 'grad_norm': 0.11605791002511978, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.27it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.61it/s]                                              {'loss': 0.7051, 'grad_norm': 0.11506376415491104, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.61it/s]                                              {'loss': 0.7257, 'grad_norm': 0.16381391882896423, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.61it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.16it/s]                                               {'loss': 0.7246, 'grad_norm': 0.1542685180902481, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.16it/s]                                               {'loss': 0.7107, 'grad_norm': 0.11903644353151321, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.16it/s] 80%|████████  | 12/15 [00:00<00:00, 16.66it/s]                                               {'loss': 0.7179, 'grad_norm': 0.09061602503061295, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.66it/s]                                               {'loss': 0.7045, 'grad_norm': 0.07997757941484451, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.66it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.7146, 'grad_norm': 0.13365909457206726, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.7235, 'grad_norm': 0.21200750768184662, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.62it/s]                                               {'train_runtime': 1.0045, 'train_samples_per_second': 423.089, 'train_steps_per_second': 14.933, 'train_loss': 0.7178635716438293, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.62it/s]100%|██████████| 15/15 [00:01<00:00, 14.94it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7309, 'grad_norm': 0.18679989874362946, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.43it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.7473, 'grad_norm': 0.19932405650615692, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.7175, 'grad_norm': 0.13542182743549347, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.47it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.60it/s]                                              {'loss': 0.7311, 'grad_norm': 0.16239480674266815, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.60it/s]                                              {'loss': 0.7367, 'grad_norm': 0.205288365483284, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.60it/s] 40%|████      | 6/15 [00:00<00:00, 17.03it/s]                                              {'loss': 0.7299, 'grad_norm': 0.1837826669216156, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 17.03it/s]                                              {'loss': 0.7535, 'grad_norm': 0.2302611917257309, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 17.03it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.60it/s]                                              {'loss': 0.733, 'grad_norm': 0.21470767259597778, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.60it/s]                                              {'loss': 0.7225, 'grad_norm': 0.11663936823606491, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.60it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.94it/s]                                               {'loss': 0.7293, 'grad_norm': 0.17301735281944275, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.94it/s]                                               {'loss': 0.7183, 'grad_norm': 0.2207408994436264, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.94it/s] 80%|████████  | 12/15 [00:00<00:00, 16.43it/s]                                               {'loss': 0.7416, 'grad_norm': 0.21466568112373352, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.43it/s]                                               {'loss': 0.7235, 'grad_norm': 0.19679710268974304, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.43it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.7274, 'grad_norm': 0.20828278362751007, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.7367, 'grad_norm': 0.2421739250421524, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.29it/s]                                               {'train_runtime': 1.0017, 'train_samples_per_second': 424.262, 'train_steps_per_second': 14.974, 'train_loss': 0.7319540023803711, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.29it/s]100%|██████████| 15/15 [00:01<00:00, 14.98it/s]
CLIENT:56
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7077, 'grad_norm': 0.09548362344503403, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.32it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.24it/s]                                              {'loss': 0.7441, 'grad_norm': 0.19447726011276245, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.24it/s]                                              {'loss': 0.7259, 'grad_norm': 0.16445858776569366, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.24it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.08it/s]                                              {'loss': 0.7205, 'grad_norm': 0.11042547971010208, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.08it/s]                                              {'loss': 0.7236, 'grad_norm': 0.12360730767250061, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.08it/s] 40%|████      | 6/15 [00:00<00:00, 15.93it/s]                                              {'loss': 0.7436, 'grad_norm': 0.2462562620639801, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.93it/s]                                              {'loss': 0.7117, 'grad_norm': 0.14053361117839813, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.93it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.86it/s]                                              {'loss': 0.7171, 'grad_norm': 0.1371450424194336, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.86it/s]                                              {'loss': 0.7388, 'grad_norm': 0.21180035173892975, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.86it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.7223, 'grad_norm': 0.17150039970874786, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.701, 'grad_norm': 0.14541009068489075, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.30it/s] 80%|████████  | 12/15 [00:00<00:00, 16.05it/s]                                               {'loss': 0.718, 'grad_norm': 0.16487343609333038, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.05it/s]                                               {'loss': 0.7289, 'grad_norm': 0.14632588624954224, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.05it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.7154, 'grad_norm': 0.16474685072898865, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.7227, 'grad_norm': 0.21334654092788696, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.41it/s]                                               {'train_runtime': 1.0315, 'train_samples_per_second': 412.04, 'train_steps_per_second': 14.543, 'train_loss': 0.7227403442064921, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.41it/s]100%|██████████| 15/15 [00:01<00:00, 14.55it/s]
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7261, 'grad_norm': 0.1585477590560913, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.99it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.37it/s]                                              {'loss': 0.731, 'grad_norm': 0.15565125644207, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.37it/s]                                              {'loss': 0.7107, 'grad_norm': 0.07784179598093033, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.37it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.08it/s]                                              {'loss': 0.7296, 'grad_norm': 0.14327290654182434, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.08it/s]                                              {'loss': 0.7306, 'grad_norm': 0.1428389698266983, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.08it/s] 40%|████      | 6/15 [00:00<00:00, 16.16it/s]                                              {'loss': 0.7141, 'grad_norm': 0.11634384095668793, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.16it/s]                                              {'loss': 0.7187, 'grad_norm': 0.14247934520244598, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.16it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.7109, 'grad_norm': 0.1465364396572113, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.7165, 'grad_norm': 0.13377249240875244, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.17it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.90it/s]                                               {'loss': 0.7131, 'grad_norm': 0.12206099182367325, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.90it/s]                                               {'loss': 0.7011, 'grad_norm': 0.11774994432926178, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.90it/s] 80%|████████  | 12/15 [00:00<00:00, 16.53it/s]                                               {'loss': 0.7407, 'grad_norm': 0.22041435539722443, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.53it/s]                                               {'loss': 0.7236, 'grad_norm': 0.1554444432258606, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.53it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7154, 'grad_norm': 0.14698104560375214, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7274, 'grad_norm': 0.1610780507326126, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.38it/s]                                               {'train_runtime': 1.0197, 'train_samples_per_second': 416.792, 'train_steps_per_second': 14.71, 'train_loss': 0.7206232746442159, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.38it/s]100%|██████████| 15/15 [00:01<00:00, 14.72it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7126, 'grad_norm': 0.10333049297332764, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.26it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.723, 'grad_norm': 0.1702749878168106, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.7419, 'grad_norm': 0.20386192202568054, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.15it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.7172, 'grad_norm': 0.10866724699735641, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.7415, 'grad_norm': 0.1986539661884308, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.51it/s] 40%|████      | 6/15 [00:00<00:00, 16.29it/s]                                              {'loss': 0.7303, 'grad_norm': 0.14994971454143524, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.29it/s]                                              {'loss': 0.7476, 'grad_norm': 0.1901775300502777, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.29it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.7111, 'grad_norm': 0.1298830807209015, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.7127, 'grad_norm': 0.15081562101840973, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.77it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.17it/s]                                               {'loss': 0.734, 'grad_norm': 0.16478967666625977, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.17it/s]                                               {'loss': 0.7143, 'grad_norm': 0.15608908236026764, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.17it/s] 80%|████████  | 12/15 [00:00<00:00, 15.90it/s]                                               {'loss': 0.7155, 'grad_norm': 0.16793502867221832, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.90it/s]                                               {'loss': 0.7093, 'grad_norm': 0.12430652230978012, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.90it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7237, 'grad_norm': 0.19945231080055237, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7366, 'grad_norm': 0.2293788343667984, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.12it/s]                                               {'train_runtime': 1.0364, 'train_samples_per_second': 410.083, 'train_steps_per_second': 14.474, 'train_loss': 0.7247585455576578, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.12it/s]100%|██████████| 15/15 [00:01<00:00, 14.48it/s]
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.725, 'grad_norm': 0.12796853482723236, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.43it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.7009, 'grad_norm': 0.07937177270650864, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.7352, 'grad_norm': 0.13032039999961853, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.44it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.72it/s]                                              {'loss': 0.7072, 'grad_norm': 0.055642109364271164, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.72it/s]                                              {'loss': 0.7298, 'grad_norm': 0.16775865852832794, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.72it/s] 40%|████      | 6/15 [00:00<00:00, 17.13it/s]                                              {'loss': 0.7061, 'grad_norm': 0.12078498303890228, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 17.13it/s]                                              {'loss': 0.7185, 'grad_norm': 0.08671800792217255, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 17.13it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.71it/s]                                              {'loss': 0.7184, 'grad_norm': 0.14026495814323425, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.71it/s]                                              {'loss': 0.7164, 'grad_norm': 0.11552509665489197, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.71it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.91it/s]                                               {'loss': 0.7164, 'grad_norm': 0.07414025068283081, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.91it/s]                                               {'loss': 0.7249, 'grad_norm': 0.13889899849891663, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.91it/s] 80%|████████  | 12/15 [00:00<00:00, 16.40it/s]                                               {'loss': 0.731, 'grad_norm': 0.15782570838928223, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.40it/s]                                               {'loss': 0.7394, 'grad_norm': 0.19543063640594482, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.40it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.708, 'grad_norm': 0.08235073834657669, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.6953, 'grad_norm': 0.0632605254650116, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.20it/s]                                               {'train_runtime': 1.0035, 'train_samples_per_second': 423.507, 'train_steps_per_second': 14.947, 'train_loss': 0.7181619167327881, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.20it/s]100%|██████████| 15/15 [00:01<00:00, 14.95it/s]
CLIENT:60
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7078, 'grad_norm': 0.10517379641532898, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.39it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.34it/s]                                              {'loss': 0.7055, 'grad_norm': 0.091703400015831, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.34it/s]                                              {'loss': 0.7079, 'grad_norm': 0.09639733284711838, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.34it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.68it/s]                                              {'loss': 0.7295, 'grad_norm': 0.13726690411567688, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.68it/s]                                              {'loss': 0.7019, 'grad_norm': 0.07448016107082367, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.68it/s] 40%|████      | 6/15 [00:00<00:00, 16.75it/s]                                              {'loss': 0.712, 'grad_norm': 0.0795423611998558, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.75it/s]                                              {'loss': 0.6914, 'grad_norm': 0.06640510261058807, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.75it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.7145, 'grad_norm': 0.11540231108665466, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.7148, 'grad_norm': 0.11927872896194458, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.30it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.95it/s]                                               {'loss': 0.7288, 'grad_norm': 0.17323452234268188, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.95it/s]                                               {'loss': 0.6792, 'grad_norm': 0.051161475479602814, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.95it/s] 80%|████████  | 12/15 [00:00<00:00, 16.34it/s]                                               {'loss': 0.7023, 'grad_norm': 0.10447296500205994, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.34it/s]                                               {'loss': 0.6965, 'grad_norm': 0.04840229079127312, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.34it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.7032, 'grad_norm': 0.1035410687327385, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.7263, 'grad_norm': 0.2032044380903244, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.63it/s]                                               {'train_runtime': 1.0048, 'train_samples_per_second': 422.975, 'train_steps_per_second': 14.929, 'train_loss': 0.7081017096837362, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.63it/s]100%|██████████| 15/15 [00:01<00:00, 14.93it/s]
CLIENT:67
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7192, 'grad_norm': 0.16040566563606262, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.52it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.41it/s]                                              {'loss': 0.7302, 'grad_norm': 0.17266158759593964, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.41it/s]                                              {'loss': 0.7168, 'grad_norm': 0.12849847972393036, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.41it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.7404, 'grad_norm': 0.1843648999929428, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.7303, 'grad_norm': 0.1746843457221985, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.24it/s] 40%|████      | 6/15 [00:00<00:00, 16.26it/s]                                              {'loss': 0.7091, 'grad_norm': 0.11192327737808228, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.26it/s]                                              {'loss': 0.7201, 'grad_norm': 0.1261328160762787, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.26it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.62it/s]                                              {'loss': 0.7329, 'grad_norm': 0.194960817694664, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.62it/s]                                              {'loss': 0.7269, 'grad_norm': 0.19412027299404144, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.62it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.00it/s]                                               {'loss': 0.7255, 'grad_norm': 0.18153826892375946, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.00it/s]                                               {'loss': 0.7092, 'grad_norm': 0.1594020277261734, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.00it/s] 80%|████████  | 12/15 [00:00<00:00, 16.49it/s]                                               {'loss': 0.7341, 'grad_norm': 0.19507744908332825, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.49it/s]                                               {'loss': 0.7156, 'grad_norm': 0.16255204379558563, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.49it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.717, 'grad_norm': 0.18106739223003387, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.7218, 'grad_norm': 0.21329882740974426, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.37it/s]                                               {'train_runtime': 1.0156, 'train_samples_per_second': 418.473, 'train_steps_per_second': 14.77, 'train_loss': 0.7232716957728068, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.37it/s]100%|██████████| 15/15 [00:01<00:00, 14.77it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 34.12it/s] 24%|██▍       | 8/33 [00:00<00:00, 29.02it/s] 33%|███▎      | 11/33 [00:00<00:00, 27.89it/s] 42%|████▏     | 14/33 [00:00<00:00, 26.84it/s] 52%|█████▏    | 17/33 [00:00<00:00, 26.28it/s] 61%|██████    | 20/33 [00:00<00:00, 25.85it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.66it/s] 79%|███████▉  | 26/33 [00:00<00:00, 25.45it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.31it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.34it/s]100%|██████████| 33/33 [00:01<00:00, 26.53it/s]
{'eval_loss': 0.7207196354866028, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3096836049856184, 'eval_runtime': 1.2845, 'eval_samples_per_second': 812.001, 'eval_steps_per_second': 25.691}
ROUND:4
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7371, 'grad_norm': 0.17727629840373993, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.52it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.45it/s]                                              {'loss': 0.7415, 'grad_norm': 0.17272241413593292, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.45it/s]                                              {'loss': 0.7274, 'grad_norm': 0.11672044545412064, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.45it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.7223, 'grad_norm': 0.1491888016462326, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.7373, 'grad_norm': 0.14469127357006073, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.26it/s] 40%|████      | 6/15 [00:00<00:00, 16.19it/s]                                              {'loss': 0.735, 'grad_norm': 0.23679308593273163, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.19it/s]                                              {'loss': 0.7306, 'grad_norm': 0.15279988944530487, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.19it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.7194, 'grad_norm': 0.15219581127166748, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.7338, 'grad_norm': 0.24156799912452698, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.18it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.7297, 'grad_norm': 0.18423251807689667, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.7244, 'grad_norm': 0.20982979238033295, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.62it/s] 80%|████████  | 12/15 [00:00<00:00, 16.16it/s]                                               {'loss': 0.7198, 'grad_norm': 0.14579196274280548, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.16it/s]                                               {'loss': 0.7311, 'grad_norm': 0.2211260199546814, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.16it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.7279, 'grad_norm': 0.1897350400686264, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.7252, 'grad_norm': 0.16742907464504242, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.95it/s]                                               {'train_runtime': 1.0337, 'train_samples_per_second': 411.145, 'train_steps_per_second': 14.511, 'train_loss': 0.7295081377029419, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.95it/s]100%|██████████| 15/15 [00:01<00:00, 14.52it/s]
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7111, 'grad_norm': 0.09912022948265076, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.23it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.59it/s]                                              {'loss': 0.7035, 'grad_norm': 0.10516074299812317, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.59it/s]                                              {'loss': 0.7314, 'grad_norm': 0.14694812893867493, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.59it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.66it/s]                                              {'loss': 0.7008, 'grad_norm': 0.08776641637086868, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.66it/s]                                              {'loss': 0.7184, 'grad_norm': 0.14589928090572357, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.66it/s] 40%|████      | 6/15 [00:00<00:00, 16.74it/s]                                              {'loss': 0.7158, 'grad_norm': 0.11258050054311752, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.74it/s]                                              {'loss': 0.718, 'grad_norm': 0.15407894551753998, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.74it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.97it/s]                                              {'loss': 0.7109, 'grad_norm': 0.08204369992017746, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.97it/s]                                              {'loss': 0.714, 'grad_norm': 0.1046314612030983, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.97it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.60it/s]                                               {'loss': 0.7136, 'grad_norm': 0.11788828670978546, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.60it/s]                                               {'loss': 0.6991, 'grad_norm': 0.10454721003770828, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.60it/s] 80%|████████  | 12/15 [00:00<00:00, 16.27it/s]                                               {'loss': 0.7218, 'grad_norm': 0.14459744095802307, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.27it/s]                                               {'loss': 0.7071, 'grad_norm': 0.0737752839922905, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.27it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.45it/s]                                               {'loss': 0.7176, 'grad_norm': 0.1730722039937973, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.45it/s]                                               {'loss': 0.6891, 'grad_norm': 0.1286434680223465, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.45it/s]                                               {'train_runtime': 1.0166, 'train_samples_per_second': 418.053, 'train_steps_per_second': 14.755, 'train_loss': 0.7114725112915039, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.45it/s]100%|██████████| 15/15 [00:01<00:00, 14.76it/s]
CLIENT:79
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7255, 'grad_norm': 0.1438627690076828, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.66it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.41it/s]                                              {'loss': 0.7341, 'grad_norm': 0.13507767021656036, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.41it/s]                                              {'loss': 0.6882, 'grad_norm': 0.05830354243516922, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.41it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.7135, 'grad_norm': 0.07493489235639572, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.7361, 'grad_norm': 0.1699003130197525, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.20it/s] 40%|████      | 6/15 [00:00<00:00, 16.13it/s]                                              {'loss': 0.7142, 'grad_norm': 0.11516409367322922, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.13it/s]                                              {'loss': 0.7406, 'grad_norm': 0.16875682771205902, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.13it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.73it/s]                                              {'loss': 0.6894, 'grad_norm': 0.09101510792970657, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.73it/s]                                              {'loss': 0.7093, 'grad_norm': 0.09751591086387634, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.73it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.64it/s]                                               {'loss': 0.7081, 'grad_norm': 0.11921349912881851, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.64it/s]                                               {'loss': 0.721, 'grad_norm': 0.11309424787759781, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.64it/s] 80%|████████  | 12/15 [00:00<00:00, 16.04it/s]                                               {'loss': 0.7031, 'grad_norm': 0.15187349915504456, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.04it/s]                                               {'loss': 0.723, 'grad_norm': 0.13746972382068634, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.04it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.7203, 'grad_norm': 0.1357545405626297, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.7264, 'grad_norm': 0.11741378903388977, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.02it/s]                                               {'train_runtime': 1.0371, 'train_samples_per_second': 409.815, 'train_steps_per_second': 14.464, 'train_loss': 0.7168543219566346, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.02it/s]100%|██████████| 15/15 [00:01<00:00, 14.47it/s]
CLIENT:14
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6945, 'grad_norm': 0.07855192571878433, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.88it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.93it/s]                                              {'loss': 0.7194, 'grad_norm': 0.12239616364240646, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.93it/s]                                              {'loss': 0.7234, 'grad_norm': 0.12923139333724976, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.93it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.86it/s]                                              {'loss': 0.7203, 'grad_norm': 0.10647893697023392, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.86it/s]                                              {'loss': 0.7064, 'grad_norm': 0.08297949284315109, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.86it/s] 40%|████      | 6/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.7202, 'grad_norm': 0.1780979037284851, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.7201, 'grad_norm': 0.13342803716659546, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.89it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.7116, 'grad_norm': 0.11845865845680237, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.7026, 'grad_norm': 0.09191983193159103, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.85it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.7106, 'grad_norm': 0.09135957062244415, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.7019, 'grad_norm': 0.14109079539775848, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.43it/s] 80%|████████  | 12/15 [00:00<00:00, 15.93it/s]                                               {'loss': 0.686, 'grad_norm': 0.11644944548606873, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.93it/s]                                               {'loss': 0.708, 'grad_norm': 0.08816350251436234, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.93it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.6888, 'grad_norm': 0.10867707431316376, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.7308, 'grad_norm': 0.21556076407432556, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.69it/s]                                               {'train_runtime': 1.0529, 'train_samples_per_second': 403.659, 'train_steps_per_second': 14.247, 'train_loss': 0.7096330245335897, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.69it/s]100%|██████████| 15/15 [00:01<00:00, 14.25it/s]
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7255, 'grad_norm': 0.18069690465927124, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.34it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.75it/s]                                              {'loss': 0.7021, 'grad_norm': 0.07422258704900742, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.75it/s]                                              {'loss': 0.7119, 'grad_norm': 0.1011236160993576, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.75it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.25it/s]                                              {'loss': 0.7208, 'grad_norm': 0.08642884343862534, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.25it/s]                                              {'loss': 0.7311, 'grad_norm': 0.15432600677013397, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.25it/s] 40%|████      | 6/15 [00:00<00:00, 16.32it/s]                                              {'loss': 0.6982, 'grad_norm': 0.10476228594779968, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.32it/s]                                              {'loss': 0.7171, 'grad_norm': 0.14385488629341125, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.32it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.69it/s]                                              {'loss': 0.7094, 'grad_norm': 0.10331212729215622, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.69it/s]                                              {'loss': 0.71, 'grad_norm': 0.12449359893798828, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.69it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.94it/s]                                               {'loss': 0.7443, 'grad_norm': 0.24036532640457153, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.94it/s]                                               {'loss': 0.6951, 'grad_norm': 0.052100468426942825, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.94it/s] 80%|████████  | 12/15 [00:00<00:00, 16.38it/s]                                               {'loss': 0.7154, 'grad_norm': 0.11363349854946136, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.38it/s]                                               {'loss': 0.7214, 'grad_norm': 0.1643373668193817, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.38it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.7059, 'grad_norm': 0.11608768999576569, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6977, 'grad_norm': 0.11191827803850174, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.41it/s]                                               {'train_runtime': 1.0112, 'train_samples_per_second': 420.306, 'train_steps_per_second': 14.834, 'train_loss': 0.7137232104937236, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.41it/s]100%|██████████| 15/15 [00:01<00:00, 14.84it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7293, 'grad_norm': 0.1978754699230194, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.44it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.7455, 'grad_norm': 0.2115723341703415, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.7163, 'grad_norm': 0.14331793785095215, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.49it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.69it/s]                                              {'loss': 0.7295, 'grad_norm': 0.17279227077960968, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.69it/s]                                              {'loss': 0.7346, 'grad_norm': 0.22013190388679504, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.69it/s] 40%|████      | 6/15 [00:00<00:00, 16.50it/s]                                              {'loss': 0.728, 'grad_norm': 0.19548481702804565, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.50it/s]                                              {'loss': 0.7508, 'grad_norm': 0.24773479998111725, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.50it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.7303, 'grad_norm': 0.23034757375717163, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.7212, 'grad_norm': 0.12466677278280258, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.18it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.96it/s]                                               {'loss': 0.727, 'grad_norm': 0.18685084581375122, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.96it/s]                                               {'loss': 0.7153, 'grad_norm': 0.239263653755188, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.96it/s] 80%|████████  | 12/15 [00:00<00:00, 16.47it/s]                                               {'loss': 0.7386, 'grad_norm': 0.23082497715950012, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.47it/s]                                               {'loss': 0.7207, 'grad_norm': 0.212755024433136, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.47it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.7243, 'grad_norm': 0.22799840569496155, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.733, 'grad_norm': 0.26352739334106445, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.18it/s]                                               {'train_runtime': 1.0168, 'train_samples_per_second': 417.986, 'train_steps_per_second': 14.752, 'train_loss': 0.7296216924985249, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.18it/s]100%|██████████| 15/15 [00:01<00:00, 14.76it/s]
CLIENT:19
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.725, 'grad_norm': 0.13505961000919342, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.86it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.56it/s]                                              {'loss': 0.7252, 'grad_norm': 0.13425378501415253, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.56it/s]                                              {'loss': 0.7313, 'grad_norm': 0.18548212945461273, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.56it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.67it/s]                                              {'loss': 0.7274, 'grad_norm': 0.15089473128318787, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.67it/s]                                              {'loss': 0.7418, 'grad_norm': 0.186153382062912, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.67it/s] 40%|████      | 6/15 [00:00<00:00, 16.66it/s]                                              {'loss': 0.7003, 'grad_norm': 0.09251227974891663, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.66it/s]                                              {'loss': 0.7365, 'grad_norm': 0.18997310101985931, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.66it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.729, 'grad_norm': 0.145272359251976, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.7123, 'grad_norm': 0.13589490950107574, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.15it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.7405, 'grad_norm': 0.1922445148229599, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.7149, 'grad_norm': 0.18499048054218292, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.69it/s] 80%|████████  | 12/15 [00:00<00:00, 16.20it/s]                                               {'loss': 0.7017, 'grad_norm': 0.09887371957302094, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.20it/s]                                               {'loss': 0.7162, 'grad_norm': 0.15506361424922943, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.20it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.7234, 'grad_norm': 0.2088470607995987, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.7208, 'grad_norm': 0.16022291779518127, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.86it/s]                                               {'train_runtime': 1.0315, 'train_samples_per_second': 412.025, 'train_steps_per_second': 14.542, 'train_loss': 0.7231048266092936, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.86it/s]100%|██████████| 15/15 [00:01<00:00, 14.55it/s]
CLIENT:28
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7177, 'grad_norm': 0.10471167415380478, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.51it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.38it/s]                                              {'loss': 0.7228, 'grad_norm': 0.15417265892028809, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.38it/s]                                              {'loss': 0.7355, 'grad_norm': 0.17607544362545013, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.38it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.7275, 'grad_norm': 0.125126451253891, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.7174, 'grad_norm': 0.137995183467865, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.63it/s] 40%|████      | 6/15 [00:00<00:00, 16.49it/s]                                              {'loss': 0.7347, 'grad_norm': 0.17623049020767212, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.49it/s]                                              {'loss': 0.7343, 'grad_norm': 0.14413055777549744, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.49it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.95it/s]                                              {'loss': 0.7189, 'grad_norm': 0.19551223516464233, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.95it/s]                                              {'loss': 0.6988, 'grad_norm': 0.08871133625507355, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.95it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.7201, 'grad_norm': 0.1381741762161255, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.7223, 'grad_norm': 0.18400363624095917, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.52it/s] 80%|████████  | 12/15 [00:00<00:00, 16.03it/s]                                               {'loss': 0.7278, 'grad_norm': 0.1279451698064804, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.03it/s]                                               {'loss': 0.7281, 'grad_norm': 0.14950411021709442, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.03it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.7343, 'grad_norm': 0.23201154172420502, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.7104, 'grad_norm': 0.09322156757116318, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.86it/s]                                               {'train_runtime': 1.0368, 'train_samples_per_second': 409.896, 'train_steps_per_second': 14.467, 'train_loss': 0.7233695030212403, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.86it/s]100%|██████████| 15/15 [00:01<00:00, 14.47it/s]
CLIENT:94
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7109, 'grad_norm': 0.12042582035064697, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.39it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.70it/s]                                              {'loss': 0.722, 'grad_norm': 0.1312936395406723, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.70it/s]                                              {'loss': 0.7422, 'grad_norm': 0.19449757039546967, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.70it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.77it/s]                                              {'loss': 0.7319, 'grad_norm': 0.1404111236333847, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.77it/s]                                              {'loss': 0.7267, 'grad_norm': 0.1562318056821823, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.77it/s] 40%|████      | 6/15 [00:00<00:00, 16.98it/s]                                              {'loss': 0.7104, 'grad_norm': 0.14041399955749512, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.98it/s]                                              {'loss': 0.7362, 'grad_norm': 0.22017645835876465, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.98it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.27it/s]                                              {'loss': 0.7262, 'grad_norm': 0.1560666710138321, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.27it/s]                                              {'loss': 0.695, 'grad_norm': 0.07017959654331207, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.27it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.78it/s]                                               {'loss': 0.7226, 'grad_norm': 0.15386785566806793, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.78it/s]                                               {'loss': 0.7065, 'grad_norm': 0.1292896568775177, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.78it/s] 80%|████████  | 12/15 [00:00<00:00, 16.25it/s]                                               {'loss': 0.7132, 'grad_norm': 0.22150453925132751, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.25it/s]                                               {'loss': 0.7093, 'grad_norm': 0.1455579400062561, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.25it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.721, 'grad_norm': 0.17292673885822296, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.719, 'grad_norm': 0.19314110279083252, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.62it/s]                                               {'train_runtime': 1.0059, 'train_samples_per_second': 422.491, 'train_steps_per_second': 14.911, 'train_loss': 0.7195360461870829, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.62it/s]100%|██████████| 15/15 [00:01<00:00, 14.92it/s]
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7248, 'grad_norm': 0.13963539898395538, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.69it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.22it/s]                                              {'loss': 0.7218, 'grad_norm': 0.13779589533805847, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.22it/s]                                              {'loss': 0.6954, 'grad_norm': 0.08187098056077957, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.22it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.08it/s]                                              {'loss': 0.7363, 'grad_norm': 0.16843533515930176, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.08it/s]                                              {'loss': 0.7095, 'grad_norm': 0.09926053881645203, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.08it/s] 40%|████      | 6/15 [00:00<00:00, 16.00it/s]                                              {'loss': 0.7056, 'grad_norm': 0.1057298332452774, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.00it/s]                                              {'loss': 0.7154, 'grad_norm': 0.14339789748191833, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.00it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.7192, 'grad_norm': 0.09547295421361923, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.722, 'grad_norm': 0.15459369122982025, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.35it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.7323, 'grad_norm': 0.16719692945480347, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.7099, 'grad_norm': 0.0886288657784462, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.56it/s] 80%|████████  | 12/15 [00:00<00:00, 16.06it/s]                                               {'loss': 0.731, 'grad_norm': 0.1391732096672058, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.06it/s]                                               {'loss': 0.7114, 'grad_norm': 0.12702953815460205, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.06it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.7051, 'grad_norm': 0.08908385783433914, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.7354, 'grad_norm': 0.22491143643856049, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.20it/s]                                               {'train_runtime': 1.0275, 'train_samples_per_second': 413.622, 'train_steps_per_second': 14.598, 'train_loss': 0.7183250665664673, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.20it/s]100%|██████████| 15/15 [00:01<00:00, 14.60it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 36.80it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.94it/s] 33%|███▎      | 11/33 [00:00<00:00, 27.55it/s] 42%|████▏     | 14/33 [00:00<00:00, 26.64it/s] 52%|█████▏    | 17/33 [00:00<00:00, 26.22it/s] 61%|██████    | 20/33 [00:00<00:00, 26.42it/s] 70%|██████▉   | 23/33 [00:00<00:00, 26.00it/s] 79%|███████▉  | 26/33 [00:00<00:00, 25.74it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.51it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.45it/s]100%|██████████| 33/33 [00:01<00:00, 26.68it/s]
{'eval_loss': 0.7195374369621277, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3096836049856184, 'eval_runtime': 1.2778, 'eval_samples_per_second': 816.254, 'eval_steps_per_second': 25.826}
ROUND:5
CLIENT:79
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7241, 'grad_norm': 0.15364830195903778, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.42it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.82it/s]                                              {'loss': 0.7328, 'grad_norm': 0.14343823492527008, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.82it/s]                                              {'loss': 0.688, 'grad_norm': 0.06090451776981354, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.82it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.10it/s]                                              {'loss': 0.7128, 'grad_norm': 0.0794600173830986, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.10it/s]                                              {'loss': 0.7344, 'grad_norm': 0.18262551724910736, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.10it/s] 40%|████      | 6/15 [00:00<00:00, 16.10it/s]                                              {'loss': 0.713, 'grad_norm': 0.12332093715667725, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.10it/s]                                              {'loss': 0.7387, 'grad_norm': 0.1819550096988678, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.10it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.05it/s]                                              {'loss': 0.6885, 'grad_norm': 0.09627341479063034, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.05it/s]                                              {'loss': 0.7083, 'grad_norm': 0.10503792017698288, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.05it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.7067, 'grad_norm': 0.1272154450416565, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.7197, 'grad_norm': 0.12179741263389587, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.31it/s] 80%|████████  | 12/15 [00:00<00:00, 15.87it/s]                                               {'loss': 0.7013, 'grad_norm': 0.1624235212802887, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.87it/s]                                               {'loss': 0.7212, 'grad_norm': 0.14757375419139862, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.87it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.7186, 'grad_norm': 0.1459500938653946, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.725, 'grad_norm': 0.12590205669403076, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.70it/s]                                               {'train_runtime': 1.0471, 'train_samples_per_second': 405.879, 'train_steps_per_second': 14.325, 'train_loss': 0.7155435442924499, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.70it/s]100%|██████████| 15/15 [00:01<00:00, 14.33it/s]
CLIENT:75
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7007, 'grad_norm': 0.08084166049957275, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.45it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.36it/s]                                              {'loss': 0.7358, 'grad_norm': 0.18971838057041168, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.36it/s]                                              {'loss': 0.7226, 'grad_norm': 0.14245973527431488, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.36it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.7155, 'grad_norm': 0.09840761125087738, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.7416, 'grad_norm': 0.19737136363983154, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.20it/s] 40%|████      | 6/15 [00:00<00:00, 16.06it/s]                                              {'loss': 0.7094, 'grad_norm': 0.12489013373851776, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.06it/s]                                              {'loss': 0.7283, 'grad_norm': 0.1938779652118683, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.06it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.99it/s]                                              {'loss': 0.6961, 'grad_norm': 0.12002865970134735, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.99it/s]                                              {'loss': 0.7085, 'grad_norm': 0.1090928316116333, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.99it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.7086, 'grad_norm': 0.11466740816831589, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.7045, 'grad_norm': 0.13160359859466553, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.55it/s] 80%|████████  | 12/15 [00:00<00:00, 15.99it/s]                                               {'loss': 0.7139, 'grad_norm': 0.21810710430145264, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.99it/s]                                               {'loss': 0.7153, 'grad_norm': 0.11476912349462509, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.99it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.7199, 'grad_norm': 0.17962253093719482, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.7174, 'grad_norm': 0.1391672044992447, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.98it/s]                                               {'train_runtime': 1.037, 'train_samples_per_second': 409.831, 'train_steps_per_second': 14.465, 'train_loss': 0.7158685525258383, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.98it/s]100%|██████████| 15/15 [00:01<00:00, 14.47it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7285, 'grad_norm': 0.1769281029701233, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.32it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.13it/s]                                              {'loss': 0.7364, 'grad_norm': 0.19052332639694214, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.13it/s]                                              {'loss': 0.7066, 'grad_norm': 0.13456204533576965, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.13it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.79it/s]                                              {'loss': 0.7173, 'grad_norm': 0.11308811604976654, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.79it/s]                                              {'loss': 0.745, 'grad_norm': 0.22852839529514313, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.79it/s] 40%|████      | 6/15 [00:00<00:00, 15.86it/s]                                              {'loss': 0.7292, 'grad_norm': 0.1770724058151245, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.86it/s]                                              {'loss': 0.712, 'grad_norm': 0.1604621261358261, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.86it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.7133, 'grad_norm': 0.15194426476955414, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.7361, 'grad_norm': 0.24983161687850952, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.58it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.7284, 'grad_norm': 0.19678039848804474, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.7058, 'grad_norm': 0.11629732698202133, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.16it/s] 80%|████████  | 12/15 [00:00<00:00, 15.72it/s]                                               {'loss': 0.7337, 'grad_norm': 0.2684856057167053, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.72it/s]                                               {'loss': 0.7227, 'grad_norm': 0.22478637099266052, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.72it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.7141, 'grad_norm': 0.15045543015003204, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.7351, 'grad_norm': 0.17100080847740173, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.83it/s]                                               {'train_runtime': 1.052, 'train_samples_per_second': 404.005, 'train_steps_per_second': 14.259, 'train_loss': 0.7242686549822489, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.83it/s]100%|██████████| 15/15 [00:01<00:00, 14.26it/s]
CLIENT:15
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7276, 'grad_norm': 0.14014241099357605, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.18it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.7128, 'grad_norm': 0.1292361319065094, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.6947, 'grad_norm': 0.0885857567191124, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.04it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.75it/s]                                              {'loss': 0.7232, 'grad_norm': 0.12814608216285706, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.75it/s]                                              {'loss': 0.7185, 'grad_norm': 0.11493197083473206, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.75it/s] 40%|████      | 6/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.7124, 'grad_norm': 0.12033119052648544, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.7123, 'grad_norm': 0.08846130222082138, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.89it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7171, 'grad_norm': 0.1516260802745819, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7031, 'grad_norm': 0.14460590481758118, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.61it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.7134, 'grad_norm': 0.1324019879102707, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.7001, 'grad_norm': 0.06309904903173447, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.23it/s] 80%|████████  | 12/15 [00:00<00:00, 15.84it/s]                                               {'loss': 0.7382, 'grad_norm': 0.22060585021972656, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.84it/s]                                               {'loss': 0.713, 'grad_norm': 0.1381368786096573, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.84it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.7061, 'grad_norm': 0.14554457366466522, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.7076, 'grad_norm': 0.11647500842809677, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.71it/s]                                               {'train_runtime': 1.0558, 'train_samples_per_second': 402.54, 'train_steps_per_second': 14.207, 'train_loss': 0.7133340001106262, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.71it/s]100%|██████████| 15/15 [00:01<00:00, 14.21it/s]
CLIENT:38
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7205, 'grad_norm': 0.12211522459983826, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.58it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.32it/s]                                              {'loss': 0.7228, 'grad_norm': 0.12509030103683472, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.32it/s]                                              {'loss': 0.7342, 'grad_norm': 0.18122880160808563, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.32it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.92it/s]                                              {'loss': 0.7312, 'grad_norm': 0.18227116763591766, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.92it/s]                                              {'loss': 0.7106, 'grad_norm': 0.09460540860891342, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.92it/s] 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.7012, 'grad_norm': 0.1315506398677826, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.7295, 'grad_norm': 0.15304823219776154, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.97it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.65it/s]                                              {'loss': 0.7149, 'grad_norm': 0.16673064231872559, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.65it/s]                                              {'loss': 0.7147, 'grad_norm': 0.12018224596977234, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.65it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.7071, 'grad_norm': 0.12607406079769135, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.7116, 'grad_norm': 0.15795879065990448, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.26it/s] 80%|████████  | 12/15 [00:00<00:00, 15.84it/s]                                               {'loss': 0.7139, 'grad_norm': 0.15296399593353271, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.84it/s]                                               {'loss': 0.7009, 'grad_norm': 0.10636589676141739, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.84it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.68it/s]                                               {'loss': 0.693, 'grad_norm': 0.1158195436000824, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.68it/s]                                               {'loss': 0.7343, 'grad_norm': 0.2821849286556244, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.68it/s]                                               {'train_runtime': 1.0496, 'train_samples_per_second': 404.903, 'train_steps_per_second': 14.291, 'train_loss': 0.7160263498624165, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.68it/s]100%|██████████| 15/15 [00:01<00:00, 14.30it/s]
CLIENT:11
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7193, 'grad_norm': 0.13786254823207855, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.17it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.7418, 'grad_norm': 0.19735462963581085, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.6985, 'grad_norm': 0.07491543889045715, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.01it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.7306, 'grad_norm': 0.172763392329216, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.7388, 'grad_norm': 0.14991329610347748, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.71it/s] 40%|████      | 6/15 [00:00<00:00, 15.83it/s]                                              {'loss': 0.7162, 'grad_norm': 0.10380254685878754, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.83it/s]                                              {'loss': 0.7271, 'grad_norm': 0.15776287019252777, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.83it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.722, 'grad_norm': 0.14020903408527374, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.7175, 'grad_norm': 0.12311273068189621, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.56it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.7248, 'grad_norm': 0.18151429295539856, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.7143, 'grad_norm': 0.1190701350569725, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.25it/s] 80%|████████  | 12/15 [00:00<00:00, 15.80it/s]                                               {'loss': 0.7294, 'grad_norm': 0.1632237285375595, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.80it/s]                                               {'loss': 0.7204, 'grad_norm': 0.17347902059555054, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.80it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.7129, 'grad_norm': 0.09802516549825668, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.7369, 'grad_norm': 0.2404509335756302, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.80it/s]                                               {'train_runtime': 1.0531, 'train_samples_per_second': 403.572, 'train_steps_per_second': 14.244, 'train_loss': 0.7233670314153036, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.80it/s]100%|██████████| 15/15 [00:01<00:00, 14.25it/s]
CLIENT:40
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7158, 'grad_norm': 0.14485515654087067, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.31it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.17it/s]                                              {'loss': 0.7036, 'grad_norm': 0.1251973658800125, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.17it/s]                                              {'loss': 0.7289, 'grad_norm': 0.1839843988418579, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.17it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.7231, 'grad_norm': 0.13796699047088623, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.7315, 'grad_norm': 0.1557723730802536, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.36it/s] 40%|████      | 6/15 [00:00<00:00, 16.71it/s]                                              {'loss': 0.7073, 'grad_norm': 0.13372012972831726, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.71it/s]                                              {'loss': 0.7006, 'grad_norm': 0.14372263848781586, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.71it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.7161, 'grad_norm': 0.13945364952087402, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.7241, 'grad_norm': 0.19131717085838318, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.04it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.60it/s]                                               {'loss': 0.7333, 'grad_norm': 0.2259531319141388, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.60it/s]                                               {'loss': 0.7011, 'grad_norm': 0.05597067251801491, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.60it/s] 80%|████████  | 12/15 [00:00<00:00, 16.05it/s]                                               {'loss': 0.7361, 'grad_norm': 0.21142922341823578, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.05it/s]                                               {'loss': 0.6984, 'grad_norm': 0.16169185936450958, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.05it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.7006, 'grad_norm': 0.16868148744106293, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.7055, 'grad_norm': 0.14569143950939178, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.91it/s]                                               {'train_runtime': 1.0347, 'train_samples_per_second': 410.756, 'train_steps_per_second': 14.497, 'train_loss': 0.7150737007459005, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.91it/s]100%|██████████| 15/15 [00:01<00:00, 14.50it/s]
CLIENT:45
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.718, 'grad_norm': 0.1815580129623413, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.53it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.71it/s]                                              {'loss': 0.7449, 'grad_norm': 0.24048177897930145, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.71it/s]                                              {'loss': 0.7211, 'grad_norm': 0.10694694519042969, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.71it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.98it/s]                                              {'loss': 0.7318, 'grad_norm': 0.19468559324741364, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.98it/s]                                              {'loss': 0.7365, 'grad_norm': 0.20420381426811218, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.98it/s] 40%|████      | 6/15 [00:00<00:00, 15.92it/s]                                              {'loss': 0.6897, 'grad_norm': 0.11009719222784042, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.92it/s]                                              {'loss': 0.7344, 'grad_norm': 0.22957657277584076, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.92it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.7122, 'grad_norm': 0.14484143257141113, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.719, 'grad_norm': 0.18332161009311676, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.78it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.7088, 'grad_norm': 0.1244848445057869, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.7128, 'grad_norm': 0.2328513264656067, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.16it/s] 80%|████████  | 12/15 [00:00<00:00, 15.75it/s]                                               {'loss': 0.7322, 'grad_norm': 0.29766741394996643, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.75it/s]                                               {'loss': 0.7081, 'grad_norm': 0.18171915411949158, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.75it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.7181, 'grad_norm': 0.258185476064682, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.7041, 'grad_norm': 0.17089834809303284, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.64it/s]                                               {'train_runtime': 1.0525, 'train_samples_per_second': 403.79, 'train_steps_per_second': 14.251, 'train_loss': 0.7194443424542745, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.64it/s]100%|██████████| 15/15 [00:01<00:00, 14.26it/s]
CLIENT:39
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7495, 'grad_norm': 0.20351138710975647, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.16it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.31it/s]                                              {'loss': 0.7055, 'grad_norm': 0.08055797964334488, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.31it/s]                                              {'loss': 0.7242, 'grad_norm': 0.14388863742351532, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.31it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.7201, 'grad_norm': 0.1099492609500885, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.7179, 'grad_norm': 0.15117551386356354, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.63it/s] 40%|████      | 6/15 [00:00<00:00, 15.91it/s]                                              {'loss': 0.6998, 'grad_norm': 0.16196605563163757, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.91it/s]                                              {'loss': 0.7084, 'grad_norm': 0.09078330546617508, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.91it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.7278, 'grad_norm': 0.17114831507205963, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.719, 'grad_norm': 0.1875767707824707, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.78it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.7154, 'grad_norm': 0.14751572906970978, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.716, 'grad_norm': 0.16452568769454956, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.20it/s] 80%|████████  | 12/15 [00:00<00:00, 15.79it/s]                                               {'loss': 0.7043, 'grad_norm': 0.1582096368074417, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.79it/s]                                               {'loss': 0.7339, 'grad_norm': 0.21770714223384857, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.79it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.7156, 'grad_norm': 0.12881328165531158, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.7088, 'grad_norm': 0.10660474002361298, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.62it/s]                                               {'train_runtime': 1.0568, 'train_samples_per_second': 402.152, 'train_steps_per_second': 14.194, 'train_loss': 0.7177482167879741, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.62it/s]100%|██████████| 15/15 [00:01<00:00, 14.20it/s]
CLIENT:62
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7301, 'grad_norm': 0.17747510969638824, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.19it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.12it/s]                                              {'loss': 0.7246, 'grad_norm': 0.14918634295463562, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.12it/s]                                              {'loss': 0.7554, 'grad_norm': 0.2931494116783142, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.12it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.7421, 'grad_norm': 0.2223808467388153, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.7166, 'grad_norm': 0.14165125787258148, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.17it/s] 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.729, 'grad_norm': 0.2544035315513611, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.7219, 'grad_norm': 0.2057034969329834, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.03it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.80it/s]                                              {'loss': 0.7312, 'grad_norm': 0.2195933312177658, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.80it/s]                                              {'loss': 0.7132, 'grad_norm': 0.2045498490333557, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.80it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.7335, 'grad_norm': 0.21334415674209595, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.7165, 'grad_norm': 0.21194428205490112, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.16it/s] 80%|████████  | 12/15 [00:00<00:00, 15.76it/s]                                               {'loss': 0.7277, 'grad_norm': 0.2685113847255707, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.76it/s]                                               {'loss': 0.7251, 'grad_norm': 0.2332392930984497, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.76it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.7203, 'grad_norm': 0.19807152450084686, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.7441, 'grad_norm': 0.31772398948669434, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.64it/s]                                               {'train_runtime': 1.0482, 'train_samples_per_second': 405.447, 'train_steps_per_second': 14.31, 'train_loss': 0.7287582993507385, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.64it/s]100%|██████████| 15/15 [00:01<00:00, 14.31it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 35.07it/s] 24%|██▍       | 8/33 [00:00<00:00, 29.59it/s] 36%|███▋      | 12/33 [00:00<00:00, 27.60it/s] 45%|████▌     | 15/33 [00:00<00:00, 26.62it/s] 55%|█████▍    | 18/33 [00:00<00:00, 25.98it/s] 64%|██████▎   | 21/33 [00:00<00:00, 25.66it/s] 73%|███████▎  | 24/33 [00:00<00:00, 25.32it/s] 82%|████████▏ | 27/33 [00:01<00:00, 25.40it/s] 91%|█████████ | 30/33 [00:01<00:00, 25.22it/s]100%|██████████| 33/33 [00:01<00:00, 26.14it/s]100%|██████████| 33/33 [00:01<00:00, 26.43it/s]
{'eval_loss': 0.7180326581001282, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3096836049856184, 'eval_runtime': 1.2901, 'eval_samples_per_second': 808.467, 'eval_steps_per_second': 25.58}
ROUND:6
CLIENT:69
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7075, 'grad_norm': 0.12058041244745255, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.15it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.63it/s]                                              {'loss': 0.7144, 'grad_norm': 0.15407468378543854, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.63it/s]                                              {'loss': 0.7223, 'grad_norm': 0.16378533840179443, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.63it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.7051, 'grad_norm': 0.11712714284658432, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.7251, 'grad_norm': 0.15850138664245605, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.33it/s] 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.7375, 'grad_norm': 0.21326012909412384, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.7062, 'grad_norm': 0.15011809766292572, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.03it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.7042, 'grad_norm': 0.13346552848815918, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.7176, 'grad_norm': 0.1835058033466339, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.70it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.7281, 'grad_norm': 0.184133380651474, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.698, 'grad_norm': 0.15151749551296234, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.03it/s] 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.7041, 'grad_norm': 0.13411273062229156, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.7088, 'grad_norm': 0.13113191723823547, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.59it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.66it/s]                                               {'loss': 0.7074, 'grad_norm': 0.12949414551258087, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.66it/s]                                               {'loss': 0.7252, 'grad_norm': 0.25365379452705383, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.66it/s]                                               {'train_runtime': 1.0593, 'train_samples_per_second': 401.222, 'train_steps_per_second': 14.161, 'train_loss': 0.7141040205955506, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.66it/s]100%|██████████| 15/15 [00:01<00:00, 14.17it/s]
CLIENT:74
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7068, 'grad_norm': 0.08604655414819717, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.73it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.42it/s]                                              {'loss': 0.7183, 'grad_norm': 0.1715690791606903, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.42it/s]                                              {'loss': 0.7301, 'grad_norm': 0.21539406478405, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.42it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.7302, 'grad_norm': 0.1738562285900116, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.6929, 'grad_norm': 0.0725579559803009, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.90it/s] 40%|████      | 6/15 [00:00<00:00, 16.02it/s]                                              {'loss': 0.7328, 'grad_norm': 0.2521319091320038, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.02it/s]                                              {'loss': 0.7092, 'grad_norm': 0.12636348605155945, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.02it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.7155, 'grad_norm': 0.2070503532886505, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.7122, 'grad_norm': 0.14108769595623016, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.78it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.6986, 'grad_norm': 0.13833507895469666, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.7127, 'grad_norm': 0.19922643899917603, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.63it/s] 80%|████████  | 12/15 [00:00<00:00, 16.18it/s]                                               {'loss': 0.7235, 'grad_norm': 0.13987882435321808, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.18it/s]                                               {'loss': 0.7093, 'grad_norm': 0.08199615776538849, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.18it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.7191, 'grad_norm': 0.24072058498859406, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.7055, 'grad_norm': 0.19005386531352997, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.98it/s]                                               {'train_runtime': 1.039, 'train_samples_per_second': 409.033, 'train_steps_per_second': 14.436, 'train_loss': 0.7144432425498962, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.98it/s]100%|██████████| 15/15 [00:01<00:00, 14.44it/s]
CLIENT:34
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7016, 'grad_norm': 0.07493501156568527, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.07it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.728, 'grad_norm': 0.14898236095905304, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.7367, 'grad_norm': 0.21998874843120575, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.30it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.75it/s]                                              {'loss': 0.7091, 'grad_norm': 0.09517721086740494, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.75it/s]                                              {'loss': 0.7466, 'grad_norm': 0.2526920735836029, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.75it/s] 40%|████      | 6/15 [00:00<00:00, 16.31it/s]                                              {'loss': 0.6969, 'grad_norm': 0.07475315779447556, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.31it/s]                                              {'loss': 0.7323, 'grad_norm': 0.22012971341609955, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.31it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.7177, 'grad_norm': 0.14052091538906097, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.6888, 'grad_norm': 0.0681108608841896, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.78it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.7152, 'grad_norm': 0.16732104122638702, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.7083, 'grad_norm': 0.1265135258436203, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.11it/s] 80%|████████  | 12/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.7174, 'grad_norm': 0.1434662789106369, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.7066, 'grad_norm': 0.13129986822605133, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.69it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.7181, 'grad_norm': 0.15785063803195953, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.7103, 'grad_norm': 0.1740446835756302, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.65it/s]                                               {'train_runtime': 1.0471, 'train_samples_per_second': 405.872, 'train_steps_per_second': 14.325, 'train_loss': 0.7155798196792602, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.65it/s]100%|██████████| 15/15 [00:01<00:00, 14.33it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7379, 'grad_norm': 0.19154484570026398, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.64it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.13it/s]                                              {'loss': 0.7216, 'grad_norm': 0.09548014402389526, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.13it/s]                                              {'loss': 0.6798, 'grad_norm': 0.07758305221796036, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.13it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.00it/s]                                              {'loss': 0.7086, 'grad_norm': 0.10609134286642075, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.00it/s]                                              {'loss': 0.7162, 'grad_norm': 0.08670398592948914, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.00it/s] 40%|████      | 6/15 [00:00<00:00, 15.81it/s]                                              {'loss': 0.7116, 'grad_norm': 0.12481715530157089, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.81it/s]                                              {'loss': 0.7116, 'grad_norm': 0.12271422892808914, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.81it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7091, 'grad_norm': 0.10095634311437607, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7146, 'grad_norm': 0.11381358653306961, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.61it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.7033, 'grad_norm': 0.06996720284223557, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.7039, 'grad_norm': 0.10781999677419662, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.49it/s] 80%|████████  | 12/15 [00:00<00:00, 15.94it/s]                                               {'loss': 0.7135, 'grad_norm': 0.19066184759140015, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.94it/s]                                               {'loss': 0.7168, 'grad_norm': 0.14180651307106018, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.94it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.7076, 'grad_norm': 0.10230114310979843, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.7083, 'grad_norm': 0.10339425504207611, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.80it/s]                                               {'train_runtime': 1.0492, 'train_samples_per_second': 405.071, 'train_steps_per_second': 14.297, 'train_loss': 0.7109582503636678, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.80it/s]100%|██████████| 15/15 [00:01<00:00, 14.30it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6982, 'grad_norm': 0.14307470619678497, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.43it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.25it/s]                                              {'loss': 0.7365, 'grad_norm': 0.22469238936901093, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.25it/s]                                              {'loss': 0.6889, 'grad_norm': 0.0647270530462265, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.25it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.07it/s]                                              {'loss': 0.6959, 'grad_norm': 0.1263398975133896, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.07it/s]                                              {'loss': 0.7191, 'grad_norm': 0.1622752994298935, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.07it/s] 40%|████      | 6/15 [00:00<00:00, 15.85it/s]                                              {'loss': 0.7243, 'grad_norm': 0.19769668579101562, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.85it/s]                                              {'loss': 0.717, 'grad_norm': 0.18816177546977997, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.85it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.7093, 'grad_norm': 0.16827309131622314, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.7028, 'grad_norm': 0.11161577701568604, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.64it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.7163, 'grad_norm': 0.13327112793922424, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.7063, 'grad_norm': 0.18636035919189453, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.50it/s] 80%|████████  | 12/15 [00:00<00:00, 16.00it/s]                                               {'loss': 0.7157, 'grad_norm': 0.16988298296928406, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.00it/s]                                               {'loss': 0.7005, 'grad_norm': 0.15656474232673645, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.00it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.7212, 'grad_norm': 0.20000289380550385, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.6952, 'grad_norm': 0.16278958320617676, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.85it/s]                                               {'train_runtime': 1.0423, 'train_samples_per_second': 407.761, 'train_steps_per_second': 14.392, 'train_loss': 0.7098083178202311, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.85it/s]100%|██████████| 15/15 [00:01<00:00, 14.40it/s]
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7211, 'grad_norm': 0.19477146863937378, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.22it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.53it/s]                                              {'loss': 0.7261, 'grad_norm': 0.19055458903312683, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.53it/s]                                              {'loss': 0.7087, 'grad_norm': 0.09164009243249893, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.53it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.69it/s]                                              {'loss': 0.7247, 'grad_norm': 0.17689363658428192, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.69it/s]                                              {'loss': 0.7256, 'grad_norm': 0.17700296640396118, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.69it/s] 40%|████      | 6/15 [00:00<00:00, 15.61it/s]                                              {'loss': 0.7103, 'grad_norm': 0.14478722214698792, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.61it/s]                                              {'loss': 0.7132, 'grad_norm': 0.17596234381198883, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.61it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.7049, 'grad_norm': 0.18182587623596191, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.7113, 'grad_norm': 0.1701449304819107, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.42it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.708, 'grad_norm': 0.15176361799240112, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.6959, 'grad_norm': 0.1486556977033615, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.36it/s] 80%|████████  | 12/15 [00:00<00:00, 16.17it/s]                                               {'loss': 0.7304, 'grad_norm': 0.27856123447418213, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.17it/s]                                               {'loss': 0.7161, 'grad_norm': 0.19908693432807922, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.17it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.00it/s]                                               {'loss': 0.7085, 'grad_norm': 0.18554814159870148, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.00it/s]                                               {'loss': 0.7197, 'grad_norm': 0.20247159898281097, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.00it/s]                                               {'train_runtime': 1.0457, 'train_samples_per_second': 406.413, 'train_steps_per_second': 14.344, 'train_loss': 0.7149647037188213, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.00it/s]100%|██████████| 15/15 [00:01<00:00, 14.35it/s]
CLIENT:32
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7103, 'grad_norm': 0.13281038403511047, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.44it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.27it/s]                                              {'loss': 0.7319, 'grad_norm': 0.2211798131465912, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.27it/s]                                              {'loss': 0.7219, 'grad_norm': 0.1767418384552002, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.27it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.06it/s]                                              {'loss': 0.7272, 'grad_norm': 0.19621096551418304, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.06it/s]                                              {'loss': 0.7293, 'grad_norm': 0.23860639333724976, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.06it/s] 40%|████      | 6/15 [00:00<00:00, 15.79it/s]                                              {'loss': 0.6918, 'grad_norm': 0.11230051517486572, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.79it/s]                                              {'loss': 0.7275, 'grad_norm': 0.24907033145427704, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.79it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.7009, 'grad_norm': 0.12407374382019043, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.7213, 'grad_norm': 0.19684219360351562, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.32it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.74it/s]                                               {'loss': 0.716, 'grad_norm': 0.24988040328025818, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.74it/s]                                               {'loss': 0.7009, 'grad_norm': 0.1906784027814865, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.74it/s] 80%|████████  | 12/15 [00:00<00:00, 16.16it/s]                                               {'loss': 0.7218, 'grad_norm': 0.15301108360290527, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.16it/s]                                               {'loss': 0.6995, 'grad_norm': 0.15599864721298218, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.16it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.7089, 'grad_norm': 0.25627607107162476, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.7141, 'grad_norm': 0.23694540560245514, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.81it/s]                                               {'train_runtime': 1.0407, 'train_samples_per_second': 408.386, 'train_steps_per_second': 14.414, 'train_loss': 0.7148870070775349, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.81it/s]100%|██████████| 15/15 [00:01<00:00, 14.42it/s]
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7333, 'grad_norm': 0.20590786635875702, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.24it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.06it/s]                                              {'loss': 0.7376, 'grad_norm': 0.19908514618873596, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.06it/s]                                              {'loss': 0.725, 'grad_norm': 0.13263241946697235, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.06it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.7188, 'grad_norm': 0.17219381034374237, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.7338, 'grad_norm': 0.16743257641792297, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.38it/s] 40%|████      | 6/15 [00:00<00:00, 16.05it/s]                                              {'loss': 0.7287, 'grad_norm': 0.2790118455886841, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.05it/s]                                              {'loss': 0.7264, 'grad_norm': 0.17927339673042297, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.05it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.7151, 'grad_norm': 0.1772744506597519, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.7263, 'grad_norm': 0.28490138053894043, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.62it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.7238, 'grad_norm': 0.21863380074501038, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.7176, 'grad_norm': 0.24673517048358917, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.04it/s] 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.7152, 'grad_norm': 0.16819220781326294, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.7234, 'grad_norm': 0.26448237895965576, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.59it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.7212, 'grad_norm': 0.22592942416667938, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.7194, 'grad_norm': 0.19453024864196777, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.62it/s]                                               {'train_runtime': 1.0575, 'train_samples_per_second': 401.892, 'train_steps_per_second': 14.184, 'train_loss': 0.7243810892105103, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.62it/s]100%|██████████| 15/15 [00:01<00:00, 14.19it/s]
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7211, 'grad_norm': 0.15605804324150085, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.56it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.699, 'grad_norm': 0.09343133121728897, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.7312, 'grad_norm': 0.15755124390125275, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.15it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.7066, 'grad_norm': 0.06405393779277802, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.724, 'grad_norm': 0.20743979513645172, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.31it/s] 40%|████      | 6/15 [00:00<00:00, 16.19it/s]                                              {'loss': 0.702, 'grad_norm': 0.14830335974693298, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.19it/s]                                              {'loss': 0.7156, 'grad_norm': 0.10583684593439102, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.19it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.7131, 'grad_norm': 0.17335976660251617, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.7123, 'grad_norm': 0.1401575356721878, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.76it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.45it/s]                                               {'loss': 0.7141, 'grad_norm': 0.09157529473304749, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.45it/s]                                               {'loss': 0.7192, 'grad_norm': 0.1729668378829956, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.45it/s] 80%|████████  | 12/15 [00:00<00:00, 15.81it/s]                                               {'loss': 0.7244, 'grad_norm': 0.19873106479644775, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.81it/s]                                               {'loss': 0.7308, 'grad_norm': 0.24653348326683044, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.81it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.7049, 'grad_norm': 0.09856591373682022, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.6944, 'grad_norm': 0.07268332690000534, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.73it/s]                                               {'train_runtime': 1.0482, 'train_samples_per_second': 405.457, 'train_steps_per_second': 14.31, 'train_loss': 0.7141758441925049, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.73it/s]100%|██████████| 15/15 [00:01<00:00, 14.32it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.725, 'grad_norm': 0.22712060809135437, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.15it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.88it/s]                                              {'loss': 0.7408, 'grad_norm': 0.2454821765422821, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.88it/s]                                              {'loss': 0.7132, 'grad_norm': 0.16199460625648499, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.88it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.7253, 'grad_norm': 0.19920098781585693, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.7289, 'grad_norm': 0.2556859254837036, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.17it/s] 40%|████      | 6/15 [00:00<00:00, 16.34it/s]                                              {'loss': 0.7227, 'grad_norm': 0.22871729731559753, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.34it/s]                                              {'loss': 0.7435, 'grad_norm': 0.2907518446445465, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.34it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.7233, 'grad_norm': 0.2728179693222046, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.7175, 'grad_norm': 0.14638718962669373, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.66it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.7208, 'grad_norm': 0.22447210550308228, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.7069, 'grad_norm': 0.2858055531978607, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.14it/s] 80%|████████  | 12/15 [00:00<00:00, 15.70it/s]                                               {'loss': 0.7302, 'grad_norm': 0.27905580401420593, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.70it/s]                                               {'loss': 0.7128, 'grad_norm': 0.2523066997528076, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.70it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.7157, 'grad_norm': 0.27409833669662476, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.7227, 'grad_norm': 0.3183805048465729, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.85it/s]                                               {'train_runtime': 1.0457, 'train_samples_per_second': 406.407, 'train_steps_per_second': 14.344, 'train_loss': 0.7232924977938334, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.85it/s]100%|██████████| 15/15 [00:01<00:00, 14.35it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 36.66it/s] 24%|██▍       | 8/33 [00:00<00:00, 29.47it/s] 36%|███▋      | 12/33 [00:00<00:00, 27.08it/s] 45%|████▌     | 15/33 [00:00<00:00, 26.28it/s] 55%|█████▍    | 18/33 [00:00<00:00, 25.97it/s] 64%|██████▎   | 21/33 [00:00<00:00, 25.54it/s] 73%|███████▎  | 24/33 [00:00<00:00, 25.46it/s] 82%|████████▏ | 27/33 [00:01<00:00, 25.26it/s] 91%|█████████ | 30/33 [00:01<00:00, 25.17it/s]100%|██████████| 33/33 [00:01<00:00, 26.26it/s]100%|██████████| 33/33 [00:01<00:00, 26.40it/s]
{'eval_loss': 0.7161593437194824, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.311601150527325, 'eval_runtime': 1.2906, 'eval_samples_per_second': 808.124, 'eval_steps_per_second': 25.569}
ROUND:7
CLIENT:71
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.701, 'grad_norm': 0.15590709447860718, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.07it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.7004, 'grad_norm': 0.07271129637956619, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.7114, 'grad_norm': 0.12199994176626205, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.32it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.7207, 'grad_norm': 0.18491284549236298, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.6908, 'grad_norm': 0.06032261624932289, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.52it/s] 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.7143, 'grad_norm': 0.17471875250339508, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.6972, 'grad_norm': 0.07537238299846649, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.03it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.7009, 'grad_norm': 0.1386500895023346, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.706, 'grad_norm': 0.13746660947799683, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.74it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.7134, 'grad_norm': 0.13773632049560547, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.6993, 'grad_norm': 0.10814543813467026, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.24it/s] 80%|████████  | 12/15 [00:00<00:00, 15.74it/s]                                               {'loss': 0.6951, 'grad_norm': 0.08893251419067383, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.74it/s]                                               {'loss': 0.7155, 'grad_norm': 0.1788090616464615, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.74it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.7066, 'grad_norm': 0.06717254966497421, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.7149, 'grad_norm': 0.1285323202610016, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.72it/s]                                               {'train_runtime': 1.0464, 'train_samples_per_second': 406.14, 'train_steps_per_second': 14.334, 'train_loss': 0.7058327476183573, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.72it/s]100%|██████████| 15/15 [00:01<00:00, 14.34it/s]
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6968, 'grad_norm': 0.10689471662044525, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.28it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.7138, 'grad_norm': 0.22952476143836975, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.7241, 'grad_norm': 0.2259448766708374, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.34it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.7196, 'grad_norm': 0.15240536630153656, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.7322, 'grad_norm': 0.267318993806839, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.88it/s] 40%|████      | 6/15 [00:00<00:00, 16.64it/s]                                              {'loss': 0.7153, 'grad_norm': 0.16534321010112762, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.64it/s]                                              {'loss': 0.7285, 'grad_norm': 0.28218814730644226, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.64it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.02it/s]                                              {'loss': 0.7071, 'grad_norm': 0.15440510213375092, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.02it/s]                                              {'loss': 0.7034, 'grad_norm': 0.10498807579278946, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.02it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.7142, 'grad_norm': 0.15143121778964996, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6965, 'grad_norm': 0.23248904943466187, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.13it/s] 80%|████████  | 12/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.7291, 'grad_norm': 0.2575405240058899, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.7135, 'grad_norm': 0.17467966675758362, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.66it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.7137, 'grad_norm': 0.22771082818508148, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.7076, 'grad_norm': 0.25291708111763, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.52it/s]                                               {'train_runtime': 1.0441, 'train_samples_per_second': 407.038, 'train_steps_per_second': 14.366, 'train_loss': 0.7143567760785421, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.52it/s]100%|██████████| 15/15 [00:01<00:00, 14.37it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7082, 'grad_norm': 0.1343798041343689, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.42it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.19it/s]                                              {'loss': 0.7149, 'grad_norm': 0.22602906823158264, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.19it/s]                                              {'loss': 0.7316, 'grad_norm': 0.2726644277572632, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.19it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.96it/s]                                              {'loss': 0.7119, 'grad_norm': 0.14128531515598297, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.96it/s]                                              {'loss': 0.7305, 'grad_norm': 0.26881542801856995, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.96it/s] 40%|████      | 6/15 [00:00<00:00, 15.76it/s]                                              {'loss': 0.7219, 'grad_norm': 0.2026013880968094, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.76it/s]                                              {'loss': 0.7357, 'grad_norm': 0.26499298214912415, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.76it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.7032, 'grad_norm': 0.1748860478401184, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.7033, 'grad_norm': 0.2033730000257492, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.76it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.7225, 'grad_norm': 0.22496052086353302, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.7033, 'grad_norm': 0.21476107835769653, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.39it/s] 80%|████████  | 12/15 [00:00<00:00, 16.04it/s]                                               {'loss': 0.7032, 'grad_norm': 0.23288729786872864, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.04it/s]                                               {'loss': 0.7003, 'grad_norm': 0.16673049330711365, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.04it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.7082, 'grad_norm': 0.27812254428863525, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.7183, 'grad_norm': 0.32414403557777405, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.85it/s]                                               {'train_runtime': 1.0466, 'train_samples_per_second': 406.076, 'train_steps_per_second': 14.332, 'train_loss': 0.7144705136617024, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.85it/s]100%|██████████| 15/15 [00:01<00:00, 14.34it/s]
CLIENT:10
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.742, 'grad_norm': 0.18673017621040344, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.29it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.13it/s]                                              {'loss': 0.7244, 'grad_norm': 0.12889109551906586, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.13it/s]                                              {'loss': 0.7251, 'grad_norm': 0.17264918982982635, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.13it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.27it/s]                                              {'loss': 0.7184, 'grad_norm': 0.14489632844924927, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.27it/s]                                              {'loss': 0.7173, 'grad_norm': 0.1486509144306183, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.27it/s] 40%|████      | 6/15 [00:00<00:00, 16.24it/s]                                              {'loss': 0.7195, 'grad_norm': 0.21053624153137207, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.24it/s]                                              {'loss': 0.7106, 'grad_norm': 0.1553986817598343, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.24it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.7241, 'grad_norm': 0.1541762351989746, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.7253, 'grad_norm': 0.23801064491271973, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.58it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.7132, 'grad_norm': 0.15935425460338593, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.7117, 'grad_norm': 0.22775526344776154, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.96it/s] 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.7139, 'grad_norm': 0.12685763835906982, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.7207, 'grad_norm': 0.2162482589483261, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.53it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.7113, 'grad_norm': 0.15456965565681458, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.7254, 'grad_norm': 0.23386436700820923, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.61it/s]                                               {'train_runtime': 1.0586, 'train_samples_per_second': 401.464, 'train_steps_per_second': 14.169, 'train_loss': 0.7201869249343872, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.61it/s]100%|██████████| 15/15 [00:01<00:00, 14.17it/s]
CLIENT:44
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7012, 'grad_norm': 0.08673655241727829, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.97it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.37it/s]                                              {'loss': 0.6871, 'grad_norm': 0.06566959619522095, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.37it/s]                                              {'loss': 0.7068, 'grad_norm': 0.09596813470125198, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.37it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.7027, 'grad_norm': 0.10374338179826736, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.6966, 'grad_norm': 0.08005896955728531, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.66it/s] 40%|████      | 6/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.6969, 'grad_norm': 0.07303831726312637, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.7314, 'grad_norm': 0.156538724899292, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.63it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.687, 'grad_norm': 0.0999893918633461, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.7128, 'grad_norm': 0.15203842520713806, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.68it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.699, 'grad_norm': 0.09570188820362091, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.7095, 'grad_norm': 0.13356873393058777, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.18it/s] 80%|████████  | 12/15 [00:00<00:00, 15.81it/s]                                               {'loss': 0.7106, 'grad_norm': 0.1132740005850792, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.81it/s]                                               {'loss': 0.6728, 'grad_norm': 0.07732729613780975, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.81it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.77it/s]                                               {'loss': 0.7064, 'grad_norm': 0.13273270428180695, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.77it/s]                                               {'loss': 0.701, 'grad_norm': 0.1361764818429947, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.77it/s]                                               {'train_runtime': 1.056, 'train_samples_per_second': 402.454, 'train_steps_per_second': 14.204, 'train_loss': 0.7014557997385661, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.77it/s]100%|██████████| 15/15 [00:01<00:00, 14.21it/s]
CLIENT:34
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7009, 'grad_norm': 0.08039701730012894, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.86it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.11it/s]                                              {'loss': 0.7261, 'grad_norm': 0.16203239560127258, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.11it/s]                                              {'loss': 0.7338, 'grad_norm': 0.23983153700828552, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.11it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.7079, 'grad_norm': 0.10246691107749939, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.7429, 'grad_norm': 0.2769199311733246, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.90it/s] 40%|████      | 6/15 [00:00<00:00, 15.78it/s]                                              {'loss': 0.6966, 'grad_norm': 0.07960539311170578, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.78it/s]                                              {'loss': 0.7288, 'grad_norm': 0.242159903049469, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.78it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.7155, 'grad_norm': 0.1528492420911789, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.689, 'grad_norm': 0.07524655014276505, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.26it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.7123, 'grad_norm': 0.18238361179828644, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.7063, 'grad_norm': 0.13965359330177307, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.51it/s] 80%|████████  | 12/15 [00:00<00:00, 15.93it/s]                                               {'loss': 0.715, 'grad_norm': 0.15671421587467194, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.93it/s]                                               {'loss': 0.7042, 'grad_norm': 0.14438395202159882, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.93it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.7152, 'grad_norm': 0.17350678145885468, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.7071, 'grad_norm': 0.19080191850662231, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.83it/s]                                               {'train_runtime': 1.0449, 'train_samples_per_second': 406.74, 'train_steps_per_second': 14.356, 'train_loss': 0.7134360392888387, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.83it/s]100%|██████████| 15/15 [00:01<00:00, 14.36it/s]
CLIENT:24
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7152, 'grad_norm': 0.16177602112293243, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.03it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.85it/s]                                              {'loss': 0.7128, 'grad_norm': 0.12125944346189499, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.85it/s]                                              {'loss': 0.7253, 'grad_norm': 0.2044260948896408, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.85it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.7321, 'grad_norm': 0.18419508635997772, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.7178, 'grad_norm': 0.13828906416893005, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.60it/s] 40%|████      | 6/15 [00:00<00:00, 15.61it/s]                                              {'loss': 0.6998, 'grad_norm': 0.17720071971416473, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.61it/s]                                              {'loss': 0.7045, 'grad_norm': 0.13827167451381683, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.61it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.7109, 'grad_norm': 0.18057414889335632, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.7002, 'grad_norm': 0.14887911081314087, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.64it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.7143, 'grad_norm': 0.1873284876346588, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.6965, 'grad_norm': 0.11309841275215149, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.23it/s] 80%|████████  | 12/15 [00:00<00:00, 15.71it/s]                                               {'loss': 0.7037, 'grad_norm': 0.22790245711803436, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.71it/s]                                               {'loss': 0.7189, 'grad_norm': 0.23126095533370972, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.71it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.706, 'grad_norm': 0.169838547706604, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.7016, 'grad_norm': 0.10446725785732269, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.70it/s]                                               {'train_runtime': 1.0586, 'train_samples_per_second': 401.482, 'train_steps_per_second': 14.17, 'train_loss': 0.7106366316477458, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.70it/s]100%|██████████| 15/15 [00:01<00:00, 14.17it/s]
CLIENT:98
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7319, 'grad_norm': 0.25503256916999817, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.11it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.42it/s]                                              {'loss': 0.7021, 'grad_norm': 0.10603468865156174, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.42it/s]                                              {'loss': 0.7226, 'grad_norm': 0.24318981170654297, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.42it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.7154, 'grad_norm': 0.22839736938476562, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.7063, 'grad_norm': 0.13461163640022278, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.57it/s] 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.715, 'grad_norm': 0.20641100406646729, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.6981, 'grad_norm': 0.11368421465158463, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.46it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.7087, 'grad_norm': 0.198369562625885, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.7229, 'grad_norm': 0.3225381076335907, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.51it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.7218, 'grad_norm': 0.20926600694656372, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.7045, 'grad_norm': 0.22234638035297394, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.19it/s] 80%|████████  | 12/15 [00:00<00:00, 15.80it/s]                                               {'loss': 0.7157, 'grad_norm': 0.22465084493160248, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.80it/s]                                               {'loss': 0.7059, 'grad_norm': 0.2104928344488144, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.80it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.7036, 'grad_norm': 0.22874398529529572, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.7072, 'grad_norm': 0.20485062897205353, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.76it/s]                                               {'train_runtime': 1.0621, 'train_samples_per_second': 400.146, 'train_steps_per_second': 14.123, 'train_loss': 0.7121135433514912, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.76it/s]100%|██████████| 15/15 [00:01<00:00, 14.13it/s]
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7177, 'grad_norm': 0.1339503973722458, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.19it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.47it/s]                                              {'loss': 0.7101, 'grad_norm': 0.12118437886238098, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.47it/s]                                              {'loss': 0.6641, 'grad_norm': 0.1287124752998352, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.47it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.7028, 'grad_norm': 0.05865459889173508, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.705, 'grad_norm': 0.09524018317461014, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.66it/s] 40%|████      | 6/15 [00:00<00:00, 15.55it/s]                                              {'loss': 0.722, 'grad_norm': 0.1485949456691742, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.55it/s]                                              {'loss': 0.71, 'grad_norm': 0.10490156710147858, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.55it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.69, 'grad_norm': 0.0709834024310112, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.7032, 'grad_norm': 0.14611344039440155, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.34it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.687, 'grad_norm': 0.06236996129155159, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.6919, 'grad_norm': 0.08801261335611343, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.87it/s] 80%|████████  | 12/15 [00:00<00:00, 15.45it/s]                                               {'loss': 0.6971, 'grad_norm': 0.15976503491401672, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.45it/s]                                               {'loss': 0.7084, 'grad_norm': 0.11243642121553421, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.45it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.6891, 'grad_norm': 0.058690253645181656, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.7236, 'grad_norm': 0.13168658316135406, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.63it/s]                                               {'train_runtime': 1.0703, 'train_samples_per_second': 397.103, 'train_steps_per_second': 14.015, 'train_loss': 0.7014705657958984, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.63it/s]100%|██████████| 15/15 [00:01<00:00, 14.02it/s]
CLIENT:95
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6916, 'grad_norm': 0.10494647920131683, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.36it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.18it/s]                                              {'loss': 0.7131, 'grad_norm': 0.19499599933624268, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.18it/s]                                              {'loss': 0.7186, 'grad_norm': 0.18879052996635437, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.18it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.7275, 'grad_norm': 0.2233833372592926, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.693, 'grad_norm': 0.11988323926925659, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.38it/s] 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.7001, 'grad_norm': 0.13646119832992554, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.7105, 'grad_norm': 0.16343559324741364, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.97it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.7138, 'grad_norm': 0.1700921505689621, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.6997, 'grad_norm': 0.16323897242546082, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.43it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.6906, 'grad_norm': 0.12471027672290802, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.6968, 'grad_norm': 0.21908451616764069, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.98it/s] 80%|████████  | 12/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.7085, 'grad_norm': 0.19976688921451569, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.6959, 'grad_norm': 0.11685671657323837, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.50it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.7027, 'grad_norm': 0.1355472058057785, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.7236, 'grad_norm': 0.36594781279563904, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.54it/s]                                               {'train_runtime': 1.061, 'train_samples_per_second': 400.569, 'train_steps_per_second': 14.138, 'train_loss': 0.7057505170504252, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.54it/s]100%|██████████| 15/15 [00:01<00:00, 14.14it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 38.14it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.77it/s] 36%|███▋      | 12/33 [00:00<00:00, 26.71it/s] 45%|████▌     | 15/33 [00:00<00:00, 25.91it/s] 55%|█████▍    | 18/33 [00:00<00:00, 25.41it/s] 64%|██████▎   | 21/33 [00:00<00:00, 25.14it/s] 73%|███████▎  | 24/33 [00:00<00:00, 25.13it/s] 82%|████████▏ | 27/33 [00:01<00:00, 25.17it/s] 91%|█████████ | 30/33 [00:01<00:00, 24.98it/s]100%|██████████| 33/33 [00:01<00:00, 26.19it/s]100%|██████████| 33/33 [00:01<00:00, 26.17it/s]
{'eval_loss': 0.7141581177711487, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.311601150527325, 'eval_runtime': 1.3023, 'eval_samples_per_second': 800.905, 'eval_steps_per_second': 25.34}
ROUND:8
CLIENT:2
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7118, 'grad_norm': 0.14798304438591003, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.93it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.44it/s]                                              {'loss': 0.712, 'grad_norm': 0.21046845614910126, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.44it/s]                                              {'loss': 0.7111, 'grad_norm': 0.1886173039674759, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.44it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.7205, 'grad_norm': 0.23646417260169983, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.7126, 'grad_norm': 0.1326276659965515, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.52it/s] 40%|████      | 6/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.7, 'grad_norm': 0.12056390196084976, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.7079, 'grad_norm': 0.15501496195793152, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.89it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.7113, 'grad_norm': 0.15990334749221802, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.706, 'grad_norm': 0.2497147023677826, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.59it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.7197, 'grad_norm': 0.2816278040409088, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.7062, 'grad_norm': 0.12433130294084549, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.14it/s] 80%|████████  | 12/15 [00:00<00:00, 15.58it/s]                                               {'loss': 0.6991, 'grad_norm': 0.12292308360338211, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.58it/s]                                               {'loss': 0.7074, 'grad_norm': 0.18704421818256378, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.58it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.707, 'grad_norm': 0.18713229894638062, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.7124, 'grad_norm': 0.20853765308856964, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.57it/s]                                               {'train_runtime': 1.0582, 'train_samples_per_second': 401.627, 'train_steps_per_second': 14.175, 'train_loss': 0.7096704721450806, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.57it/s]100%|██████████| 15/15 [00:01<00:00, 14.18it/s]
CLIENT:49
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7264, 'grad_norm': 0.19892309606075287, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.02it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.83it/s]                                              {'loss': 0.7152, 'grad_norm': 0.14255867898464203, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.83it/s]                                              {'loss': 0.7353, 'grad_norm': 0.29499751329421997, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.83it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.7171, 'grad_norm': 0.20434626936912537, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.7098, 'grad_norm': 0.18131178617477417, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.55it/s] 40%|████      | 6/15 [00:00<00:00, 15.65it/s]                                              {'loss': 0.7209, 'grad_norm': 0.22706902027130127, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.65it/s]                                              {'loss': 0.719, 'grad_norm': 0.26002979278564453, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.65it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.7133, 'grad_norm': 0.20424571633338928, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.702, 'grad_norm': 0.17195941507816315, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.50it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.7079, 'grad_norm': 0.3088158071041107, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.6964, 'grad_norm': 0.17963656783103943, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.20it/s] 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.6909, 'grad_norm': 0.1525527983903885, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.715, 'grad_norm': 0.21230733394622803, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.57it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6983, 'grad_norm': 0.2754780352115631, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6954, 'grad_norm': 0.2523798644542694, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.55it/s]                                               {'train_runtime': 1.0658, 'train_samples_per_second': 398.773, 'train_steps_per_second': 14.074, 'train_loss': 0.7108689347902933, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.55it/s]100%|██████████| 15/15 [00:01<00:00, 14.08it/s]
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7044, 'grad_norm': 0.13849033415317535, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.14it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.47it/s]                                              {'loss': 0.711, 'grad_norm': 0.13059070706367493, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.47it/s]                                              {'loss': 0.7401, 'grad_norm': 0.3228793442249298, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.47it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.7319, 'grad_norm': 0.24695704877376556, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.7061, 'grad_norm': 0.1937781572341919, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.59it/s] 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.7013, 'grad_norm': 0.1432269960641861, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.6897, 'grad_norm': 0.15460528433322906, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.48it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.7167, 'grad_norm': 0.22048881649971008, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.7077, 'grad_norm': 0.2514963746070862, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.31it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.7155, 'grad_norm': 0.19189931452274323, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6832, 'grad_norm': 0.18418747186660767, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.82it/s] 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7184, 'grad_norm': 0.23773597180843353, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7063, 'grad_norm': 0.18152569234371185, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.38it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.7028, 'grad_norm': 0.2513650953769684, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.7006, 'grad_norm': 0.19219186902046204, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.05it/s]                                               {'train_runtime': 1.0528, 'train_samples_per_second': 403.682, 'train_steps_per_second': 14.248, 'train_loss': 0.7090326110521953, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.05it/s]100%|██████████| 15/15 [00:01<00:00, 14.25it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7103, 'grad_norm': 0.1383485198020935, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.25it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.05it/s]                                              {'loss': 0.7188, 'grad_norm': 0.21428997814655304, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.05it/s]                                              {'loss': 0.7127, 'grad_norm': 0.22799357771873474, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.05it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.65it/s]                                              {'loss': 0.7115, 'grad_norm': 0.11708147823810577, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.65it/s]                                              {'loss': 0.7375, 'grad_norm': 0.332379013299942, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.65it/s] 40%|████      | 6/15 [00:00<00:00, 15.72it/s]                                              {'loss': 0.7091, 'grad_norm': 0.15797185897827148, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.72it/s]                                              {'loss': 0.7104, 'grad_norm': 0.2615174651145935, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.72it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.7091, 'grad_norm': 0.17277266085147858, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.7055, 'grad_norm': 0.1886959671974182, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.64it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.7265, 'grad_norm': 0.21813589334487915, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.6937, 'grad_norm': 0.23886501789093018, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.08it/s] 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.7036, 'grad_norm': 0.20055414736270905, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.709, 'grad_norm': 0.22892151772975922, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.54it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.7115, 'grad_norm': 0.25076305866241455, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.6942, 'grad_norm': 0.23181048035621643, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.58it/s]                                               {'train_runtime': 1.0621, 'train_samples_per_second': 400.154, 'train_steps_per_second': 14.123, 'train_loss': 0.7108954350153606, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.58it/s]100%|██████████| 15/15 [00:01<00:00, 14.13it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7066, 'grad_norm': 0.14377517998218536, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.72it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.92it/s]                                              {'loss': 0.712, 'grad_norm': 0.2432347685098648, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.92it/s]                                              {'loss': 0.7279, 'grad_norm': 0.29374292492866516, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.92it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.71, 'grad_norm': 0.15113312005996704, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.7265, 'grad_norm': 0.29203182458877563, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.63it/s] 40%|████      | 6/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.7188, 'grad_norm': 0.21796560287475586, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.7314, 'grad_norm': 0.2877139449119568, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.42it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.7003, 'grad_norm': 0.1884840875864029, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.6999, 'grad_norm': 0.22162675857543945, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.85it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.7184, 'grad_norm': 0.24196089804172516, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6992, 'grad_norm': 0.23609653115272522, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.92it/s] 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6987, 'grad_norm': 0.25444087386131287, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6971, 'grad_norm': 0.1775488257408142, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.41it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.7026, 'grad_norm': 0.30001169443130493, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.7115, 'grad_norm': 0.35408422350883484, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.48it/s]                                               {'train_runtime': 1.068, 'train_samples_per_second': 397.947, 'train_steps_per_second': 14.045, 'train_loss': 0.7107242027918498, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]100%|██████████| 15/15 [00:01<00:00, 14.05it/s]
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7246, 'grad_norm': 0.21001355350017548, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.85it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.34it/s]                                              {'loss': 0.7238, 'grad_norm': 0.19807109236717224, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.34it/s]                                              {'loss': 0.7235, 'grad_norm': 0.1762460321187973, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.34it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.7103, 'grad_norm': 0.17619316279888153, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.7222, 'grad_norm': 0.22065608203411102, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.66it/s] 40%|████      | 6/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.7208, 'grad_norm': 0.19900265336036682, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.7092, 'grad_norm': 0.13709072768688202, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.63it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7074, 'grad_norm': 0.2636956572532654, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7138, 'grad_norm': 0.24582771956920624, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.25it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.7048, 'grad_norm': 0.17973224818706512, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.7153, 'grad_norm': 0.22183367609977722, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.70it/s] 80%|████████  | 12/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.7149, 'grad_norm': 0.30077534914016724, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.7061, 'grad_norm': 0.26497185230255127, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.28it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.7003, 'grad_norm': 0.1908784955739975, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.719, 'grad_norm': 0.2240305244922638, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.74it/s]                                               {'train_runtime': 1.0669, 'train_samples_per_second': 398.364, 'train_steps_per_second': 14.06, 'train_loss': 0.7143967191378275, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.74it/s]100%|██████████| 15/15 [00:01<00:00, 14.06it/s]
CLIENT:87
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7154, 'grad_norm': 0.1274591088294983, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.91it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.7138, 'grad_norm': 0.17861580848693848, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.6969, 'grad_norm': 0.08180703222751617, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.16it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.6985, 'grad_norm': 0.09810806810855865, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.7138, 'grad_norm': 0.10120607912540436, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.38it/s] 40%|████      | 6/15 [00:00<00:00, 15.86it/s]                                              {'loss': 0.7078, 'grad_norm': 0.18368655443191528, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.86it/s]                                              {'loss': 0.7128, 'grad_norm': 0.07507914304733276, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.86it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.7162, 'grad_norm': 0.17474861443042755, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.7002, 'grad_norm': 0.12389892339706421, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.66it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.7086, 'grad_norm': 0.1509048491716385, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.6994, 'grad_norm': 0.06803887337446213, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.18it/s] 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.6999, 'grad_norm': 0.17450924217700958, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.7044, 'grad_norm': 0.0738130733370781, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.62it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.6942, 'grad_norm': 0.07497159391641617, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.727, 'grad_norm': 0.37317851185798645, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.85it/s]                                               {'train_runtime': 1.0478, 'train_samples_per_second': 405.595, 'train_steps_per_second': 14.315, 'train_loss': 0.7072700063387553, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.85it/s]100%|██████████| 15/15 [00:01<00:00, 14.32it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6945, 'grad_norm': 0.1668172925710678, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.41it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.10it/s]                                              {'loss': 0.7304, 'grad_norm': 0.26351311802864075, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.10it/s]                                              {'loss': 0.6883, 'grad_norm': 0.07231534272432327, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.10it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.82it/s]                                              {'loss': 0.6924, 'grad_norm': 0.14894267916679382, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.82it/s]                                              {'loss': 0.7144, 'grad_norm': 0.19072706997394562, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.82it/s] 40%|████      | 6/15 [00:00<00:00, 15.55it/s]                                              {'loss': 0.7184, 'grad_norm': 0.2315751165151596, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.55it/s]                                              {'loss': 0.7108, 'grad_norm': 0.2235403209924698, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.55it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.7037, 'grad_norm': 0.19739139080047607, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.6996, 'grad_norm': 0.12639304995536804, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.30it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.7116, 'grad_norm': 0.15766681730747223, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.6995, 'grad_norm': 0.22024083137512207, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.87it/s] 80%|████████  | 12/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.7094, 'grad_norm': 0.20547443628311157, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.6947, 'grad_norm': 0.18354269862174988, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.52it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.7132, 'grad_norm': 0.24206897616386414, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.689, 'grad_norm': 0.18952718377113342, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.78it/s]                                               {'train_runtime': 1.0634, 'train_samples_per_second': 399.672, 'train_steps_per_second': 14.106, 'train_loss': 0.7046539386113485, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.78it/s]100%|██████████| 15/15 [00:01<00:00, 14.11it/s]
CLIENT:99
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/25 [00:00<?, ?it/s]                                      {'loss': 0.7141, 'grad_norm': 0.17850780487060547, 'learning_rate': 0.01, 'epoch': 0.2}
  4%|▍         | 1/25 [00:00<00:01, 12.45it/s]  8%|▊         | 2/25 [00:00<00:01, 12.97it/s]                                              {'loss': 0.7119, 'grad_norm': 0.24107371270656586, 'learning_rate': 0.0096, 'epoch': 0.4}
  8%|▊         | 2/25 [00:00<00:01, 12.97it/s]                                              {'loss': 0.7262, 'grad_norm': 0.23837798833847046, 'learning_rate': 0.0092, 'epoch': 0.6}
 12%|█▏        | 3/25 [00:00<00:01, 12.97it/s] 16%|█▌        | 4/25 [00:00<00:01, 13.53it/s]                                              {'loss': 0.7251, 'grad_norm': 0.26152539253234863, 'learning_rate': 0.0088, 'epoch': 0.8}
 16%|█▌        | 4/25 [00:00<00:01, 13.53it/s]                                              {'loss': 0.6948, 'grad_norm': 0.1287640929222107, 'learning_rate': 0.0084, 'epoch': 1.0}
 20%|██        | 5/25 [00:00<00:01, 13.53it/s]                                              {'loss': 0.7123, 'grad_norm': 0.20464058220386505, 'learning_rate': 0.008, 'epoch': 1.2}
 24%|██▍       | 6/25 [00:00<00:01, 13.53it/s] 28%|██▊       | 7/25 [00:00<00:01, 15.31it/s]                                              {'loss': 0.7077, 'grad_norm': 0.20776498317718506, 'learning_rate': 0.0076, 'epoch': 1.4}
 28%|██▊       | 7/25 [00:00<00:01, 15.31it/s]                                              {'loss': 0.7067, 'grad_norm': 0.25170421600341797, 'learning_rate': 0.0072, 'epoch': 1.6}
 32%|███▏      | 8/25 [00:00<00:01, 15.31it/s] 36%|███▌      | 9/25 [00:00<00:01, 14.50it/s]                                              {'loss': 0.7245, 'grad_norm': 0.2597583830356598, 'learning_rate': 0.0068000000000000005, 'epoch': 1.8}
 36%|███▌      | 9/25 [00:00<00:01, 14.50it/s]                                              {'loss': 0.7053, 'grad_norm': 0.3776932656764984, 'learning_rate': 0.0064, 'epoch': 2.0}
 40%|████      | 10/25 [00:00<00:01, 14.50it/s]                                               {'loss': 0.7007, 'grad_norm': 0.2645956575870514, 'learning_rate': 0.006, 'epoch': 2.2}
 44%|████▍     | 11/25 [00:00<00:00, 14.50it/s] 48%|████▊     | 12/25 [00:00<00:00, 15.92it/s]                                               {'loss': 0.7034, 'grad_norm': 0.10701818764209747, 'learning_rate': 0.005600000000000001, 'epoch': 2.4}
 48%|████▊     | 12/25 [00:00<00:00, 15.92it/s]                                               {'loss': 0.6972, 'grad_norm': 0.27484381198883057, 'learning_rate': 0.005200000000000001, 'epoch': 2.6}
 52%|█████▏    | 13/25 [00:00<00:00, 15.92it/s] 56%|█████▌    | 14/25 [00:00<00:00, 15.18it/s]                                               {'loss': 0.7131, 'grad_norm': 0.3591133952140808, 'learning_rate': 0.0048, 'epoch': 2.8}
 56%|█████▌    | 14/25 [00:00<00:00, 15.18it/s]                                               {'loss': 0.7122, 'grad_norm': 0.5303633809089661, 'learning_rate': 0.0044, 'epoch': 3.0}
 60%|██████    | 15/25 [00:00<00:00, 15.18it/s]                                               {'loss': 0.6906, 'grad_norm': 0.26844197511672974, 'learning_rate': 0.004, 'epoch': 3.2}
 64%|██████▍   | 16/25 [00:01<00:00, 15.18it/s] 68%|██████▊   | 17/25 [00:01<00:00, 15.88it/s]                                               {'loss': 0.6885, 'grad_norm': 0.31585606932640076, 'learning_rate': 0.0036, 'epoch': 3.4}
 68%|██████▊   | 17/25 [00:01<00:00, 15.88it/s]                                               {'loss': 0.6884, 'grad_norm': 0.36144182085990906, 'learning_rate': 0.0032, 'epoch': 3.6}
 72%|███████▏  | 18/25 [00:01<00:00, 15.88it/s] 76%|███████▌  | 19/25 [00:01<00:00, 15.14it/s]                                               {'loss': 0.6883, 'grad_norm': 0.3695777356624603, 'learning_rate': 0.0028000000000000004, 'epoch': 3.8}
 76%|███████▌  | 19/25 [00:01<00:00, 15.14it/s]                                               {'loss': 0.6911, 'grad_norm': 0.22795282304286957, 'learning_rate': 0.0024, 'epoch': 4.0}
 80%|████████  | 20/25 [00:01<00:00, 15.14it/s]                                               {'loss': 0.7048, 'grad_norm': 0.18311607837677002, 'learning_rate': 0.002, 'epoch': 4.2}
 84%|████████▍ | 21/25 [00:01<00:00, 15.14it/s] 88%|████████▊ | 22/25 [00:01<00:00, 15.95it/s]                                               {'loss': 0.6786, 'grad_norm': 0.3426787853240967, 'learning_rate': 0.0016, 'epoch': 4.4}
 88%|████████▊ | 22/25 [00:01<00:00, 15.95it/s]                                               {'loss': 0.6744, 'grad_norm': 0.44403740763664246, 'learning_rate': 0.0012, 'epoch': 4.6}
 92%|█████████▏| 23/25 [00:01<00:00, 15.95it/s] 96%|█████████▌| 24/25 [00:01<00:00, 15.21it/s]                                               {'loss': 0.6755, 'grad_norm': 0.40422943234443665, 'learning_rate': 0.0008, 'epoch': 4.8}
 96%|█████████▌| 24/25 [00:01<00:00, 15.21it/s]                                               {'loss': 0.6653, 'grad_norm': 0.40511655807495117, 'learning_rate': 0.0004, 'epoch': 5.0}
100%|██████████| 25/25 [00:01<00:00, 15.21it/s]                                               {'train_runtime': 1.6707, 'train_samples_per_second': 407.012, 'train_steps_per_second': 14.964, 'train_loss': 0.7000215363502502, 'epoch': 5.0}
100%|██████████| 25/25 [00:01<00:00, 15.21it/s]100%|██████████| 25/25 [00:01<00:00, 14.97it/s]
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.709, 'grad_norm': 0.18531498312950134, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.90it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.21it/s]                                              {'loss': 0.7095, 'grad_norm': 0.10713926702737808, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.21it/s]                                              {'loss': 0.7282, 'grad_norm': 0.2909902334213257, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.21it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.718, 'grad_norm': 0.21045561134815216, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.6989, 'grad_norm': 0.13417832553386688, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.68it/s] 40%|████      | 6/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.6916, 'grad_norm': 0.14721223711967468, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.6944, 'grad_norm': 0.1428428292274475, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.56it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.7115, 'grad_norm': 0.23789162933826447, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.7061, 'grad_norm': 0.1571284979581833, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.51it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.6992, 'grad_norm': 0.16655781865119934, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.6964, 'grad_norm': 0.19117647409439087, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.18it/s] 80%|████████  | 12/15 [00:00<00:00, 15.83it/s]                                               {'loss': 0.7076, 'grad_norm': 0.23450623452663422, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.83it/s]                                               {'loss': 0.7051, 'grad_norm': 0.21868924796581268, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.83it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.7086, 'grad_norm': 0.17966870963573456, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.7045, 'grad_norm': 0.2081661969423294, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.76it/s]                                               {'train_runtime': 1.0561, 'train_samples_per_second': 402.418, 'train_steps_per_second': 14.203, 'train_loss': 0.7059077183405559, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.76it/s]100%|██████████| 15/15 [00:01<00:00, 14.21it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 36.07it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.70it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.76it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.85it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.31it/s] 61%|██████    | 20/33 [00:00<00:00, 25.04it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.00it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.04it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.02it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.15it/s]100%|██████████| 33/33 [00:01<00:00, 26.02it/s]
{'eval_loss': 0.7103062868118286, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3096836049856184, 'eval_runtime': 1.3095, 'eval_samples_per_second': 796.506, 'eval_steps_per_second': 25.201}
ROUND:9
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7128, 'grad_norm': 0.2100832462310791, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.93it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.66it/s]                                              {'loss': 0.6953, 'grad_norm': 0.118273064494133, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.66it/s]                                              {'loss': 0.7226, 'grad_norm': 0.21030043065547943, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.66it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.7057, 'grad_norm': 0.07923949509859085, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.7117, 'grad_norm': 0.27682992815971375, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.28it/s] 40%|████      | 6/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.6934, 'grad_norm': 0.1982196867465973, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.7094, 'grad_norm': 0.14064519107341766, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.26it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.7017, 'grad_norm': 0.22888930141925812, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.7036, 'grad_norm': 0.18849767744541168, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.20it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.7092, 'grad_norm': 0.1201997920870781, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.7071, 'grad_norm': 0.23009079694747925, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.83it/s] 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.7103, 'grad_norm': 0.2684037685394287, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.712, 'grad_norm': 0.3360876441001892, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.37it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.699, 'grad_norm': 0.12204994261264801, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6933, 'grad_norm': 0.09116672724485397, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]                                               {'train_runtime': 1.0818, 'train_samples_per_second': 392.855, 'train_steps_per_second': 13.865, 'train_loss': 0.7057894746462504, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 13.87it/s]
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7055, 'grad_norm': 0.22561430931091309, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.17it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7052, 'grad_norm': 0.21339263021945953, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7108, 'grad_norm': 0.23076239228248596, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.25it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.7219, 'grad_norm': 0.2798278331756592, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.6937, 'grad_norm': 0.14173118770122528, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.38it/s] 40%|████      | 6/15 [00:00<00:00, 15.82it/s]                                              {'loss': 0.6986, 'grad_norm': 0.2571184039115906, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.82it/s]                                              {'loss': 0.7011, 'grad_norm': 0.2530302107334137, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.82it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.6999, 'grad_norm': 0.3167935907840729, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.7023, 'grad_norm': 0.11803591996431351, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.43it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.705, 'grad_norm': 0.21080052852630615, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.691, 'grad_norm': 0.1793029010295868, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.94it/s] 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.7028, 'grad_norm': 0.3817919194698334, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.6958, 'grad_norm': 0.22804297506809235, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.59it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.7027, 'grad_norm': 0.2698344588279724, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.6982, 'grad_norm': 0.3388862609863281, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.60it/s]                                               {'train_runtime': 1.0601, 'train_samples_per_second': 400.894, 'train_steps_per_second': 14.149, 'train_loss': 0.7023013393084209, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.60it/s]100%|██████████| 15/15 [00:01<00:00, 14.15it/s]
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7036, 'grad_norm': 0.14469128847122192, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.18it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.51it/s]                                              {'loss': 0.6952, 'grad_norm': 0.15632762014865875, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.51it/s]                                              {'loss': 0.7191, 'grad_norm': 0.22390595078468323, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.51it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.6941, 'grad_norm': 0.12551461160182953, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.7051, 'grad_norm': 0.22295036911964417, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.52it/s] 40%|████      | 6/15 [00:00<00:00, 16.47it/s]                                              {'loss': 0.7058, 'grad_norm': 0.1678590029478073, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.47it/s]                                              {'loss': 0.7023, 'grad_norm': 0.24403037130832672, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.47it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.7033, 'grad_norm': 0.11806079000234604, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.7048, 'grad_norm': 0.1528017669916153, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.58it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.7015, 'grad_norm': 0.18247124552726746, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.6883, 'grad_norm': 0.15669362246990204, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.08it/s] 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.7065, 'grad_norm': 0.22517934441566467, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.701, 'grad_norm': 0.10613106936216354, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.51it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.6968, 'grad_norm': 0.2773956060409546, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.6751, 'grad_norm': 0.1920628845691681, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.49it/s]                                               {'train_runtime': 1.0544, 'train_samples_per_second': 403.091, 'train_steps_per_second': 14.227, 'train_loss': 0.700162144502004, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.49it/s]100%|██████████| 15/15 [00:01<00:00, 14.23it/s]
CLIENT:5
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.712, 'grad_norm': 0.23958273231983185, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.52it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.7124, 'grad_norm': 0.15163348615169525, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.6952, 'grad_norm': 0.09134640544652939, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.97it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.6931, 'grad_norm': 0.08385215699672699, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.7194, 'grad_norm': 0.2651793658733368, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.74it/s] 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.6975, 'grad_norm': 0.17004455626010895, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.6912, 'grad_norm': 0.17784620821475983, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.45it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.6954, 'grad_norm': 0.12781022489070892, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.6993, 'grad_norm': 0.2663012444972992, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.77it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.7056, 'grad_norm': 0.13927805423736572, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.6913, 'grad_norm': 0.19424475729465485, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.08it/s] 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.6908, 'grad_norm': 0.21911552548408508, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.7004, 'grad_norm': 0.15465368330478668, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.53it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.7067, 'grad_norm': 0.19130219519138336, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.7121, 'grad_norm': 0.25490668416023254, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.48it/s]                                               {'train_runtime': 1.0683, 'train_samples_per_second': 397.817, 'train_steps_per_second': 14.041, 'train_loss': 0.7014956792195638, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]100%|██████████| 15/15 [00:01<00:00, 14.05it/s]
CLIENT:52
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7133, 'grad_norm': 0.2051398754119873, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.90it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.29it/s]                                              {'loss': 0.712, 'grad_norm': 0.16957786679267883, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.29it/s]                                              {'loss': 0.6777, 'grad_norm': 0.10891162604093552, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.29it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.6932, 'grad_norm': 0.11940781772136688, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.7103, 'grad_norm': 0.1898806244134903, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.43it/s] 40%|████      | 6/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.7107, 'grad_norm': 0.1910741925239563, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.7055, 'grad_norm': 0.20462916791439056, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.34it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.7088, 'grad_norm': 0.20815904438495636, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.6969, 'grad_norm': 0.08785109221935272, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.29it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.696, 'grad_norm': 0.13636969029903412, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.6967, 'grad_norm': 0.1884356290102005, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.86it/s] 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.695, 'grad_norm': 0.23288920521736145, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6977, 'grad_norm': 0.09954090416431427, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.41it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.6969, 'grad_norm': 0.21744678914546967, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.695, 'grad_norm': 0.32114389538764954, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.80it/s]                                               {'train_runtime': 1.0648, 'train_samples_per_second': 399.148, 'train_steps_per_second': 14.088, 'train_loss': 0.7003819068272908, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.80it/s]100%|██████████| 15/15 [00:01<00:00, 14.09it/s]
CLIENT:41
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7045, 'grad_norm': 0.188621386885643, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.84it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.63it/s]                                              {'loss': 0.6966, 'grad_norm': 0.07230722159147263, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.63it/s]                                              {'loss': 0.7046, 'grad_norm': 0.11544230580329895, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.63it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.7057, 'grad_norm': 0.07969073206186295, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.7195, 'grad_norm': 0.23252011835575104, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.14it/s] 40%|████      | 6/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.6951, 'grad_norm': 0.1270056515932083, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.713, 'grad_norm': 0.22262106835842133, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.15it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.7005, 'grad_norm': 0.07307186722755432, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.6971, 'grad_norm': 0.08897461742162704, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.15it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.6974, 'grad_norm': 0.0942593514919281, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.6954, 'grad_norm': 0.11657867580652237, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.78it/s] 80%|████████  | 12/15 [00:00<00:00, 15.27it/s]                                               {'loss': 0.696, 'grad_norm': 0.168485626578331, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.27it/s]                                               {'loss': 0.7034, 'grad_norm': 0.1516018807888031, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.27it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.7045, 'grad_norm': 0.1561739593744278, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.7011, 'grad_norm': 0.08951807022094727, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.34it/s]                                               {'train_runtime': 1.0848, 'train_samples_per_second': 391.76, 'train_steps_per_second': 13.827, 'train_loss': 0.7022830764452617, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.34it/s]100%|██████████| 15/15 [00:01<00:00, 13.83it/s]
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7104, 'grad_norm': 0.2643888592720032, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.98it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.68it/s]                                              {'loss': 0.7156, 'grad_norm': 0.25677597522735596, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.68it/s]                                              {'loss': 0.7048, 'grad_norm': 0.11538027971982956, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.68it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.7143, 'grad_norm': 0.23775066435337067, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.715, 'grad_norm': 0.2355596274137497, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.24it/s] 40%|████      | 6/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.7022, 'grad_norm': 0.19802995026111603, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.7013, 'grad_norm': 0.23692619800567627, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.21it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.6923, 'grad_norm': 0.24305285513401031, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.6999, 'grad_norm': 0.23337693512439728, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.15it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.6971, 'grad_norm': 0.19845502078533173, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.6846, 'grad_norm': 0.20521368086338043, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.80it/s] 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.7075, 'grad_norm': 0.39043405652046204, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6995, 'grad_norm': 0.2718679904937744, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.31it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6933, 'grad_norm': 0.25418621301651, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.7029, 'grad_norm': 0.278287410736084, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]                                               {'train_runtime': 1.0861, 'train_samples_per_second': 391.306, 'train_steps_per_second': 13.811, 'train_loss': 0.702720332145691, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 13.82it/s]
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7129, 'grad_norm': 0.16268585622310638, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.44it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.7058, 'grad_norm': 0.14783279597759247, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.6688, 'grad_norm': 0.16391602158546448, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.04it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.7025, 'grad_norm': 0.06989599019289017, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.702, 'grad_norm': 0.1095263883471489, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.34it/s] 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.7169, 'grad_norm': 0.18163982033729553, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.7066, 'grad_norm': 0.12619416415691376, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.29it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.98it/s]                                              {'loss': 0.6881, 'grad_norm': 0.0836690291762352, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.98it/s]                                              {'loss': 0.698, 'grad_norm': 0.17786456644535065, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.98it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6865, 'grad_norm': 0.07384225726127625, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6891, 'grad_norm': 0.10392322391271591, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.43it/s] 80%|████████  | 12/15 [00:00<00:00, 15.80it/s]                                               {'loss': 0.691, 'grad_norm': 0.19245103001594543, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.80it/s]                                               {'loss': 0.7042, 'grad_norm': 0.1331275850534439, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.80it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.6887, 'grad_norm': 0.076031893491745, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.7184, 'grad_norm': 0.16363222897052765, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.67it/s]                                               {'train_runtime': 1.0594, 'train_samples_per_second': 401.172, 'train_steps_per_second': 14.159, 'train_loss': 0.6986284057299296, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.67it/s]100%|██████████| 15/15 [00:01<00:00, 14.16it/s]
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.722, 'grad_norm': 0.2785148024559021, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.98it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.83it/s]                                              {'loss': 0.7264, 'grad_norm': 0.26619142293930054, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.83it/s]                                              {'loss': 0.7182, 'grad_norm': 0.17330993711948395, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.83it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.7086, 'grad_norm': 0.22741618752479553, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.7237, 'grad_norm': 0.2244117707014084, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.17it/s] 40%|████      | 6/15 [00:00<00:00, 16.41it/s]                                              {'loss': 0.7096, 'grad_norm': 0.3859756588935852, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.41it/s]                                              {'loss': 0.714, 'grad_norm': 0.24178563058376312, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.41it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.73it/s]                                              {'loss': 0.7022, 'grad_norm': 0.23859906196594238, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.73it/s]                                              {'loss': 0.7042, 'grad_norm': 0.395388126373291, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.73it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.7064, 'grad_norm': 0.2964636981487274, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.6969, 'grad_norm': 0.3389674723148346, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.96it/s] 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.7019, 'grad_norm': 0.22092516720294952, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.7, 'grad_norm': 0.3665867745876312, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.48it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.7008, 'grad_norm': 0.3171744644641876, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.7027, 'grad_norm': 0.2538585662841797, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.51it/s]                                               {'train_runtime': 1.0616, 'train_samples_per_second': 400.33, 'train_steps_per_second': 14.129, 'train_loss': 0.709176500638326, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.51it/s]100%|██████████| 15/15 [00:01<00:00, 14.13it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7275, 'grad_norm': 0.25614604353904724, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.99it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.7171, 'grad_norm': 0.12314821034669876, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6825, 'grad_norm': 0.10537488758563995, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.23it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.7029, 'grad_norm': 0.13837620615959167, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.7122, 'grad_norm': 0.10961543768644333, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.11it/s] 40%|████      | 6/15 [00:00<00:00, 16.04it/s]                                              {'loss': 0.7054, 'grad_norm': 0.16076208651065826, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.04it/s]                                              {'loss': 0.7046, 'grad_norm': 0.16249918937683105, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.04it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7045, 'grad_norm': 0.12426608055830002, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7087, 'grad_norm': 0.1514960080385208, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.61it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.701, 'grad_norm': 0.08690774440765381, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.6978, 'grad_norm': 0.13802127540111542, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.12it/s] 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.7011, 'grad_norm': 0.25566014647483826, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.7077, 'grad_norm': 0.1925027072429657, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.59it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.7017, 'grad_norm': 0.12941211462020874, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.7026, 'grad_norm': 0.13083232939243317, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.59it/s]                                               {'train_runtime': 1.0552, 'train_samples_per_second': 402.77, 'train_steps_per_second': 14.215, 'train_loss': 0.7051397879918416, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.59it/s]100%|██████████| 15/15 [00:01<00:00, 14.22it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 35.32it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.51it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.76it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.85it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.39it/s] 61%|██████    | 20/33 [00:00<00:00, 25.36it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.21it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.06it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.21it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.14it/s]100%|██████████| 33/33 [00:01<00:00, 26.07it/s]
{'eval_loss': 0.7071211338043213, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3144774688398849, 'eval_runtime': 1.307, 'eval_samples_per_second': 798.025, 'eval_steps_per_second': 25.249}
ROUND:10
CLIENT:26
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7191, 'grad_norm': 0.3008264899253845, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.13it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.6995, 'grad_norm': 0.22547875344753265, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.6993, 'grad_norm': 0.09968762844800949, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.97it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.7122, 'grad_norm': 0.19617952406406403, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.705, 'grad_norm': 0.17674635350704193, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.26it/s] 40%|████      | 6/15 [00:00<00:00, 16.44it/s]                                              {'loss': 0.6933, 'grad_norm': 0.3236847519874573, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.44it/s]                                              {'loss': 0.6973, 'grad_norm': 0.30106931924819946, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.44it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.6979, 'grad_norm': 0.1444728523492813, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.6932, 'grad_norm': 0.2668086588382721, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.90it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.6921, 'grad_norm': 0.20265688002109528, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.6963, 'grad_norm': 0.22072762250900269, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.33it/s] 80%|████████  | 12/15 [00:00<00:00, 15.70it/s]                                               {'loss': 0.7079, 'grad_norm': 0.35443851351737976, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.70it/s]                                               {'loss': 0.6926, 'grad_norm': 0.23862850666046143, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.70it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.7013, 'grad_norm': 0.2732173800468445, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.7033, 'grad_norm': 0.2207961231470108, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.67it/s]                                               {'train_runtime': 1.0497, 'train_samples_per_second': 404.878, 'train_steps_per_second': 14.29, 'train_loss': 0.7006940126419068, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.67it/s]100%|██████████| 15/15 [00:01<00:00, 14.29it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7049, 'grad_norm': 0.16514267027378082, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.16it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7098, 'grad_norm': 0.2657981514930725, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7032, 'grad_norm': 0.2771665155887604, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.25it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.707, 'grad_norm': 0.13968755304813385, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.7215, 'grad_norm': 0.4129987359046936, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.40it/s] 40%|████      | 6/15 [00:00<00:00, 15.85it/s]                                              {'loss': 0.7021, 'grad_norm': 0.18964941799640656, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.85it/s]                                              {'loss': 0.6966, 'grad_norm': 0.328547865152359, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.85it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.46it/s]                                              {'loss': 0.7002, 'grad_norm': 0.20801962912082672, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.46it/s]                                              {'loss': 0.6958, 'grad_norm': 0.23061507940292358, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.46it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7138, 'grad_norm': 0.2697990834712982, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.6796, 'grad_norm': 0.2944939136505127, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.12it/s] 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.6925, 'grad_norm': 0.2417040914297104, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.6949, 'grad_norm': 0.278534471988678, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.51it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.6959, 'grad_norm': 0.30746012926101685, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.6798, 'grad_norm': 0.2855362594127655, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.51it/s]                                               {'train_runtime': 1.0584, 'train_samples_per_second': 401.533, 'train_steps_per_second': 14.172, 'train_loss': 0.69984343846639, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.51it/s]100%|██████████| 15/15 [00:01<00:00, 14.18it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7013, 'grad_norm': 0.17109999060630798, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.47it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.7016, 'grad_norm': 0.30141177773475647, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.7146, 'grad_norm': 0.3653426170349121, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.04it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.7035, 'grad_norm': 0.18095697462558746, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.7123, 'grad_norm': 0.3582986295223236, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.28it/s] 40%|████      | 6/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.7079, 'grad_norm': 0.2666146755218506, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.7158, 'grad_norm': 0.3578958213329315, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.19it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.87it/s]                                              {'loss': 0.6902, 'grad_norm': 0.2306307703256607, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.87it/s]                                              {'loss': 0.6878, 'grad_norm': 0.2805701792240143, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.87it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.7037, 'grad_norm': 0.2977176904678345, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6845, 'grad_norm': 0.2935543656349182, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.41it/s] 80%|████████  | 12/15 [00:00<00:00, 15.71it/s]                                               {'loss': 0.6826, 'grad_norm': 0.3194389045238495, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.71it/s]                                               {'loss': 0.6863, 'grad_norm': 0.20744164288043976, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.71it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.6824, 'grad_norm': 0.3736891746520996, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.6872, 'grad_norm': 0.4450877010822296, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.58it/s]                                               {'train_runtime': 1.0626, 'train_samples_per_second': 399.97, 'train_steps_per_second': 14.117, 'train_loss': 0.6974510431289673, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.58it/s]100%|██████████| 15/15 [00:01<00:00, 14.12it/s]
CLIENT:86
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7079, 'grad_norm': 0.2573893368244171, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.10it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.42it/s]                                              {'loss': 0.6976, 'grad_norm': 0.1560431867837906, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.42it/s]                                              {'loss': 0.7271, 'grad_norm': 0.19777338206768036, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.42it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.7113, 'grad_norm': 0.2161773145198822, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.7105, 'grad_norm': 0.1745053231716156, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.36it/s] 40%|████      | 6/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.6882, 'grad_norm': 0.22263272106647491, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.7118, 'grad_norm': 0.19153009355068207, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.23it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.47it/s]                                              {'loss': 0.6912, 'grad_norm': 0.13835343718528748, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.47it/s]                                              {'loss': 0.6982, 'grad_norm': 0.35978394746780396, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.47it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.6972, 'grad_norm': 0.24238015711307526, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.6854, 'grad_norm': 0.2534010708332062, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.80it/s] 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6892, 'grad_norm': 0.1214427649974823, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6993, 'grad_norm': 0.23351094126701355, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.29it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6942, 'grad_norm': 0.23636378347873688, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.697, 'grad_norm': 0.24773156642913818, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]                                               {'train_runtime': 1.0765, 'train_samples_per_second': 394.801, 'train_steps_per_second': 13.934, 'train_loss': 0.7004173835118611, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 13.94it/s]
CLIENT:76
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7046, 'grad_norm': 0.17033667862415314, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.94it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.62it/s]                                              {'loss': 0.7107, 'grad_norm': 0.16227614879608154, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.62it/s]                                              {'loss': 0.6986, 'grad_norm': 0.34570419788360596, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.62it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.7066, 'grad_norm': 0.16882432997226715, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6904, 'grad_norm': 0.1184023767709732, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.22it/s] 40%|████      | 6/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.7169, 'grad_norm': 0.4379092752933502, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6956, 'grad_norm': 0.20172050595283508, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.16it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.688, 'grad_norm': 0.19519847631454468, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.6997, 'grad_norm': 0.2921636700630188, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.11it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.6977, 'grad_norm': 0.2878209352493286, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.6917, 'grad_norm': 0.2055915892124176, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.75it/s] 80%|████████  | 12/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.707, 'grad_norm': 0.1449032425880432, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.693, 'grad_norm': 0.14276467263698578, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.24it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.6894, 'grad_norm': 0.24634112417697906, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.6829, 'grad_norm': 0.40148594975471497, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.78it/s]                                               {'train_runtime': 1.0762, 'train_samples_per_second': 394.915, 'train_steps_per_second': 13.938, 'train_loss': 0.6981852849324545, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.78it/s]100%|██████████| 15/15 [00:01<00:00, 13.94it/s]
CLIENT:14
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6886, 'grad_norm': 0.11347724497318268, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.90it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.7065, 'grad_norm': 0.19587090611457825, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.71, 'grad_norm': 0.21241925656795502, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.16it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.709, 'grad_norm': 0.17278793454170227, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.6993, 'grad_norm': 0.12108492106199265, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.12it/s] 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.7, 'grad_norm': 0.2841348648071289, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.7032, 'grad_norm': 0.22574284672737122, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.03it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.6969, 'grad_norm': 0.1948052942752838, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.6937, 'grad_norm': 0.14082764089107513, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.62it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6993, 'grad_norm': 0.1444040685892105, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6828, 'grad_norm': 0.23535102605819702, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.13it/s] 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.6721, 'grad_norm': 0.18089602887630463, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.6977, 'grad_norm': 0.13187676668167114, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.59it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.6748, 'grad_norm': 0.17397329211235046, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.6976, 'grad_norm': 0.368884414434433, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.59it/s]                                               {'train_runtime': 1.0546, 'train_samples_per_second': 402.998, 'train_steps_per_second': 14.223, 'train_loss': 0.6954311013221741, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.59it/s]100%|██████████| 15/15 [00:01<00:00, 14.23it/s]
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7173, 'grad_norm': 0.3034357726573944, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.07it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.56it/s]                                              {'loss': 0.7218, 'grad_norm': 0.2884611487388611, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.56it/s]                                              {'loss': 0.7153, 'grad_norm': 0.18702469766139984, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.56it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.7044, 'grad_norm': 0.24744772911071777, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.7194, 'grad_norm': 0.24666689336299896, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.16it/s] 40%|████      | 6/15 [00:00<00:00, 15.70it/s]                                              {'loss': 0.7015, 'grad_norm': 0.42160362005233765, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.70it/s]                                              {'loss': 0.7089, 'grad_norm': 0.262968510389328, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.70it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.6969, 'grad_norm': 0.26110804080963135, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.6949, 'grad_norm': 0.4323224723339081, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.25it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.6992, 'grad_norm': 0.3216807544231415, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.6884, 'grad_norm': 0.3697947859764099, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.75it/s] 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6966, 'grad_norm': 0.23464509844779968, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6902, 'grad_norm': 0.4067738950252533, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.43it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6922, 'grad_norm': 0.35185617208480835, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6961, 'grad_norm': 0.2742060720920563, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.45it/s]                                               {'train_runtime': 1.0703, 'train_samples_per_second': 397.071, 'train_steps_per_second': 14.014, 'train_loss': 0.70287606716156, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 14.02it/s]
CLIENT:48
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7173, 'grad_norm': 0.3357328176498413, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.11it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.77it/s]                                              {'loss': 0.71, 'grad_norm': 0.30786070227622986, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.77it/s]                                              {'loss': 0.6903, 'grad_norm': 0.1132049709558487, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.77it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.7158, 'grad_norm': 0.30970895290374756, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.7076, 'grad_norm': 0.16649393737316132, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.04it/s] 40%|████      | 6/15 [00:00<00:00, 16.30it/s]                                              {'loss': 0.706, 'grad_norm': 0.3391002118587494, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.30it/s]                                              {'loss': 0.6945, 'grad_norm': 0.24964474141597748, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.30it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.6972, 'grad_norm': 0.2662098705768585, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.699, 'grad_norm': 0.3500448763370514, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.64it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.09it/s]                                               {'loss': 0.6883, 'grad_norm': 0.22446687519550323, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.09it/s]                                               {'loss': 0.6844, 'grad_norm': 0.3865051567554474, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.09it/s] 80%|████████  | 12/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.6926, 'grad_norm': 0.29687583446502686, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.6895, 'grad_norm': 0.3514185845851898, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.50it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.682, 'grad_norm': 0.22386610507965088, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.6858, 'grad_norm': 0.3982723653316498, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.56it/s]                                               {'train_runtime': 1.0618, 'train_samples_per_second': 400.278, 'train_steps_per_second': 14.127, 'train_loss': 0.6973518451054891, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.56it/s]100%|██████████| 15/15 [00:01<00:00, 14.13it/s]
CLIENT:71
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6921, 'grad_norm': 0.2015148252248764, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.27it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.05it/s]                                              {'loss': 0.6976, 'grad_norm': 0.09099163115024567, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.05it/s]                                              {'loss': 0.7056, 'grad_norm': 0.15874843299388885, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.05it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.7091, 'grad_norm': 0.25117456912994385, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.6917, 'grad_norm': 0.08157745748758316, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.63it/s] 40%|████      | 6/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.7035, 'grad_norm': 0.23092904686927795, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.694, 'grad_norm': 0.09119907021522522, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.63it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.6917, 'grad_norm': 0.18178708851337433, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.6972, 'grad_norm': 0.18272553384304047, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.78it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.7043, 'grad_norm': 0.17908364534378052, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.6923, 'grad_norm': 0.14237020909786224, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.05it/s] 80%|████████  | 12/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.6906, 'grad_norm': 0.11170568317174911, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.703, 'grad_norm': 0.23654547333717346, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.50it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.46it/s]                                               {'loss': 0.7038, 'grad_norm': 0.07951804250478745, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.46it/s]                                               {'loss': 0.7066, 'grad_norm': 0.1649159938097, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.46it/s]                                               {'train_runtime': 1.0678, 'train_samples_per_second': 398.025, 'train_steps_per_second': 14.048, 'train_loss': 0.6988781452178955, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.46it/s]100%|██████████| 15/15 [00:01<00:00, 14.05it/s]
CLIENT:67
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6992, 'grad_norm': 0.2828720510005951, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.97it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.71it/s]                                              {'loss': 0.7082, 'grad_norm': 0.31642571091651917, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.71it/s]                                              {'loss': 0.7006, 'grad_norm': 0.22536075115203857, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.71it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.714, 'grad_norm': 0.3445395529270172, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.7047, 'grad_norm': 0.32508185505867004, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.20it/s] 40%|████      | 6/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6934, 'grad_norm': 0.1898624151945114, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.7007, 'grad_norm': 0.22832432389259338, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.16it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.698, 'grad_norm': 0.36913901567459106, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.6909, 'grad_norm': 0.3897702097892761, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.11it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.6903, 'grad_norm': 0.3604603409767151, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.6776, 'grad_norm': 0.3111015856266022, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.74it/s] 80%|████████  | 12/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.6953, 'grad_norm': 0.3707822263240814, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.6803, 'grad_norm': 0.3281257450580597, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.23it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.6772, 'grad_norm': 0.354338139295578, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.6741, 'grad_norm': 0.4471597671508789, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]                                               {'train_runtime': 1.0845, 'train_samples_per_second': 391.902, 'train_steps_per_second': 13.832, 'train_loss': 0.6936228036880493, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]100%|██████████| 15/15 [00:01<00:00, 13.84it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 32.86it/s] 24%|██▍       | 8/33 [00:00<00:00, 26.62it/s] 33%|███▎      | 11/33 [00:00<00:00, 25.97it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.73it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.35it/s] 61%|██████    | 20/33 [00:00<00:00, 25.33it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.14it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.18it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.14it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.14it/s]100%|██████████| 33/33 [00:01<00:00, 25.85it/s]
{'eval_loss': 0.7019882202148438, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.35186960690316393, 'eval_runtime': 1.3181, 'eval_samples_per_second': 791.306, 'eval_steps_per_second': 25.037}
ROUND:11
CLIENT:23
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7162, 'grad_norm': 0.3496405780315399, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.44it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.7045, 'grad_norm': 0.19272537529468536, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.7129, 'grad_norm': 0.2662563920021057, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.04it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.6965, 'grad_norm': 0.19011102616786957, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.7063, 'grad_norm': 0.3639390468597412, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.77it/s] 40%|████      | 6/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.6831, 'grad_norm': 0.2597843408584595, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.6944, 'grad_norm': 0.1850384622812271, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.44it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.39it/s]                                              {'loss': 0.6923, 'grad_norm': 0.284624844789505, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.39it/s]                                              {'loss': 0.6845, 'grad_norm': 0.46114581823349, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.39it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.7039, 'grad_norm': 0.20869934558868408, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.6811, 'grad_norm': 0.36111411452293396, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.95it/s] 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.6682, 'grad_norm': 0.29024478793144226, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.6716, 'grad_norm': 0.4575952887535095, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.40it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6913, 'grad_norm': 0.14962105453014374, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6811, 'grad_norm': 0.3102329969406128, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.48it/s]                                               {'train_runtime': 1.0709, 'train_samples_per_second': 396.868, 'train_steps_per_second': 14.007, 'train_loss': 0.6925262173016866, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]100%|██████████| 15/15 [00:01<00:00, 14.01it/s]
CLIENT:81
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6932, 'grad_norm': 0.2583463788032532, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.82it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.53it/s]                                              {'loss': 0.6978, 'grad_norm': 0.23000188171863556, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.53it/s]                                              {'loss': 0.715, 'grad_norm': 0.4472077786922455, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.53it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.7124, 'grad_norm': 0.3125630021095276, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6971, 'grad_norm': 0.34199124574661255, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.22it/s] 40%|████      | 6/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.6839, 'grad_norm': 0.2523772716522217, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.699, 'grad_norm': 0.40737006068229675, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.17it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.12it/s]                                              {'loss': 0.6899, 'grad_norm': 0.334809273481369, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.12it/s]                                              {'loss': 0.6866, 'grad_norm': 0.2710150480270386, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.12it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6925, 'grad_norm': 0.23866204917430878, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6736, 'grad_norm': 0.4823756814002991, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.76it/s] 80%|████████  | 12/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.6777, 'grad_norm': 0.36825498938560486, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.685, 'grad_norm': 0.2923857569694519, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.25it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6727, 'grad_norm': 0.35535770654678345, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6565, 'grad_norm': 0.5340777039527893, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.82it/s]                                               {'train_runtime': 1.0716, 'train_samples_per_second': 396.588, 'train_steps_per_second': 13.997, 'train_loss': 0.6888583858807882, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.82it/s]100%|██████████| 15/15 [00:01<00:00, 14.00it/s]
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.696, 'grad_norm': 0.25799432396888733, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.80it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.22it/s]                                              {'loss': 0.703, 'grad_norm': 0.1423523724079132, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.22it/s]                                              {'loss': 0.7059, 'grad_norm': 0.4114377796649933, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.22it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.7017, 'grad_norm': 0.29702040553092957, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.6889, 'grad_norm': 0.17699205875396729, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.36it/s] 40%|████      | 6/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.6813, 'grad_norm': 0.19491896033287048, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.6828, 'grad_norm': 0.18284684419631958, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.24it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.6892, 'grad_norm': 0.3334181010723114, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.6928, 'grad_norm': 0.21677397191524506, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.15it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6839, 'grad_norm': 0.2268698513507843, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.678, 'grad_norm': 0.26641833782196045, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.6841, 'grad_norm': 0.3299524486064911, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.6828, 'grad_norm': 0.2950555384159088, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.28it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.6904, 'grad_norm': 0.24800263345241547, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.6844, 'grad_norm': 0.2901788651943207, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.34it/s]                                               {'train_runtime': 1.0816, 'train_samples_per_second': 392.92, 'train_steps_per_second': 13.868, 'train_loss': 0.6896732608477275, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.34it/s]100%|██████████| 15/15 [00:01<00:00, 13.87it/s]
CLIENT:34
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6955, 'grad_norm': 0.10912803560495377, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.65it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.11it/s]                                              {'loss': 0.7117, 'grad_norm': 0.23989659547805786, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.11it/s]                                              {'loss': 0.7115, 'grad_norm': 0.35505422949790955, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.11it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.6992, 'grad_norm': 0.14054134488105774, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.7143, 'grad_norm': 0.42471224069595337, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.22it/s] 40%|████      | 6/15 [00:00<00:00, 16.19it/s]                                              {'loss': 0.6945, 'grad_norm': 0.11547428369522095, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.19it/s]                                              {'loss': 0.7018, 'grad_norm': 0.3723994791507721, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.19it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.6992, 'grad_norm': 0.21841338276863098, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.692, 'grad_norm': 0.12038801610469818, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.59it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.6906, 'grad_norm': 0.26909416913986206, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.6908, 'grad_norm': 0.20670118927955627, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.11it/s] 80%|████████  | 12/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.6976, 'grad_norm': 0.22920812666416168, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.687, 'grad_norm': 0.21458472311496735, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.50it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.694, 'grad_norm': 0.25475844740867615, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.684, 'grad_norm': 0.27467450499534607, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.57it/s]                                               {'train_runtime': 1.0595, 'train_samples_per_second': 401.115, 'train_steps_per_second': 14.157, 'train_loss': 0.6975713809331258, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.57it/s]100%|██████████| 15/15 [00:01<00:00, 14.16it/s]
CLIENT:62
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7063, 'grad_norm': 0.3120923936367035, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.03it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.24it/s]                                              {'loss': 0.7046, 'grad_norm': 0.256245493888855, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.24it/s]                                              {'loss': 0.7134, 'grad_norm': 0.520081639289856, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.24it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.7082, 'grad_norm': 0.39772695302963257, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.6951, 'grad_norm': 0.2425086945295334, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.62it/s] 40%|████      | 6/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.6842, 'grad_norm': 0.4793461263179779, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.6851, 'grad_norm': 0.37116461992263794, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.40it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.6882, 'grad_norm': 0.4144948720932007, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.6741, 'grad_norm': 0.3703935146331787, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.19it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.6894, 'grad_norm': 0.37745344638824463, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.6705, 'grad_norm': 0.39783328771591187, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.05it/s] 80%|████████  | 12/15 [00:00<00:00, 16.00it/s]                                               {'loss': 0.6645, 'grad_norm': 0.5519838929176331, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.00it/s]                                               {'loss': 0.6715, 'grad_norm': 0.429551362991333, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.00it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.6748, 'grad_norm': 0.3761836588382721, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.6644, 'grad_norm': 0.632803201675415, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.81it/s]                                               {'train_runtime': 1.0586, 'train_samples_per_second': 401.482, 'train_steps_per_second': 14.17, 'train_loss': 0.6862913131713867, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.81it/s]100%|██████████| 15/15 [00:01<00:00, 14.17it/s]
CLIENT:46
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6954, 'grad_norm': 0.1829623430967331, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.28it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.14it/s]                                              {'loss': 0.6982, 'grad_norm': 0.18388065695762634, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.14it/s]                                              {'loss': 0.7253, 'grad_norm': 0.3592420518398285, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.14it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.7035, 'grad_norm': 0.09804288297891617, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.6992, 'grad_norm': 0.3510066270828247, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.71it/s] 40%|████      | 6/15 [00:00<00:00, 15.43it/s]                                              {'loss': 0.6972, 'grad_norm': 0.26031801104545593, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.43it/s]                                              {'loss': 0.7008, 'grad_norm': 0.28877609968185425, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.43it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6885, 'grad_norm': 0.26474374532699585, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.696, 'grad_norm': 0.13004088401794434, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.31it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.7142, 'grad_norm': 0.26360592246055603, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.6835, 'grad_norm': 0.14913897216320038, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.05it/s] 80%|████████  | 12/15 [00:00<00:00, 15.45it/s]                                               {'loss': 0.6729, 'grad_norm': 0.35280904173851013, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.45it/s]                                               {'loss': 0.6849, 'grad_norm': 0.26169222593307495, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.45it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6905, 'grad_norm': 0.35595083236694336, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6995, 'grad_norm': 0.10977546125650406, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.48it/s]                                               {'train_runtime': 1.0714, 'train_samples_per_second': 396.672, 'train_steps_per_second': 14.0, 'train_loss': 0.6966508666674296, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]100%|██████████| 15/15 [00:01<00:00, 14.00it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6827, 'grad_norm': 0.2266169637441635, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.15it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.39it/s]                                              {'loss': 0.7109, 'grad_norm': 0.3659611642360687, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.39it/s]                                              {'loss': 0.687, 'grad_norm': 0.09390633553266525, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.39it/s] 27%|██▋       | 4/15 [00:00<00:00, 13.95it/s]                                              {'loss': 0.6814, 'grad_norm': 0.2047870010137558, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 13.95it/s]                                              {'loss': 0.6992, 'grad_norm': 0.26229676604270935, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 13.95it/s] 40%|████      | 6/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.6991, 'grad_norm': 0.32535263895988464, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.6909, 'grad_norm': 0.3166804015636444, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.01it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.69it/s]                                              {'loss': 0.686, 'grad_norm': 0.26466184854507446, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.69it/s]                                              {'loss': 0.6901, 'grad_norm': 0.1628095805644989, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.69it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.20it/s]                                               {'loss': 0.697, 'grad_norm': 0.2112082540988922, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.20it/s]                                               {'loss': 0.6773, 'grad_norm': 0.31860291957855225, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.20it/s] 80%|████████  | 12/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.6894, 'grad_norm': 0.28135430812835693, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.6763, 'grad_norm': 0.256731778383255, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 14.99it/s] 93%|█████████▎| 14/15 [00:00<00:00, 13.78it/s]                                               {'loss': 0.6878, 'grad_norm': 0.3419523239135742, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 13.78it/s]                                               {'loss': 0.6695, 'grad_norm': 0.25680840015411377, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.78it/s]                                               {'train_runtime': 1.125, 'train_samples_per_second': 377.767, 'train_steps_per_second': 13.333, 'train_loss': 0.6883095820744832, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.78it/s]100%|██████████| 15/15 [00:01<00:00, 13.34it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7011, 'grad_norm': 0.18192808330059052, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.00it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.79it/s]                                              {'loss': 0.703, 'grad_norm': 0.2948278486728668, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.79it/s]                                              {'loss': 0.6962, 'grad_norm': 0.3034629225730896, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.79it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.46it/s]                                              {'loss': 0.7036, 'grad_norm': 0.15538842976093292, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.46it/s]                                              {'loss': 0.7098, 'grad_norm': 0.46369698643684387, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.46it/s] 40%|████      | 6/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.6971, 'grad_norm': 0.212197408080101, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.6864, 'grad_norm': 0.37061309814453125, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.51it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6938, 'grad_norm': 0.22823172807693481, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6888, 'grad_norm': 0.2577084004878998, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.22it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.7048, 'grad_norm': 0.29780465364456177, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.6693, 'grad_norm': 0.334050714969635, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.56it/s] 80%|████████  | 12/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6847, 'grad_norm': 0.26614537835121155, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.685, 'grad_norm': 0.3082975149154663, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.13it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.25it/s]                                               {'loss': 0.6847, 'grad_norm': 0.34248220920562744, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.25it/s]                                               {'loss': 0.6695, 'grad_norm': 0.3190988600254059, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.25it/s]                                               {'train_runtime': 1.0888, 'train_samples_per_second': 390.322, 'train_steps_per_second': 13.776, 'train_loss': 0.6918711980183919, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.25it/s]100%|██████████| 15/15 [00:01<00:00, 13.78it/s]
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6955, 'grad_norm': 0.2788257598876953, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.88it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.63it/s]                                              {'loss': 0.6959, 'grad_norm': 0.2615978419780731, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.63it/s]                                              {'loss': 0.7009, 'grad_norm': 0.2827879786491394, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.63it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.7085, 'grad_norm': 0.33963093161582947, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.6874, 'grad_norm': 0.1662749946117401, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.18it/s] 40%|████      | 6/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.6855, 'grad_norm': 0.3151867091655731, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.6872, 'grad_norm': 0.30989012122154236, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.09it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6814, 'grad_norm': 0.39467841386795044, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6976, 'grad_norm': 0.13846921920776367, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.06it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.6928, 'grad_norm': 0.25625500082969666, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.6808, 'grad_norm': 0.22047823667526245, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.67it/s] 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6773, 'grad_norm': 0.4767396152019501, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6813, 'grad_norm': 0.2711259126663208, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.31it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.6848, 'grad_norm': 0.337583988904953, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.6752, 'grad_norm': 0.41893208026885986, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.74it/s]                                               {'train_runtime': 1.0777, 'train_samples_per_second': 394.367, 'train_steps_per_second': 13.919, 'train_loss': 0.6888120174407959, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.74it/s]100%|██████████| 15/15 [00:01<00:00, 13.92it/s]
CLIENT:11
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7013, 'grad_norm': 0.24565467238426208, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.07it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.29it/s]                                              {'loss': 0.7147, 'grad_norm': 0.3504197895526886, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.29it/s]                                              {'loss': 0.6929, 'grad_norm': 0.11729611456394196, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.29it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.7055, 'grad_norm': 0.30263739824295044, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.7168, 'grad_norm': 0.26446032524108887, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.90it/s] 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.7048, 'grad_norm': 0.1878155767917633, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.7022, 'grad_norm': 0.2718501389026642, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.59it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.6988, 'grad_norm': 0.2412971705198288, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.6993, 'grad_norm': 0.19729046523571014, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.35it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6905, 'grad_norm': 0.3420456647872925, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6945, 'grad_norm': 0.1995074599981308, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.94it/s] 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7012, 'grad_norm': 0.2965724766254425, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.6874, 'grad_norm': 0.29573407769203186, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.38it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.46it/s]                                               {'loss': 0.6974, 'grad_norm': 0.15257836878299713, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.46it/s]                                               {'loss': 0.6877, 'grad_norm': 0.46506714820861816, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.46it/s]                                               {'train_runtime': 1.071, 'train_samples_per_second': 396.831, 'train_steps_per_second': 14.006, 'train_loss': 0.6996726393699646, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.46it/s]100%|██████████| 15/15 [00:01<00:00, 14.01it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 37.51it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.60it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.66it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.72it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.27it/s] 61%|██████    | 20/33 [00:00<00:00, 25.41it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.13it/s] 79%|███████▉  | 26/33 [00:00<00:00, 25.17it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.12it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.15it/s]100%|██████████| 33/33 [00:01<00:00, 26.09it/s]
{'eval_loss': 0.6951135396957397, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.4793863854266539, 'eval_runtime': 1.3064, 'eval_samples_per_second': 798.367, 'eval_steps_per_second': 25.26}
ROUND:12
CLIENT:80
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7037, 'grad_norm': 0.13206805288791656, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.13it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.90it/s]                                              {'loss': 0.6913, 'grad_norm': 0.16611316800117493, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.90it/s]                                              {'loss': 0.6998, 'grad_norm': 0.4308346211910248, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.90it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.6947, 'grad_norm': 0.18638509511947632, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.6893, 'grad_norm': 0.2842061519622803, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.56it/s] 40%|████      | 6/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.6935, 'grad_norm': 0.13183321058750153, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.6841, 'grad_norm': 0.19064028561115265, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.63it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.6843, 'grad_norm': 0.13518179953098297, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.6948, 'grad_norm': 0.36069661378860474, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.76it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.6833, 'grad_norm': 0.2804701626300812, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.6803, 'grad_norm': 0.15570276975631714, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.25it/s] 80%|████████  | 12/15 [00:00<00:00, 15.70it/s]                                               {'loss': 0.7047, 'grad_norm': 0.3335588574409485, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.70it/s]                                               {'loss': 0.6738, 'grad_norm': 0.381000280380249, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.70it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.6953, 'grad_norm': 0.11532895267009735, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.6773, 'grad_norm': 0.36843836307525635, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.53it/s]                                               {'train_runtime': 1.0644, 'train_samples_per_second': 399.273, 'train_steps_per_second': 14.092, 'train_loss': 0.6900096734364828, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.53it/s]100%|██████████| 15/15 [00:01<00:00, 14.10it/s]
CLIENT:84
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6945, 'grad_norm': 0.2937842011451721, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.79it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.6918, 'grad_norm': 0.166044682264328, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.688, 'grad_norm': 0.1895410418510437, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.16it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.6932, 'grad_norm': 0.32103002071380615, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.6852, 'grad_norm': 0.12074924260377884, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.36it/s] 40%|████      | 6/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.6896, 'grad_norm': 0.26420140266418457, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.6951, 'grad_norm': 0.15310752391815186, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.23it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6787, 'grad_norm': 0.2593395709991455, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6816, 'grad_norm': 0.32012438774108887, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.10it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.6872, 'grad_norm': 0.17987161874771118, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.6706, 'grad_norm': 0.18682582676410675, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.73it/s] 80%|████████  | 12/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.6901, 'grad_norm': 0.47695720195770264, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.6709, 'grad_norm': 0.2838508188724518, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.22it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.24it/s]                                               {'loss': 0.6674, 'grad_norm': 0.25217118859291077, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.24it/s]                                               {'loss': 0.6946, 'grad_norm': 0.2634052634239197, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.24it/s]                                               {'train_runtime': 1.0869, 'train_samples_per_second': 391.021, 'train_steps_per_second': 13.801, 'train_loss': 0.6852442860603333, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.24it/s]100%|██████████| 15/15 [00:01<00:00, 13.81it/s]
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6852, 'grad_norm': 0.1550101488828659, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.86it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.62it/s]                                              {'loss': 0.6824, 'grad_norm': 0.3850288987159729, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.62it/s]                                              {'loss': 0.6921, 'grad_norm': 0.37282559275627136, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.62it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6976, 'grad_norm': 0.2509746551513672, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6894, 'grad_norm': 0.46752476692199707, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.06it/s] 40%|████      | 6/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.6909, 'grad_norm': 0.2610528767108917, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.6792, 'grad_norm': 0.4982542097568512, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.01it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.00it/s]                                              {'loss': 0.6814, 'grad_norm': 0.24666759371757507, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.00it/s]                                              {'loss': 0.6899, 'grad_norm': 0.15398019552230835, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.00it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.6868, 'grad_norm': 0.2538868486881256, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.6504, 'grad_norm': 0.3896179497241974, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.63it/s] 80%|████████  | 12/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.6787, 'grad_norm': 0.438894122838974, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.6828, 'grad_norm': 0.25887414813041687, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.33it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.6664, 'grad_norm': 0.3942606747150421, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.6556, 'grad_norm': 0.44192588329315186, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]                                               {'train_runtime': 1.0871, 'train_samples_per_second': 390.962, 'train_steps_per_second': 13.799, 'train_loss': 0.6805893858273824, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]100%|██████████| 15/15 [00:01<00:00, 13.80it/s]
CLIENT:81
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6853, 'grad_norm': 0.2893942594528198, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.74it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.17it/s]                                              {'loss': 0.6909, 'grad_norm': 0.26081007719039917, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.17it/s]                                              {'loss': 0.7004, 'grad_norm': 0.5141488909721375, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.17it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.7018, 'grad_norm': 0.3558381497859955, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.6852, 'grad_norm': 0.3825611472129822, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.34it/s] 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.6752, 'grad_norm': 0.2799329459667206, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.6827, 'grad_norm': 0.4673278331756592, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.22it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.6762, 'grad_norm': 0.38278016448020935, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.6759, 'grad_norm': 0.3130842447280884, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.49it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.6826, 'grad_norm': 0.26134538650512695, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.651, 'grad_norm': 0.5595070123672485, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.75it/s] 80%|████████  | 12/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.6622, 'grad_norm': 0.3739978075027466, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.6712, 'grad_norm': 0.3322499692440033, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.22it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.31it/s]                                               {'loss': 0.6552, 'grad_norm': 0.40673720836639404, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.31it/s]                                               {'loss': 0.6297, 'grad_norm': 0.6038586497306824, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.31it/s]                                               {'train_runtime': 1.0818, 'train_samples_per_second': 392.879, 'train_steps_per_second': 13.866, 'train_loss': 0.6750279307365418, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.31it/s]100%|██████████| 15/15 [00:01<00:00, 13.87it/s]
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.687, 'grad_norm': 0.3155873119831085, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.01it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.17it/s]                                              {'loss': 0.6881, 'grad_norm': 0.29220423102378845, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.17it/s]                                              {'loss': 0.6926, 'grad_norm': 0.312926709651947, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.17it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.6971, 'grad_norm': 0.393634557723999, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.6823, 'grad_norm': 0.18488983809947968, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.36it/s] 40%|████      | 6/15 [00:00<00:00, 15.79it/s]                                              {'loss': 0.6744, 'grad_norm': 0.35388869047164917, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.79it/s]                                              {'loss': 0.6757, 'grad_norm': 0.3484993875026703, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.79it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.6657, 'grad_norm': 0.4551665186882019, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.6941, 'grad_norm': 0.14701899886131287, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.34it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6827, 'grad_norm': 0.2827918827533722, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6724, 'grad_norm': 0.25103759765625, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.6559, 'grad_norm': 0.5672635436058044, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.6692, 'grad_norm': 0.3233746588230133, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.36it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.13it/s]                                               {'loss': 0.6696, 'grad_norm': 0.37844157218933105, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.13it/s]                                               {'loss': 0.6552, 'grad_norm': 0.5043835639953613, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.13it/s]                                               {'train_runtime': 1.077, 'train_samples_per_second': 394.598, 'train_steps_per_second': 13.927, 'train_loss': 0.677473783493042, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.13it/s]100%|██████████| 15/15 [00:01<00:00, 13.93it/s]
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6881, 'grad_norm': 0.3189035654067993, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.90it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.06it/s]                                              {'loss': 0.7003, 'grad_norm': 0.41975969076156616, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.06it/s]                                              {'loss': 0.6867, 'grad_norm': 0.2012643665075302, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.06it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.6827, 'grad_norm': 0.42815548181533813, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.6934, 'grad_norm': 0.34472838044166565, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.63it/s] 40%|████      | 6/15 [00:00<00:00, 16.01it/s]                                              {'loss': 0.6915, 'grad_norm': 0.24486491084098816, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.01it/s]                                              {'loss': 0.6832, 'grad_norm': 0.38750284910202026, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.01it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.6804, 'grad_norm': 0.4347345232963562, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.6756, 'grad_norm': 0.2878226935863495, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.77it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6752, 'grad_norm': 0.3058429956436157, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6725, 'grad_norm': 0.34340041875839233, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.13it/s] 80%|████████  | 12/15 [00:00<00:00, 15.74it/s]                                               {'loss': 0.6495, 'grad_norm': 0.6809466481208801, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.74it/s]                                               {'loss': 0.6521, 'grad_norm': 0.5406853556632996, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.74it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.6752, 'grad_norm': 0.4115159809589386, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.6776, 'grad_norm': 0.2704528272151947, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.70it/s]                                               {'train_runtime': 1.0556, 'train_samples_per_second': 402.601, 'train_steps_per_second': 14.209, 'train_loss': 0.6789389848709106, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.70it/s]100%|██████████| 15/15 [00:01<00:00, 14.21it/s]
CLIENT:36
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6911, 'grad_norm': 0.3247542083263397, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.77it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.39it/s]                                              {'loss': 0.6969, 'grad_norm': 0.3051919639110565, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.39it/s]                                              {'loss': 0.6851, 'grad_norm': 0.40612855553627014, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.39it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.04it/s]                                              {'loss': 0.6852, 'grad_norm': 0.3880636394023895, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.04it/s]                                              {'loss': 0.6968, 'grad_norm': 0.31450918316841125, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.04it/s] 40%|████      | 6/15 [00:00<00:00, 14.95it/s]                                              {'loss': 0.6728, 'grad_norm': 0.3037121891975403, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 14.95it/s]                                              {'loss': 0.678, 'grad_norm': 0.5622780919075012, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 14.95it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.67it/s]                                              {'loss': 0.6956, 'grad_norm': 0.170890673995018, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.67it/s]                                              {'loss': 0.6635, 'grad_norm': 0.45887652039527893, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.67it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.28it/s]                                               {'loss': 0.6578, 'grad_norm': 0.5002812147140503, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.28it/s]                                               {'loss': 0.6708, 'grad_norm': 0.272442102432251, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.28it/s] 80%|████████  | 12/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.6625, 'grad_norm': 0.3101402223110199, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.6717, 'grad_norm': 0.2812090218067169, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 14.80it/s] 93%|█████████▎| 14/15 [00:01<00:00, 13.73it/s]                                               {'loss': 0.6481, 'grad_norm': 0.4769206941127777, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:01<00:00, 13.73it/s]                                               {'loss': 0.6417, 'grad_norm': 0.4382086396217346, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.73it/s]                                               {'train_runtime': 1.1184, 'train_samples_per_second': 380.0, 'train_steps_per_second': 13.412, 'train_loss': 0.6744982163111369, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.73it/s]100%|██████████| 15/15 [00:01<00:00, 13.42it/s]
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6903, 'grad_norm': 0.20296728610992432, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.02it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.6982, 'grad_norm': 0.18410369753837585, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.7007, 'grad_norm': 0.5154759287834167, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.23it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.7013, 'grad_norm': 0.38362929224967957, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.681, 'grad_norm': 0.3081916272640228, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.50it/s] 40%|████      | 6/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.684, 'grad_norm': 0.22129279375076294, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.6702, 'grad_norm': 0.2239709347486496, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.31it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.6842, 'grad_norm': 0.3396049737930298, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.6702, 'grad_norm': 0.39456552267074585, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.15it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.6861, 'grad_norm': 0.30643516778945923, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.6556, 'grad_norm': 0.2767484486103058, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.74it/s] 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.6801, 'grad_norm': 0.36966079473495483, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.6775, 'grad_norm': 0.27658811211586, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.35it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.6591, 'grad_norm': 0.4036521017551422, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.6707, 'grad_norm': 0.2725072205066681, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]                                               {'train_runtime': 1.0788, 'train_samples_per_second': 393.962, 'train_steps_per_second': 13.905, 'train_loss': 0.680619192123413, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]100%|██████████| 15/15 [00:01<00:00, 13.91it/s]
CLIENT:69
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6902, 'grad_norm': 0.20498965680599213, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.97it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.71it/s]                                              {'loss': 0.6899, 'grad_norm': 0.28042829036712646, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.71it/s]                                              {'loss': 0.6964, 'grad_norm': 0.29205700755119324, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.71it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.04it/s]                                              {'loss': 0.6868, 'grad_norm': 0.19999513030052185, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.04it/s]                                              {'loss': 0.6966, 'grad_norm': 0.299251526594162, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.04it/s] 40%|████      | 6/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.6969, 'grad_norm': 0.41000980138778687, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.6782, 'grad_norm': 0.2695128917694092, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.04it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.6807, 'grad_norm': 0.2202553153038025, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.6799, 'grad_norm': 0.3355386257171631, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.36it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.687, 'grad_norm': 0.3604884147644043, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.6686, 'grad_norm': 0.25481584668159485, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.83it/s] 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6812, 'grad_norm': 0.20188376307487488, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.682, 'grad_norm': 0.21879366040229797, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.32it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.679, 'grad_norm': 0.22341927886009216, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6637, 'grad_norm': 0.48664769530296326, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]                                               {'train_runtime': 1.0838, 'train_samples_per_second': 392.149, 'train_steps_per_second': 13.841, 'train_loss': 0.6838135957717896, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 13.84it/s]
CLIENT:65
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7098, 'grad_norm': 0.17570102214813232, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.08it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.86it/s]                                              {'loss': 0.7003, 'grad_norm': 0.3401511311531067, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.86it/s]                                              {'loss': 0.7118, 'grad_norm': 0.14406810700893402, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.86it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.6819, 'grad_norm': 0.212164968252182, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.6962, 'grad_norm': 0.264816552400589, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.34it/s] 40%|████      | 6/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.6909, 'grad_norm': 0.17830389738082886, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.6967, 'grad_norm': 0.19186067581176758, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.34it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.88it/s]                                              {'loss': 0.6953, 'grad_norm': 0.15472210943698883, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.88it/s]                                              {'loss': 0.6846, 'grad_norm': 0.3579135835170746, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.88it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.7019, 'grad_norm': 0.1739729791879654, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.6946, 'grad_norm': 0.19964487850666046, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.57it/s] 80%|████████  | 12/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.6799, 'grad_norm': 0.31645649671554565, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.6809, 'grad_norm': 0.2625049352645874, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.10it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.29it/s]                                               {'loss': 0.6932, 'grad_norm': 0.23702295124530792, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.29it/s]                                               {'loss': 0.6988, 'grad_norm': 0.20084774494171143, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.29it/s]                                               {'train_runtime': 1.0838, 'train_samples_per_second': 392.122, 'train_steps_per_second': 13.84, 'train_loss': 0.69445298910141, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.29it/s]100%|██████████| 15/15 [00:01<00:00, 13.84it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 35.38it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.21it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.63it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.63it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.47it/s] 61%|██████    | 20/33 [00:00<00:00, 25.54it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.28it/s] 79%|███████▉  | 26/33 [00:00<00:00, 25.22it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.15it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.22it/s]100%|██████████| 33/33 [00:01<00:00, 26.08it/s]
{'eval_loss': 0.6865885257720947, 'eval_model_preparation_time': 0.0022, 'eval_acc': 0.6049856184084372, 'eval_runtime': 1.3069, 'eval_samples_per_second': 798.081, 'eval_steps_per_second': 25.251}
ROUND:13
CLIENT:14
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6811, 'grad_norm': 0.1392536163330078, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.70it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.83it/s]                                              {'loss': 0.6874, 'grad_norm': 0.26543760299682617, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.83it/s]                                              {'loss': 0.6895, 'grad_norm': 0.2990373969078064, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.83it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.6925, 'grad_norm': 0.22830383479595184, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.6894, 'grad_norm': 0.15730170905590057, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.01it/s] 40%|████      | 6/15 [00:00<00:00, 15.69it/s]                                              {'loss': 0.6701, 'grad_norm': 0.39402514696121216, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.69it/s]                                              {'loss': 0.6781, 'grad_norm': 0.31616002321243286, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.69it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.44it/s]                                              {'loss': 0.676, 'grad_norm': 0.24930740892887115, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.44it/s]                                              {'loss': 0.6813, 'grad_norm': 0.19543762505054474, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.44it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.6843, 'grad_norm': 0.17967642843723297, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.6555, 'grad_norm': 0.304798424243927, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.88it/s] 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.6531, 'grad_norm': 0.24323195219039917, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.6854, 'grad_norm': 0.16271187365055084, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.37it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6553, 'grad_norm': 0.2247801125049591, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6494, 'grad_norm': 0.514815628528595, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]                                               {'train_runtime': 1.0793, 'train_samples_per_second': 393.765, 'train_steps_per_second': 13.898, 'train_loss': 0.6752352515856425, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 13.90it/s]
CLIENT:74
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6908, 'grad_norm': 0.15441109240055084, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.48it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.98it/s]                                              {'loss': 0.6808, 'grad_norm': 0.34952783584594727, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.98it/s]                                              {'loss': 0.6811, 'grad_norm': 0.4383029639720917, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.98it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.6878, 'grad_norm': 0.3847859501838684, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.6815, 'grad_norm': 0.11427335441112518, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.68it/s] 40%|████      | 6/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.6669, 'grad_norm': 0.5248808860778809, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.6785, 'grad_norm': 0.2323518693447113, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.44it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6546, 'grad_norm': 0.45825061202049255, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.68, 'grad_norm': 0.2406422346830368, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.32it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.6616, 'grad_norm': 0.2472061663866043, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.6529, 'grad_norm': 0.3868766725063324, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.24it/s] 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.6896, 'grad_norm': 0.23213225603103638, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.6937, 'grad_norm': 0.12046319991350174, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.62it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6373, 'grad_norm': 0.5343465209007263, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6464, 'grad_norm': 0.3473968505859375, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.48it/s]                                               {'train_runtime': 1.0708, 'train_samples_per_second': 396.909, 'train_steps_per_second': 14.009, 'train_loss': 0.6722371141115825, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]100%|██████████| 15/15 [00:01<00:00, 14.01it/s]
CLIENT:15
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6923, 'grad_norm': 0.2978988587856293, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.07it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.27it/s]                                              {'loss': 0.6813, 'grad_norm': 0.2716735601425171, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.27it/s]                                              {'loss': 0.6792, 'grad_norm': 0.17475974559783936, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.27it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.6898, 'grad_norm': 0.2801899015903473, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.6891, 'grad_norm': 0.24157316982746124, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.61it/s] 40%|████      | 6/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.6824, 'grad_norm': 0.26030686497688293, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.6923, 'grad_norm': 0.1579885184764862, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.41it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.6717, 'grad_norm': 0.32510825991630554, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.6616, 'grad_norm': 0.31518277525901794, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.20it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.6739, 'grad_norm': 0.27260544896125793, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.6923, 'grad_norm': 0.125056654214859, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.81it/s] 80%|████████  | 12/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.6638, 'grad_norm': 0.5065280795097351, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.6684, 'grad_norm': 0.2976842224597931, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.25it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6594, 'grad_norm': 0.3064265847206116, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6738, 'grad_norm': 0.24376317858695984, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]                                               {'train_runtime': 1.078, 'train_samples_per_second': 394.265, 'train_steps_per_second': 13.915, 'train_loss': 0.678081766764323, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 13.92it/s]
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6869, 'grad_norm': 0.20134428143501282, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.08it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.69it/s]                                              {'loss': 0.676, 'grad_norm': 0.23320826888084412, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.69it/s]                                              {'loss': 0.6904, 'grad_norm': 0.35200390219688416, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.69it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.96it/s]                                              {'loss': 0.6808, 'grad_norm': 0.1702878326177597, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.96it/s]                                              {'loss': 0.6751, 'grad_norm': 0.33468809723854065, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.96it/s] 40%|████      | 6/15 [00:00<00:00, 16.17it/s]                                              {'loss': 0.684, 'grad_norm': 0.24056628346443176, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.17it/s]                                              {'loss': 0.6658, 'grad_norm': 0.38976848125457764, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.17it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.6893, 'grad_norm': 0.14134135842323303, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.6868, 'grad_norm': 0.20161855220794678, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.74it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6761, 'grad_norm': 0.2567201852798462, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6656, 'grad_norm': 0.22506126761436462, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.13it/s] 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.6731, 'grad_norm': 0.303180068731308, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.6922, 'grad_norm': 0.14198830723762512, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.53it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.65, 'grad_norm': 0.4242941439151764, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.6482, 'grad_norm': 0.2600310444831848, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.60it/s]                                               {'train_runtime': 1.0582, 'train_samples_per_second': 401.618, 'train_steps_per_second': 14.175, 'train_loss': 0.6760133504867554, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.60it/s]100%|██████████| 15/15 [00:01<00:00, 14.18it/s]
CLIENT:32
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.683, 'grad_norm': 0.2567065358161926, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.77it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.51it/s]                                              {'loss': 0.6813, 'grad_norm': 0.46027716994285583, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.51it/s]                                              {'loss': 0.6814, 'grad_norm': 0.3575354218482971, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.51it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.87it/s]                                              {'loss': 0.6812, 'grad_norm': 0.38171571493148804, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.87it/s]                                              {'loss': 0.666, 'grad_norm': 0.5271924734115601, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.87it/s] 40%|████      | 6/15 [00:00<00:00, 15.94it/s]                                              {'loss': 0.6705, 'grad_norm': 0.2172134667634964, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.94it/s]                                              {'loss': 0.6536, 'grad_norm': 0.5681334137916565, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.94it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.46it/s]                                              {'loss': 0.6703, 'grad_norm': 0.20172572135925293, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.46it/s]                                              {'loss': 0.6637, 'grad_norm': 0.4309806525707245, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.46it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.635, 'grad_norm': 0.502833366394043, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.642, 'grad_norm': 0.3680589199066162, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.92it/s] 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.6763, 'grad_norm': 0.2920798063278198, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.6587, 'grad_norm': 0.24834200739860535, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.56it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6184, 'grad_norm': 0.5382463932037354, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6308, 'grad_norm': 0.4961310625076294, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.55it/s]                                               {'train_runtime': 1.0676, 'train_samples_per_second': 398.095, 'train_steps_per_second': 14.05, 'train_loss': 0.6608134587605794, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.55it/s]100%|██████████| 15/15 [00:01<00:00, 14.05it/s]
CLIENT:59
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6724, 'grad_norm': 0.3150089681148529, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.86it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.08it/s]                                              {'loss': 0.6923, 'grad_norm': 0.231466144323349, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.08it/s]                                              {'loss': 0.6876, 'grad_norm': 0.18026204407215118, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.08it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.6932, 'grad_norm': 0.12373035401105881, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.6818, 'grad_norm': 0.2462378293275833, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.17it/s] 40%|████      | 6/15 [00:00<00:00, 15.91it/s]                                              {'loss': 0.6723, 'grad_norm': 0.47657671570777893, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.91it/s]                                              {'loss': 0.6805, 'grad_norm': 0.4294503629207611, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.91it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.6865, 'grad_norm': 0.14087112247943878, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.6753, 'grad_norm': 0.18484961986541748, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.40it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.6834, 'grad_norm': 0.148605614900589, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.6576, 'grad_norm': 0.47211697697639465, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.67it/s] 80%|████████  | 12/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.6805, 'grad_norm': 0.17093683779239655, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.6614, 'grad_norm': 0.4405098557472229, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.15it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.29it/s]                                               {'loss': 0.6662, 'grad_norm': 0.11930957436561584, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.29it/s]                                               {'loss': 0.6624, 'grad_norm': 0.21442070603370667, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.29it/s]                                               {'train_runtime': 1.0762, 'train_samples_per_second': 394.9, 'train_steps_per_second': 13.938, 'train_loss': 0.6768937389055888, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.29it/s]100%|██████████| 15/15 [00:01<00:00, 13.94it/s]
CLIENT:91
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6821, 'grad_norm': 0.2956368625164032, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.96it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.69it/s]                                              {'loss': 0.6831, 'grad_norm': 0.33145904541015625, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.69it/s]                                              {'loss': 0.6886, 'grad_norm': 0.4686809182167053, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.69it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6769, 'grad_norm': 0.3521968126296997, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6789, 'grad_norm': 0.3671165704727173, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.06it/s] 40%|████      | 6/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.6745, 'grad_norm': 0.3373705744743347, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.6699, 'grad_norm': 0.2818710207939148, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.01it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.6714, 'grad_norm': 0.34408506751060486, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.6535, 'grad_norm': 0.4383510649204254, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.99it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.6571, 'grad_norm': 0.4082239866256714, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.6575, 'grad_norm': 0.31293997168540955, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.63it/s] 80%|████████  | 12/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.645, 'grad_norm': 0.4465578496456146, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.6542, 'grad_norm': 0.39033570885658264, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.34it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.6522, 'grad_norm': 0.3563005030155182, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.6606, 'grad_norm': 0.43855857849121094, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]                                               {'train_runtime': 1.087, 'train_samples_per_second': 390.981, 'train_steps_per_second': 13.799, 'train_loss': 0.6670308470726013, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]100%|██████████| 15/15 [00:01<00:00, 13.80it/s]
CLIENT:10
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7054, 'grad_norm': 0.3478103578090668, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.99it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.20it/s]                                              {'loss': 0.7025, 'grad_norm': 0.21268346905708313, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.20it/s]                                              {'loss': 0.6907, 'grad_norm': 0.3354605436325073, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.20it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.6903, 'grad_norm': 0.2576238512992859, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.6861, 'grad_norm': 0.2760113477706909, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.74it/s] 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.6726, 'grad_norm': 0.39371755719184875, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.6754, 'grad_norm': 0.2848046123981476, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.46it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.689, 'grad_norm': 0.2827281653881073, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.6678, 'grad_norm': 0.445150226354599, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.24it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.6732, 'grad_norm': 0.3032456338405609, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.6513, 'grad_norm': 0.42967092990875244, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.84it/s] 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6889, 'grad_norm': 0.20584940910339355, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6622, 'grad_norm': 0.4211663603782654, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.29it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6715, 'grad_norm': 0.2506393790245056, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6608, 'grad_norm': 0.44261330366134644, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]                                               {'train_runtime': 1.0772, 'train_samples_per_second': 394.536, 'train_steps_per_second': 13.925, 'train_loss': 0.67918674548467, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 13.93it/s]
CLIENT:50
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6849, 'grad_norm': 0.38197436928749084, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.91it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.28it/s]                                              {'loss': 0.6827, 'grad_norm': 0.35590285062789917, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.28it/s]                                              {'loss': 0.6791, 'grad_norm': 0.36595961451530457, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.28it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.6795, 'grad_norm': 0.5162428617477417, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.6706, 'grad_norm': 0.3879426419734955, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.30it/s] 40%|████      | 6/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.6698, 'grad_norm': 0.25528377294540405, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.6407, 'grad_norm': 0.6067386865615845, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.15it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.6747, 'grad_norm': 0.28649190068244934, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.6617, 'grad_norm': 0.39530083537101746, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.70it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.649, 'grad_norm': 0.5009940266609192, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.6491, 'grad_norm': 0.38444578647613525, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.88it/s] 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.654, 'grad_norm': 0.40790897607803345, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6192, 'grad_norm': 0.6315820813179016, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.41it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.6758, 'grad_norm': 0.2040385901927948, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.619, 'grad_norm': 0.5598840117454529, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]                                               {'train_runtime': 1.0762, 'train_samples_per_second': 394.895, 'train_steps_per_second': 13.937, 'train_loss': 0.6606475234031677, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]100%|██████████| 15/15 [00:01<00:00, 13.94it/s]
CLIENT:53
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6803, 'grad_norm': 0.3395926356315613, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.48it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.74it/s]                                              {'loss': 0.6848, 'grad_norm': 0.13916392624378204, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.74it/s]                                              {'loss': 0.697, 'grad_norm': 0.35864120721817017, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.74it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.6926, 'grad_norm': 0.14881253242492676, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.6732, 'grad_norm': 0.4382224380970001, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.42it/s] 40%|████      | 6/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.6821, 'grad_norm': 0.25373613834381104, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.6941, 'grad_norm': 0.26443901658058167, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.47it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6579, 'grad_norm': 0.4026208817958832, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6834, 'grad_norm': 0.14833347499370575, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.31it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.6826, 'grad_norm': 0.3511255085468292, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.6739, 'grad_norm': 0.23453961312770844, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.88it/s] 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.6634, 'grad_norm': 0.3478606939315796, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.6655, 'grad_norm': 0.39229071140289307, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.49it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.6644, 'grad_norm': 0.32300710678100586, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.6798, 'grad_norm': 0.16216623783111572, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.64it/s]                                               {'train_runtime': 1.065, 'train_samples_per_second': 399.055, 'train_steps_per_second': 14.084, 'train_loss': 0.6783222476641337, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.64it/s]100%|██████████| 15/15 [00:01<00:00, 14.09it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 34.42it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.69it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.31it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.44it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.27it/s] 61%|██████    | 20/33 [00:00<00:00, 25.41it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.15it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.08it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.23it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.10it/s]100%|██████████| 33/33 [00:01<00:00, 25.92it/s]
{'eval_loss': 0.6766904592514038, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6845637583892618, 'eval_runtime': 1.3143, 'eval_samples_per_second': 793.563, 'eval_steps_per_second': 25.108}
ROUND:14
CLIENT:72
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6902, 'grad_norm': 0.21525262296199799, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.72it/s] 13%|█▎        | 2/15 [00:00<00:01, 13.00it/s]                                              {'loss': 0.6698, 'grad_norm': 0.4870724678039551, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 13.00it/s]                                              {'loss': 0.6561, 'grad_norm': 0.5478655099868774, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.00it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.6704, 'grad_norm': 0.420978307723999, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.6503, 'grad_norm': 0.5254651308059692, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.15it/s] 40%|████      | 6/15 [00:00<00:00, 15.86it/s]                                              {'loss': 0.6823, 'grad_norm': 0.21551577746868134, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.86it/s]                                              {'loss': 0.6908, 'grad_norm': 0.16537906229496002, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.86it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.39it/s]                                              {'loss': 0.6359, 'grad_norm': 0.5309544205665588, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.39it/s]                                              {'loss': 0.6102, 'grad_norm': 0.7117359638214111, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.39it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.6397, 'grad_norm': 0.4577886462211609, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.6454, 'grad_norm': 0.4090138375759125, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.73it/s] 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.6447, 'grad_norm': 0.36260470747947693, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.6156, 'grad_norm': 0.45093172788619995, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.26it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.6463, 'grad_norm': 0.33847010135650635, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.6359, 'grad_norm': 0.3221781551837921, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.34it/s]                                               {'train_runtime': 1.0821, 'train_samples_per_second': 392.754, 'train_steps_per_second': 13.862, 'train_loss': 0.652246888478597, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.34it/s]100%|██████████| 15/15 [00:01<00:00, 13.87it/s]
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6766, 'grad_norm': 0.17883045971393585, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.93it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.24it/s]                                              {'loss': 0.654, 'grad_norm': 0.4842682480812073, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.24it/s]                                              {'loss': 0.6631, 'grad_norm': 0.46928080916404724, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.24it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6785, 'grad_norm': 0.30821722745895386, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6503, 'grad_norm': 0.5944380760192871, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.32it/s] 40%|████      | 6/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6715, 'grad_norm': 0.2867545485496521, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6338, 'grad_norm': 0.6444522738456726, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.16it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.04it/s]                                              {'loss': 0.6627, 'grad_norm': 0.25340133905410767, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.04it/s]                                              {'loss': 0.682, 'grad_norm': 0.19312834739685059, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.04it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.6636, 'grad_norm': 0.3288881778717041, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.6131, 'grad_norm': 0.4380910098552704, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.59it/s] 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.6359, 'grad_norm': 0.5244131684303284, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.6653, 'grad_norm': 0.29260143637657166, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.35it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6261, 'grad_norm': 0.49555501341819763, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6123, 'grad_norm': 0.5256183743476868, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.82it/s]                                               {'train_runtime': 1.0756, 'train_samples_per_second': 395.135, 'train_steps_per_second': 13.946, 'train_loss': 0.6525742133458455, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.82it/s]100%|██████████| 15/15 [00:01<00:00, 13.95it/s]
CLIENT:58
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.666, 'grad_norm': 0.42859670519828796, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.11it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.91it/s]                                              {'loss': 0.6469, 'grad_norm': 0.5883218050003052, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.91it/s]                                              {'loss': 0.6652, 'grad_norm': 0.27298128604888916, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.91it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.6722, 'grad_norm': 0.4085560142993927, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.6441, 'grad_norm': 0.5976135730743408, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.15it/s] 40%|████      | 6/15 [00:00<00:00, 16.44it/s]                                              {'loss': 0.653, 'grad_norm': 0.5018536448478699, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.44it/s]                                              {'loss': 0.6355, 'grad_norm': 0.5563006401062012, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.44it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.92it/s]                                              {'loss': 0.6036, 'grad_norm': 0.5997602343559265, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.92it/s]                                              {'loss': 0.6751, 'grad_norm': 0.26920560002326965, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.92it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.6541, 'grad_norm': 0.3493328392505646, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.6064, 'grad_norm': 0.5675376057624817, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.59it/s] 80%|████████  | 12/15 [00:00<00:00, 15.90it/s]                                               {'loss': 0.5602, 'grad_norm': 0.8428265452384949, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.90it/s]                                               {'loss': 0.5733, 'grad_norm': 0.688094437122345, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.90it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.5861, 'grad_norm': 0.6300162672996521, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.6903, 'grad_norm': 0.2802767753601074, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.03it/s]                                               {'train_runtime': 1.0364, 'train_samples_per_second': 410.066, 'train_steps_per_second': 14.473, 'train_loss': 0.635472055276235, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.03it/s]100%|██████████| 15/15 [00:01<00:00, 14.48it/s]
CLIENT:2
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6842, 'grad_norm': 0.23984606564044952, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.18it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.91it/s]                                              {'loss': 0.6638, 'grad_norm': 0.42133739590644836, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.91it/s]                                              {'loss': 0.6683, 'grad_norm': 0.3835156559944153, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.91it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.6603, 'grad_norm': 0.5016912221908569, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.6872, 'grad_norm': 0.21647503972053528, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.51it/s] 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.677, 'grad_norm': 0.19196565449237823, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.6709, 'grad_norm': 0.288884699344635, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.59it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.6657, 'grad_norm': 0.3139241337776184, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.6342, 'grad_norm': 0.4880065619945526, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.62it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.629, 'grad_norm': 0.5886228680610657, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.68, 'grad_norm': 0.21970249712467194, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.08it/s] 80%|████████  | 12/15 [00:00<00:00, 15.44it/s]                                               {'loss': 0.6747, 'grad_norm': 0.2067149132490158, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.44it/s]                                               {'loss': 0.6562, 'grad_norm': 0.2751941382884979, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.44it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6512, 'grad_norm': 0.3350779116153717, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6494, 'grad_norm': 0.43276554346084595, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.48it/s]                                               {'train_runtime': 1.0694, 'train_samples_per_second': 397.434, 'train_steps_per_second': 14.027, 'train_loss': 0.6634723623593648, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]100%|██████████| 15/15 [00:01<00:00, 14.03it/s]
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6597, 'grad_norm': 0.4730975329875946, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.19it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.6838, 'grad_norm': 0.1329200714826584, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.6817, 'grad_norm': 0.21217067539691925, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.15it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.54it/s]                                              {'loss': 0.693, 'grad_norm': 0.18780413269996643, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.54it/s]                                              {'loss': 0.6689, 'grad_norm': 0.4106048047542572, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.54it/s] 40%|████      | 6/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.6646, 'grad_norm': 0.21826697885990143, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.658, 'grad_norm': 0.37538382411003113, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.88it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.6711, 'grad_norm': 0.230819433927536, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.6619, 'grad_norm': 0.34689339995384216, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.40it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.623, 'grad_norm': 0.7275116443634033, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.7016, 'grad_norm': 0.19195449352264404, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.65it/s] 80%|████████  | 12/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.6747, 'grad_norm': 0.23132577538490295, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.6414, 'grad_norm': 0.41301313042640686, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.14it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.27it/s]                                               {'loss': 0.6621, 'grad_norm': 0.23167364299297333, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.27it/s]                                               {'loss': 0.66, 'grad_norm': 0.20508038997650146, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.27it/s]                                               {'train_runtime': 1.0702, 'train_samples_per_second': 397.117, 'train_steps_per_second': 14.016, 'train_loss': 0.6670323173205058, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.27it/s]100%|██████████| 15/15 [00:01<00:00, 14.02it/s]
CLIENT:84
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6736, 'grad_norm': 0.3560369908809662, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.71it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.87it/s]                                              {'loss': 0.6811, 'grad_norm': 0.2027662992477417, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.87it/s]                                              {'loss': 0.676, 'grad_norm': 0.2273273915052414, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.87it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.6686, 'grad_norm': 0.4007928967475891, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.6811, 'grad_norm': 0.13522346317768097, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.49it/s] 40%|████      | 6/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.6698, 'grad_norm': 0.3280773460865021, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.6861, 'grad_norm': 0.14977866411209106, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.33it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.6579, 'grad_norm': 0.2985999584197998, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.6561, 'grad_norm': 0.38185685873031616, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.24it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.6751, 'grad_norm': 0.1917588859796524, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.6598, 'grad_norm': 0.1927129030227661, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.18it/s] 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.6487, 'grad_norm': 0.5938950181007385, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.6477, 'grad_norm': 0.3151274025440216, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.56it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.6506, 'grad_norm': 0.2204822599887848, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.675, 'grad_norm': 0.29383131861686707, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.47it/s]                                               {'train_runtime': 1.0752, 'train_samples_per_second': 395.271, 'train_steps_per_second': 13.951, 'train_loss': 0.6671416282653808, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.47it/s]100%|██████████| 15/15 [00:01<00:00, 13.96it/s]
CLIENT:54
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6729, 'grad_norm': 0.45555949211120605, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.16it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.54it/s]                                              {'loss': 0.6865, 'grad_norm': 0.20974542200565338, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.54it/s]                                              {'loss': 0.6724, 'grad_norm': 0.2466401606798172, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.54it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.6637, 'grad_norm': 0.366435170173645, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.6957, 'grad_norm': 0.19262324273586273, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.85it/s] 40%|████      | 6/15 [00:00<00:00, 15.54it/s]                                              {'loss': 0.6572, 'grad_norm': 0.4730178415775299, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.54it/s]                                              {'loss': 0.6435, 'grad_norm': 0.4899234175682068, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.54it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6679, 'grad_norm': 0.20318785309791565, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6747, 'grad_norm': 0.2892700135707855, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.32it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.6477, 'grad_norm': 0.33150729537010193, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.6614, 'grad_norm': 0.2723681330680847, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.87it/s] 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.642, 'grad_norm': 0.4208146929740906, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6349, 'grad_norm': 0.37753117084503174, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.41it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6452, 'grad_norm': 0.36320626735687256, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6641, 'grad_norm': 0.19375808537006378, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.48it/s]                                               {'train_runtime': 1.072, 'train_samples_per_second': 396.443, 'train_steps_per_second': 13.992, 'train_loss': 0.6619815866152445, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]100%|██████████| 15/15 [00:01<00:00, 14.00it/s]
CLIENT:75
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6821, 'grad_norm': 0.1577358990907669, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.87it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.55it/s]                                              {'loss': 0.6723, 'grad_norm': 0.4792565107345581, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.55it/s]                                              {'loss': 0.6775, 'grad_norm': 0.3442097306251526, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.55it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.6878, 'grad_norm': 0.19198182225227356, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.669, 'grad_norm': 0.5035153031349182, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.24it/s] 40%|████      | 6/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.6666, 'grad_norm': 0.30564385652542114, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.6497, 'grad_norm': 0.498515784740448, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.19it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6554, 'grad_norm': 0.245793879032135, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6708, 'grad_norm': 0.25031518936157227, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.10it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.671, 'grad_norm': 0.2405950278043747, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.6526, 'grad_norm': 0.2897050380706787, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.98it/s] 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6191, 'grad_norm': 0.5277020335197449, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.672, 'grad_norm': 0.23318670690059662, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.31it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.35it/s]                                               {'loss': 0.6408, 'grad_norm': 0.38158172369003296, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.35it/s]                                               {'loss': 0.6669, 'grad_norm': 0.28347739577293396, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.35it/s]                                               {'train_runtime': 1.0856, 'train_samples_per_second': 391.504, 'train_steps_per_second': 13.818, 'train_loss': 0.6635671138763428, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.35it/s]100%|██████████| 15/15 [00:01<00:00, 13.82it/s]
CLIENT:28
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.682, 'grad_norm': 0.2503909170627594, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.23it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.25it/s]                                              {'loss': 0.6681, 'grad_norm': 0.3986615836620331, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.25it/s]                                              {'loss': 0.6695, 'grad_norm': 0.4775117337703705, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.25it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.15it/s]                                              {'loss': 0.6837, 'grad_norm': 0.2895776629447937, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.15it/s]                                              {'loss': 0.6637, 'grad_norm': 0.33393436670303345, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.15it/s] 40%|████      | 6/15 [00:00<00:00, 16.30it/s]                                              {'loss': 0.6681, 'grad_norm': 0.4293828308582306, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.30it/s]                                              {'loss': 0.6739, 'grad_norm': 0.367001473903656, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.30it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.6248, 'grad_norm': 0.5401520133018494, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.6773, 'grad_norm': 0.1880876123905182, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.64it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.6582, 'grad_norm': 0.29655134677886963, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.6296, 'grad_norm': 0.48407620191574097, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.40it/s] 80%|████████  | 12/15 [00:00<00:00, 15.72it/s]                                               {'loss': 0.6753, 'grad_norm': 0.27355289459228516, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.72it/s]                                               {'loss': 0.6541, 'grad_norm': 0.31910866498947144, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.72it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.607, 'grad_norm': 0.6386505961418152, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6965, 'grad_norm': 0.2543202340602875, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.55it/s]                                               {'train_runtime': 1.0462, 'train_samples_per_second': 406.232, 'train_steps_per_second': 14.338, 'train_loss': 0.6621173620223999, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.55it/s]100%|██████████| 15/15 [00:01<00:00, 14.34it/s]
CLIENT:40
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6684, 'grad_norm': 0.3475539982318878, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.20it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.6664, 'grad_norm': 0.2681719958782196, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.6647, 'grad_norm': 0.46773621439933777, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.21it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.93it/s]                                              {'loss': 0.6775, 'grad_norm': 0.3150642514228821, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.93it/s]                                              {'loss': 0.6737, 'grad_norm': 0.3919730484485626, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.93it/s] 40%|████      | 6/15 [00:00<00:00, 16.54it/s]                                              {'loss': 0.6609, 'grad_norm': 0.30433306097984314, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.54it/s]                                              {'loss': 0.6474, 'grad_norm': 0.3256970942020416, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.54it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.6634, 'grad_norm': 0.30775338411331177, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.6403, 'grad_norm': 0.4805089831352234, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.63it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.628, 'grad_norm': 0.59102863073349, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.6961, 'grad_norm': 0.15950140357017517, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.08it/s] 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.6268, 'grad_norm': 0.6048839092254639, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.6277, 'grad_norm': 0.3262300193309784, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.48it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.617, 'grad_norm': 0.403429239988327, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.6489, 'grad_norm': 0.27410605549812317, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.78it/s]                                               {'train_runtime': 1.0426, 'train_samples_per_second': 407.621, 'train_steps_per_second': 14.387, 'train_loss': 0.6538126627604167, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.78it/s]100%|██████████| 15/15 [00:01<00:00, 14.39it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 36.56it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.59it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.70it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.79it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.25it/s] 61%|██████    | 20/33 [00:00<00:00, 25.17it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.27it/s] 79%|███████▉  | 26/33 [00:01<00:00, 24.98it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.13it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.16it/s]100%|██████████| 33/33 [00:01<00:00, 26.06it/s]
{'eval_loss': 0.6642216444015503, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3084, 'eval_samples_per_second': 797.14, 'eval_steps_per_second': 25.221}
ROUND:15
CLIENT:2
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6762, 'grad_norm': 0.24132056534290314, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.00it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.6469, 'grad_norm': 0.44610413908958435, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.6539, 'grad_norm': 0.3962394595146179, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.16it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.6385, 'grad_norm': 0.5504826307296753, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.6802, 'grad_norm': 0.2336144894361496, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.30it/s] 40%|████      | 6/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.6715, 'grad_norm': 0.2023499608039856, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.6601, 'grad_norm': 0.2725200653076172, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.88it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.54it/s]                                              {'loss': 0.6526, 'grad_norm': 0.32711362838745117, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.54it/s]                                              {'loss': 0.6133, 'grad_norm': 0.5030851364135742, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.54it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.6025, 'grad_norm': 0.6115484237670898, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.6758, 'grad_norm': 0.1921546757221222, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.22it/s] 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.6697, 'grad_norm': 0.2038872092962265, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.6456, 'grad_norm': 0.29648154973983765, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.56it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.50it/s]                                               {'loss': 0.6375, 'grad_norm': 0.34883999824523926, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.50it/s]                                               {'loss': 0.633, 'grad_norm': 0.38718587160110474, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.50it/s]                                               {'train_runtime': 1.0565, 'train_samples_per_second': 402.282, 'train_steps_per_second': 14.198, 'train_loss': 0.6504843831062317, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.50it/s]100%|██████████| 15/15 [00:01<00:00, 14.20it/s]
CLIENT:49
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6635, 'grad_norm': 0.40914055705070496, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.96it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.25it/s]                                              {'loss': 0.6756, 'grad_norm': 0.24979087710380554, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.25it/s]                                              {'loss': 0.6372, 'grad_norm': 0.6575536131858826, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.25it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.6487, 'grad_norm': 0.46308663487434387, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.6492, 'grad_norm': 0.3437961935997009, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.41it/s] 40%|████      | 6/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.6409, 'grad_norm': 0.4325893521308899, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.6202, 'grad_norm': 0.5110445022583008, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.24it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.12it/s]                                              {'loss': 0.6396, 'grad_norm': 0.3753398656845093, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.12it/s]                                              {'loss': 0.6436, 'grad_norm': 0.3005783259868622, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.12it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.5722, 'grad_norm': 0.6322227120399475, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.6336, 'grad_norm': 0.2704133987426758, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.69it/s] 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6627, 'grad_norm': 0.3473376929759979, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6343, 'grad_norm': 0.3030615746974945, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.32it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.5893, 'grad_norm': 0.47773081064224243, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6043, 'grad_norm': 0.36897769570350647, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]                                               {'train_runtime': 1.0788, 'train_samples_per_second': 393.946, 'train_steps_per_second': 13.904, 'train_loss': 0.6343205730120342, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 13.91it/s]
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6677, 'grad_norm': 0.2533383071422577, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.03it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.31it/s]                                              {'loss': 0.6787, 'grad_norm': 0.248810276389122, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.31it/s]                                              {'loss': 0.6299, 'grad_norm': 0.772014856338501, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.31it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.6488, 'grad_norm': 0.5417890548706055, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.6406, 'grad_norm': 0.38802996277809143, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.37it/s] 40%|████      | 6/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.6572, 'grad_norm': 0.2672586739063263, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.6441, 'grad_norm': 0.2463177591562271, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.18it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.05it/s]                                              {'loss': 0.6324, 'grad_norm': 0.43068015575408936, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.05it/s]                                              {'loss': 0.6114, 'grad_norm': 0.47109749913215637, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.05it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.6457, 'grad_norm': 0.2882416546344757, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.6144, 'grad_norm': 0.3408547043800354, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.52it/s] 80%|████████  | 12/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.6238, 'grad_norm': 0.41883182525634766, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.6381, 'grad_norm': 0.3370155394077301, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.25it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.595, 'grad_norm': 0.45593327283859253, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6378, 'grad_norm': 0.25769945979118347, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]                                               {'train_runtime': 1.081, 'train_samples_per_second': 393.153, 'train_steps_per_second': 13.876, 'train_loss': 0.637714417775472, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]100%|██████████| 15/15 [00:01<00:00, 13.88it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6737, 'grad_norm': 0.2469886839389801, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.29it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.92it/s]                                              {'loss': 0.6512, 'grad_norm': 0.4602455496788025, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.92it/s]                                              {'loss': 0.6455, 'grad_norm': 0.4453390836715698, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.92it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.6822, 'grad_norm': 0.2011512964963913, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.6191, 'grad_norm': 0.7373989224433899, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.28it/s] 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.6652, 'grad_norm': 0.2696988582611084, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.6099, 'grad_norm': 0.5552054643630981, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.22it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.52it/s]                                              {'loss': 0.6542, 'grad_norm': 0.2760789096355438, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.52it/s]                                              {'loss': 0.6379, 'grad_norm': 0.38185423612594604, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.52it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.6417, 'grad_norm': 0.4105416536331177, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.6021, 'grad_norm': 0.41188591718673706, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.72it/s] 80%|████████  | 12/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.6385, 'grad_norm': 0.3147560656070709, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.6239, 'grad_norm': 0.33750078082084656, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.23it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.28it/s]                                               {'loss': 0.6148, 'grad_norm': 0.41722944378852844, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.28it/s]                                               {'loss': 0.6163, 'grad_norm': 0.3318522274494171, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.28it/s]                                               {'train_runtime': 1.0842, 'train_samples_per_second': 391.995, 'train_steps_per_second': 13.835, 'train_loss': 0.638418730099996, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.28it/s]100%|██████████| 15/15 [00:01<00:00, 13.84it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6693, 'grad_norm': 0.2669231593608856, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.81it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.98it/s]                                              {'loss': 0.6352, 'grad_norm': 0.5044909119606018, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.98it/s]                                              {'loss': 0.6282, 'grad_norm': 0.6677998304367065, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.98it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.6652, 'grad_norm': 0.2889884114265442, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.6209, 'grad_norm': 0.6307535767555237, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.01it/s] 40%|████      | 6/15 [00:00<00:00, 15.91it/s]                                              {'loss': 0.6418, 'grad_norm': 0.4640423059463501, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.91it/s]                                              {'loss': 0.6139, 'grad_norm': 0.6664478182792664, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.91it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.6332, 'grad_norm': 0.3227054178714752, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.6228, 'grad_norm': 0.35189032554626465, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.56it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.6173, 'grad_norm': 0.4601932168006897, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.6008, 'grad_norm': 0.41643038392066956, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.33it/s] 80%|████████  | 12/15 [00:00<00:00, 15.72it/s]                                               {'loss': 0.5845, 'grad_norm': 0.49562275409698486, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.72it/s]                                               {'loss': 0.643, 'grad_norm': 0.19430790841579437, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.72it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.5688, 'grad_norm': 0.5619194507598877, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.5401, 'grad_norm': 0.6845071911811829, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.93it/s]                                               {'train_runtime': 1.0436, 'train_samples_per_second': 407.236, 'train_steps_per_second': 14.373, 'train_loss': 0.6189932425816854, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.93it/s]100%|██████████| 15/15 [00:01<00:00, 14.38it/s]
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6613, 'grad_norm': 0.4293327033519745, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.08it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.56it/s]                                              {'loss': 0.6625, 'grad_norm': 0.4152091443538666, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.56it/s]                                              {'loss': 0.6679, 'grad_norm': 0.3799121081829071, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.56it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.93it/s]                                              {'loss': 0.6538, 'grad_norm': 0.35683032870292664, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.93it/s]                                              {'loss': 0.6476, 'grad_norm': 0.4249967038631439, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.93it/s] 40%|████      | 6/15 [00:00<00:00, 15.77it/s]                                              {'loss': 0.6559, 'grad_norm': 0.37827035784721375, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.77it/s]                                              {'loss': 0.6753, 'grad_norm': 0.17680203914642334, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.77it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.6038, 'grad_norm': 0.5480461120605469, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.6154, 'grad_norm': 0.5584392547607422, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.28it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.6385, 'grad_norm': 0.31679534912109375, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.6335, 'grad_norm': 0.3640235364437103, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.73it/s] 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.5948, 'grad_norm': 0.5274174809455872, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6002, 'grad_norm': 0.45293837785720825, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.31it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.08it/s]                                               {'loss': 0.6404, 'grad_norm': 0.29642975330352783, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.08it/s]                                               {'loss': 0.6199, 'grad_norm': 0.4445871114730835, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.08it/s]                                               {'train_runtime': 1.0834, 'train_samples_per_second': 392.293, 'train_steps_per_second': 13.846, 'train_loss': 0.6380607684453329, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.08it/s]100%|██████████| 15/15 [00:01<00:00, 13.85it/s]
CLIENT:87
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6821, 'grad_norm': 0.23260118067264557, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.97it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.71it/s]                                              {'loss': 0.6605, 'grad_norm': 0.3485403060913086, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.71it/s]                                              {'loss': 0.7019, 'grad_norm': 0.20213547348976135, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.71it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6772, 'grad_norm': 0.1775585114955902, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6927, 'grad_norm': 0.16057533025741577, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.06it/s] 40%|████      | 6/15 [00:00<00:00, 14.98it/s]                                              {'loss': 0.6535, 'grad_norm': 0.35794422030448914, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 14.98it/s]                                              {'loss': 0.71, 'grad_norm': 0.18363359570503235, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 14.98it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.95it/s]                                              {'loss': 0.6603, 'grad_norm': 0.3473236560821533, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.95it/s]                                              {'loss': 0.6728, 'grad_norm': 0.1988585740327835, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.95it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.6647, 'grad_norm': 0.23692767322063446, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.7021, 'grad_norm': 0.1820446401834488, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.52it/s] 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6538, 'grad_norm': 0.3026317059993744, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6971, 'grad_norm': 0.15301711857318878, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.29it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.6942, 'grad_norm': 0.2316674143075943, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.591, 'grad_norm': 0.8242812156677246, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.61it/s]                                               {'train_runtime': 1.0859, 'train_samples_per_second': 391.394, 'train_steps_per_second': 13.814, 'train_loss': 0.6742745359738668, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.61it/s]100%|██████████| 15/15 [00:01<00:00, 13.82it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6464, 'grad_norm': 0.31962499022483826, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.10it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.6437, 'grad_norm': 0.6133753061294556, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.692, 'grad_norm': 0.21338561177253723, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.56it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.58it/s]                                              {'loss': 0.649, 'grad_norm': 0.2591957449913025, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.58it/s]                                              {'loss': 0.6532, 'grad_norm': 0.3680451214313507, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.58it/s] 40%|████      | 6/15 [00:00<00:00, 16.26it/s]                                              {'loss': 0.6387, 'grad_norm': 0.4830481708049774, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.26it/s]                                              {'loss': 0.6289, 'grad_norm': 0.4609386920928955, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.26it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.73it/s]                                              {'loss': 0.636, 'grad_norm': 0.3724897801876068, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.73it/s]                                              {'loss': 0.6718, 'grad_norm': 0.20349951088428497, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.73it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.06it/s]                                               {'loss': 0.659, 'grad_norm': 0.24547754228115082, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.06it/s]                                               {'loss': 0.6144, 'grad_norm': 0.4075951874256134, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.06it/s] 80%|████████  | 12/15 [00:00<00:00, 15.68it/s]                                               {'loss': 0.6346, 'grad_norm': 0.3751607835292816, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.68it/s]                                               {'loss': 0.6321, 'grad_norm': 0.26968619227409363, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.68it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.6203, 'grad_norm': 0.3960252106189728, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.6252, 'grad_norm': 0.26369723677635193, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.86it/s]                                               {'train_runtime': 1.0457, 'train_samples_per_second': 406.417, 'train_steps_per_second': 14.344, 'train_loss': 0.6430222670237223, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.86it/s]100%|██████████| 15/15 [00:01<00:00, 14.35it/s]
CLIENT:99
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/25 [00:00<?, ?it/s]                                      {'loss': 0.6614, 'grad_norm': 0.3605520725250244, 'learning_rate': 0.01, 'epoch': 0.2}
  4%|▍         | 1/25 [00:00<00:01, 14.16it/s]  8%|▊         | 2/25 [00:00<00:01, 14.87it/s]                                              {'loss': 0.6334, 'grad_norm': 0.5637686252593994, 'learning_rate': 0.0096, 'epoch': 0.4}
  8%|▊         | 2/25 [00:00<00:01, 14.87it/s]                                              {'loss': 0.6475, 'grad_norm': 0.5581161975860596, 'learning_rate': 0.0092, 'epoch': 0.6}
 12%|█▏        | 3/25 [00:00<00:01, 14.87it/s] 16%|█▌        | 4/25 [00:00<00:01, 13.98it/s]                                              {'loss': 0.634, 'grad_norm': 0.5345308780670166, 'learning_rate': 0.0088, 'epoch': 0.8}
 16%|█▌        | 4/25 [00:00<00:01, 13.98it/s]                                              {'loss': 0.703, 'grad_norm': 0.3375391960144043, 'learning_rate': 0.0084, 'epoch': 1.0}
 20%|██        | 5/25 [00:00<00:01, 13.98it/s] 24%|██▍       | 6/25 [00:00<00:01, 16.19it/s]                                              {'loss': 0.6388, 'grad_norm': 0.39357227087020874, 'learning_rate': 0.008, 'epoch': 1.2}
 24%|██▍       | 6/25 [00:00<00:01, 16.19it/s]                                              {'loss': 0.6331, 'grad_norm': 0.35857662558555603, 'learning_rate': 0.0076, 'epoch': 1.4}
 28%|██▊       | 7/25 [00:00<00:01, 16.19it/s] 32%|███▏      | 8/25 [00:00<00:01, 14.58it/s]                                              {'loss': 0.6103, 'grad_norm': 0.4549035131931305, 'learning_rate': 0.0072, 'epoch': 1.6}
 32%|███▏      | 8/25 [00:00<00:01, 14.58it/s]                                              {'loss': 0.6147, 'grad_norm': 0.5331133604049683, 'learning_rate': 0.0068000000000000005, 'epoch': 1.8}
 36%|███▌      | 9/25 [00:00<00:01, 14.58it/s]                                              {'loss': 0.5508, 'grad_norm': 0.675661563873291, 'learning_rate': 0.0064, 'epoch': 2.0}
 40%|████      | 10/25 [00:00<00:01, 14.58it/s] 44%|████▍     | 11/25 [00:00<00:00, 15.96it/s]                                               {'loss': 0.5986, 'grad_norm': 0.44730257987976074, 'learning_rate': 0.006, 'epoch': 2.2}
 44%|████▍     | 11/25 [00:00<00:00, 15.96it/s]                                               {'loss': 0.6931, 'grad_norm': 0.24914705753326416, 'learning_rate': 0.005600000000000001, 'epoch': 2.4}
 48%|████▊     | 12/25 [00:00<00:00, 15.96it/s] 52%|█████▏    | 13/25 [00:00<00:00, 14.81it/s]                                               {'loss': 0.5762, 'grad_norm': 0.44546324014663696, 'learning_rate': 0.005200000000000001, 'epoch': 2.6}
 52%|█████▏    | 13/25 [00:00<00:00, 14.81it/s]                                               {'loss': 0.5393, 'grad_norm': 0.5898232460021973, 'learning_rate': 0.0048, 'epoch': 2.8}
 56%|█████▌    | 14/25 [00:00<00:00, 14.81it/s]                                               {'loss': 0.4474, 'grad_norm': 1.0670902729034424, 'learning_rate': 0.0044, 'epoch': 3.0}
 60%|██████    | 15/25 [00:00<00:00, 14.81it/s] 64%|██████▍   | 16/25 [00:01<00:00, 15.56it/s]                                               {'loss': 0.5847, 'grad_norm': 0.32255345582962036, 'learning_rate': 0.004, 'epoch': 3.2}
 64%|██████▍   | 16/25 [00:01<00:00, 15.56it/s]                                               {'loss': 0.5524, 'grad_norm': 0.378385066986084, 'learning_rate': 0.0036, 'epoch': 3.4}
 68%|██████▊   | 17/25 [00:01<00:00, 15.56it/s] 72%|███████▏  | 18/25 [00:01<00:00, 14.64it/s]                                               {'loss': 0.5449, 'grad_norm': 0.3790234327316284, 'learning_rate': 0.0032, 'epoch': 3.6}
 72%|███████▏  | 18/25 [00:01<00:00, 14.64it/s]                                               {'loss': 0.5171, 'grad_norm': 0.46750858426094055, 'learning_rate': 0.0028000000000000004, 'epoch': 3.8}
 76%|███████▌  | 19/25 [00:01<00:00, 14.64it/s]                                               {'loss': 0.8692, 'grad_norm': 1.1402348279953003, 'learning_rate': 0.0024, 'epoch': 4.0}
 80%|████████  | 20/25 [00:01<00:00, 14.64it/s] 84%|████████▍ | 21/25 [00:01<00:00, 15.29it/s]                                               {'loss': 0.665, 'grad_norm': 0.3005339801311493, 'learning_rate': 0.002, 'epoch': 4.2}
 84%|████████▍ | 21/25 [00:01<00:00, 15.29it/s]                                               {'loss': 0.558, 'grad_norm': 0.29725340008735657, 'learning_rate': 0.0016, 'epoch': 4.4}
 88%|████████▊ | 22/25 [00:01<00:00, 15.29it/s] 92%|█████████▏| 23/25 [00:01<00:00, 14.46it/s]                                               {'loss': 0.4899, 'grad_norm': 0.483493447303772, 'learning_rate': 0.0012, 'epoch': 4.6}
 92%|█████████▏| 23/25 [00:01<00:00, 14.46it/s]                                               {'loss': 0.5145, 'grad_norm': 0.44047853350639343, 'learning_rate': 0.0008, 'epoch': 4.8}
 96%|█████████▌| 24/25 [00:01<00:00, 14.46it/s]                                               {'loss': 0.5453, 'grad_norm': 0.47363826632499695, 'learning_rate': 0.0004, 'epoch': 5.0}
100%|██████████| 25/25 [00:01<00:00, 14.46it/s]                                               {'train_runtime': 1.7113, 'train_samples_per_second': 397.356, 'train_steps_per_second': 14.609, 'train_loss': 0.6008932781219483, 'epoch': 5.0}
100%|██████████| 25/25 [00:01<00:00, 14.46it/s]100%|██████████| 25/25 [00:01<00:00, 14.61it/s]
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6525, 'grad_norm': 0.3976685702800751, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.24it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.6851, 'grad_norm': 0.19353748857975006, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.6318, 'grad_norm': 0.6320998072624207, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.97it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.648, 'grad_norm': 0.46894270181655884, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.6613, 'grad_norm': 0.2262045443058014, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.60it/s] 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.6576, 'grad_norm': 0.2584673762321472, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.6548, 'grad_norm': 0.20396357774734497, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.59it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6216, 'grad_norm': 0.5075353980064392, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.657, 'grad_norm': 0.3266424238681793, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.32it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.6465, 'grad_norm': 0.24299414455890656, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.6312, 'grad_norm': 0.29133105278015137, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.15it/s] 80%|████████  | 12/15 [00:00<00:00, 15.76it/s]                                               {'loss': 0.6234, 'grad_norm': 0.42357945442199707, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.76it/s]                                               {'loss': 0.6255, 'grad_norm': 0.36361831426620483, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.76it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6459, 'grad_norm': 0.2653820812702179, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6412, 'grad_norm': 0.28324034810066223, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.55it/s]                                               {'train_runtime': 1.0674, 'train_samples_per_second': 398.155, 'train_steps_per_second': 14.053, 'train_loss': 0.6455581823984782, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.55it/s]100%|██████████| 15/15 [00:01<00:00, 14.06it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 34.30it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.10it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.44it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.51it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.52it/s] 61%|██████    | 20/33 [00:00<00:00, 25.42it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.15it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.27it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.05it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.20it/s]100%|██████████| 33/33 [00:01<00:00, 25.99it/s]
{'eval_loss': 0.648061215877533, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3112, 'eval_samples_per_second': 795.45, 'eval_steps_per_second': 25.168}
ROUND:16
CLIENT:7
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6545, 'grad_norm': 0.2965603470802307, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.34it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.87it/s]                                              {'loss': 0.6805, 'grad_norm': 0.21997828781604767, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.87it/s]                                              {'loss': 0.632, 'grad_norm': 0.5023599863052368, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.87it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.54it/s]                                              {'loss': 0.6319, 'grad_norm': 0.375041663646698, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.54it/s]                                              {'loss': 0.646, 'grad_norm': 0.35505586862564087, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.54it/s] 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.6786, 'grad_norm': 0.3265305161476135, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.6259, 'grad_norm': 0.3649316430091858, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.48it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.6547, 'grad_norm': 0.22034062445163727, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.646, 'grad_norm': 0.27442729473114014, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.29it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.632, 'grad_norm': 0.30290356278419495, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.6536, 'grad_norm': 0.24122053384780884, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.74it/s] 80%|████████  | 12/15 [00:00<00:00, 15.44it/s]                                               {'loss': 0.6403, 'grad_norm': 0.32183361053466797, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.44it/s]                                               {'loss': 0.666, 'grad_norm': 0.204567089676857, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.44it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6038, 'grad_norm': 0.404641330242157, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6237, 'grad_norm': 0.2987558841705322, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]                                               {'train_runtime': 1.0798, 'train_samples_per_second': 393.604, 'train_steps_per_second': 13.892, 'train_loss': 0.6446452538172404, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 13.90it/s]
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6407, 'grad_norm': 0.42769309878349304, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.04it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.72it/s]                                              {'loss': 0.6434, 'grad_norm': 0.35234880447387695, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.72it/s]                                              {'loss': 0.6766, 'grad_norm': 0.19668085873126984, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.72it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.6229, 'grad_norm': 0.5373203754425049, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.6624, 'grad_norm': 0.195823073387146, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.19it/s] 40%|████      | 6/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.6619, 'grad_norm': 0.23353223502635956, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.6241, 'grad_norm': 0.3851660490036011, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.09it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.01it/s]                                              {'loss': 0.6731, 'grad_norm': 0.21108955144882202, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.01it/s]                                              {'loss': 0.6191, 'grad_norm': 0.41564249992370605, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.01it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.6183, 'grad_norm': 0.36341118812561035, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.6727, 'grad_norm': 0.26721715927124023, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.57it/s] 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6528, 'grad_norm': 0.2851896286010742, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6355, 'grad_norm': 0.2133549451828003, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.31it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.6634, 'grad_norm': 0.18816033005714417, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.5762, 'grad_norm': 0.530829906463623, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.70it/s]                                               {'train_runtime': 1.0836, 'train_samples_per_second': 392.198, 'train_steps_per_second': 13.842, 'train_loss': 0.6428735295931498, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.70it/s]100%|██████████| 15/15 [00:01<00:00, 13.85it/s]
CLIENT:71
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6363, 'grad_norm': 0.3183571696281433, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.19it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.6963, 'grad_norm': 0.19945847988128662, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.6795, 'grad_norm': 0.20932498574256897, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.97it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.6358, 'grad_norm': 0.3927125632762909, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.7217, 'grad_norm': 0.40226221084594727, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.21it/s] 40%|████      | 6/15 [00:00<00:00, 16.45it/s]                                              {'loss': 0.6337, 'grad_norm': 0.4561021625995636, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.45it/s]                                              {'loss': 0.6941, 'grad_norm': 0.2205822765827179, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.45it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.81it/s]                                              {'loss': 0.6484, 'grad_norm': 0.21812337636947632, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.81it/s]                                              {'loss': 0.6548, 'grad_norm': 0.24935822188854218, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.81it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.6633, 'grad_norm': 0.23085390031337738, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.667, 'grad_norm': 0.16925425827503204, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.16it/s] 80%|████████  | 12/15 [00:00<00:00, 15.76it/s]                                               {'loss': 0.6859, 'grad_norm': 0.23247186839580536, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.76it/s]                                               {'loss': 0.6495, 'grad_norm': 0.2796548306941986, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.76it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.7112, 'grad_norm': 0.2438814342021942, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.6805, 'grad_norm': 0.23439325392246246, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.84it/s]                                               {'train_runtime': 1.0498, 'train_samples_per_second': 404.856, 'train_steps_per_second': 14.289, 'train_loss': 0.6705382227897644, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.84it/s]100%|██████████| 15/15 [00:01<00:00, 14.29it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6317, 'grad_norm': 0.3065625727176666, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.74it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.50it/s]                                              {'loss': 0.6096, 'grad_norm': 0.6955384016036987, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.50it/s]                                              {'loss': 0.6987, 'grad_norm': 0.2950572371482849, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.50it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.6379, 'grad_norm': 0.2516832649707794, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.6352, 'grad_norm': 0.3885538578033447, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.18it/s] 40%|████      | 6/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.6155, 'grad_norm': 0.4652649760246277, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.6049, 'grad_norm': 0.4668363630771637, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.17it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.6191, 'grad_norm': 0.33687591552734375, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.6702, 'grad_norm': 0.22975362837314606, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.91it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6494, 'grad_norm': 0.2143339365720749, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.5953, 'grad_norm': 0.3793097734451294, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.41it/s] 80%|████████  | 12/15 [00:00<00:00, 15.72it/s]                                               {'loss': 0.6192, 'grad_norm': 0.3431330621242523, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.72it/s]                                               {'loss': 0.6233, 'grad_norm': 0.2484787553548813, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.72it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.602, 'grad_norm': 0.38165074586868286, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.6178, 'grad_norm': 0.2429712563753128, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.59it/s]                                               {'train_runtime': 1.0664, 'train_samples_per_second': 398.554, 'train_steps_per_second': 14.067, 'train_loss': 0.6286688685417176, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.59it/s]100%|██████████| 15/15 [00:01<00:00, 14.07it/s]
CLIENT:47
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.637, 'grad_norm': 0.40716344118118286, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.37it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.00it/s]                                              {'loss': 0.6363, 'grad_norm': 0.45280176401138306, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.00it/s]                                              {'loss': 0.6695, 'grad_norm': 0.2509538531303406, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.00it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.6846, 'grad_norm': 0.16230453550815582, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.599, 'grad_norm': 0.6543174982070923, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.66it/s] 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.64, 'grad_norm': 0.3016095757484436, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.6421, 'grad_norm': 0.25335127115249634, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.50it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.6107, 'grad_norm': 0.37074044346809387, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.6233, 'grad_norm': 0.3268381357192993, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.72it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.6558, 'grad_norm': 0.2767055034637451, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.5934, 'grad_norm': 0.4413522779941559, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.26it/s] 80%|████████  | 12/15 [00:00<00:00, 15.72it/s]                                               {'loss': 0.6553, 'grad_norm': 0.24236039817333221, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.72it/s]                                               {'loss': 0.5942, 'grad_norm': 0.4468936622142792, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.72it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.6493, 'grad_norm': 0.25410154461860657, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.5975, 'grad_norm': 0.32232874631881714, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.34it/s]                                               {'train_runtime': 1.0712, 'train_samples_per_second': 396.743, 'train_steps_per_second': 14.003, 'train_loss': 0.6325403372446696, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.34it/s]100%|██████████| 15/15 [00:01<00:00, 14.01it/s]
CLIENT:29
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6516, 'grad_norm': 0.24258074164390564, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.26it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.99it/s]                                              {'loss': 0.6655, 'grad_norm': 0.23352715373039246, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.99it/s]                                              {'loss': 0.6482, 'grad_norm': 0.3899087607860565, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.99it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.6628, 'grad_norm': 0.24315248429775238, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.6738, 'grad_norm': 0.2141282707452774, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.53it/s] 40%|████      | 6/15 [00:00<00:00, 15.61it/s]                                              {'loss': 0.644, 'grad_norm': 0.36017557978630066, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.61it/s]                                              {'loss': 0.6346, 'grad_norm': 0.3853382170200348, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.61it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.6577, 'grad_norm': 0.21167795360088348, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.6983, 'grad_norm': 0.25395917892456055, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.37it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.6364, 'grad_norm': 0.33642950654029846, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.6717, 'grad_norm': 0.1750449538230896, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.89it/s] 80%|████████  | 12/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.6488, 'grad_norm': 0.2414344996213913, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.6748, 'grad_norm': 0.23395489156246185, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.55it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6154, 'grad_norm': 0.34466850757598877, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6326, 'grad_norm': 0.27947959303855896, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.55it/s]                                               {'train_runtime': 1.0726, 'train_samples_per_second': 396.233, 'train_steps_per_second': 13.985, 'train_loss': 0.6544108708699544, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.55it/s]100%|██████████| 15/15 [00:01<00:00, 13.99it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.624, 'grad_norm': 0.5291781425476074, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.32it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.10it/s]                                              {'loss': 0.6199, 'grad_norm': 0.5521542429924011, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.10it/s]                                              {'loss': 0.6462, 'grad_norm': 0.28291794657707214, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.10it/s] 27%|██▋       | 4/15 [00:00<00:00, 13.88it/s]                                              {'loss': 0.6602, 'grad_norm': 0.21690775454044342, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 13.88it/s]                                              {'loss': 0.5913, 'grad_norm': 0.669761598110199, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 13.88it/s] 40%|████      | 6/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.6102, 'grad_norm': 0.5050323605537415, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.611, 'grad_norm': 0.3444405198097229, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 14.91it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.76it/s]                                              {'loss': 0.6224, 'grad_norm': 0.3587602376937866, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.76it/s]                                              {'loss': 0.5399, 'grad_norm': 0.7161667346954346, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.76it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.23it/s]                                               {'loss': 0.5841, 'grad_norm': 0.45230430364608765, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.23it/s]                                               {'loss': 0.6483, 'grad_norm': 0.2113426923751831, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.23it/s] 80%|████████  | 12/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.5331, 'grad_norm': 0.6571022868156433, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.5521, 'grad_norm': 0.5053616762161255, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 14.79it/s] 93%|█████████▎| 14/15 [00:01<00:00, 13.79it/s]                                               {'loss': 0.6131, 'grad_norm': 0.27511703968048096, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:01<00:00, 13.79it/s]                                               {'loss': 0.6255, 'grad_norm': 0.30521073937416077, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.79it/s]                                               {'train_runtime': 1.1288, 'train_samples_per_second': 376.515, 'train_steps_per_second': 13.289, 'train_loss': 0.6054187496503194, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.79it/s]100%|██████████| 15/15 [00:01<00:00, 13.29it/s]
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6179, 'grad_norm': 0.618610680103302, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.13it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.41it/s]                                              {'loss': 0.6281, 'grad_norm': 0.5640571117401123, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.41it/s]                                              {'loss': 0.6695, 'grad_norm': 0.2776135206222534, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.41it/s] 27%|██▋       | 4/15 [00:00<00:00, 13.90it/s]                                              {'loss': 0.6259, 'grad_norm': 0.3953550457954407, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 13.90it/s]                                              {'loss': 0.6422, 'grad_norm': 0.38440918922424316, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 13.90it/s] 40%|████      | 6/15 [00:00<00:00, 15.05it/s]                                              {'loss': 0.5389, 'grad_norm': 0.8035707473754883, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.05it/s]                                              {'loss': 0.6237, 'grad_norm': 0.35981157422065735, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.05it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.33it/s]                                              {'loss': 0.6045, 'grad_norm': 0.4015295207500458, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.33it/s]                                              {'loss': 0.5303, 'grad_norm': 0.6817761063575745, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.33it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.66it/s]                                               {'loss': 0.5854, 'grad_norm': 0.4179958701133728, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.66it/s]                                               {'loss': 0.5362, 'grad_norm': 0.5984102487564087, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.66it/s] 80%|████████  | 12/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.6216, 'grad_norm': 0.30104225873947144, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.5349, 'grad_norm': 0.501279354095459, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.23it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.02it/s]                                               {'loss': 0.5656, 'grad_norm': 0.4007740318775177, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.02it/s]                                               {'loss': 0.6007, 'grad_norm': 0.32013607025146484, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.02it/s]                                               {'train_runtime': 1.1079, 'train_samples_per_second': 383.601, 'train_steps_per_second': 13.539, 'train_loss': 0.5950264970461527, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.02it/s]100%|██████████| 15/15 [00:01<00:00, 13.54it/s]
CLIENT:50
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6271, 'grad_norm': 0.5259292125701904, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.39it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.95it/s]                                              {'loss': 0.6304, 'grad_norm': 0.4466571509838104, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.95it/s]                                              {'loss': 0.6256, 'grad_norm': 0.46210235357284546, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.95it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.598, 'grad_norm': 0.6572749018669128, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.6115, 'grad_norm': 0.4435313940048218, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.15it/s] 40%|████      | 6/15 [00:00<00:00, 16.31it/s]                                              {'loss': 0.6493, 'grad_norm': 0.24964091181755066, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.31it/s]                                              {'loss': 0.5446, 'grad_norm': 0.6546692848205566, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.31it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.6398, 'grad_norm': 0.24001701176166534, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.6059, 'grad_norm': 0.4182194173336029, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.61it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.5722, 'grad_norm': 0.4541800618171692, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.5979, 'grad_norm': 0.3305812180042267, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.07it/s] 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.6003, 'grad_norm': 0.39268726110458374, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.5192, 'grad_norm': 0.6275922656059265, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.47it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.6659, 'grad_norm': 0.2527669668197632, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.5405, 'grad_norm': 0.5121014714241028, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.56it/s]                                               {'train_runtime': 1.0568, 'train_samples_per_second': 402.175, 'train_steps_per_second': 14.194, 'train_loss': 0.6018668611844381, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.56it/s]100%|██████████| 15/15 [00:01<00:00, 14.20it/s]
CLIENT:9
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6313, 'grad_norm': 0.4581640064716339, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.72it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.86it/s]                                              {'loss': 0.6471, 'grad_norm': 0.35226958990097046, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.86it/s]                                              {'loss': 0.6088, 'grad_norm': 0.5839426517486572, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.86it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.6176, 'grad_norm': 0.4332783818244934, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.6071, 'grad_norm': 0.5360459089279175, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.48it/s] 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.6319, 'grad_norm': 0.4003118872642517, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.6297, 'grad_norm': 0.41504645347595215, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.48it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.587, 'grad_norm': 0.5163009762763977, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.6294, 'grad_norm': 0.2795261740684509, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.64it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.00it/s]                                               {'loss': 0.5898, 'grad_norm': 0.5288978219032288, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.00it/s]                                               {'loss': 0.6448, 'grad_norm': 0.23751159012317657, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.00it/s] 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.571, 'grad_norm': 0.41694003343582153, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.572, 'grad_norm': 0.4065076410770416, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.41it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6145, 'grad_norm': 0.2614273130893707, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.5309, 'grad_norm': 0.5526676177978516, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]                                               {'train_runtime': 1.0758, 'train_samples_per_second': 395.056, 'train_steps_per_second': 13.943, 'train_loss': 0.6075288772583007, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 13.95it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 33.56it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.93it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.30it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.52it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.39it/s] 61%|██████    | 20/33 [00:00<00:00, 25.34it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.17it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.19it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.06it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.22it/s]100%|██████████| 33/33 [00:01<00:00, 25.95it/s]
{'eval_loss': 0.6366024017333984, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.313, 'eval_samples_per_second': 794.372, 'eval_steps_per_second': 25.134}
ROUND:17
CLIENT:20
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6023, 'grad_norm': 0.5322694182395935, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.79it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.6795, 'grad_norm': 0.26025640964508057, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.64, 'grad_norm': 0.3365402817726135, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.16it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.6817, 'grad_norm': 0.22566604614257812, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.5874, 'grad_norm': 0.4787839651107788, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.22it/s] 40%|████      | 6/15 [00:00<00:00, 16.33it/s]                                              {'loss': 0.6461, 'grad_norm': 0.23916208744049072, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.33it/s]                                              {'loss': 0.6233, 'grad_norm': 0.24412673711776733, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.33it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.6836, 'grad_norm': 0.21992450952529907, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.5851, 'grad_norm': 0.4525330364704132, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.68it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.6474, 'grad_norm': 0.2957496643066406, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.6416, 'grad_norm': 0.23070783913135529, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.11it/s] 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.606, 'grad_norm': 0.3436734974384308, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.5548, 'grad_norm': 0.641233503818512, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.48it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.6435, 'grad_norm': 0.25170978903770447, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.7532, 'grad_norm': 0.6966736316680908, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.44it/s]                                               {'train_runtime': 1.064, 'train_samples_per_second': 399.447, 'train_steps_per_second': 14.098, 'train_loss': 0.6383583545684814, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]100%|██████████| 15/15 [00:01<00:00, 14.10it/s]
CLIENT:10
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6324, 'grad_norm': 0.5695896744728088, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.04it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.22it/s]                                              {'loss': 0.6728, 'grad_norm': 0.2238103449344635, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.22it/s]                                              {'loss': 0.6281, 'grad_norm': 0.3865565359592438, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.22it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.6438, 'grad_norm': 0.2974337637424469, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.6364, 'grad_norm': 0.3379788398742676, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.56it/s] 40%|████      | 6/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.5925, 'grad_norm': 0.49482864141464233, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.6242, 'grad_norm': 0.2843165397644043, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.37it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.642, 'grad_norm': 0.29118919372558594, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.572, 'grad_norm': 0.5338754057884216, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.19it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6205, 'grad_norm': 0.26146432757377625, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.5668, 'grad_norm': 0.4756084382534027, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6779, 'grad_norm': 0.34458494186401367, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.5956, 'grad_norm': 0.30459946393966675, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.29it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.6467, 'grad_norm': 0.22099018096923828, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.5822, 'grad_norm': 0.4134864807128906, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]                                               {'train_runtime': 1.0742, 'train_samples_per_second': 395.63, 'train_steps_per_second': 13.963, 'train_loss': 0.6222589294115702, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]100%|██████████| 15/15 [00:01<00:00, 13.97it/s]
CLIENT:96
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5901, 'grad_norm': 0.609429657459259, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.00it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.26it/s]                                              {'loss': 0.6378, 'grad_norm': 0.33723944425582886, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.26it/s]                                              {'loss': 0.5864, 'grad_norm': 0.6316617131233215, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.26it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.6076, 'grad_norm': 0.4689994752407074, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.5974, 'grad_norm': 0.4495027959346771, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.14it/s] 40%|████      | 6/15 [00:00<00:00, 15.96it/s]                                              {'loss': 0.5577, 'grad_norm': 0.5881913900375366, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.96it/s]                                              {'loss': 0.6415, 'grad_norm': 0.22914165258407593, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.96it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.4978, 'grad_norm': 0.7258814573287964, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.5365, 'grad_norm': 0.5469973087310791, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.57it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.5655, 'grad_norm': 0.3953050971031189, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.6138, 'grad_norm': 0.21074442565441132, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.07it/s] 80%|████████  | 12/15 [00:00<00:00, 15.60it/s]                                               {'loss': 0.4654, 'grad_norm': 0.7755850553512573, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.60it/s]                                               {'loss': 0.505, 'grad_norm': 0.5621063709259033, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.60it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.6211, 'grad_norm': 0.23772473633289337, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.5156, 'grad_norm': 0.4558713436126709, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.44it/s]                                               {'train_runtime': 1.0642, 'train_samples_per_second': 399.348, 'train_steps_per_second': 14.095, 'train_loss': 0.5692827681700389, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]100%|██████████| 15/15 [00:01<00:00, 14.10it/s]
CLIENT:16
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6419, 'grad_norm': 0.23482351005077362, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.86it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.54it/s]                                              {'loss': 0.577, 'grad_norm': 0.6033124327659607, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.54it/s]                                              {'loss': 0.623, 'grad_norm': 0.4860612154006958, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.54it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.634, 'grad_norm': 0.3191111981868744, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6023, 'grad_norm': 0.34483611583709717, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.22it/s] 40%|████      | 6/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.5785, 'grad_norm': 0.4623347222805023, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.6386, 'grad_norm': 0.24656522274017334, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.11it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.03it/s]                                              {'loss': 0.5742, 'grad_norm': 0.4239986836910248, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.03it/s]                                              {'loss': 0.5667, 'grad_norm': 0.4605150818824768, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.03it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.6068, 'grad_norm': 0.37733978033065796, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.5733, 'grad_norm': 0.3391573429107666, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.91it/s] 80%|████████  | 12/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.6057, 'grad_norm': 0.3298194408416748, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.5658, 'grad_norm': 0.3988312780857086, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.19it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.50it/s]                                               {'loss': 0.6109, 'grad_norm': 0.22253108024597168, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.50it/s]                                               {'loss': 0.5904, 'grad_norm': 0.3085120916366577, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.50it/s]                                               {'train_runtime': 1.0837, 'train_samples_per_second': 392.175, 'train_steps_per_second': 13.841, 'train_loss': 0.5992725292841593, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.50it/s]100%|██████████| 15/15 [00:01<00:00, 13.85it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6032, 'grad_norm': 0.5336822867393494, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.86it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.57it/s]                                              {'loss': 0.5984, 'grad_norm': 0.5405744314193726, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.57it/s]                                              {'loss': 0.6381, 'grad_norm': 0.2831949293613434, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.57it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.6543, 'grad_norm': 0.2018098384141922, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.5646, 'grad_norm': 0.7015120387077332, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.77it/s] 40%|████      | 6/15 [00:00<00:00, 15.83it/s]                                              {'loss': 0.5928, 'grad_norm': 0.4660763442516327, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.83it/s]                                              {'loss': 0.6003, 'grad_norm': 0.3134525418281555, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.83it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.6106, 'grad_norm': 0.34376806020736694, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.5144, 'grad_norm': 0.6837496757507324, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.51it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.17it/s]                                               {'loss': 0.569, 'grad_norm': 0.4381469190120697, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.17it/s]                                               {'loss': 0.6459, 'grad_norm': 0.20952537655830383, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.17it/s] 80%|████████  | 12/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.5101, 'grad_norm': 0.6267944574356079, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.5343, 'grad_norm': 0.49623408913612366, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.50it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.6058, 'grad_norm': 0.25817885994911194, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.6225, 'grad_norm': 0.2958751916885376, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.53it/s]                                               {'train_runtime': 1.0687, 'train_samples_per_second': 397.692, 'train_steps_per_second': 14.036, 'train_loss': 0.5909669399261475, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.53it/s]100%|██████████| 15/15 [00:01<00:00, 14.04it/s]
CLIENT:24
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6322, 'grad_norm': 0.33098217844963074, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.75it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.6654, 'grad_norm': 0.21507643163204193, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.6134, 'grad_norm': 0.4991110861301422, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.16it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.6336, 'grad_norm': 0.39066916704177856, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.6562, 'grad_norm': 0.24952432513237, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.30it/s] 40%|████      | 6/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.6067, 'grad_norm': 0.34958362579345703, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.6409, 'grad_norm': 0.21769216656684875, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.14it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.69it/s]                                              {'loss': 0.6184, 'grad_norm': 0.32120904326438904, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.69it/s]                                              {'loss': 0.6291, 'grad_norm': 0.292721152305603, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.69it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.6172, 'grad_norm': 0.2637893855571747, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.6559, 'grad_norm': 0.2256603240966797, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.81it/s] 80%|████████  | 12/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.5798, 'grad_norm': 0.39217808842658997, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.577, 'grad_norm': 0.43322518467903137, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.28it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.31it/s]                                               {'loss': 0.6246, 'grad_norm': 0.24629110097885132, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.31it/s]                                               {'loss': 0.6903, 'grad_norm': 0.4011683464050293, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.31it/s]                                               {'train_runtime': 1.082, 'train_samples_per_second': 392.796, 'train_steps_per_second': 13.863, 'train_loss': 0.6293824394543965, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.31it/s]100%|██████████| 15/15 [00:01<00:00, 13.87it/s]
CLIENT:53
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6093, 'grad_norm': 0.5163673758506775, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.14it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.88it/s]                                              {'loss': 0.6749, 'grad_norm': 0.20712940394878387, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.88it/s]                                              {'loss': 0.6246, 'grad_norm': 0.5617510676383972, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.88it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.6789, 'grad_norm': 0.2626420557498932, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.5846, 'grad_norm': 0.5231135487556458, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.45it/s] 40%|████      | 6/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.6422, 'grad_norm': 0.3111584782600403, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.6456, 'grad_norm': 0.3203096389770508, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.47it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.5735, 'grad_norm': 0.5019804239273071, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.6844, 'grad_norm': 0.3332030177116394, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.16it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.6231, 'grad_norm': 0.29550331830978394, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.6393, 'grad_norm': 0.25658363103866577, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.54it/s] 80%|████████  | 12/15 [00:00<00:00, 15.06it/s]                                               {'loss': 0.6134, 'grad_norm': 0.33877888321876526, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.06it/s]                                               {'loss': 0.6019, 'grad_norm': 0.31920376420021057, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.06it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.22it/s]                                               {'loss': 0.6123, 'grad_norm': 0.3312107026576996, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.22it/s]                                               {'loss': 0.6838, 'grad_norm': 0.3552212417125702, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.22it/s]                                               {'train_runtime': 1.0886, 'train_samples_per_second': 390.415, 'train_steps_per_second': 13.779, 'train_loss': 0.6327875057856241, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.22it/s]100%|██████████| 15/15 [00:01<00:00, 13.78it/s]
CLIENT:97
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6575, 'grad_norm': 0.19298112392425537, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.41it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.99it/s]                                              {'loss': 0.636, 'grad_norm': 0.30193573236465454, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.99it/s]                                              {'loss': 0.6751, 'grad_norm': 0.24382300674915314, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.99it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.6666, 'grad_norm': 0.3320794403553009, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.6708, 'grad_norm': 0.18492768704891205, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.11it/s] 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.6408, 'grad_norm': 0.30486348271369934, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.669, 'grad_norm': 0.2958134710788727, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.48it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.6456, 'grad_norm': 0.1957486867904663, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.6616, 'grad_norm': 0.2249937802553177, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.68it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.7379, 'grad_norm': 0.49713602662086487, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.6146, 'grad_norm': 0.36276009678840637, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.91it/s] 80%|████████  | 12/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.5658, 'grad_norm': 0.5913087725639343, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.7094, 'grad_norm': 0.3938976228237152, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.33it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.35it/s]                                               {'loss': 0.616, 'grad_norm': 0.34679698944091797, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.35it/s]                                               {'loss': 0.6221, 'grad_norm': 0.4213545620441437, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.35it/s]                                               {'train_runtime': 1.0743, 'train_samples_per_second': 395.622, 'train_steps_per_second': 13.963, 'train_loss': 0.6525887568791707, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.35it/s]100%|██████████| 15/15 [00:01<00:00, 13.97it/s]
CLIENT:41
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6393, 'grad_norm': 0.2953077554702759, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.88it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.10it/s]                                              {'loss': 0.703, 'grad_norm': 0.3256308138370514, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.10it/s]                                              {'loss': 0.6826, 'grad_norm': 0.2923945188522339, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.10it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.7494, 'grad_norm': 0.48493659496307373, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.6313, 'grad_norm': 0.452784925699234, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.63it/s] 40%|████      | 6/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.68, 'grad_norm': 0.28967413306236267, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.6303, 'grad_norm': 0.3928585946559906, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.42it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.21it/s]                                              {'loss': 0.71, 'grad_norm': 0.2929876446723938, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.21it/s]                                              {'loss': 0.7152, 'grad_norm': 0.35565441846847534, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.21it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6937, 'grad_norm': 0.23925389349460602, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6719, 'grad_norm': 0.2004251927137375, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6497, 'grad_norm': 0.2661721706390381, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6674, 'grad_norm': 0.22671155631542206, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.29it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.663, 'grad_norm': 0.27051079273223877, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.7193, 'grad_norm': 0.35555216670036316, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]                                               {'train_runtime': 1.0789, 'train_samples_per_second': 393.92, 'train_steps_per_second': 13.903, 'train_loss': 0.6804187933603922, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]100%|██████████| 15/15 [00:01<00:00, 13.91it/s]
CLIENT:47
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6224, 'grad_norm': 0.3809470236301422, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.83it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.54it/s]                                              {'loss': 0.6191, 'grad_norm': 0.444942831993103, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.54it/s]                                              {'loss': 0.6649, 'grad_norm': 0.24878044426441193, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.54it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.685, 'grad_norm': 0.1867675632238388, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.5745, 'grad_norm': 0.6663070917129517, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.24it/s] 40%|████      | 6/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6332, 'grad_norm': 0.28275418281555176, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6352, 'grad_norm': 0.25296133756637573, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.20it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.52it/s]                                              {'loss': 0.6001, 'grad_norm': 0.3536068797111511, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.52it/s]                                              {'loss': 0.6156, 'grad_norm': 0.3012602627277374, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.52it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.6505, 'grad_norm': 0.25354161858558655, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.5794, 'grad_norm': 0.44815748929977417, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.80it/s] 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6534, 'grad_norm': 0.255437433719635, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.5835, 'grad_norm': 0.4322258532047272, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.29it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.30it/s]                                               {'loss': 0.648, 'grad_norm': 0.2768043875694275, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.30it/s]                                               {'loss': 0.5897, 'grad_norm': 0.3150084912776947, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.30it/s]                                               {'train_runtime': 1.0859, 'train_samples_per_second': 391.394, 'train_steps_per_second': 13.814, 'train_loss': 0.6236407836278279, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.30it/s]100%|██████████| 15/15 [00:01<00:00, 13.82it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 35.91it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.01it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.44it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.64it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.46it/s] 61%|██████    | 20/33 [00:00<00:00, 25.35it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.14it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.24it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.08it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.24it/s]100%|██████████| 33/33 [00:01<00:00, 26.04it/s]
{'eval_loss': 0.6285549998283386, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3086, 'eval_samples_per_second': 797.053, 'eval_steps_per_second': 25.218}
ROUND:18
CLIENT:83
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5745, 'grad_norm': 0.6217525005340576, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.40it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.75it/s]                                              {'loss': 0.6158, 'grad_norm': 0.4105011820793152, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.75it/s]                                              {'loss': 0.6232, 'grad_norm': 0.28528422117233276, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.75it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.5828, 'grad_norm': 0.44075915217399597, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.6211, 'grad_norm': 0.26801401376724243, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.31it/s] 40%|████      | 6/15 [00:00<00:00, 15.93it/s]                                              {'loss': 0.5335, 'grad_norm': 0.5746331214904785, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.93it/s]                                              {'loss': 0.5979, 'grad_norm': 0.2910677492618561, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.93it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.5428, 'grad_norm': 0.4990323483943939, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.6086, 'grad_norm': 0.32861658930778503, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.57it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.6109, 'grad_norm': 0.23341858386993408, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.5161, 'grad_norm': 0.4741814136505127, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.02it/s] 80%|████████  | 12/15 [00:00<00:00, 15.42it/s]                                               {'loss': 0.5857, 'grad_norm': 0.2737644612789154, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.42it/s]                                               {'loss': 0.4875, 'grad_norm': 0.539559543132782, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.42it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.6511, 'grad_norm': 0.3190998435020447, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.592, 'grad_norm': 0.2767221927642822, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.78it/s]                                               {'train_runtime': 1.0564, 'train_samples_per_second': 402.328, 'train_steps_per_second': 14.2, 'train_loss': 0.5828955411911011, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.78it/s]100%|██████████| 15/15 [00:01<00:00, 14.21it/s]
CLIENT:1
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.734, 'grad_norm': 0.528048038482666, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.86it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.61it/s]                                              {'loss': 0.6547, 'grad_norm': 0.18621623516082764, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.61it/s]                                              {'loss': 0.5971, 'grad_norm': 0.5112751722335815, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.61it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.92it/s]                                              {'loss': 0.7005, 'grad_norm': 0.2648678421974182, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.92it/s]                                              {'loss': 0.6614, 'grad_norm': 0.1969308853149414, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.92it/s] 40%|████      | 6/15 [00:00<00:00, 16.11it/s]                                              {'loss': 0.6592, 'grad_norm': 0.35310086607933044, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.11it/s]                                              {'loss': 0.6635, 'grad_norm': 0.19115273654460907, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.11it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.54it/s]                                              {'loss': 0.6591, 'grad_norm': 0.23346786201000214, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.54it/s]                                              {'loss': 0.7312, 'grad_norm': 0.40683427453041077, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.54it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.6858, 'grad_norm': 0.21213819086551666, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.6662, 'grad_norm': 0.2055850327014923, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.02it/s] 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.6672, 'grad_norm': 0.3274437487125397, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.7284, 'grad_norm': 0.3708566427230835, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.40it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6815, 'grad_norm': 0.20328623056411743, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6066, 'grad_norm': 0.47884508967399597, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.45it/s]                                               {'train_runtime': 1.0685, 'train_samples_per_second': 397.772, 'train_steps_per_second': 14.039, 'train_loss': 0.6730851570765177, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 14.04it/s]
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5764, 'grad_norm': 0.5201985836029053, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.13it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.90it/s]                                              {'loss': 0.6842, 'grad_norm': 0.243262380361557, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.90it/s]                                              {'loss': 0.6616, 'grad_norm': 0.24364526569843292, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.90it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.6749, 'grad_norm': 0.22132903337478638, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.5965, 'grad_norm': 0.4152982234954834, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.42it/s] 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.6512, 'grad_norm': 0.2385936826467514, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.6082, 'grad_norm': 0.3071635067462921, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.46it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.6478, 'grad_norm': 0.22105230391025543, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.6278, 'grad_norm': 0.2896808385848999, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.45it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.5126, 'grad_norm': 0.7328948378562927, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.7376, 'grad_norm': 0.5008958578109741, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.85it/s] 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.6625, 'grad_norm': 0.2705248296260834, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.5904, 'grad_norm': 0.32868683338165283, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.57it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.6458, 'grad_norm': 0.18699443340301514, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.657, 'grad_norm': 0.26894041895866394, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.56it/s]                                               {'train_runtime': 1.0714, 'train_samples_per_second': 396.694, 'train_steps_per_second': 14.001, 'train_loss': 0.6356457869211832, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.56it/s]100%|██████████| 15/15 [00:01<00:00, 14.01it/s]
CLIENT:9
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6015, 'grad_norm': 0.4258537292480469, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.88it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.67it/s]                                              {'loss': 0.627, 'grad_norm': 0.29117822647094727, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.67it/s]                                              {'loss': 0.568, 'grad_norm': 0.5911321043968201, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.67it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.05it/s]                                              {'loss': 0.5896, 'grad_norm': 0.41012370586395264, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.05it/s]                                              {'loss': 0.5734, 'grad_norm': 0.48056623339653015, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.05it/s] 40%|████      | 6/15 [00:00<00:00, 14.97it/s]                                              {'loss': 0.6098, 'grad_norm': 0.33138686418533325, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 14.97it/s]                                              {'loss': 0.6065, 'grad_norm': 0.3410250246524811, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 14.97it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.98it/s]                                              {'loss': 0.5556, 'grad_norm': 0.4858035445213318, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.98it/s]                                              {'loss': 0.6231, 'grad_norm': 0.2729308307170868, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.98it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.5617, 'grad_norm': 0.4631009101867676, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.6402, 'grad_norm': 0.23265640437602997, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.58it/s] 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.5519, 'grad_norm': 0.3689350187778473, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.5515, 'grad_norm': 0.3517320156097412, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.32it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.6086, 'grad_norm': 0.2501212954521179, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.5012, 'grad_norm': 0.5168896913528442, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]                                               {'train_runtime': 1.0884, 'train_samples_per_second': 390.474, 'train_steps_per_second': 13.781, 'train_loss': 0.5846375385920207, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]100%|██████████| 15/15 [00:01<00:00, 13.79it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6533, 'grad_norm': 0.21630635857582092, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.21it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.96it/s]                                              {'loss': 0.5944, 'grad_norm': 0.4440293610095978, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.96it/s]                                              {'loss': 0.5944, 'grad_norm': 0.4571402966976166, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.96it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.6745, 'grad_norm': 0.22476448118686676, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.523, 'grad_norm': 0.8479875326156616, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.49it/s] 40%|████      | 6/15 [00:00<00:00, 15.57it/s]                                              {'loss': 0.6396, 'grad_norm': 0.2813655436038971, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.57it/s]                                              {'loss': 0.549, 'grad_norm': 0.4932093322277069, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.57it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.39it/s]                                              {'loss': 0.6364, 'grad_norm': 0.22456175088882446, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.39it/s]                                              {'loss': 0.6029, 'grad_norm': 0.35189297795295715, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.39it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.09it/s]                                               {'loss': 0.6052, 'grad_norm': 0.2941182255744934, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.09it/s]                                               {'loss': 0.5603, 'grad_norm': 0.37752681970596313, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.09it/s] 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.6293, 'grad_norm': 0.3603852093219757, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.5986, 'grad_norm': 0.29486632347106934, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.49it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.5794, 'grad_norm': 0.32469815015792847, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.5981, 'grad_norm': 0.31148186326026917, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.51it/s]                                               {'train_runtime': 1.0694, 'train_samples_per_second': 397.409, 'train_steps_per_second': 14.026, 'train_loss': 0.602570895353953, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.51it/s]100%|██████████| 15/15 [00:01<00:00, 14.03it/s]
CLIENT:28
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6574, 'grad_norm': 0.3201679587364197, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.79it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.19it/s]                                              {'loss': 0.5957, 'grad_norm': 0.4844183921813965, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.19it/s]                                              {'loss': 0.5936, 'grad_norm': 0.49797922372817993, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.19it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6422, 'grad_norm': 0.240452378988266, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6102, 'grad_norm': 0.36956945061683655, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.31it/s] 40%|████      | 6/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.604, 'grad_norm': 0.47352954745292664, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.6201, 'grad_norm': 0.3384072482585907, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.19it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.5311, 'grad_norm': 0.57857346534729, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.6936, 'grad_norm': 0.43511348962783813, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.14it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.6238, 'grad_norm': 0.25507670640945435, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.562, 'grad_norm': 0.42402926087379456, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.75it/s] 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6518, 'grad_norm': 0.34150978922843933, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6248, 'grad_norm': 0.22328613698482513, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.31it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.5106, 'grad_norm': 0.622646689414978, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.7387, 'grad_norm': 0.6379417181015015, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]                                               {'train_runtime': 1.0809, 'train_samples_per_second': 393.185, 'train_steps_per_second': 13.877, 'train_loss': 0.6173014005025228, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 13.88it/s]
CLIENT:96
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5709, 'grad_norm': 0.5635597705841064, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.98it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.65it/s]                                              {'loss': 0.6291, 'grad_norm': 0.2999151647090912, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.65it/s]                                              {'loss': 0.5673, 'grad_norm': 0.619082510471344, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.65it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.594, 'grad_norm': 0.4553931951522827, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.5855, 'grad_norm': 0.41591742634773254, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.28it/s] 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.5415, 'grad_norm': 0.5621492266654968, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.6418, 'grad_norm': 0.24865388870239258, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.22it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.12it/s]                                              {'loss': 0.4771, 'grad_norm': 0.7159304022789001, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.12it/s]                                              {'loss': 0.5227, 'grad_norm': 0.5157924294471741, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.12it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.5564, 'grad_norm': 0.3845285177230835, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.6134, 'grad_norm': 0.21440905332565308, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.02it/s] 80%|████████  | 12/15 [00:00<00:00, 16.01it/s]                                               {'loss': 0.4455, 'grad_norm': 0.7620857954025269, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.01it/s]                                               {'loss': 0.4911, 'grad_norm': 0.5378605723381042, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.01it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6225, 'grad_norm': 0.27804800868034363, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.5058, 'grad_norm': 0.42207127809524536, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.82it/s]                                               {'train_runtime': 1.0638, 'train_samples_per_second': 399.505, 'train_steps_per_second': 14.1, 'train_loss': 0.5576385955015818, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.82it/s]100%|██████████| 15/15 [00:01<00:00, 14.11it/s]
CLIENT:29
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6453, 'grad_norm': 0.29256752133369446, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.02it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.6571, 'grad_norm': 0.1994301825761795, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.6284, 'grad_norm': 0.36508694291114807, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.23it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.6567, 'grad_norm': 0.2683452069759369, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.6694, 'grad_norm': 0.20997747778892517, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.59it/s] 40%|████      | 6/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.6271, 'grad_norm': 0.33510059118270874, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.6161, 'grad_norm': 0.33085501194000244, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.36it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.6518, 'grad_norm': 0.20769597589969635, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.7073, 'grad_norm': 0.3595349192619324, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.16it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.6213, 'grad_norm': 0.3046285808086395, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.6748, 'grad_norm': 0.24758821725845337, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.02it/s] 80%|████████  | 12/15 [00:00<00:00, 15.70it/s]                                               {'loss': 0.644, 'grad_norm': 0.2482193261384964, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.70it/s]                                               {'loss': 0.6754, 'grad_norm': 0.3259405493736267, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.70it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.6034, 'grad_norm': 0.33492255210876465, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.6251, 'grad_norm': 0.2844528555870056, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]                                               {'train_runtime': 1.0768, 'train_samples_per_second': 394.672, 'train_steps_per_second': 13.93, 'train_loss': 0.6468664089838664, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]100%|██████████| 15/15 [00:01<00:00, 13.93it/s]
CLIENT:86
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.601, 'grad_norm': 0.40535062551498413, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.68it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.83it/s]                                              {'loss': 0.6532, 'grad_norm': 0.24045561254024506, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.83it/s]                                              {'loss': 0.6555, 'grad_norm': 0.2777993679046631, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.83it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.6331, 'grad_norm': 0.2785135507583618, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.6614, 'grad_norm': 0.25479573011398315, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.53it/s] 40%|████      | 6/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.6129, 'grad_norm': 0.318163126707077, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.6539, 'grad_norm': 0.20611713826656342, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.41it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.6637, 'grad_norm': 0.2564399242401123, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.5444, 'grad_norm': 0.5769011974334717, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.28it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.6097, 'grad_norm': 0.33947545289993286, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.6012, 'grad_norm': 0.3053280711174011, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.88it/s] 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.7087, 'grad_norm': 0.4347222149372101, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.6308, 'grad_norm': 0.22973190248012543, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.46it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.6273, 'grad_norm': 0.277817040681839, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.643, 'grad_norm': 0.2632015645503998, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.49it/s]                                               {'train_runtime': 1.074, 'train_samples_per_second': 395.705, 'train_steps_per_second': 13.966, 'train_loss': 0.63332097530365, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.49it/s]100%|██████████| 15/15 [00:01<00:00, 13.97it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5865, 'grad_norm': 0.5421460866928101, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.01it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.72it/s]                                              {'loss': 0.5817, 'grad_norm': 0.49893543124198914, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.72it/s]                                              {'loss': 0.633, 'grad_norm': 0.26311230659484863, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.72it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.99it/s]                                              {'loss': 0.6507, 'grad_norm': 0.21175961196422577, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.99it/s]                                              {'loss': 0.5435, 'grad_norm': 0.6812388896942139, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.99it/s] 40%|████      | 6/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.5815, 'grad_norm': 0.3807591199874878, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.5935, 'grad_norm': 0.2931608557701111, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.47it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.603, 'grad_norm': 0.3291926383972168, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.4975, 'grad_norm': 0.6439447402954102, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.28it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.5592, 'grad_norm': 0.4140058755874634, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.6463, 'grad_norm': 0.2295028418302536, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.58it/s] 80%|████████  | 12/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.4958, 'grad_norm': 0.5947268009185791, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.5231, 'grad_norm': 0.4679284393787384, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.14it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.22it/s]                                               {'loss': 0.6031, 'grad_norm': 0.236487478017807, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.22it/s]                                               {'loss': 0.6224, 'grad_norm': 0.3206082880496979, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.22it/s]                                               {'train_runtime': 1.0861, 'train_samples_per_second': 391.326, 'train_steps_per_second': 13.811, 'train_loss': 0.5813849409421284, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.22it/s]100%|██████████| 15/15 [00:01<00:00, 13.82it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 34.01it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.39it/s] 33%|███▎      | 11/33 [00:00<00:00, 25.96it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.27it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.18it/s] 61%|██████    | 20/33 [00:00<00:00, 25.28it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.00it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.17it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.08it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.06it/s]100%|██████████| 33/33 [00:01<00:00, 25.84it/s]
{'eval_loss': 0.6227386593818665, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3185, 'eval_samples_per_second': 791.053, 'eval_steps_per_second': 25.029}
ROUND:19
CLIENT:83
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5569, 'grad_norm': 0.5912299752235413, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.33it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.81it/s]                                              {'loss': 0.6068, 'grad_norm': 0.3609643578529358, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.81it/s]                                              {'loss': 0.6186, 'grad_norm': 0.29260656237602234, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.81it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.5721, 'grad_norm': 0.40287187695503235, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.6163, 'grad_norm': 0.25640958547592163, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.09it/s] 40%|████      | 6/15 [00:00<00:00, 15.79it/s]                                              {'loss': 0.5197, 'grad_norm': 0.5498974919319153, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.79it/s]                                              {'loss': 0.5927, 'grad_norm': 0.2691338360309601, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.79it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.5324, 'grad_norm': 0.4653526842594147, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6052, 'grad_norm': 0.3159677982330322, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.22it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.6093, 'grad_norm': 0.22968502342700958, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.5072, 'grad_norm': 0.44555386900901794, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.81it/s] 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.5839, 'grad_norm': 0.2797125577926636, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.4779, 'grad_norm': 0.5104789733886719, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.32it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.6549, 'grad_norm': 0.3699040114879608, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.5906, 'grad_norm': 0.278776615858078, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.43it/s]                                               {'train_runtime': 1.0688, 'train_samples_per_second': 397.661, 'train_steps_per_second': 14.035, 'train_loss': 0.5762924611568451, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.43it/s]100%|██████████| 15/15 [00:01<00:00, 14.04it/s]
CLIENT:1
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7488, 'grad_norm': 0.6143993139266968, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.02it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.29it/s]                                              {'loss': 0.6543, 'grad_norm': 0.20225059986114502, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.29it/s]                                              {'loss': 0.5847, 'grad_norm': 0.46725308895111084, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.29it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.7055, 'grad_norm': 0.31166520714759827, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.6614, 'grad_norm': 0.21659286320209503, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.28it/s] 40%|████      | 6/15 [00:00<00:00, 16.09it/s]                                              {'loss': 0.6553, 'grad_norm': 0.3650095760822296, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.09it/s]                                              {'loss': 0.664, 'grad_norm': 0.21760869026184082, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.09it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.6575, 'grad_norm': 0.20613276958465576, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.7362, 'grad_norm': 0.45420798659324646, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.53it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6867, 'grad_norm': 0.23270617425441742, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6664, 'grad_norm': 0.22620618343353271, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.94it/s] 80%|████████  | 12/15 [00:00<00:00, 15.58it/s]                                               {'loss': 0.6692, 'grad_norm': 0.32031339406967163, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.58it/s]                                               {'loss': 0.7326, 'grad_norm': 0.4196322560310364, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.58it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.682, 'grad_norm': 0.23036882281303406, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.6013, 'grad_norm': 0.47092732787132263, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.42it/s]                                               {'train_runtime': 1.0661, 'train_samples_per_second': 398.653, 'train_steps_per_second': 14.07, 'train_loss': 0.6737235983212789, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]100%|██████████| 15/15 [00:01<00:00, 14.08it/s]
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5621, 'grad_norm': 0.4928378164768219, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.03it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.20it/s]                                              {'loss': 0.6899, 'grad_norm': 0.30844393372535706, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.20it/s]                                              {'loss': 0.661, 'grad_norm': 0.26895081996917725, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.20it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.6753, 'grad_norm': 0.2614692747592926, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.5864, 'grad_norm': 0.3851553201675415, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.24it/s] 40%|████      | 6/15 [00:00<00:00, 16.41it/s]                                              {'loss': 0.6521, 'grad_norm': 0.26271146535873413, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.41it/s]                                              {'loss': 0.6033, 'grad_norm': 0.27110838890075684, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.41it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.6461, 'grad_norm': 0.22936749458312988, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.6269, 'grad_norm': 0.3005569875240326, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.70it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.4988, 'grad_norm': 0.7371653318405151, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.7456, 'grad_norm': 0.5718480348587036, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.11it/s] 80%|████████  | 12/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.6634, 'grad_norm': 0.3037848472595215, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.5869, 'grad_norm': 0.31266286969184875, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.52it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6457, 'grad_norm': 0.20461411774158478, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6595, 'grad_norm': 0.3037515878677368, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.38it/s]                                               {'train_runtime': 1.0618, 'train_samples_per_second': 400.282, 'train_steps_per_second': 14.128, 'train_loss': 0.6335446198781332, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 14.13it/s]
CLIENT:9
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5903, 'grad_norm': 0.4256925880908966, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.40it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.6215, 'grad_norm': 0.25490081310272217, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.5518, 'grad_norm': 0.558671772480011, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.01it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.65it/s]                                              {'loss': 0.5798, 'grad_norm': 0.38652244210243225, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.65it/s]                                              {'loss': 0.5614, 'grad_norm': 0.43295589089393616, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.65it/s] 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.6036, 'grad_norm': 0.32396572828292847, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.5996, 'grad_norm': 0.31103330850601196, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.45it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.5453, 'grad_norm': 0.4497254192829132, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.6228, 'grad_norm': 0.29624781012535095, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.70it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.5536, 'grad_norm': 0.4392034113407135, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.6403, 'grad_norm': 0.2553273141384125, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.03it/s] 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.5467, 'grad_norm': 0.3464975357055664, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.5455, 'grad_norm': 0.3344695270061493, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.48it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.6081, 'grad_norm': 0.2704174220561981, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.4922, 'grad_norm': 0.4973752200603485, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.44it/s]                                               {'train_runtime': 1.0687, 'train_samples_per_second': 397.672, 'train_steps_per_second': 14.035, 'train_loss': 0.5775160610675811, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]100%|██████████| 15/15 [00:01<00:00, 14.04it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6511, 'grad_norm': 0.22839033603668213, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.33it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.93it/s]                                              {'loss': 0.5824, 'grad_norm': 0.4140895903110504, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.93it/s]                                              {'loss': 0.583, 'grad_norm': 0.44329071044921875, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.93it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6772, 'grad_norm': 0.27350619435310364, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.5, 'grad_norm': 0.8458288311958313, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.23it/s] 40%|████      | 6/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6356, 'grad_norm': 0.2644343376159668, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.5383, 'grad_norm': 0.47648119926452637, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.13it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.07it/s]                                              {'loss': 0.6352, 'grad_norm': 0.23075099289417267, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.07it/s]                                              {'loss': 0.5987, 'grad_norm': 0.32238999009132385, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.07it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.68it/s]                                               {'loss': 0.6006, 'grad_norm': 0.2676558494567871, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.68it/s]                                               {'loss': 0.554, 'grad_norm': 0.353667676448822, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.68it/s] 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.6295, 'grad_norm': 0.3838216960430145, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.5955, 'grad_norm': 0.28319525718688965, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.37it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.5752, 'grad_norm': 0.3145432770252228, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.5976, 'grad_norm': 0.33390799164772034, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.43it/s]                                               {'train_runtime': 1.0828, 'train_samples_per_second': 392.494, 'train_steps_per_second': 13.853, 'train_loss': 0.596930839618047, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.43it/s]100%|██████████| 15/15 [00:01<00:00, 13.86it/s]
CLIENT:28
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6583, 'grad_norm': 0.3259362578392029, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.05it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.77it/s]                                              {'loss': 0.5839, 'grad_norm': 0.4195656180381775, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.77it/s]                                              {'loss': 0.5808, 'grad_norm': 0.4785049557685852, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.77it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.6383, 'grad_norm': 0.22419556975364685, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.6026, 'grad_norm': 0.3631860017776489, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.20it/s] 40%|████      | 6/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.5945, 'grad_norm': 0.45196446776390076, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.6146, 'grad_norm': 0.33034518361091614, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.17it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.5182, 'grad_norm': 0.5643287301063538, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.7002, 'grad_norm': 0.48058560490608215, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.59it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.6223, 'grad_norm': 0.24515488743782043, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.5551, 'grad_norm': 0.41677597165107727, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.99it/s] 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.6531, 'grad_norm': 0.36273837089538574, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.6245, 'grad_norm': 0.24434024095535278, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.62it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.4995, 'grad_norm': 0.6170191168785095, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.7486, 'grad_norm': 0.6988937258720398, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.56it/s]                                               {'train_runtime': 1.0723, 'train_samples_per_second': 396.33, 'train_steps_per_second': 13.988, 'train_loss': 0.612965722878774, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.56it/s]100%|██████████| 15/15 [00:01<00:00, 13.99it/s]
CLIENT:96
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5542, 'grad_norm': 0.5418872237205505, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.59it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.77it/s]                                              {'loss': 0.6224, 'grad_norm': 0.268961101770401, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.77it/s]                                              {'loss': 0.5502, 'grad_norm': 0.5887190699577332, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.77it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.44it/s]                                              {'loss': 0.5819, 'grad_norm': 0.44184333086013794, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.44it/s]                                              {'loss': 0.5751, 'grad_norm': 0.38335132598876953, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.44it/s] 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.5278, 'grad_norm': 0.5272913575172424, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.6439, 'grad_norm': 0.30296990275382996, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.45it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.4594, 'grad_norm': 0.6900717616081238, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.5117, 'grad_norm': 0.4716905653476715, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.85it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.549, 'grad_norm': 0.3568167984485626, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.6145, 'grad_norm': 0.23827721178531647, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.20it/s] 80%|████████  | 12/15 [00:00<00:00, 15.67it/s]                                               {'loss': 0.4291, 'grad_norm': 0.7495882511138916, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.67it/s]                                               {'loss': 0.4802, 'grad_norm': 0.516171395778656, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.67it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.30it/s]                                               {'loss': 0.6248, 'grad_norm': 0.327795147895813, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.30it/s]                                               {'loss': 0.4991, 'grad_norm': 0.3813614845275879, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.30it/s]                                               {'train_runtime': 1.0738, 'train_samples_per_second': 395.793, 'train_steps_per_second': 13.969, 'train_loss': 0.5482322414716084, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.30it/s]100%|██████████| 15/15 [00:01<00:00, 13.97it/s]
CLIENT:29
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6455, 'grad_norm': 0.32157474756240845, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.09it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.6563, 'grad_norm': 0.21069374680519104, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.6219, 'grad_norm': 0.3497757017612457, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.33it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.6559, 'grad_norm': 0.274020254611969, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.6692, 'grad_norm': 0.23339466750621796, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.40it/s] 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.621, 'grad_norm': 0.3132219910621643, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.6098, 'grad_norm': 0.3073224127292633, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.22it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.651, 'grad_norm': 0.22446313500404358, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.7125, 'grad_norm': 0.38429760932922363, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.15it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.616, 'grad_norm': 0.2815345525741577, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.6774, 'grad_norm': 0.2872331440448761, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.78it/s] 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6431, 'grad_norm': 0.26981985569000244, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6771, 'grad_norm': 0.38336536288261414, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.29it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.5989, 'grad_norm': 0.3189554810523987, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.6231, 'grad_norm': 0.2964024543762207, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.43it/s]                                               {'train_runtime': 1.0786, 'train_samples_per_second': 394.019, 'train_steps_per_second': 13.907, 'train_loss': 0.6452510833740235, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.43it/s]100%|██████████| 15/15 [00:01<00:00, 13.91it/s]
CLIENT:86
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5917, 'grad_norm': 0.3874548673629761, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.10it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6532, 'grad_norm': 0.2318343222141266, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6517, 'grad_norm': 0.31061694025993347, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.23it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.6282, 'grad_norm': 0.2680617868900299, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.6607, 'grad_norm': 0.28442609310150146, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.29it/s] 40%|████      | 6/15 [00:00<00:00, 16.00it/s]                                              {'loss': 0.6082, 'grad_norm': 0.31645309925079346, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.00it/s]                                              {'loss': 0.6527, 'grad_norm': 0.2229325920343399, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.00it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.6648, 'grad_norm': 0.314523309469223, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.5331, 'grad_norm': 0.5582147836685181, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.50it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.6044, 'grad_norm': 0.2867661714553833, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.5977, 'grad_norm': 0.29477542638778687, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.93it/s] 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.7142, 'grad_norm': 0.4836598038673401, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.6296, 'grad_norm': 0.24153943359851837, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.57it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.6269, 'grad_norm': 0.2812385857105255, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.6422, 'grad_norm': 0.283559113740921, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.56it/s]                                               {'train_runtime': 1.058, 'train_samples_per_second': 401.709, 'train_steps_per_second': 14.178, 'train_loss': 0.6306250770886739, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.56it/s]100%|██████████| 15/15 [00:01<00:00, 14.18it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5716, 'grad_norm': 0.5025241374969482, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.19it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.92it/s]                                              {'loss': 0.5679, 'grad_norm': 0.4731009602546692, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.92it/s]                                              {'loss': 0.6303, 'grad_norm': 0.2733846604824066, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.92it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.6482, 'grad_norm': 0.21821901202201843, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.5254, 'grad_norm': 0.657359778881073, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.66it/s] 40%|████      | 6/15 [00:00<00:00, 15.03it/s]                                              {'loss': 0.5732, 'grad_norm': 0.35649538040161133, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.03it/s]                                              {'loss': 0.5885, 'grad_norm': 0.28485363721847534, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.03it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.5968, 'grad_norm': 0.3179507851600647, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.4833, 'grad_norm': 0.6062995791435242, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.22it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.5515, 'grad_norm': 0.372414767742157, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.6479, 'grad_norm': 0.2719193696975708, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.65it/s] 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.4843, 'grad_norm': 0.5722704529762268, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.5143, 'grad_norm': 0.4449124336242676, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.29it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.09it/s]                                               {'loss': 0.6019, 'grad_norm': 0.24088285863399506, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.09it/s]                                               {'loss': 0.6234, 'grad_norm': 0.3375531733036041, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.09it/s]                                               {'train_runtime': 1.0922, 'train_samples_per_second': 389.135, 'train_steps_per_second': 13.734, 'train_loss': 0.57388782898585, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.09it/s]100%|██████████| 15/15 [00:01<00:00, 13.74it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 34.08it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.74it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.81it/s] 42%|████▏     | 14/33 [00:00<00:00, 26.24it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.93it/s] 61%|██████    | 20/33 [00:00<00:00, 25.49it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.47it/s] 79%|███████▉  | 26/33 [00:00<00:00, 25.20it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.19it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.28it/s]100%|██████████| 33/33 [00:01<00:00, 26.23it/s]
{'eval_loss': 0.6193996071815491, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.2996, 'eval_samples_per_second': 802.576, 'eval_steps_per_second': 25.393}
ROUND:20
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5637, 'grad_norm': 0.450899213552475, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.90it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.20it/s]                                              {'loss': 0.5197, 'grad_norm': 0.6499698758125305, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.20it/s]                                              {'loss': 0.6802, 'grad_norm': 0.416023850440979, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.20it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.5077, 'grad_norm': 0.5914283394813538, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.5768, 'grad_norm': 0.3802105188369751, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.56it/s] 40%|████      | 6/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.643, 'grad_norm': 0.35688385367393494, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.5537, 'grad_norm': 0.3350730538368225, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.41it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.5285, 'grad_norm': 0.462089478969574, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.6239, 'grad_norm': 0.33956268429756165, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.28it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.6052, 'grad_norm': 0.2660742998123169, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.5877, 'grad_norm': 0.23273521661758423, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.18it/s] 80%|████████  | 12/15 [00:00<00:00, 15.99it/s]                                               {'loss': 0.4127, 'grad_norm': 0.7967501282691956, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.99it/s]                                               {'loss': 0.4726, 'grad_norm': 0.5577036142349243, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.99it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.5709, 'grad_norm': 0.30648401379585266, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.6432, 'grad_norm': 0.38834327459335327, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.81it/s]                                               {'train_runtime': 1.0581, 'train_samples_per_second': 401.65, 'train_steps_per_second': 14.176, 'train_loss': 0.5659744501113891, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.81it/s]100%|██████████| 15/15 [00:01<00:00, 14.18it/s]
CLIENT:24
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6163, 'grad_norm': 0.23737093806266785, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.88it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.62it/s]                                              {'loss': 0.667, 'grad_norm': 0.26855936646461487, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.62it/s]                                              {'loss': 0.5766, 'grad_norm': 0.4596412777900696, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.62it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.07it/s]                                              {'loss': 0.6095, 'grad_norm': 0.3280191123485565, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.07it/s]                                              {'loss': 0.6525, 'grad_norm': 0.28047460317611694, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.07it/s] 40%|████      | 6/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.5959, 'grad_norm': 0.3379591405391693, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.6388, 'grad_norm': 0.23710781335830688, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.04it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6048, 'grad_norm': 0.25970834493637085, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.627, 'grad_norm': 0.3025476336479187, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.06it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.68it/s]                                               {'loss': 0.6104, 'grad_norm': 0.21825511753559113, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.68it/s]                                               {'loss': 0.6579, 'grad_norm': 0.289828896522522, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.68it/s] 80%|████████  | 12/15 [00:00<00:00, 15.27it/s]                                               {'loss': 0.5653, 'grad_norm': 0.36690449714660645, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.27it/s]                                               {'loss': 0.5611, 'grad_norm': 0.3640873432159424, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.27it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.62, 'grad_norm': 0.24862945079803467, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.7041, 'grad_norm': 0.5489971041679382, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]                                               {'train_runtime': 1.0878, 'train_samples_per_second': 390.694, 'train_steps_per_second': 13.789, 'train_loss': 0.6204828182856242, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]100%|██████████| 15/15 [00:01<00:00, 13.79it/s]
CLIENT:64
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6366, 'grad_norm': 0.2155277281999588, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.63it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.78it/s]                                              {'loss': 0.5267, 'grad_norm': 0.6387644410133362, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.78it/s]                                              {'loss': 0.6519, 'grad_norm': 0.28284260630607605, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.78it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.6955, 'grad_norm': 0.3875541090965271, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.5369, 'grad_norm': 0.5843531489372253, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.45it/s] 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.5768, 'grad_norm': 0.4704039990901947, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.6842, 'grad_norm': 0.33324187994003296, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.50it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.86it/s]                                              {'loss': 0.5824, 'grad_norm': 0.33454328775405884, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.86it/s]                                              {'loss': 0.4832, 'grad_norm': 0.6904578804969788, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.86it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6614, 'grad_norm': 0.28839734196662903, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.488, 'grad_norm': 0.575687050819397, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.94it/s] 80%|████████  | 12/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.6226, 'grad_norm': 0.31889835000038147, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.6357, 'grad_norm': 0.2717885673046112, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.33it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.6222, 'grad_norm': 0.3009682297706604, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.516, 'grad_norm': 0.44185343384742737, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]                                               {'train_runtime': 1.0743, 'train_samples_per_second': 395.618, 'train_steps_per_second': 13.963, 'train_loss': 0.5946707824865977, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]100%|██████████| 15/15 [00:01<00:00, 13.97it/s]
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5798, 'grad_norm': 0.3652549982070923, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.07it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.32it/s]                                              {'loss': 0.587, 'grad_norm': 0.3600466847419739, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.32it/s]                                              {'loss': 0.6112, 'grad_norm': 0.31065618991851807, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.32it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.5431, 'grad_norm': 0.5333101153373718, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.6419, 'grad_norm': 0.2928605377674103, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.37it/s] 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.5697, 'grad_norm': 0.33998358249664307, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.5682, 'grad_norm': 0.3154173195362091, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.22it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.17it/s]                                              {'loss': 0.505, 'grad_norm': 0.5177639126777649, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.17it/s]                                              {'loss': 0.7055, 'grad_norm': 0.5579342842102051, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.17it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6217, 'grad_norm': 0.2511069178581238, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6246, 'grad_norm': 0.34005409479141235, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.4666, 'grad_norm': 0.6210277676582336, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.6106, 'grad_norm': 0.2550117075443268, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.50it/s]                                               {'loss': 0.5783, 'grad_norm': 0.322015643119812, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.50it/s]                                               {'loss': 0.5382, 'grad_norm': 0.3785330653190613, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.50it/s]                                               {'train_runtime': 1.0775, 'train_samples_per_second': 394.425, 'train_steps_per_second': 13.921, 'train_loss': 0.5834210733572642, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.50it/s]100%|██████████| 15/15 [00:01<00:00, 13.93it/s]
CLIENT:97
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6595, 'grad_norm': 0.2700510323047638, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.35it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.71it/s]                                              {'loss': 0.6231, 'grad_norm': 0.24330630898475647, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.71it/s]                                              {'loss': 0.676, 'grad_norm': 0.30807164311408997, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.71it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.88it/s]                                              {'loss': 0.6565, 'grad_norm': 0.2927206754684448, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.88it/s]                                              {'loss': 0.6786, 'grad_norm': 0.3251073658466339, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.88it/s] 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.6331, 'grad_norm': 0.2882551848888397, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.6681, 'grad_norm': 0.34984755516052246, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.59it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6434, 'grad_norm': 0.23111335933208466, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6627, 'grad_norm': 0.2859369218349457, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.31it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.7677, 'grad_norm': 0.7125673294067383, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.6017, 'grad_norm': 0.34069159626960754, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.84it/s] 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.541, 'grad_norm': 0.58694988489151, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.7245, 'grad_norm': 0.47744378447532654, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.35it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.6048, 'grad_norm': 0.30664530396461487, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.6082, 'grad_norm': 0.3870777189731598, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.49it/s]                                               {'train_runtime': 1.0698, 'train_samples_per_second': 397.254, 'train_steps_per_second': 14.021, 'train_loss': 0.6499307115872701, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.49it/s]100%|██████████| 15/15 [00:01<00:00, 14.03it/s]
CLIENT:29
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6463, 'grad_norm': 0.2853444814682007, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.08it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.82it/s]                                              {'loss': 0.6575, 'grad_norm': 0.244600310921669, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.82it/s]                                              {'loss': 0.6174, 'grad_norm': 0.32013046741485596, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.82it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.6562, 'grad_norm': 0.2564927339553833, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.6696, 'grad_norm': 0.2511962950229645, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.28it/s] 40%|████      | 6/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.6168, 'grad_norm': 0.33229783177375793, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.6055, 'grad_norm': 0.28749874234199524, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.26it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.00it/s]                                              {'loss': 0.6518, 'grad_norm': 0.2524457275867462, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.00it/s]                                              {'loss': 0.7173, 'grad_norm': 0.44193795323371887, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.00it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.6127, 'grad_norm': 0.2704944610595703, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.6809, 'grad_norm': 0.32534924149513245, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.93it/s] 80%|████████  | 12/15 [00:00<00:00, 15.93it/s]                                               {'loss': 0.6431, 'grad_norm': 0.27728238701820374, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.93it/s]                                               {'loss': 0.6818, 'grad_norm': 0.3974168002605438, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.93it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.596, 'grad_norm': 0.27865442633628845, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.6226, 'grad_norm': 0.30497801303863525, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.73it/s]                                               {'train_runtime': 1.0685, 'train_samples_per_second': 397.746, 'train_steps_per_second': 14.038, 'train_loss': 0.645041020711263, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.73it/s]100%|██████████| 15/15 [00:01<00:00, 14.04it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5132, 'grad_norm': 0.6545495390892029, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.69it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.4935, 'grad_norm': 0.8008021712303162, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.6072, 'grad_norm': 0.325775146484375, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.16it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.5491, 'grad_norm': 0.4044981300830841, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.4694, 'grad_norm': 0.6829611659049988, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.31it/s] 40%|████      | 6/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.5103, 'grad_norm': 0.4941960573196411, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.4451, 'grad_norm': 0.643617570400238, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.19it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.4424, 'grad_norm': 0.5982303023338318, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.6655, 'grad_norm': 0.47956380248069763, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.09it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.68it/s]                                               {'loss': 0.5601, 'grad_norm': 0.2749997675418854, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.68it/s]                                               {'loss': 0.4082, 'grad_norm': 0.5810868740081787, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.68it/s] 80%|████████  | 12/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.4357, 'grad_norm': 0.48148900270462036, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.4889, 'grad_norm': 0.32988670468330383, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.34it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.4708, 'grad_norm': 0.3104180097579956, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.4365, 'grad_norm': 0.43053755164146423, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]                                               {'train_runtime': 1.0817, 'train_samples_per_second': 392.908, 'train_steps_per_second': 13.867, 'train_loss': 0.4997200568517049, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]100%|██████████| 15/15 [00:01<00:00, 13.87it/s]
CLIENT:70
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5359, 'grad_norm': 0.5228020548820496, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.23it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.96it/s]                                              {'loss': 0.6275, 'grad_norm': 0.21459679305553436, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.96it/s]                                              {'loss': 0.5854, 'grad_norm': 0.3594297170639038, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.96it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6126, 'grad_norm': 0.306503564119339, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.5865, 'grad_norm': 0.2775003910064697, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.20it/s] 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.5634, 'grad_norm': 0.4000079035758972, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.4853, 'grad_norm': 0.6044020652770996, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.97it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.6487, 'grad_norm': 0.28944194316864014, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.6269, 'grad_norm': 0.31902050971984863, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.28it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.4781, 'grad_norm': 0.6244866251945496, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.6412, 'grad_norm': 0.34866705536842346, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.86it/s] 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6442, 'grad_norm': 0.37657180428504944, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.4476, 'grad_norm': 0.6748335957527161, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.32it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.7186, 'grad_norm': 0.5423736572265625, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.5562, 'grad_norm': 0.2833602726459503, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.37it/s]                                               {'train_runtime': 1.0728, 'train_samples_per_second': 396.177, 'train_steps_per_second': 13.983, 'train_loss': 0.5838745931784312, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]100%|██████████| 15/15 [00:01<00:00, 13.99it/s]
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6433, 'grad_norm': 0.1928320676088333, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.80it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.57it/s]                                              {'loss': 0.6577, 'grad_norm': 0.22759495675563812, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.57it/s]                                              {'loss': 0.483, 'grad_norm': 0.830247163772583, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.57it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.5592, 'grad_norm': 0.5168694257736206, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.5807, 'grad_norm': 0.3554498255252838, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.24it/s] 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.6468, 'grad_norm': 0.4909859597682953, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.6363, 'grad_norm': 0.2744251787662506, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.22it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.5757, 'grad_norm': 0.33545416593551636, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.5557, 'grad_norm': 0.3579689860343933, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.15it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6239, 'grad_norm': 0.2911622226238251, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.5768, 'grad_norm': 0.2584110200405121, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.5818, 'grad_norm': 0.30181682109832764, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.6213, 'grad_norm': 0.3212099075317383, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.28it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.5419, 'grad_norm': 0.4060894548892975, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.6342, 'grad_norm': 0.456105500459671, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.84it/s]                                               {'train_runtime': 1.0747, 'train_samples_per_second': 395.473, 'train_steps_per_second': 13.958, 'train_loss': 0.5945531606674195, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.84it/s]100%|██████████| 15/15 [00:01<00:00, 13.96it/s]
CLIENT:11
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6218, 'grad_norm': 0.25048115849494934, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.09it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.5509, 'grad_norm': 0.5427149534225464, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.7226, 'grad_norm': 0.5316802263259888, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.33it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.39it/s]                                              {'loss': 0.5742, 'grad_norm': 0.43945467472076416, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.39it/s]                                              {'loss': 0.6134, 'grad_norm': 0.28925982117652893, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.39it/s] 40%|████      | 6/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.6799, 'grad_norm': 0.5219603776931763, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.608, 'grad_norm': 0.2565900981426239, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.17it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.6118, 'grad_norm': 0.2567098140716553, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.6532, 'grad_norm': 0.3695002496242523, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.09it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.5675, 'grad_norm': 0.31703832745552063, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.6578, 'grad_norm': 0.2625465393066406, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.67it/s] 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.6169, 'grad_norm': 0.2985333502292633, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.6055, 'grad_norm': 0.24268291890621185, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.40it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.6893, 'grad_norm': 0.37697240710258484, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.5229, 'grad_norm': 0.5223827362060547, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.52it/s]                                               {'train_runtime': 1.0795, 'train_samples_per_second': 393.703, 'train_steps_per_second': 13.895, 'train_loss': 0.6197142283121745, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.52it/s]100%|██████████| 15/15 [00:01<00:00, 13.90it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 32.91it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.91it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.28it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.45it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.03it/s] 61%|██████    | 20/33 [00:00<00:00, 24.97it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.14it/s] 79%|███████▉  | 26/33 [00:01<00:00, 24.89it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.13it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.10it/s]100%|██████████| 33/33 [00:01<00:00, 25.81it/s]
{'eval_loss': 0.6175552010536194, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3198, 'eval_samples_per_second': 790.301, 'eval_steps_per_second': 25.005}
ROUND:21
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5738, 'grad_norm': 0.33259809017181396, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.27it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.07it/s]                                              {'loss': 0.5815, 'grad_norm': 0.332136869430542, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.07it/s]                                              {'loss': 0.6088, 'grad_norm': 0.3030278980731964, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.07it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.5338, 'grad_norm': 0.5181246995925903, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6439, 'grad_norm': 0.31569385528564453, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.16it/s] 40%|████      | 6/15 [00:00<00:00, 16.16it/s]                                              {'loss': 0.5669, 'grad_norm': 0.31909599900245667, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.16it/s]                                              {'loss': 0.5649, 'grad_norm': 0.2904389202594757, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.16it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.4981, 'grad_norm': 0.5016887187957764, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.7108, 'grad_norm': 0.6241862177848816, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.58it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.622, 'grad_norm': 0.2730659544467926, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.6255, 'grad_norm': 0.3517819046974182, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.08it/s] 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.4608, 'grad_norm': 0.6143751740455627, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.611, 'grad_norm': 0.26683974266052246, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.49it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.577, 'grad_norm': 0.3262687027454376, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.537, 'grad_norm': 0.3967027962207794, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.47it/s]                                               {'train_runtime': 1.059, 'train_samples_per_second': 401.337, 'train_steps_per_second': 14.165, 'train_loss': 0.5810480932394664, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.47it/s]100%|██████████| 15/15 [00:01<00:00, 14.17it/s]
CLIENT:67
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5495, 'grad_norm': 0.4094526171684265, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.65it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.5219, 'grad_norm': 0.5336769819259644, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6018, 'grad_norm': 0.3256183862686157, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.06it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.84it/s]                                              {'loss': 0.5025, 'grad_norm': 0.6065329313278198, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.84it/s]                                              {'loss': 0.5313, 'grad_norm': 0.4033944308757782, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.84it/s] 40%|████      | 6/15 [00:00<00:00, 15.76it/s]                                              {'loss': 0.638, 'grad_norm': 0.3631596863269806, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.76it/s]                                              {'loss': 0.6218, 'grad_norm': 0.27897390723228455, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.76it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.4788, 'grad_norm': 0.5361523032188416, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.4878, 'grad_norm': 0.47733214497566223, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.13it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.515, 'grad_norm': 0.3503526747226715, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.5415, 'grad_norm': 0.27424952387809753, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.75it/s] 80%|████████  | 12/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.5159, 'grad_norm': 0.37972185015678406, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.5365, 'grad_norm': 0.3150959014892578, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.24it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.5378, 'grad_norm': 0.36396679282188416, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.4696, 'grad_norm': 0.4400181770324707, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]                                               {'train_runtime': 1.0746, 'train_samples_per_second': 395.486, 'train_steps_per_second': 13.958, 'train_loss': 0.5366416692733764, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 13.96it/s]
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6029, 'grad_norm': 0.24494719505310059, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.03it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.73it/s]                                              {'loss': 0.5688, 'grad_norm': 0.34404420852661133, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.73it/s]                                              {'loss': 0.5714, 'grad_norm': 0.3424362540245056, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.73it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.02it/s]                                              {'loss': 0.5503, 'grad_norm': 0.41945844888687134, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.02it/s]                                              {'loss': 0.6452, 'grad_norm': 0.2755931615829468, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.02it/s] 40%|████      | 6/15 [00:00<00:00, 16.26it/s]                                              {'loss': 0.5438, 'grad_norm': 0.4122072756290436, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.26it/s]                                              {'loss': 0.5942, 'grad_norm': 0.20061036944389343, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.26it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.5969, 'grad_norm': 0.24271242320537567, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.55, 'grad_norm': 0.33524516224861145, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.66it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.5644, 'grad_norm': 0.281553715467453, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6165, 'grad_norm': 0.2974793016910553, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.13it/s] 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.5584, 'grad_norm': 0.3549072742462158, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.5497, 'grad_norm': 0.3068956434726715, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.53it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.6458, 'grad_norm': 0.36772990226745605, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.5699, 'grad_norm': 0.2980988621711731, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.56it/s]                                               {'train_runtime': 1.0611, 'train_samples_per_second': 400.523, 'train_steps_per_second': 14.136, 'train_loss': 0.5818841060002645, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.56it/s]100%|██████████| 15/15 [00:01<00:00, 14.14it/s]
CLIENT:64
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6373, 'grad_norm': 0.22230680286884308, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.56it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.81it/s]                                              {'loss': 0.5144, 'grad_norm': 0.6255196332931519, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.81it/s]                                              {'loss': 0.651, 'grad_norm': 0.3139096200466156, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.81it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.6997, 'grad_norm': 0.4491211771965027, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.5258, 'grad_norm': 0.5759568810462952, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.68it/s] 40%|████      | 6/15 [00:00<00:00, 15.99it/s]                                              {'loss': 0.5697, 'grad_norm': 0.45438575744628906, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.99it/s]                                              {'loss': 0.6874, 'grad_norm': 0.39308783411979675, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.99it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.82it/s]                                              {'loss': 0.5783, 'grad_norm': 0.3303530514240265, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.82it/s]                                              {'loss': 0.4735, 'grad_norm': 0.6643514633178711, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.82it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6632, 'grad_norm': 0.32102298736572266, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.4813, 'grad_norm': 0.5607115030288696, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.32it/s] 80%|████████  | 12/15 [00:00<00:00, 15.88it/s]                                               {'loss': 0.6238, 'grad_norm': 0.337927907705307, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.88it/s]                                               {'loss': 0.6358, 'grad_norm': 0.28687363862991333, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.88it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.6233, 'grad_norm': 0.2692364752292633, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.5123, 'grad_norm': 0.43067899346351624, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.74it/s]                                               {'train_runtime': 1.0533, 'train_samples_per_second': 403.476, 'train_steps_per_second': 14.24, 'train_loss': 0.5917841533819834, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.74it/s]100%|██████████| 15/15 [00:01<00:00, 14.25it/s]
CLIENT:96
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5312, 'grad_norm': 0.5065896511077881, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.79it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.6164, 'grad_norm': 0.23443438112735748, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.5275, 'grad_norm': 0.5482263565063477, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.97it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.5669, 'grad_norm': 0.37185826897621155, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.5634, 'grad_norm': 0.33787065744400024, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.13it/s] 40%|████      | 6/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.5103, 'grad_norm': 0.4732879400253296, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.65, 'grad_norm': 0.3687334656715393, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.88it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.4362, 'grad_norm': 0.6476246118545532, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.499, 'grad_norm': 0.4266239404678345, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.45it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.5421, 'grad_norm': 0.31057843565940857, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.6183, 'grad_norm': 0.30430278182029724, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.73it/s] 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.4089, 'grad_norm': 0.7195236086845398, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.4675, 'grad_norm': 0.47106069326400757, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.26it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.6299, 'grad_norm': 0.40648260712623596, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.4924, 'grad_norm': 0.3453143835067749, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]                                               {'train_runtime': 1.0748, 'train_samples_per_second': 395.408, 'train_steps_per_second': 13.956, 'train_loss': 0.5373391906420389, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]100%|██████████| 15/15 [00:01<00:00, 13.96it/s]
CLIENT:83
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5321, 'grad_norm': 0.561636209487915, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.38it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.5977, 'grad_norm': 0.3021736741065979, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.6168, 'grad_norm': 0.2598963975906372, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.04it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.5592, 'grad_norm': 0.35275930166244507, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6121, 'grad_norm': 0.23147538304328918, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.16it/s] 40%|████      | 6/15 [00:00<00:00, 16.36it/s]                                              {'loss': 0.5015, 'grad_norm': 0.5237798094749451, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.36it/s]                                              {'loss': 0.5879, 'grad_norm': 0.23725658655166626, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.36it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.5202, 'grad_norm': 0.4212983250617981, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.6026, 'grad_norm': 0.319621741771698, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.76it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.09it/s]                                               {'loss': 0.6092, 'grad_norm': 0.2670231759548187, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.09it/s]                                               {'loss': 0.4971, 'grad_norm': 0.40930408239364624, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.09it/s] 80%|████████  | 12/15 [00:00<00:00, 15.67it/s]                                               {'loss': 0.5828, 'grad_norm': 0.30320459604263306, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.67it/s]                                               {'loss': 0.4672, 'grad_norm': 0.476455956697464, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.67it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.661, 'grad_norm': 0.4586279094219208, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.5903, 'grad_norm': 0.3095221519470215, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.56it/s]                                               {'train_runtime': 1.0585, 'train_samples_per_second': 401.495, 'train_steps_per_second': 14.17, 'train_loss': 0.5691733022530874, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.56it/s]100%|██████████| 15/15 [00:01<00:00, 14.18it/s]
CLIENT:98
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.4985, 'grad_norm': 0.6162329912185669, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.04it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.31it/s]                                              {'loss': 0.6994, 'grad_norm': 0.49933019280433655, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.31it/s]                                              {'loss': 0.5546, 'grad_norm': 0.4391518235206604, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.31it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.5254, 'grad_norm': 0.49773597717285156, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.6405, 'grad_norm': 0.28290659189224243, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.41it/s] 40%|████      | 6/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.6002, 'grad_norm': 0.48458752036094666, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.6897, 'grad_norm': 0.653483510017395, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.24it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.5734, 'grad_norm': 0.2764383554458618, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.4389, 'grad_norm': 0.9841159582138062, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.10it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.5908, 'grad_norm': 0.27577266097068787, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.5577, 'grad_norm': 0.30046525597572327, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.98it/s] 80%|████████  | 12/15 [00:00<00:00, 15.68it/s]                                               {'loss': 0.5921, 'grad_norm': 0.32797083258628845, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.68it/s]                                               {'loss': 0.5758, 'grad_norm': 0.2693903148174286, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.68it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.5637, 'grad_norm': 0.2932673394680023, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.6033, 'grad_norm': 0.2834182381629944, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]                                               {'train_runtime': 1.0766, 'train_samples_per_second': 394.767, 'train_steps_per_second': 13.933, 'train_loss': 0.580258826414744, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]100%|██████████| 15/15 [00:01<00:00, 13.94it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6063, 'grad_norm': 0.2578931748867035, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.55it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.90it/s]                                              {'loss': 0.5134, 'grad_norm': 0.6151733994483948, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.90it/s]                                              {'loss': 0.7503, 'grad_norm': 0.6473605632781982, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.90it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.6265, 'grad_norm': 0.2241121083498001, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.6034, 'grad_norm': 0.24809618294239044, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.15it/s] 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.5708, 'grad_norm': 0.3657088875770569, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.5618, 'grad_norm': 0.3528098165988922, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.97it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.5956, 'grad_norm': 0.2616616487503052, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6809, 'grad_norm': 0.429498553276062, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.23it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.6444, 'grad_norm': 0.26722490787506104, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.5696, 'grad_norm': 0.3228175938129425, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.81it/s] 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.6012, 'grad_norm': 0.2675171494483948, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.6156, 'grad_norm': 0.2693239748477936, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.26it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.5752, 'grad_norm': 0.298118531703949, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6155, 'grad_norm': 0.28276509046554565, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]                                               {'train_runtime': 1.074, 'train_samples_per_second': 395.734, 'train_steps_per_second': 13.967, 'train_loss': 0.6087041695912679, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 13.97it/s]
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5942, 'grad_norm': 0.29324162006378174, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.76it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.57it/s]                                              {'loss': 0.6094, 'grad_norm': 0.2612050473690033, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.57it/s]                                              {'loss': 0.71, 'grad_norm': 0.52457195520401, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.57it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.5549, 'grad_norm': 0.4644213318824768, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.661, 'grad_norm': 0.26160743832588196, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.09it/s] 40%|████      | 6/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.6654, 'grad_norm': 0.34016501903533936, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.593, 'grad_norm': 0.27021777629852295, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.11it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.6778, 'grad_norm': 0.3543911278247833, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.5899, 'grad_norm': 0.30996131896972656, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.14it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.5903, 'grad_norm': 0.26935696601867676, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6753, 'grad_norm': 0.3678785264492035, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.6424, 'grad_norm': 0.30111342668533325, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.6331, 'grad_norm': 0.2752663791179657, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.6673, 'grad_norm': 0.31131020188331604, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.5365, 'grad_norm': 0.48571884632110596, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.91it/s]                                               {'train_runtime': 1.0726, 'train_samples_per_second': 396.236, 'train_steps_per_second': 13.985, 'train_loss': 0.6267060558001201, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.91it/s]100%|██████████| 15/15 [00:01<00:00, 13.99it/s]
CLIENT:15
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6078, 'grad_norm': 0.23910322785377502, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.03it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.621, 'grad_norm': 0.26429247856140137, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.6988, 'grad_norm': 0.47815805673599243, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.19it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.62it/s]                                              {'loss': 0.6289, 'grad_norm': 0.21990658342838287, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.62it/s]                                              {'loss': 0.6549, 'grad_norm': 0.293316125869751, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.62it/s] 40%|████      | 6/15 [00:00<00:00, 16.00it/s]                                              {'loss': 0.6416, 'grad_norm': 0.27925488352775574, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.00it/s]                                              {'loss': 0.6961, 'grad_norm': 0.393633097410202, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.00it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.5966, 'grad_norm': 0.2504751682281494, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.6048, 'grad_norm': 0.2883089482784271, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.59it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.6291, 'grad_norm': 0.25189390778541565, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.7158, 'grad_norm': 0.5514754056930542, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.08it/s] 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.5492, 'grad_norm': 0.5039108395576477, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.6239, 'grad_norm': 0.24169766902923584, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.47it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.6129, 'grad_norm': 0.25937435030937195, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.6562, 'grad_norm': 0.3115077018737793, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.61it/s]                                               {'train_runtime': 1.056, 'train_samples_per_second': 402.452, 'train_steps_per_second': 14.204, 'train_loss': 0.6358410835266113, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.61it/s]100%|██████████| 15/15 [00:01<00:00, 14.21it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 35.01it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.71it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.18it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.45it/s] 52%|█████▏    | 17/33 [00:00<00:00, 24.98it/s] 61%|██████    | 20/33 [00:00<00:00, 24.71it/s] 70%|██████▉   | 23/33 [00:00<00:00, 24.93it/s] 79%|███████▉  | 26/33 [00:01<00:00, 24.99it/s] 88%|████████▊ | 29/33 [00:01<00:00, 24.96it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.14it/s]100%|██████████| 33/33 [00:01<00:00, 25.80it/s]
{'eval_loss': 0.6169236898422241, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3206, 'eval_samples_per_second': 789.793, 'eval_steps_per_second': 24.989}
ROUND:22
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.631, 'grad_norm': 0.3111753463745117, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.81it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.09it/s]                                              {'loss': 0.5379, 'grad_norm': 0.4095773696899414, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.09it/s]                                              {'loss': 0.4789, 'grad_norm': 0.6276265382766724, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.09it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.628, 'grad_norm': 0.2899165749549866, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.4933, 'grad_norm': 0.5623868703842163, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.45it/s] 40%|████      | 6/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.5548, 'grad_norm': 0.4710424542427063, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.4819, 'grad_norm': 0.5517290234565735, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.32it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.597, 'grad_norm': 0.2870548367500305, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.5956, 'grad_norm': 0.3515769839286804, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.20it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.09it/s]                                               {'loss': 0.5502, 'grad_norm': 0.28951790928840637, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.09it/s]                                               {'loss': 0.553, 'grad_norm': 0.27665475010871887, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.09it/s] 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.52, 'grad_norm': 0.35370945930480957, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.6502, 'grad_norm': 0.42181655764579773, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.46it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.4903, 'grad_norm': 0.3791656792163849, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.4307, 'grad_norm': 0.5607925057411194, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]                                               {'train_runtime': 1.0772, 'train_samples_per_second': 394.526, 'train_steps_per_second': 13.924, 'train_loss': 0.5462026615937551, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]100%|██████████| 15/15 [00:01<00:00, 13.93it/s]
CLIENT:62
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.542, 'grad_norm': 0.49260881543159485, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.88it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.05it/s]                                              {'loss': 0.6002, 'grad_norm': 0.2590464949607849, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.05it/s]                                              {'loss': 0.4086, 'grad_norm': 0.9790422320365906, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.05it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.4901, 'grad_norm': 0.5789680480957031, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.6284, 'grad_norm': 0.27788570523262024, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.68it/s] 40%|████      | 6/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.4222, 'grad_norm': 0.717819333076477, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.5191, 'grad_norm': 0.353552907705307, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.42it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.4794, 'grad_norm': 0.45796412229537964, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.5506, 'grad_norm': 0.3687569499015808, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.23it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.5384, 'grad_norm': 0.28405603766441345, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.5229, 'grad_norm': 0.2482070028781891, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.4247, 'grad_norm': 0.48952561616897583, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.5154, 'grad_norm': 0.2793169617652893, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.40it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.5622, 'grad_norm': 0.3112196922302246, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.3699, 'grad_norm': 0.5936082005500793, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]                                               {'train_runtime': 1.0739, 'train_samples_per_second': 395.751, 'train_steps_per_second': 13.968, 'train_loss': 0.5049525717894237, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 13.97it/s]
CLIENT:26
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5448, 'grad_norm': 0.43961623311042786, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.95it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.29it/s]                                              {'loss': 0.6112, 'grad_norm': 0.29800617694854736, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.29it/s]                                              {'loss': 0.7638, 'grad_norm': 0.7055003046989441, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.29it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.6333, 'grad_norm': 0.2753608226776123, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.6489, 'grad_norm': 0.2444283813238144, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.33it/s] 40%|████      | 6/15 [00:00<00:00, 16.40it/s]                                              {'loss': 0.5355, 'grad_norm': 0.4458443522453308, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.40it/s]                                              {'loss': 0.5579, 'grad_norm': 0.37100139260292053, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.40it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.686, 'grad_norm': 0.4025856554508209, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.5606, 'grad_norm': 0.36501261591911316, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.70it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.6405, 'grad_norm': 0.29068252444267273, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.6321, 'grad_norm': 0.30565065145492554, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.14it/s] 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.5319, 'grad_norm': 0.5364990234375, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.6113, 'grad_norm': 0.24797429144382477, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.57it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.5978, 'grad_norm': 0.34518638253211975, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.6482, 'grad_norm': 0.307987242937088, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.52it/s]                                               {'train_runtime': 1.0592, 'train_samples_per_second': 401.262, 'train_steps_per_second': 14.162, 'train_loss': 0.6135919014612834, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.52it/s]100%|██████████| 15/15 [00:01<00:00, 14.17it/s]
CLIENT:41
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6279, 'grad_norm': 0.35643288493156433, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.33it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.02it/s]                                              {'loss': 0.7561, 'grad_norm': 0.7061840891838074, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.02it/s]                                              {'loss': 0.709, 'grad_norm': 0.5132695436477661, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.02it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.8129, 'grad_norm': 0.8537169098854065, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.5974, 'grad_norm': 0.3390854299068451, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.62it/s] 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.6928, 'grad_norm': 0.3499152660369873, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.6056, 'grad_norm': 0.2968796193599701, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.59it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.52it/s]                                              {'loss': 0.7294, 'grad_norm': 0.4713503420352936, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.52it/s]                                              {'loss': 0.7361, 'grad_norm': 0.5575976967811584, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.52it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.00it/s]                                               {'loss': 0.7043, 'grad_norm': 0.33858248591423035, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.00it/s]                                               {'loss': 0.6699, 'grad_norm': 0.24428243935108185, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.00it/s] 80%|████████  | 12/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.6422, 'grad_norm': 0.2941037714481354, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.6617, 'grad_norm': 0.2607271373271942, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.39it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6587, 'grad_norm': 0.220026433467865, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.7304, 'grad_norm': 0.4925132095813751, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.45it/s]                                               {'train_runtime': 1.0715, 'train_samples_per_second': 396.637, 'train_steps_per_second': 13.999, 'train_loss': 0.6889457980791728, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 14.00it/s]
CLIENT:35
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5662, 'grad_norm': 0.34525585174560547, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.87it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.6546, 'grad_norm': 0.30809009075164795, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.6768, 'grad_norm': 0.4493924677371979, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.14it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.5986, 'grad_norm': 0.27695712447166443, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6198, 'grad_norm': 0.25187307596206665, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.13it/s] 40%|████      | 6/15 [00:00<00:00, 15.99it/s]                                              {'loss': 0.6685, 'grad_norm': 0.3666408360004425, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.99it/s]                                              {'loss': 0.6571, 'grad_norm': 0.3601459562778473, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.99it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.5959, 'grad_norm': 0.2625664174556732, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.6337, 'grad_norm': 0.3641592562198639, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.60it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.6245, 'grad_norm': 0.22538433969020844, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.6659, 'grad_norm': 0.3667052686214447, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.96it/s] 80%|████████  | 12/15 [00:00<00:00, 15.64it/s]                                               {'loss': 0.5322, 'grad_norm': 0.5041050314903259, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.64it/s]                                               {'loss': 0.6596, 'grad_norm': 0.3105757534503937, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.64it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.6245, 'grad_norm': 0.2592780590057373, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.5707, 'grad_norm': 0.4193348288536072, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.60it/s]                                               {'train_runtime': 1.0572, 'train_samples_per_second': 401.996, 'train_steps_per_second': 14.188, 'train_loss': 0.6232436418533325, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.60it/s]100%|██████████| 15/15 [00:01<00:00, 14.19it/s]
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5913, 'grad_norm': 0.2581730782985687, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.14it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.92it/s]                                              {'loss': 0.6079, 'grad_norm': 0.2562703788280487, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.92it/s]                                              {'loss': 0.7178, 'grad_norm': 0.5612925291061401, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.92it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.5482, 'grad_norm': 0.42504170536994934, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.6635, 'grad_norm': 0.3101598620414734, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.18it/s] 40%|████      | 6/15 [00:00<00:00, 16.52it/s]                                              {'loss': 0.6677, 'grad_norm': 0.38154125213623047, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.52it/s]                                              {'loss': 0.5916, 'grad_norm': 0.25821352005004883, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.52it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.6798, 'grad_norm': 0.38378992676734924, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.5882, 'grad_norm': 0.31801703572273254, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.85it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.5889, 'grad_norm': 0.2601584196090698, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.6762, 'grad_norm': 0.38194888830184937, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.48it/s] 80%|████████  | 12/15 [00:00<00:00, 15.77it/s]                                               {'loss': 0.6424, 'grad_norm': 0.30188310146331787, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.77it/s]                                               {'loss': 0.6333, 'grad_norm': 0.28810158371925354, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.77it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.668, 'grad_norm': 0.32055550813674927, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.5347, 'grad_norm': 0.48057109117507935, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.71it/s]                                               {'train_runtime': 1.0498, 'train_samples_per_second': 404.852, 'train_steps_per_second': 14.289, 'train_loss': 0.6266276836395264, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.71it/s]100%|██████████| 15/15 [00:01<00:00, 14.29it/s]
CLIENT:36
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5847, 'grad_norm': 0.2677907347679138, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.06it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.75it/s]                                              {'loss': 0.591, 'grad_norm': 0.31971636414527893, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.75it/s]                                              {'loss': 0.5199, 'grad_norm': 0.5054818987846375, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.75it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.5258, 'grad_norm': 0.45423954725265503, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.6017, 'grad_norm': 0.27658239006996155, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.15it/s] 40%|████      | 6/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.5942, 'grad_norm': 0.33024612069129944, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.4472, 'grad_norm': 0.6921449899673462, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.11it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.05it/s]                                              {'loss': 0.7205, 'grad_norm': 0.6631112098693848, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.05it/s]                                              {'loss': 0.5192, 'grad_norm': 0.41565942764282227, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.05it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.4814, 'grad_norm': 0.48323020339012146, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.6263, 'grad_norm': 0.3382115960121155, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.95it/s] 80%|████████  | 12/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.6112, 'grad_norm': 0.36892837285995483, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.6313, 'grad_norm': 0.33881881833076477, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.34it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.5216, 'grad_norm': 0.35966870188713074, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.5254, 'grad_norm': 0.33044716715812683, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.67it/s]                                               {'train_runtime': 1.0759, 'train_samples_per_second': 395.008, 'train_steps_per_second': 13.941, 'train_loss': 0.5667583644390106, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.67it/s]100%|██████████| 15/15 [00:01<00:00, 13.95it/s]
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6727, 'grad_norm': 0.4049874544143677, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.93it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.64it/s]                                              {'loss': 0.5242, 'grad_norm': 0.4906485378742218, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.64it/s]                                              {'loss': 0.5287, 'grad_norm': 0.5008521676063538, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.64it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.6211, 'grad_norm': 0.2488899976015091, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.4938, 'grad_norm': 0.5636109113693237, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.35it/s] 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.6451, 'grad_norm': 0.38094183802604675, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.4654, 'grad_norm': 0.7026400566101074, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.29it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.6389, 'grad_norm': 0.3085549473762512, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.7039, 'grad_norm': 0.6789969205856323, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.13it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.6259, 'grad_norm': 0.35499534010887146, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.5268, 'grad_norm': 0.3311839699745178, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.74it/s] 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.5419, 'grad_norm': 0.3735092580318451, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.6474, 'grad_norm': 0.3937341272830963, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.26it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.33it/s]                                               {'loss': 0.5406, 'grad_norm': 0.37710222601890564, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.33it/s]                                               {'loss': 0.5418, 'grad_norm': 0.3290540277957916, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.33it/s]                                               {'train_runtime': 1.0784, 'train_samples_per_second': 394.09, 'train_steps_per_second': 13.909, 'train_loss': 0.5812113424142201, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.33it/s]100%|██████████| 15/15 [00:01<00:00, 13.91it/s]
CLIENT:77
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6178, 'grad_norm': 0.253993421792984, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.83it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.02it/s]                                              {'loss': 0.6921, 'grad_norm': 0.3675353229045868, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.02it/s]                                              {'loss': 0.6782, 'grad_norm': 0.4134555459022522, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.02it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.7143, 'grad_norm': 0.46912822127342224, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.6622, 'grad_norm': 0.28044652938842773, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.19it/s] 40%|████      | 6/15 [00:00<00:00, 15.72it/s]                                              {'loss': 0.5743, 'grad_norm': 0.4166756272315979, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.72it/s]                                              {'loss': 0.6365, 'grad_norm': 0.2606610655784607, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.72it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.47it/s]                                              {'loss': 0.68, 'grad_norm': 0.36227425932884216, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.47it/s]                                              {'loss': 0.6384, 'grad_norm': 0.3028062880039215, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.47it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.6745, 'grad_norm': 0.32888171076774597, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.5995, 'grad_norm': 0.3071424067020416, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.15it/s] 80%|████████  | 12/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.712, 'grad_norm': 0.43911534547805786, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.6117, 'grad_norm': 0.2682853043079376, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.52it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.7301, 'grad_norm': 0.5226017832756042, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.6127, 'grad_norm': 0.36406829953193665, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.52it/s]                                               {'train_runtime': 1.0612, 'train_samples_per_second': 400.504, 'train_steps_per_second': 14.135, 'train_loss': 0.655623730023702, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.52it/s]100%|██████████| 15/15 [00:01<00:00, 14.14it/s]
CLIENT:21
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5704, 'grad_norm': 0.2817802429199219, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.11it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.40it/s]                                              {'loss': 0.6608, 'grad_norm': 0.33483850955963135, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.40it/s]                                              {'loss': 0.5188, 'grad_norm': 0.5475594401359558, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.40it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.5991, 'grad_norm': 0.34569957852363586, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.5706, 'grad_norm': 0.36730116605758667, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.43it/s] 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.6431, 'grad_norm': 0.3703964054584503, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.4859, 'grad_norm': 0.5691628456115723, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.29it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.7119, 'grad_norm': 0.5423671007156372, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.5531, 'grad_norm': 0.3490849435329437, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.59it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6289, 'grad_norm': 0.2836839258670807, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6042, 'grad_norm': 0.26796039938926697, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.76it/s] 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.5589, 'grad_norm': 0.33045846223831177, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.5757, 'grad_norm': 0.3672953248023987, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.26it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.29it/s]                                               {'loss': 0.6448, 'grad_norm': 0.3717688322067261, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.29it/s]                                               {'loss': 0.5572, 'grad_norm': 0.31333327293395996, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.29it/s]                                               {'train_runtime': 1.0795, 'train_samples_per_second': 393.704, 'train_steps_per_second': 13.895, 'train_loss': 0.5922355572382609, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.29it/s]100%|██████████| 15/15 [00:01<00:00, 13.90it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 37.39it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.98it/s] 36%|███▋      | 12/33 [00:00<00:00, 26.82it/s] 45%|████▌     | 15/33 [00:00<00:00, 26.35it/s] 55%|█████▍    | 18/33 [00:00<00:00, 25.76it/s] 64%|██████▎   | 21/33 [00:00<00:00, 25.62it/s] 73%|███████▎  | 24/33 [00:00<00:00, 25.41it/s] 82%|████████▏ | 27/33 [00:01<00:00, 25.23it/s] 91%|█████████ | 30/33 [00:01<00:00, 25.25it/s]100%|██████████| 33/33 [00:01<00:00, 26.06it/s]100%|██████████| 33/33 [00:01<00:00, 26.31it/s]
{'eval_loss': 0.616912305355072, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.2958, 'eval_samples_per_second': 804.937, 'eval_steps_per_second': 25.468}
ROUND:23
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6046, 'grad_norm': 0.2554011344909668, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.01it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.28it/s]                                              {'loss': 0.4987, 'grad_norm': 0.5816488862037659, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.28it/s]                                              {'loss': 0.7646, 'grad_norm': 0.7666616439819336, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.28it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.6281, 'grad_norm': 0.25001850724220276, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.6017, 'grad_norm': 0.23436644673347473, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.37it/s] 40%|████      | 6/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.5668, 'grad_norm': 0.34055790305137634, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.5581, 'grad_norm': 0.3334888517856598, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.18it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.5945, 'grad_norm': 0.260353147983551, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6842, 'grad_norm': 0.48279300332069397, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.06it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.6464, 'grad_norm': 0.30456966161727905, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.5678, 'grad_norm': 0.31832438707351685, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.64it/s] 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.6007, 'grad_norm': 0.26999637484550476, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.6155, 'grad_norm': 0.3225870430469513, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.37it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.5736, 'grad_norm': 0.2902072072029114, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.6163, 'grad_norm': 0.29930439591407776, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.86it/s]                                               {'train_runtime': 1.0731, 'train_samples_per_second': 396.041, 'train_steps_per_second': 13.978, 'train_loss': 0.6081063787142436, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.86it/s]100%|██████████| 15/15 [00:01<00:00, 13.98it/s]
CLIENT:54
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5593, 'grad_norm': 0.3304811418056488, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.89it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.46it/s]                                              {'loss': 0.6829, 'grad_norm': 0.41870230436325073, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.46it/s]                                              {'loss': 0.6474, 'grad_norm': 0.37465959787368774, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.46it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.5888, 'grad_norm': 0.27498331665992737, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.7071, 'grad_norm': 0.44134342670440674, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.76it/s] 40%|████      | 6/15 [00:00<00:00, 15.77it/s]                                              {'loss': 0.5534, 'grad_norm': 0.38911518454551697, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.77it/s]                                              {'loss': 0.5401, 'grad_norm': 0.37218108773231506, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.77it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.33it/s]                                              {'loss': 0.6646, 'grad_norm': 0.3569197356700897, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.33it/s]                                              {'loss': 0.6778, 'grad_norm': 0.5254223346710205, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.33it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.6101, 'grad_norm': 0.23333954811096191, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.6445, 'grad_norm': 0.2780381143093109, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.78it/s] 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.574, 'grad_norm': 0.35856038331985474, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.5807, 'grad_norm': 0.32032516598701477, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.43it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.5932, 'grad_norm': 0.26245662569999695, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.6906, 'grad_norm': 0.5684021711349487, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.34it/s]                                               {'train_runtime': 1.075, 'train_samples_per_second': 395.336, 'train_steps_per_second': 13.953, 'train_loss': 0.6209669629732768, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.34it/s]100%|██████████| 15/15 [00:01<00:00, 13.96it/s]
CLIENT:27
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.4612, 'grad_norm': 0.6944811344146729, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.08it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.5824, 'grad_norm': 0.259057879447937, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.6556, 'grad_norm': 0.4420570731163025, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.30it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.6599, 'grad_norm': 0.35054296255111694, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.5456, 'grad_norm': 0.3876691460609436, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.48it/s] 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.4615, 'grad_norm': 0.6648443937301636, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.6031, 'grad_norm': 0.2727087736129761, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.29it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.5143, 'grad_norm': 0.3647303879261017, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.5393, 'grad_norm': 0.3189943730831146, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.15it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.5471, 'grad_norm': 0.31758731603622437, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.541, 'grad_norm': 0.30913546681404114, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.62it/s] 80%|████████  | 12/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.5761, 'grad_norm': 0.31424033641815186, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.5234, 'grad_norm': 0.31783413887023926, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.36it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.5808, 'grad_norm': 0.2653084099292755, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.5751, 'grad_norm': 0.3323361575603485, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.65it/s]                                               {'train_runtime': 1.0723, 'train_samples_per_second': 396.35, 'train_steps_per_second': 13.989, 'train_loss': 0.5577527344226837, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.65it/s]100%|██████████| 15/15 [00:01<00:00, 13.99it/s]
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5681, 'grad_norm': 0.29748180508613586, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.10it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.59it/s]                                              {'loss': 0.576, 'grad_norm': 0.2988426685333252, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.59it/s]                                              {'loss': 0.6075, 'grad_norm': 0.3012853264808655, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.59it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.96it/s]                                              {'loss': 0.5232, 'grad_norm': 0.487944096326828, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.96it/s]                                              {'loss': 0.6476, 'grad_norm': 0.3672901391983032, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.96it/s] 40%|████      | 6/15 [00:00<00:00, 15.73it/s]                                              {'loss': 0.5643, 'grad_norm': 0.3185045123100281, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.73it/s]                                              {'loss': 0.5618, 'grad_norm': 0.2725217044353485, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.73it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.4905, 'grad_norm': 0.47611814737319946, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.7181, 'grad_norm': 0.6917848587036133, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.41it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.6224, 'grad_norm': 0.290548712015152, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.6277, 'grad_norm': 0.3949100077152252, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.93it/s] 80%|████████  | 12/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.4543, 'grad_norm': 0.6010886430740356, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.6114, 'grad_norm': 0.2968488931655884, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.55it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.575, 'grad_norm': 0.3241763114929199, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.5346, 'grad_norm': 0.3866156041622162, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.55it/s]                                               {'train_runtime': 1.0651, 'train_samples_per_second': 399.036, 'train_steps_per_second': 14.084, 'train_loss': 0.5788281778494517, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.55it/s]100%|██████████| 15/15 [00:01<00:00, 14.09it/s]
CLIENT:39
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.53, 'grad_norm': 0.47388356924057007, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.99it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.699, 'grad_norm': 0.4954586923122406, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.6166, 'grad_norm': 0.29580193758010864, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.19it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.10it/s]                                              {'loss': 0.6635, 'grad_norm': 0.3514471650123596, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.10it/s]                                              {'loss': 0.5921, 'grad_norm': 0.277408629655838, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.10it/s] 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.5688, 'grad_norm': 0.33582866191864014, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.7115, 'grad_norm': 0.4838752746582031, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.97it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.5552, 'grad_norm': 0.3847559988498688, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.5597, 'grad_norm': 0.3248371183872223, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.60it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.6018, 'grad_norm': 0.2528643310070038, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.5845, 'grad_norm': 0.26376017928123474, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.25it/s] 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.5859, 'grad_norm': 0.31039613485336304, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.5249, 'grad_norm': 0.4677785038948059, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.57it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.6547, 'grad_norm': 0.34764087200164795, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.6811, 'grad_norm': 0.459092378616333, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.56it/s]                                               {'train_runtime': 1.056, 'train_samples_per_second': 402.452, 'train_steps_per_second': 14.204, 'train_loss': 0.6086137215296428, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.56it/s]100%|██████████| 15/15 [00:01<00:00, 14.21it/s]
CLIENT:65
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6808, 'grad_norm': 0.37145906686782837, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.06it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.5791, 'grad_norm': 0.29556411504745483, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.7339, 'grad_norm': 0.5970056653022766, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.30it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.6451, 'grad_norm': 0.28234297037124634, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.625, 'grad_norm': 0.2551540732383728, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.25it/s] 40%|████      | 6/15 [00:00<00:00, 15.55it/s]                                              {'loss': 0.6754, 'grad_norm': 0.4189251661300659, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.55it/s]                                              {'loss': 0.6622, 'grad_norm': 0.2796999216079712, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.55it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6985, 'grad_norm': 0.4338388442993164, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.5682, 'grad_norm': 0.4338147044181824, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.23it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6813, 'grad_norm': 0.28597959876060486, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6618, 'grad_norm': 0.2678248882293701, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.82it/s] 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.5963, 'grad_norm': 0.35671937465667725, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.6216, 'grad_norm': 0.24752306938171387, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.33it/s]                                               {'loss': 0.6502, 'grad_norm': 0.24417373538017273, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.33it/s]                                               {'loss': 0.6819, 'grad_norm': 0.36948856711387634, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.33it/s]                                               {'train_runtime': 1.0807, 'train_samples_per_second': 393.261, 'train_steps_per_second': 13.88, 'train_loss': 0.6507526199022929, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.33it/s]100%|██████████| 15/15 [00:01<00:00, 13.88it/s]
CLIENT:7
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6396, 'grad_norm': 0.2762962877750397, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.89it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.6858, 'grad_norm': 0.39984506368637085, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.5684, 'grad_norm': 0.34346142411231995, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.23it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.5882, 'grad_norm': 0.2739675045013428, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.6109, 'grad_norm': 0.26540812849998474, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.45it/s] 40%|████      | 6/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.7229, 'grad_norm': 0.6189671754837036, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.5949, 'grad_norm': 0.27344149351119995, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.32it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.6498, 'grad_norm': 0.2931938171386719, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.6351, 'grad_norm': 0.2989095449447632, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.60it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6207, 'grad_norm': 0.27527546882629395, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.641, 'grad_norm': 0.2625475525856018, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.82it/s] 80%|████████  | 12/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.633, 'grad_norm': 0.2846226394176483, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.6734, 'grad_norm': 0.2912880778312683, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.34it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.5817, 'grad_norm': 0.3306124210357666, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.6157, 'grad_norm': 0.30626213550567627, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]                                               {'train_runtime': 1.0722, 'train_samples_per_second': 396.389, 'train_steps_per_second': 13.99, 'train_loss': 0.6307459950447083, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]100%|██████████| 15/15 [00:01<00:00, 14.00it/s]
CLIENT:26
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5421, 'grad_norm': 0.42083635926246643, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.91it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.62it/s]                                              {'loss': 0.6115, 'grad_norm': 0.31902480125427246, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.62it/s]                                              {'loss': 0.7671, 'grad_norm': 0.7282219529151917, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.62it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6331, 'grad_norm': 0.2763611078262329, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6492, 'grad_norm': 0.2559814453125, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.31it/s] 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.5338, 'grad_norm': 0.44070589542388916, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.5572, 'grad_norm': 0.36704036593437195, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.22it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.687, 'grad_norm': 0.39927205443382263, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.5595, 'grad_norm': 0.3635326325893402, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.10it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.641, 'grad_norm': 0.2953515648841858, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.632, 'grad_norm': 0.31409573554992676, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.73it/s] 80%|████████  | 12/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.5304, 'grad_norm': 0.545587420463562, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.611, 'grad_norm': 0.25441041588783264, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.24it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.597, 'grad_norm': 0.33097049593925476, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.6485, 'grad_norm': 0.3099749684333801, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.47it/s]                                               {'train_runtime': 1.0807, 'train_samples_per_second': 393.266, 'train_steps_per_second': 13.88, 'train_loss': 0.6133507331212361, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.47it/s]100%|██████████| 15/15 [00:01<00:00, 13.89it/s]
CLIENT:24
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6143, 'grad_norm': 0.2371600866317749, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.93it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.39it/s]                                              {'loss': 0.6768, 'grad_norm': 0.41566169261932373, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.39it/s]                                              {'loss': 0.5619, 'grad_norm': 0.3772261142730713, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.39it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.86it/s]                                              {'loss': 0.6002, 'grad_norm': 0.27944743633270264, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.86it/s]                                              {'loss': 0.6562, 'grad_norm': 0.3347845673561096, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.86it/s] 40%|████      | 6/15 [00:00<00:00, 15.62it/s]                                              {'loss': 0.595, 'grad_norm': 0.37018299102783203, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.62it/s]                                              {'loss': 0.6421, 'grad_norm': 0.2804431915283203, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.62it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.6019, 'grad_norm': 0.251947820186615, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.6282, 'grad_norm': 0.3500249981880188, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.43it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.6095, 'grad_norm': 0.24408148229122162, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.6598, 'grad_norm': 0.33518022298812866, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.95it/s] 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.5612, 'grad_norm': 0.35299769043922424, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.5577, 'grad_norm': 0.3543544411659241, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.51it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.6194, 'grad_norm': 0.26062455773353577, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.7098, 'grad_norm': 0.6062670946121216, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.52it/s]                                               {'train_runtime': 1.0657, 'train_samples_per_second': 398.788, 'train_steps_per_second': 14.075, 'train_loss': 0.6195948918660482, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.52it/s]100%|██████████| 15/15 [00:01<00:00, 14.08it/s]
CLIENT:60
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6463, 'grad_norm': 0.31460192799568176, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.25it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.69it/s]                                              {'loss': 0.6646, 'grad_norm': 0.349743127822876, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.69it/s]                                              {'loss': 0.666, 'grad_norm': 0.4559915363788605, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.69it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.6013, 'grad_norm': 0.27866408228874207, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.6933, 'grad_norm': 0.4439640939235687, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.18it/s] 40%|████      | 6/15 [00:00<00:00, 15.70it/s]                                              {'loss': 0.6991, 'grad_norm': 0.5419542193412781, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.70it/s]                                              {'loss': 0.7084, 'grad_norm': 0.5061274170875549, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.70it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.6231, 'grad_norm': 0.24090784788131714, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.6571, 'grad_norm': 0.26798561215400696, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.48it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.5578, 'grad_norm': 0.42198362946510315, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.7297, 'grad_norm': 0.5859572887420654, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.93it/s] 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6594, 'grad_norm': 0.2922089397907257, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.7426, 'grad_norm': 0.5768347382545471, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.54it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.90it/s]                                               {'loss': 0.6302, 'grad_norm': 0.23983252048492432, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.90it/s]                                               {'loss': 0.5376, 'grad_norm': 0.5977032780647278, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.90it/s]                                               {'train_runtime': 1.0537, 'train_samples_per_second': 403.337, 'train_steps_per_second': 14.235, 'train_loss': 0.6544361591339112, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.90it/s]100%|██████████| 15/15 [00:01<00:00, 14.24it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 36.71it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.42it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.54it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.53it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.23it/s] 61%|██████    | 20/33 [00:00<00:00, 25.01it/s] 70%|██████▉   | 23/33 [00:00<00:00, 24.73it/s] 79%|███████▉  | 26/33 [00:01<00:00, 24.56it/s] 88%|████████▊ | 29/33 [00:01<00:00, 24.48it/s] 97%|█████████▋| 32/33 [00:01<00:00, 24.52it/s]100%|██████████| 33/33 [00:01<00:00, 25.66it/s]
{'eval_loss': 0.6167987585067749, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3275, 'eval_samples_per_second': 785.672, 'eval_steps_per_second': 24.858}
ROUND:24
CLIENT:38
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6301, 'grad_norm': 0.27610206604003906, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.65it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.78it/s]                                              {'loss': 0.6229, 'grad_norm': 0.27899086475372314, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.78it/s]                                              {'loss': 0.5635, 'grad_norm': 0.35672879219055176, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.78it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.95it/s]                                              {'loss': 0.5445, 'grad_norm': 0.3955881595611572, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.95it/s]                                              {'loss': 0.6718, 'grad_norm': 0.3749426007270813, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.95it/s] 40%|████      | 6/15 [00:00<00:00, 16.26it/s]                                              {'loss': 0.6069, 'grad_norm': 0.32007819414138794, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.26it/s]                                              {'loss': 0.5925, 'grad_norm': 0.25162258744239807, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.26it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.5771, 'grad_norm': 0.25323617458343506, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.6473, 'grad_norm': 0.35245808959007263, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.70it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.6407, 'grad_norm': 0.3437524139881134, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.5706, 'grad_norm': 0.25670742988586426, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.07it/s] 80%|████████  | 12/15 [00:00<00:00, 15.65it/s]                                               {'loss': 0.5934, 'grad_norm': 0.3453823924064636, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.65it/s]                                               {'loss': 0.6549, 'grad_norm': 0.3213629722595215, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.65it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.46it/s]                                               {'loss': 0.6346, 'grad_norm': 0.32131868600845337, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.46it/s]                                               {'loss': 0.4333, 'grad_norm': 0.7828303575515747, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.46it/s]                                               {'train_runtime': 1.0644, 'train_samples_per_second': 399.287, 'train_steps_per_second': 14.092, 'train_loss': 0.5989354054133097, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.46it/s]100%|██████████| 15/15 [00:01<00:00, 14.10it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6307, 'grad_norm': 0.30325233936309814, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.80it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.54it/s]                                              {'loss': 0.536, 'grad_norm': 0.4124659597873688, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.54it/s]                                              {'loss': 0.4764, 'grad_norm': 0.6354578733444214, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.54it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.33it/s]                                              {'loss': 0.6282, 'grad_norm': 0.29448962211608887, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.33it/s]                                              {'loss': 0.4906, 'grad_norm': 0.5567560791969299, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.33it/s] 40%|████      | 6/15 [00:00<00:00, 15.27it/s]                                              {'loss': 0.5535, 'grad_norm': 0.4543457627296448, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.27it/s]                                              {'loss': 0.4792, 'grad_norm': 0.5480561256408691, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.27it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.5968, 'grad_norm': 0.2944211959838867, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.5958, 'grad_norm': 0.34944382309913635, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.18it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.5496, 'grad_norm': 0.28275641798973083, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.5525, 'grad_norm': 0.2756044268608093, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.11it/s] 80%|████████  | 12/15 [00:00<00:00, 15.61it/s]                                               {'loss': 0.5195, 'grad_norm': 0.3564970791339874, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.61it/s]                                               {'loss': 0.6516, 'grad_norm': 0.43358415365219116, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.61it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.31it/s]                                               {'loss': 0.4892, 'grad_norm': 0.3790595829486847, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.31it/s]                                               {'loss': 0.4291, 'grad_norm': 0.5589399337768555, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.31it/s]                                               {'train_runtime': 1.0813, 'train_samples_per_second': 393.041, 'train_steps_per_second': 13.872, 'train_loss': 0.5452388862768809, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.31it/s]100%|██████████| 15/15 [00:01<00:00, 13.88it/s]
CLIENT:32
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.633, 'grad_norm': 0.26870301365852356, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.42it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.5219, 'grad_norm': 0.5156325101852417, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.5686, 'grad_norm': 0.33205756545066833, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.01it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.5651, 'grad_norm': 0.34142088890075684, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.4886, 'grad_norm': 0.5656027793884277, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.19it/s] 40%|████      | 6/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.6805, 'grad_norm': 0.5060279965400696, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.4815, 'grad_norm': 0.5176005959510803, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.09it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.03it/s]                                              {'loss': 0.6604, 'grad_norm': 0.3747239410877228, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.03it/s]                                              {'loss': 0.5621, 'grad_norm': 0.35866832733154297, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.03it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.4919, 'grad_norm': 0.4590492844581604, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.5612, 'grad_norm': 0.28657034039497375, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.61it/s] 80%|████████  | 12/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.6528, 'grad_norm': 0.45176196098327637, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.6422, 'grad_norm': 0.36021140217781067, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.36it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.4938, 'grad_norm': 0.4324064254760742, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.5328, 'grad_norm': 0.3674526810646057, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]                                               {'train_runtime': 1.0837, 'train_samples_per_second': 392.185, 'train_steps_per_second': 13.842, 'train_loss': 0.5690883974234263, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]100%|██████████| 15/15 [00:01<00:00, 13.85it/s]
CLIENT:50
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5599, 'grad_norm': 0.34002619981765747, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.32it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.99it/s]                                              {'loss': 0.5691, 'grad_norm': 0.35170668363571167, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.99it/s]                                              {'loss': 0.5664, 'grad_norm': 0.3602910041809082, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.99it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.4933, 'grad_norm': 0.5638639330863953, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.5582, 'grad_norm': 0.28926876187324524, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.19it/s] 40%|████      | 6/15 [00:00<00:00, 16.37it/s]                                              {'loss': 0.6774, 'grad_norm': 0.5630375742912292, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.37it/s]                                              {'loss': 0.4603, 'grad_norm': 0.524307906627655, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.37it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.81it/s]                                              {'loss': 0.6388, 'grad_norm': 0.38044849038124084, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.81it/s]                                              {'loss': 0.572, 'grad_norm': 0.30962860584259033, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.81it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.5256, 'grad_norm': 0.33139804005622864, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.5739, 'grad_norm': 0.2467578500509262, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.20it/s] 80%|████████  | 12/15 [00:00<00:00, 15.78it/s]                                               {'loss': 0.5826, 'grad_norm': 0.34436407685279846, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.78it/s]                                               {'loss': 0.4532, 'grad_norm': 0.5013043284416199, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.78it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.6916, 'grad_norm': 0.5497013330459595, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.499, 'grad_norm': 0.38075020909309387, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.11it/s]                                               {'train_runtime': 1.0447, 'train_samples_per_second': 406.82, 'train_steps_per_second': 14.358, 'train_loss': 0.5614141225814819, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.11it/s]100%|██████████| 15/15 [00:01<00:00, 14.36it/s]
CLIENT:10
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5785, 'grad_norm': 0.38056597113609314, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.75it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.90it/s]                                              {'loss': 0.6801, 'grad_norm': 0.3877429962158203, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.90it/s]                                              {'loss': 0.5971, 'grad_norm': 0.29186657071113586, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.90it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.6262, 'grad_norm': 0.2578810453414917, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.6145, 'grad_norm': 0.3093682825565338, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.55it/s] 40%|████      | 6/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.5535, 'grad_norm': 0.3740946352481842, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.6145, 'grad_norm': 0.24573200941085815, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.35it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.6288, 'grad_norm': 0.25399065017700195, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.531, 'grad_norm': 0.470626562833786, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.27it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.6135, 'grad_norm': 0.26953381299972534, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.5346, 'grad_norm': 0.3934292197227478, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.83it/s] 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.6943, 'grad_norm': 0.5196847915649414, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.5823, 'grad_norm': 0.24859872460365295, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.48it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.6524, 'grad_norm': 0.3369581401348114, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.5636, 'grad_norm': 0.3487665355205536, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.51it/s]                                               {'train_runtime': 1.0734, 'train_samples_per_second': 395.954, 'train_steps_per_second': 13.975, 'train_loss': 0.6043315847714742, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.51it/s]100%|██████████| 15/15 [00:01<00:00, 13.98it/s]
CLIENT:94
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6156, 'grad_norm': 0.26773130893707275, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.02it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.6165, 'grad_norm': 0.24891869723796844, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.4945, 'grad_norm': 0.6714248657226562, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.30it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.5836, 'grad_norm': 0.2755873203277588, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.5656, 'grad_norm': 0.3337048292160034, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.45it/s] 40%|████      | 6/15 [00:00<00:00, 15.92it/s]                                              {'loss': 0.5944, 'grad_norm': 0.3020307421684265, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.92it/s]                                              {'loss': 0.4777, 'grad_norm': 0.5748429298400879, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.92it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.54it/s]                                              {'loss': 0.5773, 'grad_norm': 0.30147579312324524, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.54it/s]                                              {'loss': 0.7872, 'grad_norm': 0.9700175523757935, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.54it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.5916, 'grad_norm': 0.24138464033603668, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.6262, 'grad_norm': 0.3314003050327301, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.88it/s] 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.4833, 'grad_norm': 0.6483420729637146, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.5998, 'grad_norm': 0.29942718148231506, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.32it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.5703, 'grad_norm': 0.27742376923561096, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.5412, 'grad_norm': 0.3326896131038666, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.34it/s]                                               {'train_runtime': 1.0752, 'train_samples_per_second': 395.266, 'train_steps_per_second': 13.951, 'train_loss': 0.5816579540570577, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.34it/s]100%|██████████| 15/15 [00:01<00:00, 13.96it/s]
CLIENT:19
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5898, 'grad_norm': 0.3075469732284546, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.34it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.03it/s]                                              {'loss': 0.6271, 'grad_norm': 0.2583104968070984, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.03it/s]                                              {'loss': 0.5156, 'grad_norm': 0.514905571937561, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.03it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.5803, 'grad_norm': 0.2690069079399109, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.5297, 'grad_norm': 0.4588329493999481, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.71it/s] 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.69, 'grad_norm': 0.5200868248939514, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.5442, 'grad_norm': 0.3202260434627533, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.50it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.5933, 'grad_norm': 0.3150058686733246, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.6365, 'grad_norm': 0.348661869764328, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.35it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.5292, 'grad_norm': 0.38702404499053955, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.5289, 'grad_norm': 0.3273496925830841, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.94it/s] 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.7202, 'grad_norm': 0.6661573052406311, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.5784, 'grad_norm': 0.2706998884677887, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.47it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.5365, 'grad_norm': 0.3458483815193176, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.6002, 'grad_norm': 0.32702672481536865, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.51it/s]                                               {'train_runtime': 1.0701, 'train_samples_per_second': 397.167, 'train_steps_per_second': 14.018, 'train_loss': 0.5866467475891113, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.51it/s]100%|██████████| 15/15 [00:01<00:00, 14.02it/s]
CLIENT:44
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7152, 'grad_norm': 0.5248906016349792, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.08it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.7393, 'grad_norm': 0.622793972492218, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.7065, 'grad_norm': 0.46670472621917725, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.97it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.683, 'grad_norm': 0.3282596468925476, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.7125, 'grad_norm': 0.44078534841537476, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.56it/s] 40%|████      | 6/15 [00:00<00:00, 15.64it/s]                                              {'loss': 0.7781, 'grad_norm': 0.7343253493309021, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.64it/s]                                              {'loss': 0.6331, 'grad_norm': 0.3096455931663513, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.64it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.8202, 'grad_norm': 0.9236279726028442, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.6358, 'grad_norm': 0.33466392755508423, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.41it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.6794, 'grad_norm': 0.27079054713249207, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.6603, 'grad_norm': 0.20448140799999237, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.34it/s] 80%|████████  | 12/15 [00:00<00:00, 15.88it/s]                                               {'loss': 0.801, 'grad_norm': 0.8041393160820007, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.88it/s]                                               {'loss': 0.7538, 'grad_norm': 0.6240474581718445, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.88it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.6629, 'grad_norm': 0.2434726059436798, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.6424, 'grad_norm': 0.28687578439712524, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.54it/s]                                               {'train_runtime': 1.0641, 'train_samples_per_second': 399.399, 'train_steps_per_second': 14.096, 'train_loss': 0.7082396864891052, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.54it/s]100%|██████████| 15/15 [00:01<00:00, 14.10it/s]
CLIENT:86
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5764, 'grad_norm': 0.30709710717201233, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.42it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.14it/s]                                              {'loss': 0.6625, 'grad_norm': 0.37659281492233276, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.14it/s]                                              {'loss': 0.647, 'grad_norm': 0.41244393587112427, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.14it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.6231, 'grad_norm': 0.22750380635261536, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.6653, 'grad_norm': 0.34905654191970825, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.40it/s] 40%|████      | 6/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.6019, 'grad_norm': 0.3085809648036957, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.653, 'grad_norm': 0.3095247149467468, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.18it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.91it/s]                                              {'loss': 0.6676, 'grad_norm': 0.37928709387779236, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.91it/s]                                              {'loss': 0.517, 'grad_norm': 0.5588966012001038, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.91it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.5969, 'grad_norm': 0.2323172241449356, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.5948, 'grad_norm': 0.2716325521469116, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.34it/s] 80%|████████  | 12/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.725, 'grad_norm': 0.5970441102981567, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.6285, 'grad_norm': 0.2613793909549713, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.03it/s] 93%|█████████▎| 14/15 [00:00<00:00, 13.97it/s]                                               {'loss': 0.6282, 'grad_norm': 0.28065481781959534, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 13.97it/s]                                               {'loss': 0.6424, 'grad_norm': 0.3287375867366791, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.97it/s]                                               {'train_runtime': 1.121, 'train_samples_per_second': 379.116, 'train_steps_per_second': 13.381, 'train_loss': 0.6286424001057943, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.97it/s]100%|██████████| 15/15 [00:01<00:00, 13.39it/s]
CLIENT:23
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5375, 'grad_norm': 0.4562342166900635, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.61it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.46it/s]                                              {'loss': 0.6716, 'grad_norm': 0.37040454149246216, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.46it/s]                                              {'loss': 0.5952, 'grad_norm': 0.38368216156959534, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.46it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.6476, 'grad_norm': 0.3256913125514984, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.5377, 'grad_norm': 0.4006807804107666, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.14it/s] 40%|████      | 6/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.5867, 'grad_norm': 0.35901764035224915, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.6699, 'grad_norm': 0.36367109417915344, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.14it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.54it/s]                                              {'loss': 0.5933, 'grad_norm': 0.27403637766838074, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.54it/s]                                              {'loss': 0.4694, 'grad_norm': 0.661537766456604, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.54it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6744, 'grad_norm': 0.4529622793197632, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.5446, 'grad_norm': 0.3535526394844055, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.82it/s] 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.5672, 'grad_norm': 0.31155550479888916, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.4733, 'grad_norm': 0.5927934050559998, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.35it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.7077, 'grad_norm': 0.5483169555664062, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.5929, 'grad_norm': 0.3001422584056854, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]                                               {'train_runtime': 1.096, 'train_samples_per_second': 387.78, 'train_steps_per_second': 13.686, 'train_loss': 0.5912762522697449, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 13.69it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 33.24it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.62it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.41it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.50it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.39it/s] 61%|██████    | 20/33 [00:00<00:00, 25.32it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.06it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.25it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.02it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.23it/s]100%|██████████| 33/33 [00:01<00:00, 25.94it/s]
{'eval_loss': 0.6169313192367554, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3139, 'eval_samples_per_second': 793.804, 'eval_steps_per_second': 25.116}
ROUND:25
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.602, 'grad_norm': 0.26246070861816406, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.86it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.22it/s]                                              {'loss': 0.5613, 'grad_norm': 0.296418696641922, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.22it/s]                                              {'loss': 0.5651, 'grad_norm': 0.31795254349708557, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.22it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.541, 'grad_norm': 0.36810070276260376, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.6495, 'grad_norm': 0.3549876809120178, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.28it/s] 40%|████      | 6/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.5376, 'grad_norm': 0.42854011058807373, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.5934, 'grad_norm': 0.2054632604122162, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.15it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.5959, 'grad_norm': 0.25500616431236267, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.5471, 'grad_norm': 0.3305390477180481, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.06it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.66it/s]                                               {'loss': 0.5623, 'grad_norm': 0.2807871699333191, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.66it/s]                                               {'loss': 0.6182, 'grad_norm': 0.328304260969162, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.66it/s] 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.5562, 'grad_norm': 0.3429509401321411, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.5466, 'grad_norm': 0.3031497895717621, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.37it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.649, 'grad_norm': 0.3996848165988922, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.5685, 'grad_norm': 0.2928563356399536, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.88it/s]                                               {'train_runtime': 1.084, 'train_samples_per_second': 392.062, 'train_steps_per_second': 13.837, 'train_loss': 0.5795746962229411, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.88it/s]100%|██████████| 15/15 [00:01<00:00, 13.84it/s]
CLIENT:65
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6813, 'grad_norm': 0.39960208535194397, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.39it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.87it/s]                                              {'loss': 0.5779, 'grad_norm': 0.2886531949043274, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.87it/s]                                              {'loss': 0.7362, 'grad_norm': 0.6124050617218018, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.87it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.6458, 'grad_norm': 0.30336782336235046, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.6247, 'grad_norm': 0.25926637649536133, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.12it/s] 40%|████      | 6/15 [00:00<00:00, 15.78it/s]                                              {'loss': 0.6752, 'grad_norm': 0.4225063621997833, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.78it/s]                                              {'loss': 0.6617, 'grad_norm': 0.27471593022346497, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.78it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.6984, 'grad_norm': 0.44648507237434387, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.5667, 'grad_norm': 0.43685993552207947, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.50it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.6812, 'grad_norm': 0.28940942883491516, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.6614, 'grad_norm': 0.2616197466850281, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.91it/s] 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.5951, 'grad_norm': 0.360415518283844, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.6216, 'grad_norm': 0.24462851881980896, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.40it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.65, 'grad_norm': 0.24515767395496368, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.6813, 'grad_norm': 0.3664875626564026, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.42it/s]                                               {'train_runtime': 1.0816, 'train_samples_per_second': 392.923, 'train_steps_per_second': 13.868, 'train_loss': 0.6505811452865601, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]100%|██████████| 15/15 [00:01<00:00, 13.87it/s]
CLIENT:95
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6719, 'grad_norm': 0.4365623891353607, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.93it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.35it/s]                                              {'loss': 0.5605, 'grad_norm': 0.3233630955219269, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.35it/s]                                              {'loss': 0.5875, 'grad_norm': 0.3349713981151581, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.35it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.5353, 'grad_norm': 0.4703487157821655, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.6502, 'grad_norm': 0.3033216595649719, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.58it/s] 40%|████      | 6/15 [00:00<00:00, 15.25it/s]                                              {'loss': 0.6548, 'grad_norm': 0.3652307093143463, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.25it/s]                                              {'loss': 0.6155, 'grad_norm': 0.26997604966163635, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.25it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.01it/s]                                              {'loss': 0.5993, 'grad_norm': 0.2508828938007355, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.01it/s]                                              {'loss': 0.631, 'grad_norm': 0.3484556972980499, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.01it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.30it/s]                                               {'loss': 0.6727, 'grad_norm': 0.4447743594646454, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.30it/s]                                               {'loss': 0.5561, 'grad_norm': 0.3471486568450928, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.30it/s] 80%|████████  | 12/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.5905, 'grad_norm': 0.2946310043334961, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.6944, 'grad_norm': 0.5385131239891052, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 14.86it/s] 93%|█████████▎| 14/15 [00:00<00:00, 13.98it/s]                                               {'loss': 0.6534, 'grad_norm': 0.321455180644989, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 13.98it/s]                                               {'loss': 0.4318, 'grad_norm': 0.8252090215682983, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.98it/s]                                               {'train_runtime': 1.1207, 'train_samples_per_second': 379.218, 'train_steps_per_second': 13.384, 'train_loss': 0.6069885710875194, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.98it/s]100%|██████████| 15/15 [00:01<00:00, 13.39it/s]
CLIENT:84
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6055, 'grad_norm': 0.2539580762386322, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.41it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.20it/s]                                              {'loss': 0.6881, 'grad_norm': 0.4258650839328766, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.20it/s]                                              {'loss': 0.6695, 'grad_norm': 0.40760287642478943, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.20it/s] 27%|██▋       | 4/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.5768, 'grad_norm': 0.2914567291736603, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.7196, 'grad_norm': 0.5933847427368164, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 13.99it/s] 40%|████      | 6/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.6222, 'grad_norm': 0.3146892488002777, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.6963, 'grad_norm': 0.43598300218582153, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 14.90it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.76it/s]                                              {'loss': 0.6126, 'grad_norm': 0.24350488185882568, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.76it/s]                                              {'loss': 0.5927, 'grad_norm': 0.37601956725120544, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.76it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.24it/s]                                               {'loss': 0.6762, 'grad_norm': 0.36822348833084106, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.24it/s]                                               {'loss': 0.6576, 'grad_norm': 0.29040831327438354, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.24it/s] 80%|████████  | 12/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.5316, 'grad_norm': 0.5541877746582031, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.6028, 'grad_norm': 0.2576547861099243, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 14.81it/s] 93%|█████████▎| 14/15 [00:00<00:00, 13.93it/s]                                               {'loss': 0.6499, 'grad_norm': 0.300794392824173, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 13.93it/s]                                               {'loss': 0.6538, 'grad_norm': 0.41090893745422363, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.93it/s]                                               {'train_runtime': 1.1343, 'train_samples_per_second': 374.682, 'train_steps_per_second': 13.224, 'train_loss': 0.6370108882586162, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.93it/s]100%|██████████| 15/15 [00:01<00:00, 13.23it/s]
CLIENT:61
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7308, 'grad_norm': 0.6088248491287231, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.53it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.6355, 'grad_norm': 0.23379985988140106, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.4446, 'grad_norm': 0.8269674181938171, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.01it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.55, 'grad_norm': 0.4064286947250366, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.5937, 'grad_norm': 0.30175885558128357, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.28it/s] 40%|████      | 6/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.8177, 'grad_norm': 0.9608779549598694, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.5747, 'grad_norm': 0.276445209980011, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.18it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.5842, 'grad_norm': 0.3160969316959381, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.7828, 'grad_norm': 0.8984282612800598, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.14it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.5668, 'grad_norm': 0.3053947389125824, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.7137, 'grad_norm': 0.5168772339820862, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.74it/s] 80%|████████  | 12/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.5578, 'grad_norm': 0.3763180077075958, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.6509, 'grad_norm': 0.2888794243335724, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.28it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.5994, 'grad_norm': 0.2578074634075165, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.6074, 'grad_norm': 0.2805308997631073, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]                                               {'train_runtime': 1.0985, 'train_samples_per_second': 386.893, 'train_steps_per_second': 13.655, 'train_loss': 0.6273345311482748, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]100%|██████████| 15/15 [00:01<00:00, 13.66it/s]
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.584, 'grad_norm': 0.3546682894229889, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.73it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.98it/s]                                              {'loss': 0.5814, 'grad_norm': 0.27485212683677673, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.98it/s]                                              {'loss': 0.6228, 'grad_norm': 0.31709352135658264, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.98it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.5977, 'grad_norm': 0.2702276408672333, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.5761, 'grad_norm': 0.3001902103424072, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.62it/s] 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.605, 'grad_norm': 0.28791242837905884, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.6967, 'grad_norm': 0.500956654548645, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.38it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.4995, 'grad_norm': 0.4825240671634674, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.5474, 'grad_norm': 0.35648033022880554, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.20it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.6209, 'grad_norm': 0.3005301356315613, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.5932, 'grad_norm': 0.25497597455978394, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.08it/s] 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.5187, 'grad_norm': 0.4196861684322357, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.5391, 'grad_norm': 0.3417900502681732, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.35it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.6356, 'grad_norm': 0.33215582370758057, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.5819, 'grad_norm': 0.3493040204048157, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]                                               {'train_runtime': 1.0887, 'train_samples_per_second': 390.377, 'train_steps_per_second': 13.778, 'train_loss': 0.5866832355658214, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]100%|██████████| 15/15 [00:01<00:00, 13.78it/s]
CLIENT:30
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.693, 'grad_norm': 0.4640520215034485, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.75it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.14it/s]                                              {'loss': 0.6054, 'grad_norm': 0.25522592663764954, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.14it/s]                                              {'loss': 0.5448, 'grad_norm': 0.44939756393432617, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.14it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6113, 'grad_norm': 0.21313714981079102, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6389, 'grad_norm': 0.2932622730731964, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.20it/s] 40%|████      | 6/15 [00:00<00:00, 16.10it/s]                                              {'loss': 0.5966, 'grad_norm': 0.32527846097946167, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.10it/s]                                              {'loss': 0.6695, 'grad_norm': 0.3643988072872162, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.10it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.5847, 'grad_norm': 0.3005547523498535, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.6151, 'grad_norm': 0.31247860193252563, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.61it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6223, 'grad_norm': 0.2691550850868225, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.7136, 'grad_norm': 0.512596070766449, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.13it/s] 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.443, 'grad_norm': 0.8093875050544739, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.6176, 'grad_norm': 0.25267618894577026, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.53it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.5829, 'grad_norm': 0.2801367938518524, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.6918, 'grad_norm': 0.47929272055625916, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.60it/s]                                               {'train_runtime': 1.0674, 'train_samples_per_second': 398.172, 'train_steps_per_second': 14.053, 'train_loss': 0.6153610130151113, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.60it/s]100%|██████████| 15/15 [00:01<00:00, 14.06it/s]
CLIENT:69
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6612, 'grad_norm': 0.3773173987865448, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.95it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.5944, 'grad_norm': 0.22915388643741608, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.6062, 'grad_norm': 0.29962918162345886, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.30it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.6638, 'grad_norm': 0.3580206632614136, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.624, 'grad_norm': 0.2612926661968231, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.48it/s] 40%|████      | 6/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.5245, 'grad_norm': 0.524380087852478, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.6167, 'grad_norm': 0.26598748564720154, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.34it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.6352, 'grad_norm': 0.266492635011673, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.6056, 'grad_norm': 0.37578079104423523, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.20it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.5791, 'grad_norm': 0.3190496265888214, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.6267, 'grad_norm': 0.2737411856651306, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.80it/s] 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6791, 'grad_norm': 0.49289676547050476, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6536, 'grad_norm': 0.28845176100730896, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.29it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6497, 'grad_norm': 0.3201014995574951, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.5217, 'grad_norm': 0.4833638370037079, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.92it/s]                                               {'train_runtime': 1.078, 'train_samples_per_second': 394.26, 'train_steps_per_second': 13.915, 'train_loss': 0.6161171356836955, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.92it/s]100%|██████████| 15/15 [00:01<00:00, 13.92it/s]
CLIENT:35
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5627, 'grad_norm': 0.3029032349586487, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.42it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.76it/s]                                              {'loss': 0.6551, 'grad_norm': 0.33120986819267273, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.76it/s]                                              {'loss': 0.6801, 'grad_norm': 0.49175822734832764, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.76it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.5963, 'grad_norm': 0.2696605920791626, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.6198, 'grad_norm': 0.2568783760070801, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.04it/s] 40%|████      | 6/15 [00:00<00:00, 16.33it/s]                                              {'loss': 0.6699, 'grad_norm': 0.3976552188396454, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.33it/s]                                              {'loss': 0.6574, 'grad_norm': 0.3658485412597656, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.33it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.83it/s]                                              {'loss': 0.5954, 'grad_norm': 0.259709894657135, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.83it/s]                                              {'loss': 0.634, 'grad_norm': 0.3732694685459137, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.83it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6244, 'grad_norm': 0.22441506385803223, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6666, 'grad_norm': 0.3717838525772095, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.43it/s] 80%|████████  | 12/15 [00:00<00:00, 15.73it/s]                                               {'loss': 0.5307, 'grad_norm': 0.49972131848335266, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.73it/s]                                               {'loss': 0.6602, 'grad_norm': 0.3146956264972687, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.73it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.6243, 'grad_norm': 0.26785168051719666, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.5704, 'grad_norm': 0.42507416009902954, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.62it/s]                                               {'train_runtime': 1.0609, 'train_samples_per_second': 400.62, 'train_steps_per_second': 14.14, 'train_loss': 0.6231454968452453, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.62it/s]100%|██████████| 15/15 [00:01<00:00, 14.15it/s]
CLIENT:1
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7994, 'grad_norm': 0.8494170904159546, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.74it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.48it/s]                                              {'loss': 0.6604, 'grad_norm': 0.34646666049957275, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.48it/s]                                              {'loss': 0.5602, 'grad_norm': 0.4163552522659302, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.48it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.7273, 'grad_norm': 0.5263773202896118, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.6657, 'grad_norm': 0.32398906350135803, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.24it/s] 40%|████      | 6/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6455, 'grad_norm': 0.38348931074142456, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6696, 'grad_norm': 0.25966256856918335, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.20it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.6552, 'grad_norm': 0.24709780514240265, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.748, 'grad_norm': 0.5932152271270752, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.11it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.6856, 'grad_norm': 0.3056039810180664, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.6681, 'grad_norm': 0.2460869401693344, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.03it/s] 80%|████████  | 12/15 [00:00<00:00, 15.81it/s]                                               {'loss': 0.6739, 'grad_norm': 0.3505017161369324, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.81it/s]                                               {'loss': 0.7403, 'grad_norm': 0.49515458941459656, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.81it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.6837, 'grad_norm': 0.23024992644786835, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.5951, 'grad_norm': 0.4876420497894287, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.64it/s]                                               {'train_runtime': 1.0848, 'train_samples_per_second': 391.77, 'train_steps_per_second': 13.827, 'train_loss': 0.6785437226295471, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.64it/s]100%|██████████| 15/15 [00:01<00:00, 13.83it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 37.16it/s] 24%|██▍       | 8/33 [00:00<00:00, 29.56it/s] 36%|███▋      | 12/33 [00:00<00:00, 26.97it/s] 45%|████▌     | 15/33 [00:00<00:00, 26.50it/s] 55%|█████▍    | 18/33 [00:00<00:00, 25.85it/s] 64%|██████▎   | 21/33 [00:00<00:00, 25.62it/s] 73%|███████▎  | 24/33 [00:00<00:00, 25.41it/s] 82%|████████▏ | 27/33 [00:01<00:00, 25.29it/s] 91%|█████████ | 30/33 [00:01<00:00, 25.27it/s]100%|██████████| 33/33 [00:01<00:00, 26.09it/s]100%|██████████| 33/33 [00:01<00:00, 26.38it/s]
{'eval_loss': 0.6167346835136414, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.2927, 'eval_samples_per_second': 806.813, 'eval_steps_per_second': 25.527}
ROUND:26
CLIENT:23
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5372, 'grad_norm': 0.4522251784801483, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.73it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.57it/s]                                              {'loss': 0.6719, 'grad_norm': 0.36188575625419617, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.57it/s]                                              {'loss': 0.5935, 'grad_norm': 0.3860780596733093, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.57it/s] 27%|██▋       | 4/15 [00:00<00:00, 13.98it/s]                                              {'loss': 0.6465, 'grad_norm': 0.3301323652267456, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 13.98it/s]                                              {'loss': 0.5377, 'grad_norm': 0.4001455307006836, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 13.98it/s] 40%|████      | 6/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.5884, 'grad_norm': 0.4077320098876953, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.67, 'grad_norm': 0.3637174963951111, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.01it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.5935, 'grad_norm': 0.271699994802475, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.4702, 'grad_norm': 0.6733412146568298, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.08it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.6747, 'grad_norm': 0.4648571312427521, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.5439, 'grad_norm': 0.3433195650577545, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.04it/s] 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.5668, 'grad_norm': 0.30931544303894043, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.4727, 'grad_norm': 0.5917946100234985, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.49it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.7074, 'grad_norm': 0.5565407872200012, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.5924, 'grad_norm': 0.29153385758399963, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]                                               {'train_runtime': 1.0941, 'train_samples_per_second': 388.441, 'train_steps_per_second': 13.71, 'train_loss': 0.591122567653656, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]100%|██████████| 15/15 [00:01<00:00, 13.71it/s]
CLIENT:81
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5606, 'grad_norm': 0.30488869547843933, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.65it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.42it/s]                                              {'loss': 0.6235, 'grad_norm': 0.2799607813358307, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.42it/s]                                              {'loss': 0.4447, 'grad_norm': 0.7976504564285278, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.42it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.5504, 'grad_norm': 0.39981094002723694, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.5484, 'grad_norm': 0.3395056426525116, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.18it/s] 40%|████      | 6/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.5818, 'grad_norm': 0.34181270003318787, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.5019, 'grad_norm': 0.42582300305366516, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.14it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.5333, 'grad_norm': 0.326347291469574, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6281, 'grad_norm': 0.43628713488578796, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.10it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.6378, 'grad_norm': 0.3381751775741577, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.4303, 'grad_norm': 0.5805098414421082, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.74it/s] 80%|████████  | 12/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.5795, 'grad_norm': 0.3777952492237091, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.601, 'grad_norm': 0.3063381612300873, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.66it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.21it/s]                                               {'loss': 0.5574, 'grad_norm': 0.25007396936416626, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.21it/s]                                               {'loss': 0.4421, 'grad_norm': 0.49713772535324097, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.21it/s]                                               {'train_runtime': 1.1058, 'train_samples_per_second': 384.336, 'train_steps_per_second': 13.565, 'train_loss': 0.5480580608050029, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.21it/s]100%|██████████| 15/15 [00:01<00:00, 13.57it/s]
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.59, 'grad_norm': 0.2729397714138031, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.04it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.60it/s]                                              {'loss': 0.7029, 'grad_norm': 0.4821120500564575, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.60it/s]                                              {'loss': 0.5076, 'grad_norm': 0.543479323387146, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.60it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.5621, 'grad_norm': 0.3661825656890869, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.6578, 'grad_norm': 0.38590556383132935, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.13it/s] 40%|████      | 6/15 [00:00<00:00, 14.95it/s]                                              {'loss': 0.6517, 'grad_norm': 0.3315819799900055, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 14.95it/s]                                              {'loss': 0.6674, 'grad_norm': 0.3895009756088257, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 14.95it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.81it/s]                                              {'loss': 0.5359, 'grad_norm': 0.40189680457115173, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.81it/s]                                              {'loss': 0.639, 'grad_norm': 0.4005008339881897, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.81it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.23it/s]                                               {'loss': 0.6322, 'grad_norm': 0.2945408523082733, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.23it/s]                                               {'loss': 0.609, 'grad_norm': 0.25242879986763, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.23it/s] 80%|████████  | 12/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.5747, 'grad_norm': 0.30914050340652466, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.591, 'grad_norm': 0.27744007110595703, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 14.82it/s] 93%|█████████▎| 14/15 [00:00<00:00, 13.98it/s]                                               {'loss': 0.6289, 'grad_norm': 0.28136032819747925, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 13.98it/s]                                               {'loss': 0.6424, 'grad_norm': 0.36588236689567566, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.98it/s]                                               {'train_runtime': 1.1292, 'train_samples_per_second': 376.382, 'train_steps_per_second': 13.284, 'train_loss': 0.6128422856330872, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.98it/s]100%|██████████| 15/15 [00:01<00:00, 13.29it/s]
CLIENT:34
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7384, 'grad_norm': 0.648253858089447, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.80it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.50it/s]                                              {'loss': 0.6297, 'grad_norm': 0.21205772459506989, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.50it/s]                                              {'loss': 0.5328, 'grad_norm': 0.512718677520752, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.50it/s] 27%|██▋       | 4/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.6785, 'grad_norm': 0.3922576904296875, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.5149, 'grad_norm': 0.6143675446510315, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 13.99it/s] 40%|████      | 6/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.7496, 'grad_norm': 0.7301567792892456, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.5299, 'grad_norm': 0.5002395510673523, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.12it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.62it/s]                                              {'loss': 0.6283, 'grad_norm': 0.2679072320461273, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.62it/s]                                              {'loss': 0.806, 'grad_norm': 0.9823675751686096, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.62it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.18it/s]                                               {'loss': 0.6094, 'grad_norm': 0.23826132714748383, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.18it/s]                                               {'loss': 0.6473, 'grad_norm': 0.25416627526283264, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.18it/s] 80%|████████  | 12/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.63, 'grad_norm': 0.26594650745391846, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6346, 'grad_norm': 0.2396591454744339, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 14.94it/s] 93%|█████████▎| 14/15 [00:00<00:00, 13.76it/s]                                               {'loss': 0.6236, 'grad_norm': 0.24237488210201263, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 13.76it/s]                                               {'loss': 0.6403, 'grad_norm': 0.2872261703014374, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.76it/s]                                               {'train_runtime': 1.1405, 'train_samples_per_second': 372.631, 'train_steps_per_second': 13.152, 'train_loss': 0.6395623763402303, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.76it/s]100%|██████████| 15/15 [00:01<00:00, 13.16it/s]
CLIENT:62
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5373, 'grad_norm': 0.4266522228717804, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.26it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.79it/s]                                              {'loss': 0.6006, 'grad_norm': 0.26366785168647766, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.79it/s]                                              {'loss': 0.4033, 'grad_norm': 0.9769132733345032, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.79it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.4868, 'grad_norm': 0.5707188844680786, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.6289, 'grad_norm': 0.27997130155563354, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.48it/s] 40%|████      | 6/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.4192, 'grad_norm': 0.7278889417648315, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.5184, 'grad_norm': 0.34186142683029175, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.34it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.4774, 'grad_norm': 0.450179785490036, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.5506, 'grad_norm': 0.37700796127319336, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.23it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.5377, 'grad_norm': 0.2792661488056183, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.5225, 'grad_norm': 0.2511589527130127, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.85it/s] 80%|████████  | 12/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.4234, 'grad_norm': 0.48088234663009644, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.5147, 'grad_norm': 0.28097665309906006, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.34it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.5629, 'grad_norm': 0.3184417486190796, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.3683, 'grad_norm': 0.5912248492240906, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.89it/s]                                               {'train_runtime': 1.0846, 'train_samples_per_second': 391.86, 'train_steps_per_second': 13.83, 'train_loss': 0.5034760455290477, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.89it/s]100%|██████████| 15/15 [00:01<00:00, 13.84it/s]
CLIENT:46
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6563, 'grad_norm': 0.3051978051662445, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.92it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.73it/s]                                              {'loss': 0.6462, 'grad_norm': 0.27670109272003174, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.73it/s]                                              {'loss': 0.5652, 'grad_norm': 0.4048939347267151, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.73it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.7613, 'grad_norm': 0.6876609325408936, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.5342, 'grad_norm': 0.47121661901474, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.14it/s] 40%|████      | 6/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.5887, 'grad_norm': 0.3419165313243866, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.5961, 'grad_norm': 0.26948508620262146, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.12it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.05it/s]                                              {'loss': 0.6021, 'grad_norm': 0.26120084524154663, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.05it/s]                                              {'loss': 0.7163, 'grad_norm': 0.5094200372695923, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.05it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6282, 'grad_norm': 0.2685548961162567, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.669, 'grad_norm': 0.3146134912967682, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.71it/s] 80%|████████  | 12/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.5443, 'grad_norm': 0.45584720373153687, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.6077, 'grad_norm': 0.2796041965484619, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.24it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.5597, 'grad_norm': 0.3794274628162384, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.7663, 'grad_norm': 0.7125603556632996, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.53it/s]                                               {'train_runtime': 1.0928, 'train_samples_per_second': 388.906, 'train_steps_per_second': 13.726, 'train_loss': 0.6294375737508138, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.53it/s]100%|██████████| 15/15 [00:01<00:00, 13.73it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6043, 'grad_norm': 0.25838541984558105, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.12it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.64it/s]                                              {'loss': 0.4986, 'grad_norm': 0.5951165556907654, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.64it/s]                                              {'loss': 0.7636, 'grad_norm': 0.7652159333229065, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.64it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.21it/s]                                              {'loss': 0.628, 'grad_norm': 0.24988070130348206, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.21it/s]                                              {'loss': 0.602, 'grad_norm': 0.23324941098690033, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.21it/s] 40%|████      | 6/15 [00:00<00:00, 15.05it/s]                                              {'loss': 0.5669, 'grad_norm': 0.34320682287216187, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.05it/s]                                              {'loss': 0.5586, 'grad_norm': 0.3231734335422516, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.05it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.5952, 'grad_norm': 0.25630632042884827, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.6839, 'grad_norm': 0.4827283024787903, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.99it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.647, 'grad_norm': 0.3085520267486572, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.5677, 'grad_norm': 0.32214993238449097, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.48it/s] 80%|████████  | 12/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.6005, 'grad_norm': 0.2742834985256195, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.6147, 'grad_norm': 0.30198177695274353, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.22it/s] 93%|█████████▎| 14/15 [00:00<00:00, 13.99it/s]                                               {'loss': 0.5735, 'grad_norm': 0.29725566506385803, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 13.99it/s]                                               {'loss': 0.6165, 'grad_norm': 0.31639564037323, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.99it/s]                                               {'train_runtime': 1.1183, 'train_samples_per_second': 380.037, 'train_steps_per_second': 13.413, 'train_loss': 0.6080758551756541, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.99it/s]100%|██████████| 15/15 [00:01<00:00, 13.42it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6535, 'grad_norm': 0.30153265595436096, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.11it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.66it/s]                                              {'loss': 0.5604, 'grad_norm': 0.34353214502334595, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.66it/s]                                              {'loss': 0.5628, 'grad_norm': 0.3987623453140259, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.66it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.6908, 'grad_norm': 0.4269462823867798, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.4504, 'grad_norm': 0.7749265432357788, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.20it/s] 40%|████      | 6/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.6355, 'grad_norm': 0.3493570387363434, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.5237, 'grad_norm': 0.4506427049636841, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.04it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.97it/s]                                              {'loss': 0.6372, 'grad_norm': 0.28303781151771545, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.97it/s]                                              {'loss': 0.5929, 'grad_norm': 0.27160948514938354, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.97it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.5966, 'grad_norm': 0.26619279384613037, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.543, 'grad_norm': 0.32644346356391907, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.45it/s] 80%|████████  | 12/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.6348, 'grad_norm': 0.44644755125045776, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.5932, 'grad_norm': 0.27979227900505066, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.14it/s] 93%|█████████▎| 14/15 [00:00<00:00, 13.96it/s]                                               {'loss': 0.571, 'grad_norm': 0.29985931515693665, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 13.96it/s]                                               {'loss': 0.5974, 'grad_norm': 0.367355614900589, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.96it/s]                                               {'train_runtime': 1.1205, 'train_samples_per_second': 379.279, 'train_steps_per_second': 13.386, 'train_loss': 0.589552491903305, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.96it/s]100%|██████████| 15/15 [00:01<00:00, 13.39it/s]
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5682, 'grad_norm': 0.30562472343444824, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.22it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.59it/s]                                              {'loss': 0.5755, 'grad_norm': 0.28655868768692017, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.59it/s]                                              {'loss': 0.6077, 'grad_norm': 0.301331490278244, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.59it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.5232, 'grad_norm': 0.4881094992160797, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.6471, 'grad_norm': 0.3657183051109314, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.01it/s] 40%|████      | 6/15 [00:00<00:00, 15.71it/s]                                              {'loss': 0.564, 'grad_norm': 0.3132762610912323, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.71it/s]                                              {'loss': 0.5619, 'grad_norm': 0.2759101092815399, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.71it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.4904, 'grad_norm': 0.47560617327690125, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.7173, 'grad_norm': 0.6887997388839722, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.11it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.6222, 'grad_norm': 0.30018913745880127, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.6276, 'grad_norm': 0.39964181184768677, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.49it/s] 80%|████████  | 12/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.4539, 'grad_norm': 0.5986751317977905, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.6112, 'grad_norm': 0.29413068294525146, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.02it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.11it/s]                                               {'loss': 0.5748, 'grad_norm': 0.3280186057090759, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.11it/s]                                               {'loss': 0.5342, 'grad_norm': 0.38484323024749756, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.11it/s]                                               {'train_runtime': 1.1111, 'train_samples_per_second': 382.52, 'train_steps_per_second': 13.501, 'train_loss': 0.5786186436812083, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.11it/s]100%|██████████| 15/15 [00:01<00:00, 13.51it/s]
CLIENT:11
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6175, 'grad_norm': 0.24405437707901, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.41it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.24it/s]                                              {'loss': 0.5292, 'grad_norm': 0.5201554894447327, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.24it/s]                                              {'loss': 0.7488, 'grad_norm': 0.745540976524353, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.24it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.38it/s]                                              {'loss': 0.5598, 'grad_norm': 0.3795057237148285, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.38it/s]                                              {'loss': 0.6081, 'grad_norm': 0.2586902678012848, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.38it/s] 40%|████      | 6/15 [00:00<00:00, 15.27it/s]                                              {'loss': 0.6894, 'grad_norm': 0.6297645568847656, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.27it/s]                                              {'loss': 0.6057, 'grad_norm': 0.25636112689971924, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.27it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.98it/s]                                              {'loss': 0.6105, 'grad_norm': 0.25808802247047424, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.98it/s]                                              {'loss': 0.6522, 'grad_norm': 0.35704103112220764, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.98it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.5629, 'grad_norm': 0.2917976379394531, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.659, 'grad_norm': 0.3041417598724365, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.43it/s] 80%|████████  | 12/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.6169, 'grad_norm': 0.30967292189598083, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.6042, 'grad_norm': 0.23570279777050018, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.19it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.04it/s]                                               {'loss': 0.6936, 'grad_norm': 0.4396245777606964, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.04it/s]                                               {'loss': 0.5178, 'grad_norm': 0.5221951007843018, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.04it/s]                                               {'train_runtime': 1.1232, 'train_samples_per_second': 378.369, 'train_steps_per_second': 13.354, 'train_loss': 0.6183751940727233, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.04it/s]100%|██████████| 15/15 [00:01<00:00, 13.36it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 37.88it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.97it/s] 36%|███▋      | 12/33 [00:00<00:00, 26.54it/s] 45%|████▌     | 15/33 [00:00<00:00, 25.86it/s] 55%|█████▍    | 18/33 [00:00<00:00, 25.31it/s] 64%|██████▎   | 21/33 [00:00<00:00, 25.22it/s] 73%|███████▎  | 24/33 [00:00<00:00, 25.27it/s] 82%|████████▏ | 27/33 [00:01<00:00, 24.99it/s] 91%|█████████ | 30/33 [00:01<00:00, 25.22it/s]100%|██████████| 33/33 [00:01<00:00, 26.04it/s]100%|██████████| 33/33 [00:01<00:00, 26.13it/s]
{'eval_loss': 0.6169692873954773, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3048, 'eval_samples_per_second': 799.331, 'eval_steps_per_second': 25.29}
ROUND:27
CLIENT:65
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6812, 'grad_norm': 0.41484391689300537, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.24it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.76it/s]                                              {'loss': 0.5774, 'grad_norm': 0.29605022072792053, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.76it/s]                                              {'loss': 0.7375, 'grad_norm': 0.6225764155387878, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.76it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.6463, 'grad_norm': 0.3176115155220032, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.6242, 'grad_norm': 0.2610975205898285, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.62it/s] 40%|████      | 6/15 [00:00<00:00, 15.43it/s]                                              {'loss': 0.6752, 'grad_norm': 0.42203208804130554, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.43it/s]                                              {'loss': 0.6616, 'grad_norm': 0.2719345986843109, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.43it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.6976, 'grad_norm': 0.4557955265045166, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.5658, 'grad_norm': 0.43223151564598083, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.30it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.6808, 'grad_norm': 0.2924468219280243, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.6608, 'grad_norm': 0.25483277440071106, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.91it/s] 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.594, 'grad_norm': 0.35631346702575684, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6214, 'grad_norm': 0.24705663323402405, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.43it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.6498, 'grad_norm': 0.24138420820236206, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.6811, 'grad_norm': 0.3683953881263733, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.54it/s]                                               {'train_runtime': 1.0796, 'train_samples_per_second': 393.659, 'train_steps_per_second': 13.894, 'train_loss': 0.6503148396809896, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.54it/s]100%|██████████| 15/15 [00:01<00:00, 13.90it/s]
CLIENT:21
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5684, 'grad_norm': 0.26811379194259644, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.25it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.6641, 'grad_norm': 0.35954463481903076, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.511, 'grad_norm': 0.5321305990219116, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.01it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.5973, 'grad_norm': 0.3528560996055603, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.567, 'grad_norm': 0.3415510356426239, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.74it/s] 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.6433, 'grad_norm': 0.37876272201538086, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.4789, 'grad_norm': 0.5440316796302795, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.46it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.7174, 'grad_norm': 0.5865905284881592, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.5511, 'grad_norm': 0.3228682577610016, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.30it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6289, 'grad_norm': 0.30550792813301086, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6035, 'grad_norm': 0.27015313506126404, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.94it/s] 80%|████████  | 12/15 [00:00<00:00, 15.44it/s]                                               {'loss': 0.5577, 'grad_norm': 0.34707921743392944, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.44it/s]                                               {'loss': 0.5745, 'grad_norm': 0.38516494631767273, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.44it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.6463, 'grad_norm': 0.38253673911094666, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.5562, 'grad_norm': 0.3160042464733124, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.02it/s]                                               {'train_runtime': 1.0624, 'train_samples_per_second': 400.052, 'train_steps_per_second': 14.119, 'train_loss': 0.5910347600777944, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.02it/s]100%|██████████| 15/15 [00:01<00:00, 14.13it/s]
CLIENT:87
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6737, 'grad_norm': 0.3996182382106781, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.99it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.70it/s]                                              {'loss': 0.6091, 'grad_norm': 0.2483265995979309, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.70it/s]                                              {'loss': 0.7816, 'grad_norm': 0.8450630903244019, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.70it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.6938, 'grad_norm': 0.42567044496536255, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.7113, 'grad_norm': 0.45102035999298096, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.24it/s] 40%|████      | 6/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.5964, 'grad_norm': 0.3388133943080902, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.7439, 'grad_norm': 0.5701810717582703, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.18it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.6094, 'grad_norm': 0.2507404685020447, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.6727, 'grad_norm': 0.30395469069480896, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.48it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.6447, 'grad_norm': 0.22252105176448822, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.7267, 'grad_norm': 0.4771750867366791, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.72it/s] 80%|████████  | 12/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.6139, 'grad_norm': 0.3306000232696533, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.7175, 'grad_norm': 0.41273605823516846, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.24it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.29it/s]                                               {'loss': 0.7037, 'grad_norm': 0.4706047773361206, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.29it/s]                                               {'loss': 0.4911, 'grad_norm': 0.9022552371025085, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.29it/s]                                               {'train_runtime': 1.1033, 'train_samples_per_second': 385.219, 'train_steps_per_second': 13.596, 'train_loss': 0.6659542957941691, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.29it/s]100%|██████████| 15/15 [00:01<00:00, 13.60it/s]
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.602, 'grad_norm': 0.263359010219574, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.88it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.29it/s]                                              {'loss': 0.5607, 'grad_norm': 0.2921751141548157, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.29it/s]                                              {'loss': 0.5641, 'grad_norm': 0.3050483763217926, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.29it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.47it/s]                                              {'loss': 0.5401, 'grad_norm': 0.36628735065460205, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.47it/s]                                              {'loss': 0.65, 'grad_norm': 0.3692309558391571, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.47it/s] 40%|████      | 6/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.537, 'grad_norm': 0.43423643708229065, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.5932, 'grad_norm': 0.21421879529953003, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.35it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.5955, 'grad_norm': 0.2537495791912079, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.547, 'grad_norm': 0.3325715661048889, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.01it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.5619, 'grad_norm': 0.27849918603897095, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.6183, 'grad_norm': 0.3283446133136749, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.47it/s] 80%|████████  | 12/15 [00:00<00:00, 15.82it/s]                                               {'loss': 0.5558, 'grad_norm': 0.3435424566268921, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.82it/s]                                               {'loss': 0.5462, 'grad_norm': 0.3002071678638458, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.82it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.66it/s]                                               {'loss': 0.6494, 'grad_norm': 0.40964508056640625, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.66it/s]                                               {'loss': 0.5682, 'grad_norm': 0.2929784655570984, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.66it/s]                                               {'train_runtime': 1.0714, 'train_samples_per_second': 396.678, 'train_steps_per_second': 14.0, 'train_loss': 0.5792837341626486, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.66it/s]100%|██████████| 15/15 [00:01<00:00, 14.01it/s]
CLIENT:46
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6578, 'grad_norm': 0.3284965753555298, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.05it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.6471, 'grad_norm': 0.293043851852417, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.5627, 'grad_norm': 0.3917730450630188, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.14it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.766, 'grad_norm': 0.7155589461326599, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.5312, 'grad_norm': 0.4632052183151245, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.89it/s] 40%|████      | 6/15 [00:00<00:00, 15.92it/s]                                              {'loss': 0.5877, 'grad_norm': 0.34005746245384216, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.92it/s]                                              {'loss': 0.5956, 'grad_norm': 0.2653903663158417, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.92it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.6018, 'grad_norm': 0.25069621205329895, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.7177, 'grad_norm': 0.5224080085754395, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.59it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.628, 'grad_norm': 0.2719172537326813, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.6696, 'grad_norm': 0.32771292328834534, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.15it/s] 80%|████████  | 12/15 [00:00<00:00, 16.00it/s]                                               {'loss': 0.5438, 'grad_norm': 0.45139583945274353, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.00it/s]                                               {'loss': 0.6073, 'grad_norm': 0.28400343656539917, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.00it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.5589, 'grad_norm': 0.3784887492656708, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.7677, 'grad_norm': 0.7205075621604919, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.59it/s]                                               {'train_runtime': 1.0589, 'train_samples_per_second': 401.351, 'train_steps_per_second': 14.165, 'train_loss': 0.6295315583546957, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.59it/s]100%|██████████| 15/15 [00:01<00:00, 14.17it/s]
CLIENT:74
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7066, 'grad_norm': 0.5157585740089417, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.64it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.86it/s]                                              {'loss': 0.5634, 'grad_norm': 0.2929469645023346, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.86it/s]                                              {'loss': 0.524, 'grad_norm': 0.4881856143474579, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.86it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.5604, 'grad_norm': 0.34522175788879395, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.7206, 'grad_norm': 0.5407575964927673, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.63it/s] 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.4856, 'grad_norm': 0.6352499127388, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.6526, 'grad_norm': 0.3426957428455353, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.03it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.5235, 'grad_norm': 0.4467005431652069, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.6608, 'grad_norm': 0.4129026532173157, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.36it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.6209, 'grad_norm': 0.2754051983356476, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.5603, 'grad_norm': 0.27613404393196106, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.91it/s] 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6671, 'grad_norm': 0.471043199300766, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.7238, 'grad_norm': 0.5574359893798828, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.41it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.5029, 'grad_norm': 0.4718046486377716, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6135, 'grad_norm': 0.3366472125053406, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.45it/s]                                               {'train_runtime': 1.0793, 'train_samples_per_second': 393.779, 'train_steps_per_second': 13.898, 'train_loss': 0.6057231386502584, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 13.90it/s]
CLIENT:44
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7199, 'grad_norm': 0.5669410824775696, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.77it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.32it/s]                                              {'loss': 0.7448, 'grad_norm': 0.6535866260528564, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.32it/s]                                              {'loss': 0.71, 'grad_norm': 0.5092412233352661, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.32it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.6841, 'grad_norm': 0.3511602580547333, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.715, 'grad_norm': 0.4531501829624176, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.85it/s] 40%|████      | 6/15 [00:00<00:00, 15.61it/s]                                              {'loss': 0.781, 'grad_norm': 0.770753026008606, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.61it/s]                                              {'loss': 0.6307, 'grad_norm': 0.2573043406009674, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.61it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.8217, 'grad_norm': 0.9492722153663635, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.6377, 'grad_norm': 0.32660168409347534, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.40it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.6797, 'grad_norm': 0.27712008357048035, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.66, 'grad_norm': 0.2089095264673233, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.96it/s] 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.7988, 'grad_norm': 0.7868620157241821, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.7541, 'grad_norm': 0.6332864165306091, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.51it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.6633, 'grad_norm': 0.24472853541374207, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.6429, 'grad_norm': 0.2848537266254425, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.86it/s]                                               {'train_runtime': 1.0686, 'train_samples_per_second': 397.711, 'train_steps_per_second': 14.037, 'train_loss': 0.7095806201299032, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.86it/s]100%|██████████| 15/15 [00:01<00:00, 14.04it/s]
CLIENT:60
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.648, 'grad_norm': 0.343781977891922, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.34it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.6657, 'grad_norm': 0.37369275093078613, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.6662, 'grad_norm': 0.49189460277557373, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.01it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.6002, 'grad_norm': 0.2690117061138153, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.6944, 'grad_norm': 0.46490389108657837, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.23it/s] 40%|████      | 6/15 [00:00<00:00, 15.92it/s]                                              {'loss': 0.6995, 'grad_norm': 0.533341646194458, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.92it/s]                                              {'loss': 0.7099, 'grad_norm': 0.5308880805969238, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.92it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.6227, 'grad_norm': 0.24501879513263702, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.6565, 'grad_norm': 0.26978859305381775, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.28it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.5567, 'grad_norm': 0.4227859675884247, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.7309, 'grad_norm': 0.6044414043426514, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.91it/s] 80%|████████  | 12/15 [00:00<00:00, 15.42it/s]                                               {'loss': 0.6595, 'grad_norm': 0.2891932427883148, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.42it/s]                                               {'loss': 0.7417, 'grad_norm': 0.5817647576332092, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.42it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6304, 'grad_norm': 0.24507088959217072, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.5372, 'grad_norm': 0.6002233624458313, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.48it/s]                                               {'train_runtime': 1.0815, 'train_samples_per_second': 392.972, 'train_steps_per_second': 13.87, 'train_loss': 0.6546288053194682, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]100%|██████████| 15/15 [00:01<00:00, 13.88it/s]
CLIENT:72
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6749, 'grad_norm': 0.4121144413948059, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.18it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.22it/s]                                              {'loss': 0.5237, 'grad_norm': 0.45365431904792786, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.22it/s]                                              {'loss': 0.5102, 'grad_norm': 0.5866175889968872, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.22it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.81it/s]                                              {'loss': 0.5494, 'grad_norm': 0.3790777325630188, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.81it/s]                                              {'loss': 0.5039, 'grad_norm': 0.5230036377906799, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.81it/s] 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.7332, 'grad_norm': 0.696152925491333, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.7504, 'grad_norm': 0.7679678201675415, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.50it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.5127, 'grad_norm': 0.46276840567588806, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.4221, 'grad_norm': 0.7484918236732483, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.28it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.5595, 'grad_norm': 0.26069310307502747, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.5641, 'grad_norm': 0.28636863827705383, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.03it/s] 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.5867, 'grad_norm': 0.3034696578979492, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.5348, 'grad_norm': 0.30891987681388855, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.46it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6062, 'grad_norm': 0.26890042424201965, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6145, 'grad_norm': 0.35119572281837463, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.45it/s]                                               {'train_runtime': 1.0823, 'train_samples_per_second': 392.672, 'train_steps_per_second': 13.859, 'train_loss': 0.5764277239640554, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 13.86it/s]
CLIENT:5
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5562, 'grad_norm': 0.33634302020072937, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.75it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.57it/s]                                              {'loss': 0.655, 'grad_norm': 0.32829535007476807, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.57it/s]                                              {'loss': 0.7725, 'grad_norm': 0.7651214003562927, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.57it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.02it/s]                                              {'loss': 0.7308, 'grad_norm': 0.6125847101211548, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.02it/s]                                              {'loss': 0.5389, 'grad_norm': 0.4391859173774719, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.02it/s] 40%|████      | 6/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.6377, 'grad_norm': 0.2999761700630188, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.01it/s]                                              {'loss': 0.6333, 'grad_norm': 0.293854296207428, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.01it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6768, 'grad_norm': 0.32409656047821045, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.5512, 'grad_norm': 0.43196043372154236, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.06it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.6879, 'grad_norm': 0.37397974729537964, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.6158, 'grad_norm': 0.2433902770280838, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.72it/s] 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.572, 'grad_norm': 0.37950876355171204, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.6742, 'grad_norm': 0.3083588182926178, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.26it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.6326, 'grad_norm': 0.24532431364059448, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.5872, 'grad_norm': 0.39512887597084045, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]                                               {'train_runtime': 1.1013, 'train_samples_per_second': 385.896, 'train_steps_per_second': 13.62, 'train_loss': 0.6347958485285441, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]100%|██████████| 15/15 [00:01<00:00, 13.63it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 37.87it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.85it/s] 36%|███▋      | 12/33 [00:00<00:00, 26.66it/s] 45%|████▌     | 15/33 [00:00<00:00, 25.94it/s] 55%|█████▍    | 18/33 [00:00<00:00, 25.39it/s] 64%|██████▎   | 21/33 [00:00<00:00, 25.21it/s] 73%|███████▎  | 24/33 [00:00<00:00, 25.33it/s] 82%|████████▏ | 27/33 [00:01<00:00, 24.98it/s] 91%|█████████ | 30/33 [00:01<00:00, 25.14it/s]100%|██████████| 33/33 [00:01<00:00, 26.13it/s]100%|██████████| 33/33 [00:01<00:00, 26.17it/s]
{'eval_loss': 0.6166480779647827, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3028, 'eval_samples_per_second': 800.597, 'eval_steps_per_second': 25.331}
ROUND:28
CLIENT:67
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5423, 'grad_norm': 0.3926050066947937, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.83it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.5102, 'grad_norm': 0.5003060102462769, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.6009, 'grad_norm': 0.3009410500526428, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.09it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.4896, 'grad_norm': 0.5832754373550415, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.5255, 'grad_norm': 0.3750559985637665, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.11it/s] 40%|████      | 6/15 [00:00<00:00, 15.96it/s]                                              {'loss': 0.642, 'grad_norm': 0.41191041469573975, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.96it/s]                                              {'loss': 0.623, 'grad_norm': 0.32982078194618225, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.96it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.4712, 'grad_norm': 0.5043660402297974, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.4832, 'grad_norm': 0.4587882459163666, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.50it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.5118, 'grad_norm': 0.3325226902961731, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.5426, 'grad_norm': 0.35812556743621826, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.99it/s] 80%|████████  | 12/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.5116, 'grad_norm': 0.4063948690891266, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.537, 'grad_norm': 0.32000282406806946, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.63it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.5372, 'grad_norm': 0.27932655811309814, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.4654, 'grad_norm': 0.4203968346118927, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.54it/s]                                               {'train_runtime': 1.0693, 'train_samples_per_second': 397.441, 'train_steps_per_second': 14.027, 'train_loss': 0.5328806499640147, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.54it/s]100%|██████████| 15/15 [00:01<00:00, 14.03it/s]
CLIENT:14
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7076, 'grad_norm': 0.5365833044052124, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.55it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.99it/s]                                              {'loss': 0.6278, 'grad_norm': 0.2688995599746704, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.99it/s]                                              {'loss': 0.6037, 'grad_norm': 0.298048198223114, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.99it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.646, 'grad_norm': 0.26985839009284973, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.6923, 'grad_norm': 0.3975401818752289, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.28it/s] 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.5687, 'grad_norm': 0.3743709325790405, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.6247, 'grad_norm': 0.23050753772258759, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.22it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.6404, 'grad_norm': 0.29677435755729675, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.6785, 'grad_norm': 0.39899441599845886, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.19it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.6688, 'grad_norm': 0.31471672654151917, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.6002, 'grad_norm': 0.2627018690109253, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.11it/s] 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6439, 'grad_norm': 0.480871319770813, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6871, 'grad_norm': 0.3949066400527954, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.54it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6329, 'grad_norm': 0.28171151876449585, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.5358, 'grad_norm': 0.5239897966384888, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]                                               {'train_runtime': 1.0931, 'train_samples_per_second': 388.813, 'train_steps_per_second': 13.723, 'train_loss': 0.6372307856877645, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 13.73it/s]
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5359, 'grad_norm': 0.40110456943511963, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.77it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.57it/s]                                              {'loss': 0.544, 'grad_norm': 0.3903615176677704, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.57it/s]                                              {'loss': 0.7233, 'grad_norm': 0.5825471878051758, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.57it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.5771, 'grad_norm': 0.31850484013557434, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.5767, 'grad_norm': 0.27752408385276794, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.16it/s] 40%|████      | 6/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6297, 'grad_norm': 0.3858911097049713, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.5845, 'grad_norm': 0.2550027370452881, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.13it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.5738, 'grad_norm': 0.31660887598991394, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.5816, 'grad_norm': 0.32904863357543945, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.11it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.6268, 'grad_norm': 0.2849660813808441, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.6023, 'grad_norm': 0.25222301483154297, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.04it/s] 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.4693, 'grad_norm': 0.6350670456886292, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.5845, 'grad_norm': 0.24028612673282623, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.49it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.5635, 'grad_norm': 0.27989327907562256, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.5963, 'grad_norm': 0.35793396830558777, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]                                               {'train_runtime': 1.0886, 'train_samples_per_second': 390.416, 'train_steps_per_second': 13.779, 'train_loss': 0.5846142868200938, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]100%|██████████| 15/15 [00:01<00:00, 13.78it/s]
CLIENT:86
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5769, 'grad_norm': 0.30725669860839844, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.24it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.63it/s]                                              {'loss': 0.6626, 'grad_norm': 0.3712567090988159, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.63it/s]                                              {'loss': 0.6454, 'grad_norm': 0.4014473259449005, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.63it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.623, 'grad_norm': 0.23488262295722961, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.6643, 'grad_norm': 0.3401730954647064, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.91it/s] 40%|████      | 6/15 [00:00<00:00, 15.61it/s]                                              {'loss': 0.6022, 'grad_norm': 0.3096030056476593, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.61it/s]                                              {'loss': 0.6522, 'grad_norm': 0.30169063806533813, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.61it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.667, 'grad_norm': 0.368365079164505, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.5183, 'grad_norm': 0.5605396628379822, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.37it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.5968, 'grad_norm': 0.2427699863910675, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.5954, 'grad_norm': 0.27369824051856995, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.96it/s] 80%|████████  | 12/15 [00:00<00:00, 15.45it/s]                                               {'loss': 0.725, 'grad_norm': 0.5963295102119446, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.45it/s]                                               {'loss': 0.6282, 'grad_norm': 0.25982269644737244, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.45it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6289, 'grad_norm': 0.2846807539463043, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6421, 'grad_norm': 0.331486314535141, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.38it/s]                                               {'train_runtime': 1.0798, 'train_samples_per_second': 393.591, 'train_steps_per_second': 13.891, 'train_loss': 0.6285548567771911, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 13.90it/s]
CLIENT:13
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5615, 'grad_norm': 0.3359379470348358, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.93it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.46it/s]                                              {'loss': 0.5363, 'grad_norm': 0.4433276355266571, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.46it/s]                                              {'loss': 0.4658, 'grad_norm': 0.6541348099708557, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.46it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.4815, 'grad_norm': 0.5660273432731628, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.5286, 'grad_norm': 0.375182181596756, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.50it/s] 40%|████      | 6/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.5816, 'grad_norm': 0.3282828629016876, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.5006, 'grad_norm': 0.3600969612598419, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.21it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.4805, 'grad_norm': 0.41009923815727234, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.5986, 'grad_norm': 0.3797469735145569, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.08it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6446, 'grad_norm': 0.42763179540634155, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.4723, 'grad_norm': 0.35225504636764526, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.55it/s] 80%|████████  | 12/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.3856, 'grad_norm': 0.6603321433067322, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.6184, 'grad_norm': 0.42609071731567383, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.28it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.28it/s]                                               {'loss': 0.3446, 'grad_norm': 0.7878515720367432, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.28it/s]                                               {'loss': 0.6132, 'grad_norm': 0.44846031069755554, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.28it/s]                                               {'train_runtime': 1.1003, 'train_samples_per_second': 386.274, 'train_steps_per_second': 13.633, 'train_loss': 0.5209010481834412, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.28it/s]100%|██████████| 15/15 [00:01<00:00, 13.64it/s]
CLIENT:18
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5251, 'grad_norm': 0.4647236168384552, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.11it/s] 13%|█▎        | 2/15 [00:00<00:01, 11.97it/s]                                              {'loss': 0.6056, 'grad_norm': 0.25700703263282776, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 11.97it/s]                                              {'loss': 0.6817, 'grad_norm': 0.4856339693069458, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:01, 11.97it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.33it/s]                                              {'loss': 0.5152, 'grad_norm': 0.5710176229476929, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.33it/s]                                              {'loss': 0.6483, 'grad_norm': 0.29471689462661743, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.33it/s] 40%|████      | 6/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.6359, 'grad_norm': 0.38868269324302673, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.7076, 'grad_norm': 0.5092159509658813, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.26it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.5915, 'grad_norm': 0.2572859227657318, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.4207, 'grad_norm': 0.805899977684021, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.99it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.5448, 'grad_norm': 0.3570026457309723, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.5936, 'grad_norm': 0.24845340847969055, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.49it/s] 80%|████████  | 12/15 [00:00<00:00, 15.27it/s]                                               {'loss': 0.67, 'grad_norm': 0.462248831987381, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.27it/s]                                               {'loss': 0.5716, 'grad_norm': 0.3048366606235504, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.27it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.26it/s]                                               {'loss': 0.6689, 'grad_norm': 0.4479540288448334, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.26it/s]                                               {'loss': 0.5116, 'grad_norm': 0.40902790427207947, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.26it/s]                                               {'train_runtime': 1.1108, 'train_samples_per_second': 382.619, 'train_steps_per_second': 13.504, 'train_loss': 0.592810054620107, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.26it/s]100%|██████████| 15/15 [00:01<00:00, 13.51it/s]
CLIENT:30
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6898, 'grad_norm': 0.423845112323761, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.88it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.44it/s]                                              {'loss': 0.6054, 'grad_norm': 0.2593771815299988, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.44it/s]                                              {'loss': 0.5472, 'grad_norm': 0.4519070088863373, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.44it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.6111, 'grad_norm': 0.21064865589141846, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.638, 'grad_norm': 0.28000667691230774, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.78it/s] 40%|████      | 6/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.5974, 'grad_norm': 0.3176214098930359, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.6686, 'grad_norm': 0.35226404666900635, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.53it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.5851, 'grad_norm': 0.3043181896209717, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.615, 'grad_norm': 0.30002331733703613, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.10it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.25it/s]                                               {'loss': 0.622, 'grad_norm': 0.2656444013118744, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.25it/s]                                               {'loss': 0.7128, 'grad_norm': 0.49982401728630066, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.25it/s] 80%|████████  | 12/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.4443, 'grad_norm': 0.8061887621879578, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.6177, 'grad_norm': 0.2591991424560547, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 14.95it/s] 93%|█████████▎| 14/15 [00:00<00:00, 13.97it/s]                                               {'loss': 0.5829, 'grad_norm': 0.2856089174747467, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 13.97it/s]                                               {'loss': 0.6915, 'grad_norm': 0.47831353545188904, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.97it/s]                                               {'train_runtime': 1.1066, 'train_samples_per_second': 384.066, 'train_steps_per_second': 13.555, 'train_loss': 0.6152545114358267, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.97it/s]100%|██████████| 15/15 [00:01<00:00, 13.56it/s]
CLIENT:36
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5841, 'grad_norm': 0.26448675990104675, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.07it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.5889, 'grad_norm': 0.29793038964271545, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.52, 'grad_norm': 0.5111755728721619, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.30it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.5244, 'grad_norm': 0.44415587186813354, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.6004, 'grad_norm': 0.27353110909461975, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.37it/s] 40%|████      | 6/15 [00:00<00:00, 16.26it/s]                                              {'loss': 0.5953, 'grad_norm': 0.3375161588191986, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.26it/s]                                              {'loss': 0.4449, 'grad_norm': 0.6892860531806946, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.26it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.7221, 'grad_norm': 0.675147533416748, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.5187, 'grad_norm': 0.41282400488853455, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.57it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.4805, 'grad_norm': 0.47724097967147827, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.627, 'grad_norm': 0.3535841107368469, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.05it/s] 80%|████████  | 12/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.6125, 'grad_norm': 0.3752334713935852, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.6317, 'grad_norm': 0.3410545587539673, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.66it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.68it/s]                                               {'loss': 0.5211, 'grad_norm': 0.3516698181629181, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.68it/s]                                               {'loss': 0.5253, 'grad_norm': 0.34832778573036194, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.68it/s]                                               {'train_runtime': 1.0549, 'train_samples_per_second': 402.891, 'train_steps_per_second': 14.22, 'train_loss': 0.5664533575375875, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.68it/s]100%|██████████| 15/15 [00:01<00:00, 14.22it/s]
CLIENT:52
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5945, 'grad_norm': 0.24954459071159363, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.11it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.6499, 'grad_norm': 0.3195778727531433, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.718, 'grad_norm': 0.6187984347343445, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.29it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.6798, 'grad_norm': 0.4083188772201538, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.615, 'grad_norm': 0.24773024022579193, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.51it/s] 40%|████      | 6/15 [00:00<00:00, 15.93it/s]                                              {'loss': 0.6356, 'grad_norm': 0.3301014304161072, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.93it/s]                                              {'loss': 0.6224, 'grad_norm': 0.23570670187473297, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.93it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.612, 'grad_norm': 0.2727810740470886, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.7471, 'grad_norm': 0.6756641268730164, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.59it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.681, 'grad_norm': 0.3372389078140259, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.6172, 'grad_norm': 0.24503421783447266, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.18it/s] 80%|████████  | 12/15 [00:00<00:00, 15.60it/s]                                               {'loss': 0.5931, 'grad_norm': 0.32685333490371704, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.60it/s]                                               {'loss': 0.7163, 'grad_norm': 0.5067253708839417, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.60it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.6061, 'grad_norm': 0.2588662803173065, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.5572, 'grad_norm': 0.45310282707214355, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.60it/s]                                               {'train_runtime': 1.0552, 'train_samples_per_second': 402.776, 'train_steps_per_second': 14.216, 'train_loss': 0.6430134574572245, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.60it/s]100%|██████████| 15/15 [00:01<00:00, 14.22it/s]
CLIENT:95
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6689, 'grad_norm': 0.4092770516872406, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.98it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.40it/s]                                              {'loss': 0.5616, 'grad_norm': 0.320722371339798, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.40it/s]                                              {'loss': 0.5869, 'grad_norm': 0.3344419002532959, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.40it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.537, 'grad_norm': 0.47255757451057434, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.649, 'grad_norm': 0.2938728332519531, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.66it/s] 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.6536, 'grad_norm': 0.35232773423194885, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.6147, 'grad_norm': 0.26126033067703247, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.59it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.5996, 'grad_norm': 0.25777676701545715, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.6303, 'grad_norm': 0.34206560254096985, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.14it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.6713, 'grad_norm': 0.4323238134384155, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.5568, 'grad_norm': 0.35024651885032654, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.75it/s] 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.5907, 'grad_norm': 0.29283133149147034, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.6935, 'grad_norm': 0.5295141935348511, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.653, 'grad_norm': 0.31581079959869385, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.4328, 'grad_norm': 0.8246403932571411, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.72it/s]                                               {'train_runtime': 1.0737, 'train_samples_per_second': 395.823, 'train_steps_per_second': 13.97, 'train_loss': 0.6066634635130564, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.72it/s]100%|██████████| 15/15 [00:01<00:00, 13.98it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 36.08it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.22it/s] 33%|███▎      | 11/33 [00:00<00:00, 27.15it/s] 42%|████▏     | 14/33 [00:00<00:00, 26.08it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.49it/s] 61%|██████    | 20/33 [00:00<00:00, 25.06it/s] 70%|██████▉   | 23/33 [00:00<00:00, 24.94it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.10it/s] 88%|████████▊ | 29/33 [00:01<00:00, 24.97it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.15it/s]100%|██████████| 33/33 [00:01<00:00, 26.06it/s]
{'eval_loss': 0.6167448163032532, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.308, 'eval_samples_per_second': 797.425, 'eval_steps_per_second': 25.23}
ROUND:29
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5897, 'grad_norm': 0.24578620493412018, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.14it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.607, 'grad_norm': 0.24476377665996552, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.7235, 'grad_norm': 0.5949313640594482, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.97it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.52it/s]                                              {'loss': 0.5434, 'grad_norm': 0.3989892601966858, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.52it/s]                                              {'loss': 0.6651, 'grad_norm': 0.34285467863082886, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.52it/s] 40%|████      | 6/15 [00:00<00:00, 15.65it/s]                                              {'loss': 0.6676, 'grad_norm': 0.40568622946739197, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.65it/s]                                              {'loss': 0.5915, 'grad_norm': 0.2621385455131531, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.65it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.44it/s]                                              {'loss': 0.6802, 'grad_norm': 0.39240142703056335, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.44it/s]                                              {'loss': 0.5864, 'grad_norm': 0.29868584871292114, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.44it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.5877, 'grad_norm': 0.25937122106552124, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.6744, 'grad_norm': 0.3846278786659241, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.03it/s] 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.6426, 'grad_norm': 0.3030530512332916, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.6336, 'grad_norm': 0.2933499217033386, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.48it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.6676, 'grad_norm': 0.32458382844924927, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.5328, 'grad_norm': 0.4909898042678833, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.58it/s]                                               {'train_runtime': 1.0685, 'train_samples_per_second': 397.771, 'train_steps_per_second': 14.039, 'train_loss': 0.6262142101923625, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.58it/s]100%|██████████| 15/15 [00:01<00:00, 14.04it/s]
CLIENT:99
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/25 [00:00<?, ?it/s]                                      {'loss': 0.6076, 'grad_norm': 0.23892033100128174, 'learning_rate': 0.01, 'epoch': 0.2}
  4%|▍         | 1/25 [00:00<00:01, 13.02it/s]  8%|▊         | 2/25 [00:00<00:01, 14.06it/s]                                              {'loss': 0.5063, 'grad_norm': 0.5265037417411804, 'learning_rate': 0.0096, 'epoch': 0.4}
  8%|▊         | 2/25 [00:00<00:01, 14.06it/s]                                              {'loss': 0.5398, 'grad_norm': 0.36973994970321655, 'learning_rate': 0.0092, 'epoch': 0.6}
 12%|█▏        | 3/25 [00:00<00:01, 14.06it/s] 16%|█▌        | 4/25 [00:00<00:01, 14.30it/s]                                              {'loss': 0.5254, 'grad_norm': 0.44691553711891174, 'learning_rate': 0.0088, 'epoch': 0.8}
 16%|█▌        | 4/25 [00:00<00:01, 14.30it/s]                                              {'loss': 0.7902, 'grad_norm': 1.076625943183899, 'learning_rate': 0.0084, 'epoch': 1.0}
 20%|██        | 5/25 [00:00<00:01, 14.30it/s]                                              {'loss': 0.5901, 'grad_norm': 0.2300616353750229, 'learning_rate': 0.008, 'epoch': 1.2}
 24%|██▍       | 6/25 [00:00<00:01, 14.30it/s] 28%|██▊       | 7/25 [00:00<00:01, 15.57it/s]                                              {'loss': 0.5979, 'grad_norm': 0.2750086486339569, 'learning_rate': 0.0076, 'epoch': 1.4}
 28%|██▊       | 7/25 [00:00<00:01, 15.57it/s]                                              {'loss': 0.5428, 'grad_norm': 0.3155736029148102, 'learning_rate': 0.0072, 'epoch': 1.6}
 32%|███▏      | 8/25 [00:00<00:01, 15.57it/s] 36%|███▌      | 9/25 [00:00<00:01, 14.60it/s]                                              {'loss': 0.5471, 'grad_norm': 0.31499651074409485, 'learning_rate': 0.0068000000000000005, 'epoch': 1.8}
 36%|███▌      | 9/25 [00:00<00:01, 14.60it/s]                                              {'loss': 0.4514, 'grad_norm': 0.665859043598175, 'learning_rate': 0.0064, 'epoch': 2.0}
 40%|████      | 10/25 [00:00<00:01, 14.60it/s] 44%|████▍     | 11/25 [00:00<00:00, 15.75it/s]                                               {'loss': 0.5433, 'grad_norm': 0.3640722930431366, 'learning_rate': 0.006, 'epoch': 2.2}
 44%|████▍     | 11/25 [00:00<00:00, 15.75it/s]                                               {'loss': 0.7324, 'grad_norm': 0.7048887014389038, 'learning_rate': 0.005600000000000001, 'epoch': 2.4}
 48%|████▊     | 12/25 [00:00<00:00, 15.75it/s] 52%|█████▏    | 13/25 [00:00<00:00, 14.59it/s]                                               {'loss': 0.5297, 'grad_norm': 0.2964926064014435, 'learning_rate': 0.005200000000000001, 'epoch': 2.6}
 52%|█████▏    | 13/25 [00:00<00:00, 14.59it/s]                                               {'loss': 0.467, 'grad_norm': 0.48637673258781433, 'learning_rate': 0.0048, 'epoch': 2.8}
 56%|█████▌    | 14/25 [00:00<00:00, 14.59it/s]                                               {'loss': 0.3096, 'grad_norm': 1.1241170167922974, 'learning_rate': 0.0044, 'epoch': 3.0}
 60%|██████    | 15/25 [00:00<00:00, 14.59it/s] 64%|██████▍   | 16/25 [00:01<00:00, 15.65it/s]                                               {'loss': 0.5627, 'grad_norm': 0.25414782762527466, 'learning_rate': 0.004, 'epoch': 3.2}
 64%|██████▍   | 16/25 [00:01<00:00, 15.65it/s]                                               {'loss': 0.5242, 'grad_norm': 0.2521122992038727, 'learning_rate': 0.0036, 'epoch': 3.4}
 68%|██████▊   | 17/25 [00:01<00:00, 15.65it/s] 72%|███████▏  | 18/25 [00:01<00:00, 15.43it/s]                                               {'loss': 0.5191, 'grad_norm': 0.27575454115867615, 'learning_rate': 0.0032, 'epoch': 3.6}
 72%|███████▏  | 18/25 [00:01<00:00, 15.43it/s]                                               {'loss': 0.4763, 'grad_norm': 0.3537389039993286, 'learning_rate': 0.0028000000000000004, 'epoch': 3.8}
 76%|███████▌  | 19/25 [00:01<00:00, 15.43it/s]                                               {'loss': 1.0061, 'grad_norm': 1.8963751792907715, 'learning_rate': 0.0024, 'epoch': 4.0}
 80%|████████  | 20/25 [00:01<00:00, 15.43it/s] 84%|████████▍ | 21/25 [00:01<00:00, 15.96it/s]                                               {'loss': 0.6908, 'grad_norm': 0.6386004090309143, 'learning_rate': 0.002, 'epoch': 4.2}
 84%|████████▍ | 21/25 [00:01<00:00, 15.96it/s]                                               {'loss': 0.5467, 'grad_norm': 0.2675776481628418, 'learning_rate': 0.0016, 'epoch': 4.4}
 88%|████████▊ | 22/25 [00:01<00:00, 15.96it/s] 92%|█████████▏| 23/25 [00:01<00:00, 14.88it/s]                                               {'loss': 0.453, 'grad_norm': 0.39857620000839233, 'learning_rate': 0.0012, 'epoch': 4.6}
 92%|█████████▏| 23/25 [00:01<00:00, 14.88it/s]                                               {'loss': 0.4866, 'grad_norm': 0.33389753103256226, 'learning_rate': 0.0008, 'epoch': 4.8}
 96%|█████████▌| 24/25 [00:01<00:00, 14.88it/s]                                               {'loss': 0.5329, 'grad_norm': 0.5707301497459412, 'learning_rate': 0.0004, 'epoch': 5.0}
100%|██████████| 25/25 [00:01<00:00, 14.88it/s]                                               {'train_runtime': 1.689, 'train_samples_per_second': 402.605, 'train_steps_per_second': 14.802, 'train_loss': 0.5631561732292175, 'epoch': 5.0}
100%|██████████| 25/25 [00:01<00:00, 14.88it/s]100%|██████████| 25/25 [00:01<00:00, 14.80it/s]
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6308, 'grad_norm': 0.2785564064979553, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.12it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.21it/s]                                              {'loss': 0.6756, 'grad_norm': 0.39438989758491516, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.21it/s]                                              {'loss': 0.893, 'grad_norm': 1.3455963134765625, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.21it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.7793, 'grad_norm': 0.7698426842689514, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.6912, 'grad_norm': 0.3946807384490967, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.35it/s] 40%|████      | 6/15 [00:00<00:00, 15.78it/s]                                              {'loss': 0.6346, 'grad_norm': 0.31573277711868286, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.78it/s]                                              {'loss': 0.7032, 'grad_norm': 0.4059247076511383, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.78it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.6989, 'grad_norm': 0.4218147099018097, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.6286, 'grad_norm': 0.3042203485965729, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.50it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.7167, 'grad_norm': 0.5040742754936218, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.6782, 'grad_norm': 0.249959796667099, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.05it/s] 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.6237, 'grad_norm': 0.3124145269393921, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.6578, 'grad_norm': 0.2415902316570282, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.51it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.7129, 'grad_norm': 0.4416287839412689, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.6746, 'grad_norm': 0.2833491563796997, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.74it/s]                                               {'train_runtime': 1.0549, 'train_samples_per_second': 402.887, 'train_steps_per_second': 14.22, 'train_loss': 0.6932602365811665, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.74it/s]100%|██████████| 15/15 [00:01<00:00, 14.22it/s]
CLIENT:61
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.731, 'grad_norm': 0.5905858278274536, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.20it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.92it/s]                                              {'loss': 0.6347, 'grad_norm': 0.22783049941062927, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.92it/s]                                              {'loss': 0.4454, 'grad_norm': 0.8350796699523926, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.92it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.07it/s]                                              {'loss': 0.5502, 'grad_norm': 0.40459686517715454, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.07it/s]                                              {'loss': 0.5943, 'grad_norm': 0.3037313222885132, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.07it/s] 40%|████      | 6/15 [00:00<00:00, 15.66it/s]                                              {'loss': 0.8182, 'grad_norm': 0.9830357432365417, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.66it/s]                                              {'loss': 0.5748, 'grad_norm': 0.2889387309551239, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.66it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.5833, 'grad_norm': 0.3053530752658844, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.7811, 'grad_norm': 0.902525007724762, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.32it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.5672, 'grad_norm': 0.3125506043434143, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.7123, 'grad_norm': 0.5297786593437195, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.86it/s] 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.5578, 'grad_norm': 0.3735339939594269, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.6507, 'grad_norm': 0.28486740589141846, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.46it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.5998, 'grad_norm': 0.2644697427749634, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.6072, 'grad_norm': 0.29319608211517334, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.47it/s]                                               {'train_runtime': 1.0647, 'train_samples_per_second': 399.161, 'train_steps_per_second': 14.088, 'train_loss': 0.6272063791751862, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.47it/s]100%|██████████| 15/15 [00:01<00:00, 14.09it/s]
CLIENT:66
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6075, 'grad_norm': 0.30543988943099976, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.70it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.6759, 'grad_norm': 0.4130845069885254, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.6013, 'grad_norm': 0.3329146206378937, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.08it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.73it/s]                                              {'loss': 0.6924, 'grad_norm': 0.40637728571891785, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.73it/s]                                              {'loss': 0.6078, 'grad_norm': 0.26793617010116577, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.73it/s] 40%|████      | 6/15 [00:00<00:00, 16.17it/s]                                              {'loss': 0.6117, 'grad_norm': 0.29547590017318726, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.17it/s]                                              {'loss': 0.5961, 'grad_norm': 0.26586228609085083, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.17it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.7085, 'grad_norm': 0.4740404784679413, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.5548, 'grad_norm': 0.4174385666847229, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.71it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.6795, 'grad_norm': 0.3783431947231293, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.6683, 'grad_norm': 0.3572814464569092, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.14it/s] 80%|████████  | 12/15 [00:00<00:00, 15.71it/s]                                               {'loss': 0.4781, 'grad_norm': 0.6888383626937866, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.71it/s]                                               {'loss': 0.6341, 'grad_norm': 0.29003867506980896, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.71it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.6264, 'grad_norm': 0.26634618639945984, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.6485, 'grad_norm': 0.33470332622528076, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.63it/s]                                               {'train_runtime': 1.0541, 'train_samples_per_second': 403.171, 'train_steps_per_second': 14.23, 'train_loss': 0.6260573367277781, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.63it/s]100%|██████████| 15/15 [00:01<00:00, 14.23it/s]
CLIENT:36
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5835, 'grad_norm': 0.26816871762275696, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.24it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.588, 'grad_norm': 0.28897395730018616, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.5177, 'grad_norm': 0.5040673017501831, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.64it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.5218, 'grad_norm': 0.43621423840522766, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6, 'grad_norm': 0.2770332098007202, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.16it/s] 40%|████      | 6/15 [00:00<00:00, 15.71it/s]                                              {'loss': 0.595, 'grad_norm': 0.34663519263267517, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.71it/s]                                              {'loss': 0.4419, 'grad_norm': 0.6859637498855591, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.71it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.7241, 'grad_norm': 0.691807210445404, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.5173, 'grad_norm': 0.4111853539943695, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.41it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.479, 'grad_norm': 0.47723352909088135, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.6272, 'grad_norm': 0.3607625365257263, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.05it/s] 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.6126, 'grad_norm': 0.38300973176956177, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.6318, 'grad_norm': 0.3522653579711914, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.47it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.5203, 'grad_norm': 0.34767380356788635, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.5246, 'grad_norm': 0.349937379360199, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.45it/s]                                               {'train_runtime': 1.0639, 'train_samples_per_second': 399.482, 'train_steps_per_second': 14.099, 'train_loss': 0.565654973189036, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 14.10it/s]
CLIENT:23
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5357, 'grad_norm': 0.45247265696525574, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.39it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.6731, 'grad_norm': 0.3741866946220398, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.5919, 'grad_norm': 0.3709460198879242, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.97it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.07it/s]                                              {'loss': 0.6463, 'grad_norm': 0.3437003791332245, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.07it/s]                                              {'loss': 0.5367, 'grad_norm': 0.4001864790916443, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.07it/s] 40%|████      | 6/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.5892, 'grad_norm': 0.40832608938217163, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.6708, 'grad_norm': 0.37517544627189636, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 14.91it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.03it/s]                                              {'loss': 0.5932, 'grad_norm': 0.2787981629371643, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.03it/s]                                              {'loss': 0.4688, 'grad_norm': 0.6766104102134705, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.03it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.66it/s]                                               {'loss': 0.6759, 'grad_norm': 0.48324868083000183, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.66it/s]                                               {'loss': 0.543, 'grad_norm': 0.3473801016807556, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.66it/s] 80%|████████  | 12/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.5661, 'grad_norm': 0.31139519810676575, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.4711, 'grad_norm': 0.5890797972679138, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.39it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.7082, 'grad_norm': 0.575221598148346, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.592, 'grad_norm': 0.2907099425792694, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]                                               {'train_runtime': 1.0844, 'train_samples_per_second': 391.911, 'train_steps_per_second': 13.832, 'train_loss': 0.5907880345980326, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]100%|██████████| 15/15 [00:01<00:00, 13.84it/s]
CLIENT:83
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5154, 'grad_norm': 0.5037661194801331, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.87it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.19it/s]                                              {'loss': 0.5942, 'grad_norm': 0.26781943440437317, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.19it/s]                                              {'loss': 0.6192, 'grad_norm': 0.3227708637714386, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.19it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.5523, 'grad_norm': 0.32543206214904785, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.612, 'grad_norm': 0.2580462694168091, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.63it/s] 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.4906, 'grad_norm': 0.5003476738929749, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.5862, 'grad_norm': 0.23387505114078522, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.46it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.513, 'grad_norm': 0.3879966735839844, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.6011, 'grad_norm': 0.3391703963279724, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.72it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.6099, 'grad_norm': 0.3104425072669983, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.491, 'grad_norm': 0.38992515206336975, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.03it/s] 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.5829, 'grad_norm': 0.32006675004959106, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.4616, 'grad_norm': 0.4587864279747009, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.57it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.33it/s]                                               {'loss': 0.6644, 'grad_norm': 0.5252659320831299, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.33it/s]                                               {'loss': 0.5906, 'grad_norm': 0.3266935348510742, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.33it/s]                                               {'train_runtime': 1.0732, 'train_samples_per_second': 396.019, 'train_steps_per_second': 13.977, 'train_loss': 0.5656340102354686, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.33it/s]100%|██████████| 15/15 [00:01<00:00, 13.98it/s]
CLIENT:77
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6167, 'grad_norm': 0.24541285634040833, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.45it/s] 13%|█▎        | 2/15 [00:00<00:01, 13.00it/s]                                              {'loss': 0.6938, 'grad_norm': 0.4060894548892975, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 13.00it/s]                                              {'loss': 0.6815, 'grad_norm': 0.4382508099079132, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.00it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.7186, 'grad_norm': 0.508275032043457, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.662, 'grad_norm': 0.2839778959751129, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.72it/s] 40%|████      | 6/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.5693, 'grad_norm': 0.3750561773777008, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.6351, 'grad_norm': 0.2714407444000244, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.41it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.6816, 'grad_norm': 0.3796081840991974, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.6373, 'grad_norm': 0.30704566836357117, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.30it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.6724, 'grad_norm': 0.3145122826099396, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.5977, 'grad_norm': 0.2896176278591156, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.25it/s] 80%|████████  | 12/15 [00:00<00:00, 15.65it/s]                                               {'loss': 0.7123, 'grad_norm': 0.4718281030654907, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.65it/s]                                               {'loss': 0.6109, 'grad_norm': 0.27603042125701904, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.65it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.7301, 'grad_norm': 0.546128511428833, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.6109, 'grad_norm': 0.34036096930503845, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.58it/s]                                               {'train_runtime': 1.0643, 'train_samples_per_second': 399.331, 'train_steps_per_second': 14.094, 'train_loss': 0.6553453763326009, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.58it/s]100%|██████████| 15/15 [00:01<00:00, 14.10it/s]
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6017, 'grad_norm': 0.2575872838497162, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.27it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.03it/s]                                              {'loss': 0.5613, 'grad_norm': 0.3006938099861145, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.03it/s]                                              {'loss': 0.5646, 'grad_norm': 0.3054647445678711, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.03it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.541, 'grad_norm': 0.37655043601989746, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.6492, 'grad_norm': 0.3652527928352356, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.63it/s] 40%|████      | 6/15 [00:00<00:00, 15.65it/s]                                              {'loss': 0.5379, 'grad_norm': 0.44790714979171753, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.65it/s]                                              {'loss': 0.593, 'grad_norm': 0.2139551192522049, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.65it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.89it/s]                                              {'loss': 0.5953, 'grad_norm': 0.25648030638694763, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.89it/s]                                              {'loss': 0.5471, 'grad_norm': 0.3337043821811676, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.89it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.00it/s]                                               {'loss': 0.5619, 'grad_norm': 0.272271990776062, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.00it/s]                                               {'loss': 0.618, 'grad_norm': 0.32928407192230225, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.00it/s] 80%|████████  | 12/15 [00:00<00:00, 15.96it/s]                                               {'loss': 0.556, 'grad_norm': 0.3437618315219879, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.96it/s]                                               {'loss': 0.5462, 'grad_norm': 0.30908891558647156, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.96it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.6488, 'grad_norm': 0.4059443473815918, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.5685, 'grad_norm': 0.2927161753177643, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.70it/s]                                               {'train_runtime': 1.0592, 'train_samples_per_second': 401.261, 'train_steps_per_second': 14.162, 'train_loss': 0.579376204808553, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.70it/s]100%|██████████| 15/15 [00:01<00:00, 14.17it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:347: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 35.72it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.47it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.55it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.80it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.68it/s] 61%|██████    | 20/33 [00:00<00:00, 25.29it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.21it/s] 79%|███████▉  | 26/33 [00:00<00:00, 25.32it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.04it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.32it/s]100%|██████████| 33/33 [00:01<00:00, 26.14it/s]
{'eval_loss': 0.6167947053909302, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3037, 'eval_samples_per_second': 800.017, 'eval_steps_per_second': 25.312}

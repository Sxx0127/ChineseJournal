nohup: ignoring input
/home/suxiaoxin/.conda/envs/sxx/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at ./data/glue/glue.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at ./model/models--distilbert-base-multilingual-cased/ and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 294,912 || all params: 135,621,122 || trainable%: 0.2175
ROUND:0
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:00<00:08,  1.56it/s]                                              {'loss': 0.7191, 'grad_norm': 0.11958245187997818, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:08,  1.56it/s]                                              {'loss': 0.7148, 'grad_norm': 0.07216303050518036, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:08,  1.56it/s] 20%|██        | 3/15 [00:00<00:02,  4.79it/s]                                              {'loss': 0.7452, 'grad_norm': 0.18087656795978546, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:02,  4.79it/s]                                              {'loss': 0.7308, 'grad_norm': 0.13203905522823334, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:02,  4.79it/s] 33%|███▎      | 5/15 [00:00<00:01,  7.26it/s]                                              {'loss': 0.7071, 'grad_norm': 0.0865335464477539, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:01,  7.26it/s]                                              {'loss': 0.7003, 'grad_norm': 0.10097866505384445, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:01,  7.26it/s] 47%|████▋     | 7/15 [00:01<00:00,  9.45it/s]                                              {'loss': 0.7043, 'grad_norm': 0.08856232464313507, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:01<00:00,  9.45it/s]                                              {'loss': 0.7296, 'grad_norm': 0.14227613806724548, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:01<00:00,  9.45it/s] 60%|██████    | 9/15 [00:01<00:00, 11.57it/s]                                              {'loss': 0.7175, 'grad_norm': 0.09786652773618698, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:01<00:00, 11.57it/s]                                              {'loss': 0.7125, 'grad_norm': 0.099840447306633, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:01<00:00, 11.57it/s] 73%|███████▎  | 11/15 [00:01<00:00, 12.28it/s]                                               {'loss': 0.7124, 'grad_norm': 0.11294767260551453, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:01<00:00, 12.28it/s]                                               {'loss': 0.7278, 'grad_norm': 0.13550159335136414, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:01<00:00, 12.28it/s] 87%|████████▋ | 13/15 [00:01<00:00, 13.62it/s]                                               {'loss': 0.7246, 'grad_norm': 0.12203196436166763, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:01<00:00, 13.62it/s]                                               {'loss': 0.7244, 'grad_norm': 0.10392099618911743, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:01<00:00, 13.62it/s]100%|██████████| 15/15 [00:01<00:00, 14.73it/s]                                               {'loss': 0.7221, 'grad_norm': 0.11947678029537201, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.73it/s]                                               {'train_runtime': 1.5768, 'train_samples_per_second': 269.541, 'train_steps_per_second': 9.513, 'train_loss': 0.7195064743359884, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.73it/s]100%|██████████| 15/15 [00:01<00:00,  9.52it/s]
CLIENT:64
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7118, 'grad_norm': 0.0946613997220993, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.74it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.7459, 'grad_norm': 0.1785169243812561, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.7193, 'grad_norm': 0.09966570883989334, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.76it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.10it/s]                                              {'loss': 0.697, 'grad_norm': 0.0567469447851181, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.10it/s]                                              {'loss': 0.7439, 'grad_norm': 0.1703050136566162, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.10it/s] 40%|████      | 6/15 [00:00<00:00, 16.87it/s]                                              {'loss': 0.7268, 'grad_norm': 0.1293361634016037, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.87it/s]                                              {'loss': 0.7164, 'grad_norm': 0.07866671681404114, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.87it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.7327, 'grad_norm': 0.13340778648853302, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.7453, 'grad_norm': 0.1802590787410736, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.24it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.78it/s]                                               {'loss': 0.7215, 'grad_norm': 0.0896233543753624, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.78it/s]                                               {'loss': 0.7257, 'grad_norm': 0.17063136398792267, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.78it/s] 80%|████████  | 12/15 [00:00<00:00, 16.38it/s]                                               {'loss': 0.7115, 'grad_norm': 0.10992161184549332, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.38it/s]                                               {'loss': 0.7283, 'grad_norm': 0.09962784498929977, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.38it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.7293, 'grad_norm': 0.1261046975851059, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.7444, 'grad_norm': 0.16526572406291962, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.15it/s]                                               {'train_runtime': 1.0156, 'train_samples_per_second': 418.484, 'train_steps_per_second': 14.77, 'train_loss': 0.7266492565472921, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.15it/s]100%|██████████| 15/15 [00:01<00:00, 14.78it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7341, 'grad_norm': 0.14404992759227753, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.80it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.69it/s]                                              {'loss': 0.7426, 'grad_norm': 0.1543341726064682, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.69it/s]                                              {'loss': 0.7106, 'grad_norm': 0.11128024011850357, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.69it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.02it/s]                                              {'loss': 0.7212, 'grad_norm': 0.09083036333322525, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.02it/s]                                              {'loss': 0.7543, 'grad_norm': 0.18114161491394043, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.02it/s] 40%|████      | 6/15 [00:00<00:00, 17.23it/s]                                              {'loss': 0.7364, 'grad_norm': 0.1399066299200058, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 17.23it/s]                                              {'loss': 0.7195, 'grad_norm': 0.1244511604309082, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 17.23it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.74it/s]                                              {'loss': 0.7204, 'grad_norm': 0.12060774117708206, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.74it/s]                                              {'loss': 0.7494, 'grad_norm': 0.18647626042366028, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.74it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.16it/s]                                               {'loss': 0.7394, 'grad_norm': 0.14694131910800934, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.16it/s]                                               {'loss': 0.7121, 'grad_norm': 0.08772963285446167, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.16it/s] 80%|████████  | 12/15 [00:00<00:00, 16.93it/s]                                               {'loss': 0.7504, 'grad_norm': 0.19395990669727325, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.93it/s]                                               {'loss': 0.7372, 'grad_norm': 0.16157710552215576, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.93it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.83it/s]                                               {'loss': 0.7238, 'grad_norm': 0.10801224410533905, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.83it/s]                                               {'loss': 0.7464, 'grad_norm': 0.12663112580776215, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.83it/s]                                               {'train_runtime': 0.9843, 'train_samples_per_second': 431.763, 'train_steps_per_second': 15.239, 'train_loss': 0.7331624706586202, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.83it/s]100%|██████████| 15/15 [00:00<00:00, 15.24it/s]
CLIENT:80
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7158, 'grad_norm': 0.06345166265964508, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.43it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.11it/s]                                              {'loss': 0.7092, 'grad_norm': 0.08479905128479004, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.11it/s]                                              {'loss': 0.7462, 'grad_norm': 0.17152105271816254, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.11it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.7163, 'grad_norm': 0.08600851893424988, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.7249, 'grad_norm': 0.12933969497680664, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.04it/s] 40%|████      | 6/15 [00:00<00:00, 16.11it/s]                                              {'loss': 0.7041, 'grad_norm': 0.06694648414850235, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.11it/s]                                              {'loss': 0.7093, 'grad_norm': 0.08600499480962753, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.11it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.7012, 'grad_norm': 0.06855862587690353, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.7442, 'grad_norm': 0.14019691944122314, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.72it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.7218, 'grad_norm': 0.1110503152012825, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.7002, 'grad_norm': 0.06875668466091156, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.39it/s] 80%|████████  | 12/15 [00:00<00:00, 16.57it/s]                                               {'loss': 0.7541, 'grad_norm': 0.12981557846069336, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.57it/s]                                               {'loss': 0.7305, 'grad_norm': 0.14027614891529083, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.57it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6933, 'grad_norm': 0.047728873789310455, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.7343, 'grad_norm': 0.14036023616790771, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.32it/s]                                               {'train_runtime': 1.0291, 'train_samples_per_second': 412.99, 'train_steps_per_second': 14.576, 'train_loss': 0.7203700145085653, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.32it/s]100%|██████████| 15/15 [00:01<00:00, 14.58it/s]
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7167, 'grad_norm': 0.10332008451223373, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.06it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.7353, 'grad_norm': 0.13156865537166595, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.7406, 'grad_norm': 0.15156932175159454, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.16it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.93it/s]                                              {'loss': 0.7247, 'grad_norm': 0.13452623784542084, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.93it/s]                                              {'loss': 0.7248, 'grad_norm': 0.0985746905207634, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.93it/s] 40%|████      | 6/15 [00:00<00:00, 15.94it/s]                                              {'loss': 0.7343, 'grad_norm': 0.15165294706821442, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.94it/s]                                              {'loss': 0.7295, 'grad_norm': 0.12374522536993027, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.94it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.07it/s]                                              {'loss': 0.7282, 'grad_norm': 0.11678509414196014, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.07it/s]                                              {'loss': 0.7255, 'grad_norm': 0.13898475468158722, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.07it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.732, 'grad_norm': 0.1299656331539154, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.7141, 'grad_norm': 0.10696740448474884, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.24it/s] 80%|████████  | 12/15 [00:00<00:00, 16.10it/s]                                               {'loss': 0.7456, 'grad_norm': 0.15017753839492798, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.10it/s]                                               {'loss': 0.7311, 'grad_norm': 0.14464637637138367, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.10it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.7076, 'grad_norm': 0.09921196103096008, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.7339, 'grad_norm': 0.14342725276947021, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.99it/s]                                               {'train_runtime': 1.0463, 'train_samples_per_second': 406.176, 'train_steps_per_second': 14.336, 'train_loss': 0.7282661676406861, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.99it/s]100%|██████████| 15/15 [00:01<00:00, 14.34it/s]
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7363, 'grad_norm': 0.13605909049510956, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.83it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.92it/s]                                              {'loss': 0.7352, 'grad_norm': 0.12865322828292847, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.92it/s]                                              {'loss': 0.7335, 'grad_norm': 0.11338043212890625, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.92it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.06it/s]                                              {'loss': 0.7213, 'grad_norm': 0.11569284647703171, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.06it/s]                                              {'loss': 0.7373, 'grad_norm': 0.14028963446617126, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.06it/s] 40%|████      | 6/15 [00:00<00:00, 16.99it/s]                                              {'loss': 0.7337, 'grad_norm': 0.124580979347229, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.99it/s]                                              {'loss': 0.7186, 'grad_norm': 0.08700016885995865, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.99it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.7282, 'grad_norm': 0.15891626477241516, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.7332, 'grad_norm': 0.14332829415798187, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.56it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.88it/s]                                               {'loss': 0.7199, 'grad_norm': 0.10787546634674072, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.88it/s]                                               {'loss': 0.7347, 'grad_norm': 0.12947647273540497, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.88it/s] 80%|████████  | 12/15 [00:00<00:00, 16.66it/s]                                               {'loss': 0.742, 'grad_norm': 0.16990216076374054, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.66it/s]                                               {'loss': 0.7316, 'grad_norm': 0.1501123458147049, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.66it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.65it/s]                                               {'loss': 0.7187, 'grad_norm': 0.11199651658535004, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.65it/s]                                               {'loss': 0.7401, 'grad_norm': 0.12883377075195312, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.65it/s]                                               {'train_runtime': 0.9914, 'train_samples_per_second': 428.669, 'train_steps_per_second': 15.129, 'train_loss': 0.7309550484021504, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.65it/s]100%|██████████| 15/15 [00:00<00:00, 15.13it/s]
CLIENT:43
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7412, 'grad_norm': 0.13124507665634155, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.80it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.7059, 'grad_norm': 0.05745895579457283, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.7452, 'grad_norm': 0.16622228920459747, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.41it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.95it/s]                                              {'loss': 0.7247, 'grad_norm': 0.11031117290258408, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.95it/s]                                              {'loss': 0.7296, 'grad_norm': 0.11163890361785889, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.95it/s] 40%|████      | 6/15 [00:00<00:00, 16.91it/s]                                              {'loss': 0.7194, 'grad_norm': 0.11496668308973312, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.91it/s]                                              {'loss': 0.7178, 'grad_norm': 0.11969682574272156, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.91it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.80it/s]                                              {'loss': 0.739, 'grad_norm': 0.12909899652004242, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.80it/s]                                              {'loss': 0.6988, 'grad_norm': 0.07705914229154587, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.80it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.22it/s]                                               {'loss': 0.7559, 'grad_norm': 0.2071959525346756, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.22it/s]                                               {'loss': 0.7195, 'grad_norm': 0.07922465354204178, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.22it/s] 80%|████████  | 12/15 [00:00<00:00, 16.66it/s]                                               {'loss': 0.6929, 'grad_norm': 0.05808309465646744, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.66it/s]                                               {'loss': 0.7484, 'grad_norm': 0.1700689196586609, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.66it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.7087, 'grad_norm': 0.06731483340263367, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.7283, 'grad_norm': 0.10091682523488998, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.39it/s]                                               {'train_runtime': 0.9992, 'train_samples_per_second': 425.347, 'train_steps_per_second': 15.012, 'train_loss': 0.7250160336494446, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.39it/s]100%|██████████| 15/15 [00:00<00:00, 15.02it/s]
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.724, 'grad_norm': 0.13537174463272095, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.89it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.41it/s]                                              {'loss': 0.7476, 'grad_norm': 0.17219460010528564, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.41it/s]                                              {'loss': 0.7073, 'grad_norm': 0.1024608165025711, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.41it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.7347, 'grad_norm': 0.16998440027236938, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.7369, 'grad_norm': 0.1339777708053589, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.45it/s] 40%|████      | 6/15 [00:00<00:00, 16.81it/s]                                              {'loss': 0.7235, 'grad_norm': 0.10357128083705902, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.81it/s]                                              {'loss': 0.7371, 'grad_norm': 0.14344823360443115, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.81it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.97it/s]                                              {'loss': 0.7461, 'grad_norm': 0.164344921708107, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.97it/s]                                              {'loss': 0.717, 'grad_norm': 0.10690239816904068, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.97it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.7271, 'grad_norm': 0.12645898759365082, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.7255, 'grad_norm': 0.11230257898569107, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.57it/s] 80%|████████  | 12/15 [00:00<00:00, 16.31it/s]                                               {'loss': 0.757, 'grad_norm': 0.21575333178043365, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.31it/s]                                               {'loss': 0.7463, 'grad_norm': 0.1812201887369156, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.31it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.743, 'grad_norm': 0.13401919603347778, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.725, 'grad_norm': 0.10715566575527191, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.46it/s]                                               {'train_runtime': 1.0146, 'train_samples_per_second': 418.897, 'train_steps_per_second': 14.785, 'train_loss': 0.7332125226656596, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.46it/s]100%|██████████| 15/15 [00:01<00:00, 14.79it/s]
CLIENT:7
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7258, 'grad_norm': 0.1117490753531456, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.32it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.08it/s]                                              {'loss': 0.7212, 'grad_norm': 0.07693099230527878, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.08it/s]                                              {'loss': 0.7427, 'grad_norm': 0.14841870963573456, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.08it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.99it/s]                                              {'loss': 0.7185, 'grad_norm': 0.12372800707817078, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.99it/s]                                              {'loss': 0.7393, 'grad_norm': 0.1201358512043953, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.99it/s] 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.6964, 'grad_norm': 0.0766185075044632, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.73, 'grad_norm': 0.12968146800994873, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.03it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.96it/s]                                              {'loss': 0.7208, 'grad_norm': 0.0954829752445221, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.96it/s]                                              {'loss': 0.7258, 'grad_norm': 0.11040719598531723, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.96it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.7184, 'grad_norm': 0.10715842247009277, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.7216, 'grad_norm': 0.10726518929004669, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.50it/s] 80%|████████  | 12/15 [00:00<00:00, 16.06it/s]                                               {'loss': 0.7309, 'grad_norm': 0.11655930429697037, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.06it/s]                                               {'loss': 0.7165, 'grad_norm': 0.08143005520105362, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.06it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.01it/s]                                               {'loss': 0.7307, 'grad_norm': 0.1391429752111435, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.01it/s]                                               {'loss': 0.7155, 'grad_norm': 0.11850478500127792, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.01it/s]                                               {'train_runtime': 1.0432, 'train_samples_per_second': 407.395, 'train_steps_per_second': 14.379, 'train_loss': 0.7236016829808553, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.01it/s]100%|██████████| 15/15 [00:01<00:00, 14.38it/s]
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7272, 'grad_norm': 0.11839815229177475, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.64it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.04it/s]                                              {'loss': 0.702, 'grad_norm': 0.07507026195526123, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.04it/s]                                              {'loss': 0.7375, 'grad_norm': 0.12003201246261597, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.04it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.05it/s]                                              {'loss': 0.7076, 'grad_norm': 0.0537523478269577, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.05it/s]                                              {'loss': 0.7334, 'grad_norm': 0.15347087383270264, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.05it/s] 40%|████      | 6/15 [00:00<00:00, 16.75it/s]                                              {'loss': 0.7087, 'grad_norm': 0.1109938770532608, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.75it/s]                                              {'loss': 0.7205, 'grad_norm': 0.07876058667898178, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.75it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.7224, 'grad_norm': 0.12650653719902039, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.7196, 'grad_norm': 0.10418958961963654, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.34it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.72it/s]                                               {'loss': 0.7182, 'grad_norm': 0.06888007372617722, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.72it/s]                                               {'loss': 0.7296, 'grad_norm': 0.12291159480810165, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.72it/s] 80%|████████  | 12/15 [00:00<00:00, 16.31it/s]                                               {'loss': 0.7364, 'grad_norm': 0.13812457025051117, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.31it/s]                                               {'loss': 0.7468, 'grad_norm': 0.17136827111244202, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.31it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.7107, 'grad_norm': 0.07415115088224411, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.6963, 'grad_norm': 0.059769630432128906, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.14it/s]                                               {'train_runtime': 1.0146, 'train_samples_per_second': 418.883, 'train_steps_per_second': 14.784, 'train_loss': 0.7211312095324198, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.14it/s]100%|██████████| 15/15 [00:01<00:00, 14.79it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 34.76it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.89it/s] 33%|███▎      | 11/33 [00:00<00:00, 28.48it/s] 42%|████▏     | 14/33 [00:00<00:00, 27.52it/s] 52%|█████▏    | 17/33 [00:00<00:00, 26.84it/s] 61%|██████    | 20/33 [00:00<00:00, 26.42it/s] 70%|██████▉   | 23/33 [00:00<00:00, 26.74it/s] 79%|███████▉  | 26/33 [00:00<00:00, 26.45it/s] 88%|████████▊ | 29/33 [00:01<00:00, 26.19it/s] 97%|█████████▋| 32/33 [00:01<00:00, 26.22it/s]100%|██████████| 33/33 [00:01<00:00, 27.35it/s]
{'eval_loss': 0.7229359745979309, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3096836049856184, 'eval_runtime': 1.247, 'eval_samples_per_second': 836.422, 'eval_steps_per_second': 26.464}
ROUND:1
CLIENT:26
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7422, 'grad_norm': 0.1551530510187149, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.76it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.67it/s]                                              {'loss': 0.7171, 'grad_norm': 0.12524409592151642, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.67it/s]                                              {'loss': 0.704, 'grad_norm': 0.060904476791620255, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.67it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.7278, 'grad_norm': 0.10413400083780289, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.7201, 'grad_norm': 0.09814641624689102, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.53it/s] 40%|████      | 6/15 [00:00<00:00, 16.43it/s]                                              {'loss': 0.7221, 'grad_norm': 0.16199691593647003, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.43it/s]                                              {'loss': 0.726, 'grad_norm': 0.14723409712314606, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.43it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.7114, 'grad_norm': 0.0755295604467392, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.7204, 'grad_norm': 0.13515602052211761, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.41it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.76it/s]                                               {'loss': 0.7128, 'grad_norm': 0.09884435683488846, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.76it/s]                                               {'loss': 0.7205, 'grad_norm': 0.11344007402658463, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.76it/s] 80%|████████  | 12/15 [00:00<00:00, 16.42it/s]                                               {'loss': 0.7481, 'grad_norm': 0.16196882724761963, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.42it/s]                                               {'loss': 0.7207, 'grad_norm': 0.11632479727268219, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.42it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.734, 'grad_norm': 0.12893228232860565, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.7297, 'grad_norm': 0.10879722982645035, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.30it/s]                                               {'train_runtime': 1.0099, 'train_samples_per_second': 420.846, 'train_steps_per_second': 14.853, 'train_loss': 0.723788070678711, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.30it/s]100%|██████████| 15/15 [00:01<00:00, 14.86it/s]
CLIENT:80
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7154, 'grad_norm': 0.06320486217737198, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.44it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.83it/s]                                              {'loss': 0.7086, 'grad_norm': 0.08473270386457443, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.83it/s]                                              {'loss': 0.7448, 'grad_norm': 0.17211133241653442, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.83it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.96it/s]                                              {'loss': 0.7156, 'grad_norm': 0.0860765129327774, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.96it/s]                                              {'loss': 0.7238, 'grad_norm': 0.12966614961624146, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.96it/s] 40%|████      | 6/15 [00:00<00:00, 17.35it/s]                                              {'loss': 0.7038, 'grad_norm': 0.0671539455652237, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 17.35it/s]                                              {'loss': 0.7087, 'grad_norm': 0.08645636588335037, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 17.35it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.66it/s]                                              {'loss': 0.7007, 'grad_norm': 0.06875337660312653, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.66it/s]                                              {'loss': 0.7431, 'grad_norm': 0.14119942486286163, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.66it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.84it/s]                                               {'loss': 0.721, 'grad_norm': 0.11161389946937561, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.84it/s]                                               {'loss': 0.6997, 'grad_norm': 0.06930358707904816, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.84it/s] 80%|████████  | 12/15 [00:00<00:00, 16.46it/s]                                               {'loss': 0.7531, 'grad_norm': 0.13036474585533142, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.46it/s]                                               {'loss': 0.7294, 'grad_norm': 0.14124175906181335, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.46it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6933, 'grad_norm': 0.047529131174087524, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.7332, 'grad_norm': 0.1419530212879181, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.29it/s]                                               {'train_runtime': 1.0063, 'train_samples_per_second': 422.353, 'train_steps_per_second': 14.907, 'train_loss': 0.7196152130762736, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.29it/s]100%|██████████| 15/15 [00:01<00:00, 14.91it/s]
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7112, 'grad_norm': 0.09439343214035034, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.56it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.21it/s]                                              {'loss': 0.7172, 'grad_norm': 0.08957410603761673, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.21it/s]                                              {'loss': 0.7575, 'grad_norm': 0.20548802614212036, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.21it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.76it/s]                                              {'loss': 0.7457, 'grad_norm': 0.15517820417881012, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.76it/s]                                              {'loss': 0.7176, 'grad_norm': 0.12257169932126999, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.76it/s] 40%|████      | 6/15 [00:00<00:00, 16.74it/s]                                              {'loss': 0.7097, 'grad_norm': 0.09273751080036163, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.74it/s]                                              {'loss': 0.6998, 'grad_norm': 0.09629571437835693, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.74it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.7331, 'grad_norm': 0.1334880292415619, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.7265, 'grad_norm': 0.15036678314208984, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.19it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.79it/s]                                               {'loss': 0.7306, 'grad_norm': 0.1126503199338913, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.79it/s]                                               {'loss': 0.6981, 'grad_norm': 0.11334361881017685, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.79it/s] 80%|████████  | 12/15 [00:00<00:00, 16.46it/s]                                               {'loss': 0.739, 'grad_norm': 0.13855445384979248, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.46it/s]                                               {'loss': 0.7218, 'grad_norm': 0.10345764458179474, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.46it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.27it/s]                                               {'loss': 0.7262, 'grad_norm': 0.14666278660297394, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.27it/s]                                               {'loss': 0.7176, 'grad_norm': 0.11444676667451859, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.27it/s]                                               {'train_runtime': 1.0095, 'train_samples_per_second': 420.993, 'train_steps_per_second': 14.859, 'train_loss': 0.7234459718068441, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.27it/s]100%|██████████| 15/15 [00:01<00:00, 14.86it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7445, 'grad_norm': 0.1454271525144577, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.59it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.07it/s]                                              {'loss': 0.7244, 'grad_norm': 0.0747263953089714, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.07it/s]                                              {'loss': 0.6782, 'grad_norm': 0.06062890961766243, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.07it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.46it/s]                                              {'loss': 0.7124, 'grad_norm': 0.0823642835021019, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.46it/s]                                              {'loss': 0.7191, 'grad_norm': 0.06896895170211792, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.46it/s] 40%|████      | 6/15 [00:00<00:00, 17.14it/s]                                              {'loss': 0.7159, 'grad_norm': 0.09600704163312912, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 17.14it/s]                                              {'loss': 0.7166, 'grad_norm': 0.09273940324783325, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 17.14it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.7126, 'grad_norm': 0.0816672146320343, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.7189, 'grad_norm': 0.08680572360754013, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.37it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.91it/s]                                               {'loss': 0.7053, 'grad_norm': 0.05509943515062332, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.91it/s]                                               {'loss': 0.7087, 'grad_norm': 0.0800744965672493, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.91it/s] 80%|████████  | 12/15 [00:00<00:00, 16.56it/s]                                               {'loss': 0.723, 'grad_norm': 0.1403939127922058, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.56it/s]                                               {'loss': 0.7238, 'grad_norm': 0.10170792043209076, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.56it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.7123, 'grad_norm': 0.07810365408658981, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.7128, 'grad_norm': 0.07810040563344955, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.33it/s]                                               {'train_runtime': 1.0021, 'train_samples_per_second': 424.119, 'train_steps_per_second': 14.969, 'train_loss': 0.71523464123408, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.33it/s]100%|██████████| 15/15 [00:01<00:00, 14.97it/s]
CLIENT:77
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.724, 'grad_norm': 0.10941676050424576, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.99it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.40it/s]                                              {'loss': 0.7203, 'grad_norm': 0.08055506646633148, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.40it/s]                                              {'loss': 0.7168, 'grad_norm': 0.10016781091690063, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.40it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.72it/s]                                              {'loss': 0.7084, 'grad_norm': 0.06141923740506172, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.72it/s]                                              {'loss': 0.7249, 'grad_norm': 0.09533210098743439, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.72it/s] 40%|████      | 6/15 [00:00<00:00, 17.15it/s]                                              {'loss': 0.7283, 'grad_norm': 0.14437277615070343, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 17.15it/s]                                              {'loss': 0.7284, 'grad_norm': 0.11010068655014038, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 17.15it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.7106, 'grad_norm': 0.07760640233755112, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.7251, 'grad_norm': 0.1049901619553566, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.42it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.99it/s]                                               {'loss': 0.7238, 'grad_norm': 0.08612200617790222, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.99it/s]                                               {'loss': 0.7269, 'grad_norm': 0.12913581728935242, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.99it/s] 80%|████████  | 12/15 [00:00<00:00, 16.45it/s]                                               {'loss': 0.7196, 'grad_norm': 0.07901032269001007, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.45it/s]                                               {'loss': 0.7318, 'grad_norm': 0.12496693432331085, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.45it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.7157, 'grad_norm': 0.057542238384485245, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.7543, 'grad_norm': 0.1361638307571411, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.21it/s]                                               {'train_runtime': 1.0123, 'train_samples_per_second': 419.846, 'train_steps_per_second': 14.818, 'train_loss': 0.7239191214243571, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.21it/s]100%|██████████| 15/15 [00:01<00:00, 14.82it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7135, 'grad_norm': 0.09600545465946198, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.51it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.42it/s]                                              {'loss': 0.7247, 'grad_norm': 0.15718398988246918, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.42it/s]                                              {'loss': 0.7442, 'grad_norm': 0.18820257484912872, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.42it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.7186, 'grad_norm': 0.10145504772663116, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.7447, 'grad_norm': 0.18134137988090515, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.30it/s] 40%|████      | 6/15 [00:00<00:00, 16.16it/s]                                              {'loss': 0.7331, 'grad_norm': 0.13561925292015076, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.16it/s]                                              {'loss': 0.7519, 'grad_norm': 0.1694697141647339, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.16it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.97it/s]                                              {'loss': 0.7144, 'grad_norm': 0.11654882878065109, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.97it/s]                                              {'loss': 0.7169, 'grad_norm': 0.1337479203939438, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.97it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.7394, 'grad_norm': 0.14407601952552795, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.7198, 'grad_norm': 0.13503554463386536, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.29it/s] 80%|████████  | 12/15 [00:00<00:00, 16.13it/s]                                               {'loss': 0.7217, 'grad_norm': 0.1425980180501938, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.13it/s]                                               {'loss': 0.7142, 'grad_norm': 0.10746601223945618, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.13it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.7321, 'grad_norm': 0.16807854175567627, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.7465, 'grad_norm': 0.19135929644107819, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.25it/s]                                               {'train_runtime': 1.028, 'train_samples_per_second': 413.423, 'train_steps_per_second': 14.591, 'train_loss': 0.7290346185366313, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.25it/s]100%|██████████| 15/15 [00:01<00:00, 14.60it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7328, 'grad_norm': 0.1722692996263504, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.33it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.81it/s]                                              {'loss': 0.7493, 'grad_norm': 0.1835029572248459, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.81it/s]                                              {'loss': 0.7191, 'grad_norm': 0.12482983618974686, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.81it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.7335, 'grad_norm': 0.1482001692056656, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.7404, 'grad_norm': 0.18584232032299042, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.15it/s] 40%|████      | 6/15 [00:00<00:00, 16.42it/s]                                              {'loss': 0.7339, 'grad_norm': 0.1660420149564743, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.42it/s]                                              {'loss': 0.7594, 'grad_norm': 0.20359128713607788, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.42it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.81it/s]                                              {'loss': 0.7394, 'grad_norm': 0.18790721893310547, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.81it/s]                                              {'loss': 0.7261, 'grad_norm': 0.10186784714460373, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.81it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.08it/s]                                               {'loss': 0.7356, 'grad_norm': 0.14830422401428223, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.08it/s]                                               {'loss': 0.7273, 'grad_norm': 0.1858450025320053, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.08it/s] 80%|████████  | 12/15 [00:00<00:00, 16.60it/s]                                               {'loss': 0.7509, 'grad_norm': 0.17729409039020538, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.60it/s]                                               {'loss': 0.7327, 'grad_norm': 0.1634436696767807, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.60it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.7373, 'grad_norm': 0.1684938371181488, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.7485, 'grad_norm': 0.19306187331676483, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.66it/s]                                               {'train_runtime': 0.9965, 'train_samples_per_second': 426.472, 'train_steps_per_second': 15.052, 'train_loss': 0.7377402305603027, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.66it/s]100%|██████████| 15/15 [00:00<00:00, 15.06it/s]
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7287, 'grad_norm': 0.15804940462112427, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.41it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.82it/s]                                              {'loss': 0.7032, 'grad_norm': 0.06636524945497513, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.82it/s]                                              {'loss': 0.7136, 'grad_norm': 0.09032583236694336, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.82it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.7225, 'grad_norm': 0.0766177624464035, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.7347, 'grad_norm': 0.13314470648765564, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.44it/s] 40%|████      | 6/15 [00:00<00:00, 16.44it/s]                                              {'loss': 0.7006, 'grad_norm': 0.09163693338632584, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.44it/s]                                              {'loss': 0.721, 'grad_norm': 0.12236535549163818, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.44it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.7124, 'grad_norm': 0.08886472880840302, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.7135, 'grad_norm': 0.1055385172367096, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.46it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.22it/s]                                               {'loss': 0.7529, 'grad_norm': 0.19833038747310638, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.22it/s]                                               {'loss': 0.6952, 'grad_norm': 0.04513945057988167, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.22it/s] 80%|████████  | 12/15 [00:00<00:00, 17.08it/s]                                               {'loss': 0.7193, 'grad_norm': 0.09663435816764832, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 17.08it/s]                                               {'loss': 0.7281, 'grad_norm': 0.13376563787460327, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 17.08it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.45it/s]                                               {'loss': 0.7105, 'grad_norm': 0.09761832654476166, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.45it/s]                                               {'loss': 0.7019, 'grad_norm': 0.09400255978107452, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.45it/s]                                               {'train_runtime': 1.0014, 'train_samples_per_second': 424.409, 'train_steps_per_second': 14.979, 'train_loss': 0.7171958128611247, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.45it/s]100%|██████████| 15/15 [00:01<00:00, 14.98it/s]
CLIENT:20
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7318, 'grad_norm': 0.13777779042720795, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.58it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.7054, 'grad_norm': 0.06605225801467896, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.7344, 'grad_norm': 0.12093625217676163, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.99it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.705, 'grad_norm': 0.05411216989159584, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.7243, 'grad_norm': 0.15020357072353363, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.53it/s] 40%|████      | 6/15 [00:00<00:00, 16.97it/s]                                              {'loss': 0.7049, 'grad_norm': 0.08780059218406677, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.97it/s]                                              {'loss': 0.716, 'grad_norm': 0.11879132688045502, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.97it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.7127, 'grad_norm': 0.06886876374483109, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.7252, 'grad_norm': 0.13571658730506897, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.38it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.88it/s]                                               {'loss': 0.733, 'grad_norm': 0.10721083730459213, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.88it/s]                                               {'loss': 0.7112, 'grad_norm': 0.09200701862573624, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.88it/s] 80%|████████  | 12/15 [00:00<00:00, 16.44it/s]                                               {'loss': 0.7243, 'grad_norm': 0.1130727231502533, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.44it/s]                                               {'loss': 0.7482, 'grad_norm': 0.16722671687602997, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.44it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.7087, 'grad_norm': 0.08857449144124985, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.6812, 'grad_norm': 0.056738510727882385, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.33it/s]                                               {'train_runtime': 1.0062, 'train_samples_per_second': 422.392, 'train_steps_per_second': 14.908, 'train_loss': 0.7177448193232219, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.33it/s]100%|██████████| 15/15 [00:01<00:00, 14.91it/s]
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7229, 'grad_norm': 0.13526515662670135, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.55it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.67it/s]                                              {'loss': 0.7461, 'grad_norm': 0.1723257154226303, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.67it/s]                                              {'loss': 0.7067, 'grad_norm': 0.1022116020321846, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.67it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.87it/s]                                              {'loss': 0.7333, 'grad_norm': 0.17038051784038544, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.87it/s]                                              {'loss': 0.7358, 'grad_norm': 0.13456903398036957, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.87it/s] 40%|████      | 6/15 [00:00<00:00, 16.45it/s]                                              {'loss': 0.7227, 'grad_norm': 0.10410653054714203, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.45it/s]                                              {'loss': 0.736, 'grad_norm': 0.14463545382022858, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.45it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.7447, 'grad_norm': 0.16512396931648254, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.18it/s]                                              {'loss': 0.7162, 'grad_norm': 0.10765627026557922, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.18it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.02it/s]                                               {'loss': 0.7261, 'grad_norm': 0.127579927444458, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.02it/s]                                               {'loss': 0.7246, 'grad_norm': 0.113265261054039, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.02it/s] 80%|████████  | 12/15 [00:00<00:00, 16.45it/s]                                               {'loss': 0.7552, 'grad_norm': 0.21784690022468567, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.45it/s]                                               {'loss': 0.7448, 'grad_norm': 0.18293194472789764, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.45it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.742, 'grad_norm': 0.13516074419021606, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.7242, 'grad_norm': 0.1075051873922348, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.14it/s]                                               {'train_runtime': 1.0156, 'train_samples_per_second': 418.478, 'train_steps_per_second': 14.77, 'train_loss': 0.7320822874704996, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.14it/s]100%|██████████| 15/15 [00:01<00:00, 14.78it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 34.58it/s] 24%|██▍       | 8/33 [00:00<00:00, 29.38it/s] 33%|███▎      | 11/33 [00:00<00:00, 27.70it/s] 42%|████▏     | 14/33 [00:00<00:00, 26.76it/s] 52%|█████▏    | 17/33 [00:00<00:00, 26.32it/s] 61%|██████    | 20/33 [00:00<00:00, 26.06it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.81it/s] 79%|███████▉  | 26/33 [00:00<00:00, 26.20it/s] 88%|████████▊ | 29/33 [00:01<00:00, 26.11it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.99it/s]100%|██████████| 33/33 [00:01<00:00, 26.94it/s]
{'eval_loss': 0.7220536470413208, 'eval_model_preparation_time': 0.0026, 'eval_acc': 0.3096836049856184, 'eval_runtime': 1.2687, 'eval_samples_per_second': 822.083, 'eval_steps_per_second': 26.01}
ROUND:2
CLIENT:75
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.702, 'grad_norm': 0.06839857995510101, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.46it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.7393, 'grad_norm': 0.15661613643169403, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.7252, 'grad_norm': 0.11707665771245956, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.09it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.7175, 'grad_norm': 0.0820145234465599, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.7463, 'grad_norm': 0.15992578864097595, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.88it/s] 40%|████      | 6/15 [00:00<00:00, 16.82it/s]                                              {'loss': 0.7123, 'grad_norm': 0.1010463535785675, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.82it/s]                                              {'loss': 0.734, 'grad_norm': 0.152462899684906, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.82it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.80it/s]                                              {'loss': 0.6999, 'grad_norm': 0.0973498746752739, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.80it/s]                                              {'loss': 0.712, 'grad_norm': 0.08727218210697174, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.80it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.21it/s]                                               {'loss': 0.7124, 'grad_norm': 0.09429279714822769, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.21it/s]                                               {'loss': 0.7097, 'grad_norm': 0.10220137983560562, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.21it/s] 80%|████████  | 12/15 [00:00<00:00, 16.63it/s]                                               {'loss': 0.7233, 'grad_norm': 0.16925601661205292, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.63it/s]                                               {'loss': 0.7202, 'grad_norm': 0.08916641771793365, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.63it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.7282, 'grad_norm': 0.13694775104522705, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.7234, 'grad_norm': 0.11062025278806686, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.40it/s]                                               {'train_runtime': 0.9979, 'train_samples_per_second': 425.875, 'train_steps_per_second': 15.031, 'train_loss': 0.720387856165568, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.40it/s]100%|██████████| 15/15 [00:00<00:00, 15.04it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7021, 'grad_norm': 0.11001003533601761, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.87it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.41it/s]                                              {'loss': 0.7428, 'grad_norm': 0.17062875628471375, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.41it/s]                                              {'loss': 0.6896, 'grad_norm': 0.053520236164331436, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.41it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.6998, 'grad_norm': 0.09685806185007095, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.7246, 'grad_norm': 0.12320668995380402, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.24it/s] 40%|████      | 6/15 [00:00<00:00, 16.12it/s]                                              {'loss': 0.7314, 'grad_norm': 0.14583469927310944, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.12it/s]                                              {'loss': 0.7246, 'grad_norm': 0.13787437975406647, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.12it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.75it/s]                                              {'loss': 0.7167, 'grad_norm': 0.12362831830978394, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.75it/s]                                              {'loss': 0.7073, 'grad_norm': 0.08668781816959381, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.75it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.7228, 'grad_norm': 0.09727304428815842, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.716, 'grad_norm': 0.13206909596920013, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.31it/s] 80%|████████  | 12/15 [00:00<00:00, 16.09it/s]                                               {'loss': 0.7246, 'grad_norm': 0.11958601325750351, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.09it/s]                                               {'loss': 0.7092, 'grad_norm': 0.1109054759144783, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.09it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.733, 'grad_norm': 0.1357654482126236, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.7047, 'grad_norm': 0.11566781252622604, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.16it/s]                                               {'train_runtime': 1.0315, 'train_samples_per_second': 412.013, 'train_steps_per_second': 14.542, 'train_loss': 0.716597851117452, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.16it/s]100%|██████████| 15/15 [00:01<00:00, 14.55it/s]
CLIENT:46
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7104, 'grad_norm': 0.09411047399044037, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.08it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.42it/s]                                              {'loss': 0.7128, 'grad_norm': 0.089882031083107, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.42it/s]                                              {'loss': 0.7577, 'grad_norm': 0.16848483681678772, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.42it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.06it/s]                                              {'loss': 0.7096, 'grad_norm': 0.058644771575927734, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.06it/s]                                              {'loss': 0.7337, 'grad_norm': 0.16586123406887054, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.06it/s] 40%|████      | 6/15 [00:00<00:00, 15.94it/s]                                              {'loss': 0.7226, 'grad_norm': 0.12140678614377975, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.94it/s]                                              {'loss': 0.7321, 'grad_norm': 0.1322641521692276, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.94it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.7181, 'grad_norm': 0.12227115035057068, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.7071, 'grad_norm': 0.06979107111692429, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.62it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.7448, 'grad_norm': 0.11513081192970276, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.7, 'grad_norm': 0.07880719751119614, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.21it/s] 80%|████████  | 12/15 [00:00<00:00, 16.05it/s]                                               {'loss': 0.7176, 'grad_norm': 0.15026500821113586, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.05it/s]                                               {'loss': 0.7186, 'grad_norm': 0.11465589702129364, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.05it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7386, 'grad_norm': 0.1569228321313858, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7029, 'grad_norm': 0.05933418124914169, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.12it/s]                                               {'train_runtime': 1.0376, 'train_samples_per_second': 409.594, 'train_steps_per_second': 14.456, 'train_loss': 0.7217793226242065, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.12it/s]100%|██████████| 15/15 [00:01<00:00, 14.46it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7434, 'grad_norm': 0.14674045145511627, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.58it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.05it/s]                                              {'loss': 0.7239, 'grad_norm': 0.07521916925907135, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.05it/s]                                              {'loss': 0.6784, 'grad_norm': 0.06123567745089531, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.05it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.01it/s]                                              {'loss': 0.7118, 'grad_norm': 0.08324213325977325, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.01it/s]                                              {'loss': 0.7186, 'grad_norm': 0.06921550631523132, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.01it/s] 40%|████      | 6/15 [00:00<00:00, 16.46it/s]                                              {'loss': 0.7153, 'grad_norm': 0.09685077518224716, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.46it/s]                                              {'loss': 0.7159, 'grad_norm': 0.09382341057062149, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.46it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.99it/s]                                              {'loss': 0.7121, 'grad_norm': 0.0817754939198494, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.99it/s]                                              {'loss': 0.7184, 'grad_norm': 0.08739817142486572, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.99it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.705, 'grad_norm': 0.055356305092573166, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.7081, 'grad_norm': 0.08068005740642548, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.50it/s] 80%|████████  | 12/15 [00:00<00:00, 16.19it/s]                                               {'loss': 0.7219, 'grad_norm': 0.14228659868240356, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.19it/s]                                               {'loss': 0.7231, 'grad_norm': 0.10339450091123581, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.19it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.7118, 'grad_norm': 0.07867051661014557, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.7123, 'grad_norm': 0.0786905288696289, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.20it/s]                                               {'train_runtime': 1.0197, 'train_samples_per_second': 416.782, 'train_steps_per_second': 14.71, 'train_loss': 0.7146682937939962, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.20it/s]100%|██████████| 15/15 [00:01<00:00, 14.72it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7314, 'grad_norm': 0.1737515926361084, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.61it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.52it/s]                                              {'loss': 0.7478, 'grad_norm': 0.1853482574224472, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.52it/s]                                              {'loss': 0.7181, 'grad_norm': 0.12647637724876404, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.52it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.70it/s]                                              {'loss': 0.7323, 'grad_norm': 0.15002773702144623, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.70it/s]                                              {'loss': 0.7389, 'grad_norm': 0.18813030421733856, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.70it/s] 40%|████      | 6/15 [00:00<00:00, 16.50it/s]                                              {'loss': 0.7325, 'grad_norm': 0.16863153874874115, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.50it/s]                                              {'loss': 0.7577, 'grad_norm': 0.20695607364177704, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.50it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.7378, 'grad_norm': 0.19123771786689758, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.7253, 'grad_norm': 0.10329437255859375, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.14it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.73it/s]                                               {'loss': 0.7344, 'grad_norm': 0.1513424813747406, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.73it/s]                                               {'loss': 0.7258, 'grad_norm': 0.18902423977851868, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.73it/s] 80%|████████  | 12/15 [00:00<00:00, 16.26it/s]                                               {'loss': 0.7494, 'grad_norm': 0.18096379935741425, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.26it/s]                                               {'loss': 0.7313, 'grad_norm': 0.16632682085037231, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.26it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.7359, 'grad_norm': 0.17220328748226166, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.7469, 'grad_norm': 0.19655734300613403, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.94it/s]                                               {'train_runtime': 1.0257, 'train_samples_per_second': 414.342, 'train_steps_per_second': 14.624, 'train_loss': 0.7363614678382874, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.94it/s]100%|██████████| 15/15 [00:01<00:00, 14.63it/s]
CLIENT:39
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7533, 'grad_norm': 0.16721248626708984, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.50it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.67it/s]                                              {'loss': 0.7068, 'grad_norm': 0.0676877573132515, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.67it/s]                                              {'loss': 0.7271, 'grad_norm': 0.11912853270769119, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.67it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.91it/s]                                              {'loss': 0.7225, 'grad_norm': 0.09161031246185303, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.91it/s]                                              {'loss': 0.7217, 'grad_norm': 0.12243233621120453, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.91it/s] 40%|████      | 6/15 [00:00<00:00, 16.47it/s]                                              {'loss': 0.7042, 'grad_norm': 0.13174860179424286, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.47it/s]                                              {'loss': 0.7104, 'grad_norm': 0.07590214163064957, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.47it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.06it/s]                                              {'loss': 0.7337, 'grad_norm': 0.13548894226551056, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.06it/s]                                              {'loss': 0.7259, 'grad_norm': 0.1479768007993698, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.06it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.7212, 'grad_norm': 0.11456546187400818, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.7229, 'grad_norm': 0.12731172144412994, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.46it/s] 80%|████████  | 12/15 [00:00<00:00, 16.23it/s]                                               {'loss': 0.7112, 'grad_norm': 0.12332885712385178, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.23it/s]                                               {'loss': 0.7441, 'grad_norm': 0.1636778861284256, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.23it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.7216, 'grad_norm': 0.10093270987272263, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.7134, 'grad_norm': 0.08259256929159164, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.22it/s]                                               {'train_runtime': 1.0221, 'train_samples_per_second': 415.825, 'train_steps_per_second': 14.676, 'train_loss': 0.7226605256398518, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.22it/s]100%|██████████| 15/15 [00:01<00:00, 14.68it/s]
CLIENT:23
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7469, 'grad_norm': 0.1623958796262741, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.05it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.7212, 'grad_norm': 0.10029271245002747, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.7363, 'grad_norm': 0.12347479164600372, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.35it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.54it/s]                                              {'loss': 0.7143, 'grad_norm': 0.09859045594930649, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.54it/s]                                              {'loss': 0.7434, 'grad_norm': 0.16424719989299774, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.54it/s] 40%|████      | 6/15 [00:00<00:00, 16.48it/s]                                              {'loss': 0.7105, 'grad_norm': 0.12317570298910141, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.48it/s]                                              {'loss': 0.7156, 'grad_norm': 0.09268227219581604, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.48it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.726, 'grad_norm': 0.12642399966716766, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.7394, 'grad_norm': 0.19376711547374725, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.11it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.73it/s]                                               {'loss': 0.7281, 'grad_norm': 0.092855304479599, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.73it/s]                                               {'loss': 0.7296, 'grad_norm': 0.15701895952224731, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.73it/s] 80%|████████  | 12/15 [00:00<00:00, 16.30it/s]                                               {'loss': 0.7085, 'grad_norm': 0.13356757164001465, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.30it/s]                                               {'loss': 0.7347, 'grad_norm': 0.18159261345863342, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.7119, 'grad_norm': 0.07540417462587357, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.7271, 'grad_norm': 0.1368962824344635, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.19it/s]                                               {'train_runtime': 1.0149, 'train_samples_per_second': 418.768, 'train_steps_per_second': 14.78, 'train_loss': 0.7262381792068482, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.19it/s]100%|██████████| 15/15 [00:01<00:00, 14.79it/s]
CLIENT:20
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7307, 'grad_norm': 0.13913114368915558, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.30it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.705, 'grad_norm': 0.06612984836101532, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.7335, 'grad_norm': 0.12211187928915024, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.23it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.08it/s]                                              {'loss': 0.7047, 'grad_norm': 0.05449282005429268, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.08it/s]                                              {'loss': 0.7231, 'grad_norm': 0.15075410902500153, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.08it/s] 40%|████      | 6/15 [00:00<00:00, 15.94it/s]                                              {'loss': 0.7043, 'grad_norm': 0.08855900168418884, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.94it/s]                                              {'loss': 0.715, 'grad_norm': 0.11973422020673752, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.94it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.7122, 'grad_norm': 0.0693410187959671, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.7241, 'grad_norm': 0.137049600481987, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.56it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.7322, 'grad_norm': 0.10893778502941132, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.7105, 'grad_norm': 0.09353191405534744, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.85it/s] 80%|████████  | 12/15 [00:00<00:00, 16.38it/s]                                               {'loss': 0.7234, 'grad_norm': 0.11520104110240936, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.38it/s]                                               {'loss': 0.7468, 'grad_norm': 0.16922242939472198, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.38it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.708, 'grad_norm': 0.0897011011838913, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6813, 'grad_norm': 0.05748400092124939, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.41it/s]                                               {'train_runtime': 1.0117, 'train_samples_per_second': 420.087, 'train_steps_per_second': 14.827, 'train_loss': 0.7169936060905456, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.41it/s]100%|██████████| 15/15 [00:01<00:00, 14.83it/s]
CLIENT:70
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.726, 'grad_norm': 0.1568860411643982, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.51it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.7056, 'grad_norm': 0.09407297521829605, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.7268, 'grad_norm': 0.14030739665031433, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.52it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.54it/s]                                              {'loss': 0.7275, 'grad_norm': 0.11427381634712219, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.54it/s]                                              {'loss': 0.7195, 'grad_norm': 0.13013848662376404, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.54it/s] 40%|████      | 6/15 [00:00<00:00, 16.92it/s]                                              {'loss': 0.7363, 'grad_norm': 0.14669252932071686, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.92it/s]                                              {'loss': 0.735, 'grad_norm': 0.1940840482711792, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.92it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.7195, 'grad_norm': 0.09156792610883713, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.7054, 'grad_norm': 0.10599647462368011, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.51it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.90it/s]                                               {'loss': 0.735, 'grad_norm': 0.18223616480827332, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.90it/s]                                               {'loss': 0.7042, 'grad_norm': 0.08859875798225403, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.90it/s] 80%|████████  | 12/15 [00:00<00:00, 16.37it/s]                                               {'loss': 0.7211, 'grad_norm': 0.10267624258995056, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.37it/s]                                               {'loss': 0.7446, 'grad_norm': 0.18837815523147583, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.37it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.6959, 'grad_norm': 0.06833143532276154, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.7211, 'grad_norm': 0.15190887451171875, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.08it/s]                                               {'train_runtime': 1.0083, 'train_samples_per_second': 421.508, 'train_steps_per_second': 14.877, 'train_loss': 0.7215688745180766, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.08it/s]100%|██████████| 15/15 [00:01<00:00, 14.88it/s]
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7225, 'grad_norm': 0.09456514567136765, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.52it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.01it/s]                                              {'loss': 0.7143, 'grad_norm': 0.08698432147502899, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.01it/s]                                              {'loss': 0.6596, 'grad_norm': 0.08899460732936859, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.01it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.7036, 'grad_norm': 0.04499007388949394, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.7083, 'grad_norm': 0.07016958296298981, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.97it/s] 40%|████      | 6/15 [00:00<00:00, 16.95it/s]                                              {'loss': 0.7274, 'grad_norm': 0.10356612503528595, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.95it/s]                                              {'loss': 0.7138, 'grad_norm': 0.07501976191997528, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.95it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.6921, 'grad_norm': 0.053451940417289734, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.709, 'grad_norm': 0.10179763287305832, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.29it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.6878, 'grad_norm': 0.047639861702919006, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.6955, 'grad_norm': 0.06262102723121643, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.69it/s] 80%|████████  | 12/15 [00:00<00:00, 16.21it/s]                                               {'loss': 0.7042, 'grad_norm': 0.11057901382446289, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.21it/s]                                               {'loss': 0.7133, 'grad_norm': 0.07863441854715347, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.21it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.6898, 'grad_norm': 0.041901517659425735, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.7298, 'grad_norm': 0.09461904317140579, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.14it/s]                                               {'train_runtime': 1.0179, 'train_samples_per_second': 417.515, 'train_steps_per_second': 14.736, 'train_loss': 0.7047264456748963, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.14it/s]100%|██████████| 15/15 [00:01<00:00, 14.74it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 36.18it/s] 24%|██▍       | 8/33 [00:00<00:00, 29.91it/s] 36%|███▋      | 12/33 [00:00<00:00, 28.04it/s] 45%|████▌     | 15/33 [00:00<00:00, 27.10it/s] 55%|█████▍    | 18/33 [00:00<00:00, 26.48it/s] 64%|██████▎   | 21/33 [00:00<00:00, 26.06it/s] 73%|███████▎  | 24/33 [00:00<00:00, 25.75it/s] 82%|████████▏ | 27/33 [00:01<00:00, 25.58it/s] 91%|█████████ | 30/33 [00:01<00:00, 25.46it/s]100%|██████████| 33/33 [00:01<00:00, 26.57it/s]100%|██████████| 33/33 [00:01<00:00, 26.83it/s]
{'eval_loss': 0.7211998105049133, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3096836049856184, 'eval_runtime': 1.2704, 'eval_samples_per_second': 821.002, 'eval_steps_per_second': 25.976}
ROUND:3
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7207, 'grad_norm': 0.13813957571983337, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.55it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.41it/s]                                              {'loss': 0.7433, 'grad_norm': 0.1760835349559784, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.41it/s]                                              {'loss': 0.7053, 'grad_norm': 0.1036258339881897, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.41it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.25it/s]                                              {'loss': 0.7305, 'grad_norm': 0.17524419724941254, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.25it/s]                                              {'loss': 0.7336, 'grad_norm': 0.13877250254154205, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.25it/s] 40%|████      | 6/15 [00:00<00:00, 16.18it/s]                                              {'loss': 0.7212, 'grad_norm': 0.10696674883365631, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.18it/s]                                              {'loss': 0.7336, 'grad_norm': 0.14979122579097748, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.18it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.93it/s]                                              {'loss': 0.742, 'grad_norm': 0.17084985971450806, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.93it/s]                                              {'loss': 0.7146, 'grad_norm': 0.11197555810213089, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.93it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.7241, 'grad_norm': 0.1326717585325241, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.7228, 'grad_norm': 0.11763513088226318, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.32it/s] 80%|████████  | 12/15 [00:00<00:00, 16.01it/s]                                               {'loss': 0.7517, 'grad_norm': 0.22649502754211426, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.01it/s]                                               {'loss': 0.7418, 'grad_norm': 0.18961836397647858, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.01it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.7398, 'grad_norm': 0.14031285047531128, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.7226, 'grad_norm': 0.11044598370790482, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.08it/s]                                               {'train_runtime': 1.0298, 'train_samples_per_second': 412.721, 'train_steps_per_second': 14.567, 'train_loss': 0.7298498193422953, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.08it/s]100%|██████████| 15/15 [00:01<00:00, 14.57it/s]
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6998, 'grad_norm': 0.07900939136743546, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.91it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.63it/s]                                              {'loss': 0.721, 'grad_norm': 0.164295956492424, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.63it/s]                                              {'loss': 0.7318, 'grad_norm': 0.16305847465991974, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.63it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.7251, 'grad_norm': 0.10929303616285324, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.7431, 'grad_norm': 0.18461158871650696, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.35it/s] 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.7222, 'grad_norm': 0.11806408315896988, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.7425, 'grad_norm': 0.1913069486618042, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.03it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.7152, 'grad_norm': 0.10861939191818237, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.7083, 'grad_norm': 0.07631291449069977, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.30it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.83it/s]                                               {'loss': 0.7232, 'grad_norm': 0.09970556199550629, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.83it/s]                                               {'loss': 0.7116, 'grad_norm': 0.1498773843050003, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.83it/s] 80%|████████  | 12/15 [00:00<00:00, 16.30it/s]                                               {'loss': 0.7456, 'grad_norm': 0.1606290489435196, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.30it/s]                                               {'loss': 0.7252, 'grad_norm': 0.11681075394153595, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.7297, 'grad_norm': 0.14191828668117523, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.7257, 'grad_norm': 0.16041019558906555, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.36it/s]                                               {'train_runtime': 1.0165, 'train_samples_per_second': 418.107, 'train_steps_per_second': 14.757, 'train_loss': 0.7246644973754883, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.36it/s]100%|██████████| 15/15 [00:01<00:00, 14.76it/s]
CLIENT:76
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7159, 'grad_norm': 0.09502150118350983, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.37it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.7216, 'grad_norm': 0.09562292695045471, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.7217, 'grad_norm': 0.18013443052768707, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.58it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.85it/s]                                              {'loss': 0.7185, 'grad_norm': 0.09561439603567123, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.85it/s]                                              {'loss': 0.6973, 'grad_norm': 0.06888501346111298, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.85it/s] 40%|████      | 6/15 [00:00<00:00, 16.47it/s]                                              {'loss': 0.7532, 'grad_norm': 0.2210412621498108, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.47it/s]                                              {'loss': 0.7127, 'grad_norm': 0.1080792099237442, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.47it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.706, 'grad_norm': 0.1070849671959877, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.7275, 'grad_norm': 0.15135516226291656, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.31it/s] 67%|██████▋   | 10/15 [00:00<00:00, 16.10it/s]                                               {'loss': 0.7267, 'grad_norm': 0.14049962162971497, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 16.10it/s]                                               {'loss': 0.7125, 'grad_norm': 0.10927120596170425, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 16.10it/s] 80%|████████  | 12/15 [00:00<00:00, 16.50it/s]                                               {'loss': 0.7191, 'grad_norm': 0.08414682000875473, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.50it/s]                                               {'loss': 0.7059, 'grad_norm': 0.07383441925048828, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.50it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.7176, 'grad_norm': 0.1191852018237114, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.7283, 'grad_norm': 0.1869702786207199, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.30it/s]                                               {'train_runtime': 1.0082, 'train_samples_per_second': 421.55, 'train_steps_per_second': 14.878, 'train_loss': 0.7189486185709636, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.30it/s]100%|██████████| 15/15 [00:01<00:00, 14.88it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.73, 'grad_norm': 0.17626190185546875, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.75it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.50it/s]                                              {'loss': 0.7463, 'grad_norm': 0.18876740336418152, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.50it/s]                                              {'loss': 0.7172, 'grad_norm': 0.12822450697422028, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.50it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.27it/s]                                              {'loss': 0.7311, 'grad_norm': 0.15271936357021332, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.27it/s]                                              {'loss': 0.7373, 'grad_norm': 0.19196434319019318, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.27it/s] 40%|████      | 6/15 [00:00<00:00, 16.64it/s]                                              {'loss': 0.7312, 'grad_norm': 0.17150864005088806, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.64it/s]                                              {'loss': 0.7561, 'grad_norm': 0.21127955615520477, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.64it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.7362, 'grad_norm': 0.1955004781484604, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.7246, 'grad_norm': 0.10586118698120117, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.15it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.73it/s]                                               {'loss': 0.7332, 'grad_norm': 0.1543716937303543, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.73it/s]                                               {'loss': 0.7242, 'grad_norm': 0.19371676445007324, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.73it/s] 80%|████████  | 12/15 [00:00<00:00, 16.21it/s]                                               {'loss': 0.7479, 'grad_norm': 0.18559633195400238, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.21it/s]                                               {'loss': 0.7299, 'grad_norm': 0.1698135882616043, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.21it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.7345, 'grad_norm': 0.1770232766866684, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.7453, 'grad_norm': 0.20170694589614868, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.11it/s]                                               {'train_runtime': 1.0229, 'train_samples_per_second': 415.466, 'train_steps_per_second': 14.664, 'train_loss': 0.7350062847137451, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.11it/s]100%|██████████| 15/15 [00:01<00:00, 14.67it/s]
CLIENT:56
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7072, 'grad_norm': 0.09079526364803314, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.33it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.7432, 'grad_norm': 0.18359503149986267, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.7252, 'grad_norm': 0.1555851250886917, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.15it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.87it/s]                                              {'loss': 0.7203, 'grad_norm': 0.10448569059371948, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.87it/s]                                              {'loss': 0.7238, 'grad_norm': 0.11653254926204681, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.87it/s] 40%|████      | 6/15 [00:00<00:00, 16.06it/s]                                              {'loss': 0.7445, 'grad_norm': 0.23001691699028015, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.06it/s]                                              {'loss': 0.7127, 'grad_norm': 0.13070784509181976, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.06it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.7186, 'grad_norm': 0.1269237995147705, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.7417, 'grad_norm': 0.19340278208255768, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.90it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.74it/s]                                               {'loss': 0.7254, 'grad_norm': 0.15532059967517853, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.74it/s]                                               {'loss': 0.7039, 'grad_norm': 0.131829172372818, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.74it/s] 80%|████████  | 12/15 [00:00<00:00, 16.07it/s]                                               {'loss': 0.7217, 'grad_norm': 0.14697833359241486, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.07it/s]                                               {'loss': 0.7326, 'grad_norm': 0.12903651595115662, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.07it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.7199, 'grad_norm': 0.14433664083480835, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.7288, 'grad_norm': 0.18535302579402924, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.98it/s]                                               {'train_runtime': 1.0369, 'train_samples_per_second': 409.862, 'train_steps_per_second': 14.466, 'train_loss': 0.7246209502220153, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.98it/s]100%|██████████| 15/15 [00:01<00:00, 14.47it/s]
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7254, 'grad_norm': 0.14913912117481232, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.51it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.93it/s]                                              {'loss': 0.7302, 'grad_norm': 0.14716075360774994, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.93it/s]                                              {'loss': 0.7105, 'grad_norm': 0.0737517699599266, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.93it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.7294, 'grad_norm': 0.13536211848258972, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.7308, 'grad_norm': 0.13436149060726166, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.46it/s] 40%|████      | 6/15 [00:00<00:00, 16.15it/s]                                              {'loss': 0.7145, 'grad_norm': 0.10947547852993011, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.15it/s]                                              {'loss': 0.7197, 'grad_norm': 0.13275150954723358, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.15it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.7123, 'grad_norm': 0.13512536883354187, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.7179, 'grad_norm': 0.12277895957231522, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.56it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.92it/s]                                               {'loss': 0.7149, 'grad_norm': 0.11135242879390717, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.92it/s]                                               {'loss': 0.7031, 'grad_norm': 0.10590020567178726, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.92it/s] 80%|████████  | 12/15 [00:00<00:00, 16.35it/s]                                               {'loss': 0.7452, 'grad_norm': 0.1966608464717865, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.35it/s]                                               {'loss': 0.7271, 'grad_norm': 0.13788610696792603, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.35it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7188, 'grad_norm': 0.13040490448474884, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7313, 'grad_norm': 0.14129464328289032, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.38it/s]                                               {'train_runtime': 1.0119, 'train_samples_per_second': 419.985, 'train_steps_per_second': 14.823, 'train_loss': 0.7220726251602173, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.38it/s]100%|██████████| 15/15 [00:01<00:00, 14.83it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7121, 'grad_norm': 0.097636379301548, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.01it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.7222, 'grad_norm': 0.16069598495960236, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.7412, 'grad_norm': 0.19264109432697296, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.33it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.06it/s]                                              {'loss': 0.7171, 'grad_norm': 0.10310745239257812, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.06it/s]                                              {'loss': 0.7417, 'grad_norm': 0.18700668215751648, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.06it/s] 40%|████      | 6/15 [00:00<00:00, 16.04it/s]                                              {'loss': 0.731, 'grad_norm': 0.13969062268733978, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.04it/s]                                              {'loss': 0.7491, 'grad_norm': 0.17609409987926483, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.04it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.7126, 'grad_norm': 0.11985938996076584, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.7149, 'grad_norm': 0.13829828798770905, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.78it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.65it/s]                                               {'loss': 0.7371, 'grad_norm': 0.1489076167345047, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.65it/s]                                               {'loss': 0.7176, 'grad_norm': 0.1397838592529297, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.65it/s] 80%|████████  | 12/15 [00:00<00:00, 15.98it/s]                                               {'loss': 0.7194, 'grad_norm': 0.14831890165805817, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.98it/s]                                               {'loss': 0.7125, 'grad_norm': 0.110773004591465, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.98it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.7294, 'grad_norm': 0.17502139508724213, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.7433, 'grad_norm': 0.1991375982761383, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.99it/s]                                               {'train_runtime': 1.0368, 'train_samples_per_second': 409.926, 'train_steps_per_second': 14.468, 'train_loss': 0.7267415603001912, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.99it/s]100%|██████████| 15/15 [00:01<00:00, 14.47it/s]
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7244, 'grad_norm': 0.12136799097061157, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.18it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.50it/s]                                              {'loss': 0.7005, 'grad_norm': 0.07495076209306717, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.50it/s]                                              {'loss': 0.7348, 'grad_norm': 0.12319536507129669, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.50it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.92it/s]                                              {'loss': 0.7072, 'grad_norm': 0.053186528384685516, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.92it/s]                                              {'loss': 0.7297, 'grad_norm': 0.1579284965991974, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.92it/s] 40%|████      | 6/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.7062, 'grad_norm': 0.11372631788253784, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.7189, 'grad_norm': 0.08139338344335556, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.89it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.88it/s]                                              {'loss': 0.7194, 'grad_norm': 0.13107256591320038, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.88it/s]                                              {'loss': 0.7173, 'grad_norm': 0.1078900471329689, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.88it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.717, 'grad_norm': 0.07014808058738708, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.7267, 'grad_norm': 0.12747472524642944, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.49it/s] 80%|████████  | 12/15 [00:00<00:00, 15.99it/s]                                               {'loss': 0.7332, 'grad_norm': 0.14361706376075745, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.99it/s]                                               {'loss': 0.7426, 'grad_norm': 0.1773015558719635, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.99it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.7092, 'grad_norm': 0.07588378340005875, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.6958, 'grad_norm': 0.060287512838840485, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.96it/s]                                               {'train_runtime': 1.0346, 'train_samples_per_second': 410.774, 'train_steps_per_second': 14.498, 'train_loss': 0.7188583453496297, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.96it/s]100%|██████████| 15/15 [00:01<00:00, 14.50it/s]
CLIENT:60
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7074, 'grad_norm': 0.09947143495082855, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.37it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.92it/s]                                              {'loss': 0.7051, 'grad_norm': 0.08661381900310516, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.92it/s]                                              {'loss': 0.7075, 'grad_norm': 0.09063024818897247, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.92it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.7292, 'grad_norm': 0.12975075840950012, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.7018, 'grad_norm': 0.07035571336746216, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.56it/s] 40%|████      | 6/15 [00:00<00:00, 16.26it/s]                                              {'loss': 0.712, 'grad_norm': 0.07464273273944855, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.26it/s]                                              {'loss': 0.6916, 'grad_norm': 0.06206310912966728, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.26it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.80it/s]                                              {'loss': 0.7151, 'grad_norm': 0.10785982757806778, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.80it/s]                                              {'loss': 0.7156, 'grad_norm': 0.11232338100671768, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.80it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.7304, 'grad_norm': 0.1594810038805008, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.6795, 'grad_norm': 0.047928985208272934, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.16it/s] 80%|████████  | 12/15 [00:00<00:00, 15.80it/s]                                               {'loss': 0.7035, 'grad_norm': 0.09539440274238586, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.80it/s]                                               {'loss': 0.6967, 'grad_norm': 0.04586082324385643, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.80it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.7046, 'grad_norm': 0.09459538757801056, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.7294, 'grad_norm': 0.18397076427936554, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.71it/s]                                               {'train_runtime': 1.0435, 'train_samples_per_second': 407.294, 'train_steps_per_second': 14.375, 'train_loss': 0.7086218396822611, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.71it/s]100%|██████████| 15/15 [00:01<00:00, 14.38it/s]
CLIENT:67
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7184, 'grad_norm': 0.1514655500650406, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.34it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.91it/s]                                              {'loss': 0.7294, 'grad_norm': 0.16296134889125824, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.91it/s]                                              {'loss': 0.7164, 'grad_norm': 0.12117759138345718, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.91it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.66it/s]                                              {'loss': 0.7403, 'grad_norm': 0.17455267906188965, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.66it/s]                                              {'loss': 0.7306, 'grad_norm': 0.16434191167354584, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.66it/s] 40%|████      | 6/15 [00:00<00:00, 16.38it/s]                                              {'loss': 0.7097, 'grad_norm': 0.1045016497373581, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.38it/s]                                              {'loss': 0.7212, 'grad_norm': 0.11701671034097672, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.38it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.7355, 'grad_norm': 0.1779450923204422, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.73, 'grad_norm': 0.1752980500459671, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.74it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.729, 'grad_norm': 0.1610618382692337, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.7127, 'grad_norm': 0.1410437971353531, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.62it/s] 80%|████████  | 12/15 [00:00<00:00, 15.95it/s]                                               {'loss': 0.7389, 'grad_norm': 0.17198067903518677, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.95it/s]                                               {'loss': 0.72, 'grad_norm': 0.14109830558300018, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.95it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.7224, 'grad_norm': 0.15636248886585236, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.7282, 'grad_norm': 0.1827450841665268, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.88it/s]                                               {'train_runtime': 1.0341, 'train_samples_per_second': 410.983, 'train_steps_per_second': 14.505, 'train_loss': 0.7254976193110149, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.88it/s]100%|██████████| 15/15 [00:01<00:00, 14.51it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 39.50it/s] 24%|██▍       | 8/33 [00:00<00:00, 29.75it/s] 36%|███▋      | 12/33 [00:00<00:00, 27.45it/s] 45%|████▌     | 15/33 [00:00<00:00, 26.58it/s] 55%|█████▍    | 18/33 [00:00<00:00, 26.06it/s] 64%|██████▎   | 21/33 [00:00<00:00, 25.71it/s] 73%|███████▎  | 24/33 [00:00<00:00, 25.48it/s] 82%|████████▏ | 27/33 [00:01<00:00, 25.34it/s] 91%|█████████ | 30/33 [00:01<00:00, 25.45it/s]100%|██████████| 33/33 [00:01<00:00, 26.72it/s]
{'eval_loss': 0.7201448678970337, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3096836049856184, 'eval_runtime': 1.2756, 'eval_samples_per_second': 817.633, 'eval_steps_per_second': 25.87}
ROUND:4
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7362, 'grad_norm': 0.16237883269786835, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.69it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.28it/s]                                              {'loss': 0.7408, 'grad_norm': 0.1578737199306488, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.28it/s]                                              {'loss': 0.7272, 'grad_norm': 0.10715997219085693, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.28it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.7223, 'grad_norm': 0.1359744518995285, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.7377, 'grad_norm': 0.13009127974510193, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.34it/s] 40%|████      | 6/15 [00:00<00:00, 16.27it/s]                                              {'loss': 0.7365, 'grad_norm': 0.21344241499900818, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.27it/s]                                              {'loss': 0.732, 'grad_norm': 0.13667906820774078, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.27it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.94it/s]                                              {'loss': 0.7214, 'grad_norm': 0.13527421653270721, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.94it/s]                                              {'loss': 0.7379, 'grad_norm': 0.21113328635692596, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.94it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.7335, 'grad_norm': 0.15874771773815155, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.7295, 'grad_norm': 0.17946185171604156, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.59it/s] 80%|████████  | 12/15 [00:00<00:00, 16.09it/s]                                               {'loss': 0.7234, 'grad_norm': 0.12638774514198303, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.09it/s]                                               {'loss': 0.7376, 'grad_norm': 0.18488813936710358, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.09it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.7339, 'grad_norm': 0.15716399252414703, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.7307, 'grad_norm': 0.1392914354801178, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.94it/s]                                               {'train_runtime': 1.0391, 'train_samples_per_second': 408.993, 'train_steps_per_second': 14.435, 'train_loss': 0.7320362567901612, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.94it/s]100%|██████████| 15/15 [00:01<00:00, 14.44it/s]
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7107, 'grad_norm': 0.09011785686016083, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.78it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.14it/s]                                              {'loss': 0.703, 'grad_norm': 0.09644947946071625, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.14it/s]                                              {'loss': 0.7309, 'grad_norm': 0.13411913812160492, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.14it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.06it/s]                                              {'loss': 0.7007, 'grad_norm': 0.08087309449911118, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.06it/s]                                              {'loss': 0.7184, 'grad_norm': 0.13252295553684235, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.06it/s] 40%|████      | 6/15 [00:00<00:00, 16.00it/s]                                              {'loss': 0.7161, 'grad_norm': 0.1022995114326477, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.00it/s]                                              {'loss': 0.7189, 'grad_norm': 0.13825826346874237, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.00it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7116, 'grad_norm': 0.07424675673246384, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.715, 'grad_norm': 0.0935269296169281, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.61it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.715, 'grad_norm': 0.10462875664234161, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.7005, 'grad_norm': 0.09262518584728241, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.04it/s] 80%|████████  | 12/15 [00:00<00:00, 15.79it/s]                                               {'loss': 0.724, 'grad_norm': 0.12839509546756744, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.79it/s]                                               {'loss': 0.7082, 'grad_norm': 0.06561379134654999, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.79it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.721, 'grad_norm': 0.14864711463451385, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.14it/s]                                               {'loss': 0.6916, 'grad_norm': 0.11216890066862106, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.14it/s]                                               {'train_runtime': 1.0402, 'train_samples_per_second': 408.568, 'train_steps_per_second': 14.42, 'train_loss': 0.7123652974764506, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.14it/s]100%|██████████| 15/15 [00:01<00:00, 14.43it/s]
CLIENT:79
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7248, 'grad_norm': 0.13128212094306946, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.15it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.7335, 'grad_norm': 0.12338047474622726, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.6881, 'grad_norm': 0.054323308169841766, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.04it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.7135, 'grad_norm': 0.06851933896541595, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.7362, 'grad_norm': 0.15429699420928955, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.72it/s] 40%|████      | 6/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.7145, 'grad_norm': 0.10455557703971863, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.7416, 'grad_norm': 0.15195444226264954, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.88it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.69, 'grad_norm': 0.08276000618934631, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.7101, 'grad_norm': 0.08815080672502518, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.13it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.7096, 'grad_norm': 0.10458304733037949, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.7226, 'grad_norm': 0.09901251643896103, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.55it/s] 80%|████████  | 12/15 [00:00<00:00, 16.20it/s]                                               {'loss': 0.7055, 'grad_norm': 0.13465924561023712, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.20it/s]                                               {'loss': 0.7255, 'grad_norm': 0.11883952468633652, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.20it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.7229, 'grad_norm': 0.11810427159070969, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.7285, 'grad_norm': 0.10202536731958389, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.10it/s]                                               {'train_runtime': 1.0332, 'train_samples_per_second': 411.35, 'train_steps_per_second': 14.518, 'train_loss': 0.7177971919377645, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.10it/s]100%|██████████| 15/15 [00:01<00:00, 14.52it/s]
CLIENT:14
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6941, 'grad_norm': 0.07194732874631882, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.52it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.92it/s]                                              {'loss': 0.7188, 'grad_norm': 0.1118052527308464, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.92it/s]                                              {'loss': 0.7228, 'grad_norm': 0.11845976859331131, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.92it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.58it/s]                                              {'loss': 0.72, 'grad_norm': 0.09777180105447769, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.58it/s]                                              {'loss': 0.7064, 'grad_norm': 0.07652607560157776, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.58it/s] 40%|████      | 6/15 [00:00<00:00, 16.31it/s]                                              {'loss': 0.7203, 'grad_norm': 0.16245397925376892, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.31it/s]                                              {'loss': 0.7206, 'grad_norm': 0.12070958316326141, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.31it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.7124, 'grad_norm': 0.10746476799249649, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.7032, 'grad_norm': 0.0848551094532013, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.77it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.65it/s]                                               {'loss': 0.7116, 'grad_norm': 0.08207575976848602, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.65it/s]                                               {'loss': 0.7038, 'grad_norm': 0.12408447265625, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.65it/s] 80%|████████  | 12/15 [00:00<00:00, 16.09it/s]                                               {'loss': 0.6876, 'grad_norm': 0.10387315601110458, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.09it/s]                                               {'loss': 0.7093, 'grad_norm': 0.07960010319948196, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.09it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.6905, 'grad_norm': 0.09495637565851212, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.7351, 'grad_norm': 0.18765275180339813, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.07it/s]                                               {'train_runtime': 1.0289, 'train_samples_per_second': 413.05, 'train_steps_per_second': 14.578, 'train_loss': 0.7104402701059978, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.07it/s]100%|██████████| 15/15 [00:01<00:00, 14.58it/s]
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7246, 'grad_norm': 0.16536659002304077, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.84it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.7018, 'grad_norm': 0.06812737137079239, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.7116, 'grad_norm': 0.0926964059472084, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.28it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.7207, 'grad_norm': 0.07933201640844345, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.7312, 'grad_norm': 0.14077498018741608, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.30it/s] 40%|████      | 6/15 [00:00<00:00, 16.33it/s]                                              {'loss': 0.6985, 'grad_norm': 0.09529360383749008, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.33it/s]                                              {'loss': 0.7179, 'grad_norm': 0.12960663437843323, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.33it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.06it/s]                                              {'loss': 0.7102, 'grad_norm': 0.09282497316598892, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.06it/s]                                              {'loss': 0.7111, 'grad_norm': 0.11088531464338303, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.06it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.7476, 'grad_norm': 0.21126624941825867, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.6951, 'grad_norm': 0.04728156700730324, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.30it/s] 80%|████████  | 12/15 [00:00<00:00, 15.88it/s]                                               {'loss': 0.7172, 'grad_norm': 0.10102169960737228, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.88it/s]                                               {'loss': 0.7246, 'grad_norm': 0.14211654663085938, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.88it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.09it/s]                                               {'loss': 0.7081, 'grad_norm': 0.10236619412899017, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.09it/s]                                               {'loss': 0.6998, 'grad_norm': 0.0975819081068039, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.09it/s]                                               {'train_runtime': 1.0289, 'train_samples_per_second': 413.076, 'train_steps_per_second': 14.579, 'train_loss': 0.7146764953931173, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.09it/s]100%|██████████| 15/15 [00:01<00:00, 14.58it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7283, 'grad_norm': 0.1805170178413391, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.47it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.92it/s]                                              {'loss': 0.7445, 'grad_norm': 0.19383437931537628, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.92it/s]                                              {'loss': 0.7161, 'grad_norm': 0.13106171786785126, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.92it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.7296, 'grad_norm': 0.15741725265979767, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.7355, 'grad_norm': 0.19837534427642822, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.40it/s] 40%|████      | 6/15 [00:00<00:00, 16.02it/s]                                              {'loss': 0.7295, 'grad_norm': 0.17679047584533691, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.02it/s]                                              {'loss': 0.754, 'grad_norm': 0.2183123379945755, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.02it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.03it/s]                                              {'loss': 0.7343, 'grad_norm': 0.20189912617206573, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.03it/s]                                              {'loss': 0.7236, 'grad_norm': 0.10883158445358276, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.03it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7317, 'grad_norm': 0.15916216373443604, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7223, 'grad_norm': 0.20032398402690887, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.12it/s] 80%|████████  | 12/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.7461, 'grad_norm': 0.19204093515872955, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.7282, 'grad_norm': 0.17564113438129425, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.66it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.7327, 'grad_norm': 0.18286214768886566, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.7433, 'grad_norm': 0.20941324532032013, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.73it/s]                                               {'train_runtime': 1.0455, 'train_samples_per_second': 406.492, 'train_steps_per_second': 14.347, 'train_loss': 0.733310862382253, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.73it/s]100%|██████████| 15/15 [00:01<00:00, 14.35it/s]
CLIENT:19
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7244, 'grad_norm': 0.12363370507955551, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.81it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.11it/s]                                              {'loss': 0.7246, 'grad_norm': 0.12277541309595108, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.11it/s]                                              {'loss': 0.7307, 'grad_norm': 0.16945424675941467, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.11it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.7272, 'grad_norm': 0.13792262971401215, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.7422, 'grad_norm': 0.16893713176250458, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.40it/s] 40%|████      | 6/15 [00:00<00:00, 16.86it/s]                                              {'loss': 0.7007, 'grad_norm': 0.08461533486843109, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.86it/s]                                              {'loss': 0.7382, 'grad_norm': 0.16989067196846008, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.86it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.7307, 'grad_norm': 0.1290510594844818, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.7142, 'grad_norm': 0.12098826467990875, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.23it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.7442, 'grad_norm': 0.16613280773162842, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.7189, 'grad_norm': 0.15973937511444092, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.39it/s] 80%|████████  | 12/15 [00:00<00:00, 15.90it/s]                                               {'loss': 0.7037, 'grad_norm': 0.08776188641786575, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.90it/s]                                               {'loss': 0.7202, 'grad_norm': 0.13079199194908142, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.90it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.7292, 'grad_norm': 0.1756303906440735, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.7254, 'grad_norm': 0.13369515538215637, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.74it/s]                                               {'train_runtime': 1.039, 'train_samples_per_second': 409.042, 'train_steps_per_second': 14.437, 'train_loss': 0.7249667882919312, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.74it/s]100%|██████████| 15/15 [00:01<00:00, 14.44it/s]
CLIENT:28
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7173, 'grad_norm': 0.09515024721622467, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.38it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.722, 'grad_norm': 0.14086709916591644, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.7349, 'grad_norm': 0.16137182712554932, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.16it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.88it/s]                                              {'loss': 0.7274, 'grad_norm': 0.11471249163150787, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.88it/s]                                              {'loss': 0.7176, 'grad_norm': 0.12495803833007812, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.88it/s] 40%|████      | 6/15 [00:00<00:00, 16.07it/s]                                              {'loss': 0.7355, 'grad_norm': 0.15995542705059052, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.07it/s]                                              {'loss': 0.7354, 'grad_norm': 0.12964217364788055, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.07it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.86it/s]                                              {'loss': 0.7211, 'grad_norm': 0.1740151345729828, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.86it/s]                                              {'loss': 0.6997, 'grad_norm': 0.08012611418962479, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.86it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.7224, 'grad_norm': 0.12107854336500168, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.726, 'grad_norm': 0.15886159241199493, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.19it/s] 80%|████████  | 12/15 [00:00<00:00, 15.75it/s]                                               {'loss': 0.7306, 'grad_norm': 0.1123654842376709, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.75it/s]                                               {'loss': 0.7317, 'grad_norm': 0.12692642211914062, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.75it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.68it/s]                                               {'loss': 0.7404, 'grad_norm': 0.19620689749717712, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.68it/s]                                               {'loss': 0.7125, 'grad_norm': 0.08263114094734192, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.68it/s]                                               {'train_runtime': 1.0512, 'train_samples_per_second': 404.286, 'train_steps_per_second': 14.269, 'train_loss': 0.7249591867129008, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.68it/s]100%|██████████| 15/15 [00:01<00:00, 14.27it/s]
CLIENT:94
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7103, 'grad_norm': 0.11017440259456635, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.25it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.7214, 'grad_norm': 0.12008107453584671, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.7415, 'grad_norm': 0.17786704003810883, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.01it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.7318, 'grad_norm': 0.12791548669338226, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.727, 'grad_norm': 0.14236189424991608, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.76it/s] 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.7111, 'grad_norm': 0.12623204290866852, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.738, 'grad_norm': 0.19580340385437012, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.97it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.75it/s]                                              {'loss': 0.728, 'grad_norm': 0.13864146173000336, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.75it/s]                                              {'loss': 0.6952, 'grad_norm': 0.06413466483354568, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.75it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.7255, 'grad_norm': 0.1344119757413864, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.7091, 'grad_norm': 0.11179199069738388, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.26it/s] 80%|████████  | 12/15 [00:00<00:00, 15.87it/s]                                               {'loss': 0.7184, 'grad_norm': 0.1898454874753952, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.87it/s]                                               {'loss': 0.713, 'grad_norm': 0.12429730594158173, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.87it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.7255, 'grad_norm': 0.14674124121665955, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.7244, 'grad_norm': 0.16305486857891083, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.67it/s]                                               {'train_runtime': 1.0498, 'train_samples_per_second': 404.834, 'train_steps_per_second': 14.288, 'train_loss': 0.721361509958903, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.67it/s]100%|██████████| 15/15 [00:01<00:00, 14.29it/s]
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7241, 'grad_norm': 0.1275755912065506, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.69it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.7212, 'grad_norm': 0.12616384029388428, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.6952, 'grad_norm': 0.07573812454938889, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.11it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.7361, 'grad_norm': 0.1531815081834793, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.7096, 'grad_norm': 0.09017246961593628, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.12it/s] 40%|████      | 6/15 [00:00<00:00, 16.07it/s]                                              {'loss': 0.7059, 'grad_norm': 0.09598933160305023, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.07it/s]                                              {'loss': 0.7163, 'grad_norm': 0.1291825771331787, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.07it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.80it/s]                                              {'loss': 0.7201, 'grad_norm': 0.08614587783813477, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.80it/s]                                              {'loss': 0.7238, 'grad_norm': 0.13801583647727966, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.80it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.7347, 'grad_norm': 0.14795126020908356, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.7112, 'grad_norm': 0.07819663733243942, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.46it/s] 80%|████████  | 12/15 [00:00<00:00, 15.98it/s]                                               {'loss': 0.7335, 'grad_norm': 0.12258660048246384, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.98it/s]                                               {'loss': 0.714, 'grad_norm': 0.10985907912254333, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.98it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.7068, 'grad_norm': 0.07796313613653183, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.7406, 'grad_norm': 0.1911318600177765, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.86it/s]                                               {'train_runtime': 1.0392, 'train_samples_per_second': 408.96, 'train_steps_per_second': 14.434, 'train_loss': 0.7195459683736165, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.86it/s]100%|██████████| 15/15 [00:01<00:00, 14.44it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 35.31it/s] 24%|██▍       | 8/33 [00:00<00:00, 29.01it/s] 33%|███▎      | 11/33 [00:00<00:00, 27.28it/s] 42%|████▏     | 14/33 [00:00<00:00, 26.31it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.80it/s] 61%|██████    | 20/33 [00:00<00:00, 25.54it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.30it/s] 79%|███████▉  | 26/33 [00:00<00:00, 25.32it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.17it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.18it/s]100%|██████████| 33/33 [00:01<00:00, 26.27it/s]
{'eval_loss': 0.7191187739372253, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3096836049856184, 'eval_runtime': 1.2974, 'eval_samples_per_second': 803.919, 'eval_steps_per_second': 25.436}
ROUND:5
CLIENT:79
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7236, 'grad_norm': 0.13551746308803558, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.40it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.20it/s]                                              {'loss': 0.7324, 'grad_norm': 0.12660863995552063, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.20it/s]                                              {'loss': 0.6879, 'grad_norm': 0.05514223128557205, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.20it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.98it/s]                                              {'loss': 0.713, 'grad_norm': 0.07025977224111557, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.98it/s]                                              {'loss': 0.7348, 'grad_norm': 0.1592707484960556, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.98it/s] 40%|████      | 6/15 [00:00<00:00, 15.73it/s]                                              {'loss': 0.7136, 'grad_norm': 0.10758134722709656, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.73it/s]                                              {'loss': 0.7402, 'grad_norm': 0.15700474381446838, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.73it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.6893, 'grad_norm': 0.08483446389436722, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.7095, 'grad_norm': 0.09059339016675949, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.85it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.7087, 'grad_norm': 0.1081082820892334, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.7218, 'grad_norm': 0.10229560732841492, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.15it/s] 80%|████████  | 12/15 [00:00<00:00, 15.75it/s]                                               {'loss': 0.7043, 'grad_norm': 0.13827300071716309, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.75it/s]                                               {'loss': 0.7244, 'grad_norm': 0.12295742332935333, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.75it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.7219, 'grad_norm': 0.12141519039869308, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.7277, 'grad_norm': 0.10562567412853241, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.58it/s]                                               {'train_runtime': 1.0574, 'train_samples_per_second': 401.913, 'train_steps_per_second': 14.185, 'train_loss': 0.7168760736783345, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.58it/s]100%|██████████| 15/15 [00:01<00:00, 14.19it/s]
CLIENT:75
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7006, 'grad_norm': 0.07121402770280838, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.23it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.7351, 'grad_norm': 0.1671750247478485, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.7223, 'grad_norm': 0.1258828490972519, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.15it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.96it/s]                                              {'loss': 0.7156, 'grad_norm': 0.08710680156946182, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.96it/s]                                              {'loss': 0.742, 'grad_norm': 0.1731913685798645, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.96it/s] 40%|████      | 6/15 [00:00<00:00, 15.78it/s]                                              {'loss': 0.7099, 'grad_norm': 0.1091633290052414, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.78it/s]                                              {'loss': 0.7299, 'grad_norm': 0.16699957847595215, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.78it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.02it/s]                                              {'loss': 0.6974, 'grad_norm': 0.10421735793352127, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.02it/s]                                              {'loss': 0.7099, 'grad_norm': 0.093205526471138, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.02it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.44it/s]                                               {'loss': 0.7102, 'grad_norm': 0.09930668026208878, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.44it/s]                                               {'loss': 0.707, 'grad_norm': 0.11015696078538895, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.44it/s] 80%|████████  | 12/15 [00:00<00:00, 16.08it/s]                                               {'loss': 0.7188, 'grad_norm': 0.1829357147216797, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.08it/s]                                               {'loss': 0.718, 'grad_norm': 0.09654109179973602, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.08it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.7245, 'grad_norm': 0.1486797332763672, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.7207, 'grad_norm': 0.1177135705947876, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.99it/s]                                               {'train_runtime': 1.04, 'train_samples_per_second': 408.673, 'train_steps_per_second': 14.424, 'train_loss': 0.7174601793289185, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.99it/s]100%|██████████| 15/15 [00:01<00:00, 14.43it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7279, 'grad_norm': 0.15575751662254333, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.24it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.59it/s]                                              {'loss': 0.7359, 'grad_norm': 0.1677749902009964, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.59it/s]                                              {'loss': 0.7063, 'grad_norm': 0.11914829164743423, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.59it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.79it/s]                                              {'loss': 0.7174, 'grad_norm': 0.09961431473493576, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.79it/s]                                              {'loss': 0.7462, 'grad_norm': 0.19962793588638306, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.79it/s] 40%|████      | 6/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.7305, 'grad_norm': 0.1532030701637268, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.714, 'grad_norm': 0.13765978813171387, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.88it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.7155, 'grad_norm': 0.1302550882101059, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.741, 'grad_norm': 0.21034087240695953, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.58it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7328, 'grad_norm': 0.16422730684280396, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7086, 'grad_norm': 0.09667833894491196, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.12it/s] 80%|████████  | 12/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.7416, 'grad_norm': 0.21820509433746338, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.7299, 'grad_norm': 0.18094851076602936, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.85it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.719, 'grad_norm': 0.12166424095630646, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.7409, 'grad_norm': 0.14021438360214233, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.89it/s]                                               {'train_runtime': 1.0483, 'train_samples_per_second': 405.406, 'train_steps_per_second': 14.308, 'train_loss': 0.7271535793940226, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.89it/s]100%|██████████| 15/15 [00:01<00:00, 14.31it/s]
CLIENT:15
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7273, 'grad_norm': 0.1233505979180336, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.52it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.27it/s]                                              {'loss': 0.7124, 'grad_norm': 0.11449902504682541, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.27it/s]                                              {'loss': 0.6946, 'grad_norm': 0.0795096755027771, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.27it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.7232, 'grad_norm': 0.11256890743970871, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.7188, 'grad_norm': 0.10122261941432953, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.12it/s] 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.7129, 'grad_norm': 0.10550923645496368, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.7129, 'grad_norm': 0.07746387273073196, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.97it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.67it/s]                                              {'loss': 0.7186, 'grad_norm': 0.12998606264591217, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.67it/s]                                              {'loss': 0.7047, 'grad_norm': 0.12399926781654358, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.67it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.7153, 'grad_norm': 0.11334427446126938, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.7009, 'grad_norm': 0.055037952959537506, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.07it/s] 80%|████████  | 12/15 [00:00<00:00, 15.70it/s]                                               {'loss': 0.7423, 'grad_norm': 0.1856287121772766, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.70it/s]                                               {'loss': 0.7158, 'grad_norm': 0.11640959978103638, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.70it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.7092, 'grad_norm': 0.12024237960577011, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.7101, 'grad_norm': 0.0971212238073349, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.60it/s]                                               {'train_runtime': 1.055, 'train_samples_per_second': 402.856, 'train_steps_per_second': 14.218, 'train_loss': 0.7145974238713583, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.60it/s]100%|██████████| 15/15 [00:01<00:00, 14.22it/s]
CLIENT:38
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7201, 'grad_norm': 0.10770072042942047, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.87it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.26it/s]                                              {'loss': 0.7224, 'grad_norm': 0.11096777766942978, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.26it/s]                                              {'loss': 0.734, 'grad_norm': 0.1597357839345932, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.26it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.7313, 'grad_norm': 0.15984386205673218, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.7109, 'grad_norm': 0.08354124426841736, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.29it/s] 40%|████      | 6/15 [00:00<00:00, 16.25it/s]                                              {'loss': 0.7019, 'grad_norm': 0.11498004198074341, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.25it/s]                                              {'loss': 0.7309, 'grad_norm': 0.13261644542217255, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.25it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.84it/s]                                              {'loss': 0.717, 'grad_norm': 0.14238031208515167, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.84it/s]                                              {'loss': 0.7164, 'grad_norm': 0.10292408615350723, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.84it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.7093, 'grad_norm': 0.10800027847290039, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.715, 'grad_norm': 0.13168755173683167, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.34it/s] 80%|████████  | 12/15 [00:00<00:00, 15.84it/s]                                               {'loss': 0.7174, 'grad_norm': 0.12716323137283325, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.84it/s]                                               {'loss': 0.7034, 'grad_norm': 0.08807886391878128, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.84it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.696, 'grad_norm': 0.09537091106176376, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.7425, 'grad_norm': 0.2279617339372635, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.81it/s]                                               {'train_runtime': 1.0404, 'train_samples_per_second': 408.495, 'train_steps_per_second': 14.417, 'train_loss': 0.7179064710934957, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.81it/s]100%|██████████| 15/15 [00:01<00:00, 14.42it/s]
CLIENT:11
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7189, 'grad_norm': 0.12171562016010284, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.37it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.61it/s]                                              {'loss': 0.7411, 'grad_norm': 0.1739337295293808, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.61it/s]                                              {'loss': 0.6984, 'grad_norm': 0.0669565424323082, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.61it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.7308, 'grad_norm': 0.15158289670944214, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.7393, 'grad_norm': 0.13157886266708374, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.85it/s] 40%|████      | 6/15 [00:00<00:00, 15.79it/s]                                              {'loss': 0.7167, 'grad_norm': 0.09169099479913712, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.79it/s]                                              {'loss': 0.7287, 'grad_norm': 0.13644133508205414, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.79it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.7239, 'grad_norm': 0.11916539818048477, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.7191, 'grad_norm': 0.10583184659481049, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.49it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.7283, 'grad_norm': 0.1515667885541916, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.7165, 'grad_norm': 0.10172215849161148, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.21it/s] 80%|████████  | 12/15 [00:00<00:00, 15.75it/s]                                               {'loss': 0.733, 'grad_norm': 0.1345311552286148, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.75it/s]                                               {'loss': 0.725, 'grad_norm': 0.1407652646303177, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.75it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.7152, 'grad_norm': 0.08351512253284454, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.7435, 'grad_norm': 0.19361892342567444, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.71it/s]                                               {'train_runtime': 1.0533, 'train_samples_per_second': 403.485, 'train_steps_per_second': 14.241, 'train_loss': 0.7252189834912618, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.71it/s]100%|██████████| 15/15 [00:01<00:00, 14.25it/s]
CLIENT:40
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7154, 'grad_norm': 0.12759843468666077, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.58it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.24it/s]                                              {'loss': 0.7032, 'grad_norm': 0.11067593842744827, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.24it/s]                                              {'loss': 0.7287, 'grad_norm': 0.1622767448425293, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.24it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.7232, 'grad_norm': 0.12240590155124664, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.7321, 'grad_norm': 0.13673849403858185, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.04it/s] 40%|████      | 6/15 [00:00<00:00, 15.83it/s]                                              {'loss': 0.7082, 'grad_norm': 0.11763152480125427, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.83it/s]                                              {'loss': 0.7021, 'grad_norm': 0.12426178902387619, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.83it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.81it/s]                                              {'loss': 0.7179, 'grad_norm': 0.12013762444257736, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.81it/s]                                              {'loss': 0.7271, 'grad_norm': 0.16064158082008362, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.81it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.7379, 'grad_norm': 0.18928037583827972, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.7018, 'grad_norm': 0.04877885803580284, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.99it/s] 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.7416, 'grad_norm': 0.17510442435741425, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.7029, 'grad_norm': 0.13278326392173767, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.59it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.7056, 'grad_norm': 0.13718032836914062, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.7097, 'grad_norm': 0.11929579824209213, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.51it/s]                                               {'train_runtime': 1.0604, 'train_samples_per_second': 400.811, 'train_steps_per_second': 14.146, 'train_loss': 0.7171477715174357, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.51it/s]100%|██████████| 15/15 [00:01<00:00, 14.15it/s]
CLIENT:45
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7175, 'grad_norm': 0.1599874049425125, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.65it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.7442, 'grad_norm': 0.2125198096036911, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.7212, 'grad_norm': 0.09376926720142365, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.33it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.7321, 'grad_norm': 0.170872300863266, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.7377, 'grad_norm': 0.17753426730632782, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.33it/s] 40%|████      | 6/15 [00:00<00:00, 15.75it/s]                                              {'loss': 0.6903, 'grad_norm': 0.0971158891916275, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.75it/s]                                              {'loss': 0.7376, 'grad_norm': 0.1956445276737213, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.75it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.7148, 'grad_norm': 0.12446537613868713, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.7225, 'grad_norm': 0.1528117060661316, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.50it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.17it/s]                                               {'loss': 0.7118, 'grad_norm': 0.10434912145137787, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.17it/s]                                               {'loss': 0.7194, 'grad_norm': 0.18968774378299713, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.17it/s] 80%|████████  | 12/15 [00:00<00:00, 15.67it/s]                                               {'loss': 0.7416, 'grad_norm': 0.23874948918819427, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.67it/s]                                               {'loss': 0.7142, 'grad_norm': 0.14748336374759674, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.67it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.7273, 'grad_norm': 0.2066253423690796, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.7103, 'grad_norm': 0.13674934208393097, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.75it/s]                                               {'train_runtime': 1.0523, 'train_samples_per_second': 403.895, 'train_steps_per_second': 14.255, 'train_loss': 0.7228320161501567, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.75it/s]100%|██████████| 15/15 [00:01<00:00, 14.26it/s]
CLIENT:39
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7488, 'grad_norm': 0.1795402467250824, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.56it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.18it/s]                                              {'loss': 0.7053, 'grad_norm': 0.07126398384571075, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.18it/s]                                              {'loss': 0.7242, 'grad_norm': 0.1261661946773529, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.18it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.43it/s]                                              {'loss': 0.7203, 'grad_norm': 0.09735845774412155, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.43it/s]                                              {'loss': 0.7185, 'grad_norm': 0.13215042650699615, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.43it/s] 40%|████      | 6/15 [00:00<00:00, 16.16it/s]                                              {'loss': 0.7008, 'grad_norm': 0.14130181074142456, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.16it/s]                                              {'loss': 0.7089, 'grad_norm': 0.078983835875988, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.16it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.73, 'grad_norm': 0.14635969698429108, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.7219, 'grad_norm': 0.15930889546871185, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.70it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.7182, 'grad_norm': 0.12393514066934586, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.7194, 'grad_norm': 0.13758119940757751, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.16it/s] 80%|████████  | 12/15 [00:00<00:00, 15.88it/s]                                               {'loss': 0.708, 'grad_norm': 0.13236504793167114, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.88it/s]                                               {'loss': 0.7395, 'grad_norm': 0.17825840413570404, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.88it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.719, 'grad_norm': 0.10798946022987366, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.78it/s]                                               {'loss': 0.7116, 'grad_norm': 0.08879056572914124, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.78it/s]                                               {'train_runtime': 1.0458, 'train_samples_per_second': 406.401, 'train_steps_per_second': 14.344, 'train_loss': 0.7196279247601827, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.78it/s]100%|██████████| 15/15 [00:01<00:00, 14.35it/s]
CLIENT:62
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7295, 'grad_norm': 0.15674474835395813, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.25it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.7243, 'grad_norm': 0.13100160658359528, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.7549, 'grad_norm': 0.2585604786872864, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.48it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.70it/s]                                              {'loss': 0.7425, 'grad_norm': 0.19575773179531097, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.70it/s]                                              {'loss': 0.7174, 'grad_norm': 0.12433210015296936, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.70it/s] 40%|████      | 6/15 [00:00<00:00, 16.19it/s]                                              {'loss': 0.7316, 'grad_norm': 0.21939046680927277, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.19it/s]                                              {'loss': 0.725, 'grad_norm': 0.17573919892311096, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.19it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.82it/s]                                              {'loss': 0.7357, 'grad_norm': 0.18464025855064392, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.82it/s]                                              {'loss': 0.718, 'grad_norm': 0.17013701796531677, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.82it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.7397, 'grad_norm': 0.17490531504154205, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.7234, 'grad_norm': 0.17031385004520416, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.13it/s] 80%|████████  | 12/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.7373, 'grad_norm': 0.21004894375801086, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.7344, 'grad_norm': 0.18267278373241425, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.66it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.7284, 'grad_norm': 0.15586595237255096, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.758, 'grad_norm': 0.24465489387512207, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.13it/s]                                               {'train_runtime': 1.0321, 'train_samples_per_second': 411.791, 'train_steps_per_second': 14.534, 'train_loss': 0.7333397428194682, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.13it/s]100%|██████████| 15/15 [00:01<00:00, 14.54it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 35.14it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.91it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.96it/s] 42%|████▏     | 14/33 [00:00<00:00, 26.15it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.61it/s] 61%|██████    | 20/33 [00:00<00:00, 25.30it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.10it/s] 79%|███████▉  | 26/33 [00:01<00:00, 24.93it/s] 88%|████████▊ | 29/33 [00:01<00:00, 24.87it/s] 97%|█████████▋| 32/33 [00:01<00:00, 24.89it/s]100%|██████████| 33/33 [00:01<00:00, 26.01it/s]
{'eval_loss': 0.7180073857307434, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3096836049856184, 'eval_runtime': 1.3095, 'eval_samples_per_second': 796.468, 'eval_steps_per_second': 25.2}
ROUND:6
CLIENT:69
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7075, 'grad_norm': 0.10323841869831085, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.04it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.49it/s]                                              {'loss': 0.7143, 'grad_norm': 0.13032038509845734, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.49it/s]                                              {'loss': 0.7225, 'grad_norm': 0.13871274888515472, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.49it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.7055, 'grad_norm': 0.10007289797067642, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.7261, 'grad_norm': 0.133758544921875, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.91it/s] 40%|████      | 6/15 [00:00<00:00, 16.01it/s]                                              {'loss': 0.7394, 'grad_norm': 0.1781519502401352, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.01it/s]                                              {'loss': 0.7081, 'grad_norm': 0.1257915049791336, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.01it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.7063, 'grad_norm': 0.11234875023365021, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.7211, 'grad_norm': 0.1493847370147705, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.90it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.7324, 'grad_norm': 0.14757151901721954, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.7015, 'grad_norm': 0.12377955764532089, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.28it/s] 80%|████████  | 12/15 [00:00<00:00, 15.83it/s]                                               {'loss': 0.7076, 'grad_norm': 0.10900215804576874, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.83it/s]                                               {'loss': 0.7128, 'grad_norm': 0.1038387194275856, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.83it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.7115, 'grad_norm': 0.10189736634492874, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.7338, 'grad_norm': 0.1979178935289383, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.67it/s]                                               {'train_runtime': 1.0498, 'train_samples_per_second': 404.835, 'train_steps_per_second': 14.288, 'train_loss': 0.7166827201843262, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.67it/s]100%|██████████| 15/15 [00:01<00:00, 14.29it/s]
CLIENT:74
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7068, 'grad_norm': 0.07339857518672943, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.13it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.58it/s]                                              {'loss': 0.7182, 'grad_norm': 0.1460355520248413, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.58it/s]                                              {'loss': 0.7304, 'grad_norm': 0.1831599920988083, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.58it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.7307, 'grad_norm': 0.14616520702838898, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.6932, 'grad_norm': 0.06210557743906975, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.63it/s] 40%|████      | 6/15 [00:00<00:00, 16.71it/s]                                              {'loss': 0.7352, 'grad_norm': 0.20916211605072021, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.71it/s]                                              {'loss': 0.7106, 'grad_norm': 0.10593746602535248, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.71it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.10it/s]                                              {'loss': 0.7192, 'grad_norm': 0.1686129868030548, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.10it/s]                                              {'loss': 0.7147, 'grad_norm': 0.11575854569673538, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.10it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.7018, 'grad_norm': 0.11350136250257492, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.7182, 'grad_norm': 0.15974195301532745, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.51it/s] 80%|████████  | 12/15 [00:00<00:00, 15.92it/s]                                               {'loss': 0.7271, 'grad_norm': 0.11345315724611282, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.92it/s]                                               {'loss': 0.7114, 'grad_norm': 0.0693771243095398, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.92it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.7271, 'grad_norm': 0.18683654069900513, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.7122, 'grad_norm': 0.151229590177536, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.87it/s]                                               {'train_runtime': 1.0337, 'train_samples_per_second': 411.14, 'train_steps_per_second': 14.511, 'train_loss': 0.717131233215332, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.87it/s]100%|██████████| 15/15 [00:01<00:00, 14.52it/s]
CLIENT:34
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7018, 'grad_norm': 0.06320458650588989, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.35it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.7279, 'grad_norm': 0.12574754655361176, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.7368, 'grad_norm': 0.18682792782783508, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.91it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.02it/s]                                              {'loss': 0.7094, 'grad_norm': 0.08141912519931793, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.02it/s]                                              {'loss': 0.7481, 'grad_norm': 0.2121283859014511, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.02it/s] 40%|████      | 6/15 [00:00<00:00, 16.38it/s]                                              {'loss': 0.6971, 'grad_norm': 0.064127616584301, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.38it/s]                                              {'loss': 0.7351, 'grad_norm': 0.18090061843395233, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.38it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.72, 'grad_norm': 0.11541728675365448, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.6885, 'grad_norm': 0.05836944654583931, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.90it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.67it/s]                                               {'loss': 0.719, 'grad_norm': 0.13434700667858124, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.67it/s]                                               {'loss': 0.7111, 'grad_norm': 0.10276278108358383, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.67it/s] 80%|████████  | 12/15 [00:00<00:00, 16.28it/s]                                               {'loss': 0.7209, 'grad_norm': 0.11494132876396179, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.28it/s]                                               {'loss': 0.7103, 'grad_norm': 0.10520263016223907, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.28it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.7227, 'grad_norm': 0.12618966400623322, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.7153, 'grad_norm': 0.13729336857795715, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.02it/s]                                               {'train_runtime': 1.0243, 'train_samples_per_second': 414.932, 'train_steps_per_second': 14.645, 'train_loss': 0.7176065842310587, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.02it/s]100%|██████████| 15/15 [00:01<00:00, 14.65it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.738, 'grad_norm': 0.1620146632194519, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.33it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.67it/s]                                              {'loss': 0.7216, 'grad_norm': 0.08222812414169312, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.67it/s]                                              {'loss': 0.6796, 'grad_norm': 0.06665818393230438, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.67it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.99it/s]                                              {'loss': 0.709, 'grad_norm': 0.08976190537214279, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.99it/s]                                              {'loss': 0.7167, 'grad_norm': 0.07425802946090698, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.99it/s] 40%|████      | 6/15 [00:00<00:00, 16.02it/s]                                              {'loss': 0.7123, 'grad_norm': 0.10529076308012009, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.02it/s]                                              {'loss': 0.7127, 'grad_norm': 0.10377296060323715, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.02it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.7099, 'grad_norm': 0.08665281534194946, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.7157, 'grad_norm': 0.09492918103933334, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.59it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.704, 'grad_norm': 0.059078436344861984, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.7055, 'grad_norm': 0.08933389186859131, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.13it/s] 80%|████████  | 12/15 [00:00<00:00, 16.04it/s]                                               {'loss': 0.7168, 'grad_norm': 0.1568780541419983, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.04it/s]                                               {'loss': 0.7194, 'grad_norm': 0.11627756059169769, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.04it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.7095, 'grad_norm': 0.08480829000473022, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.7101, 'grad_norm': 0.08641398698091507, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.64it/s]                                               {'train_runtime': 1.0531, 'train_samples_per_second': 403.567, 'train_steps_per_second': 14.244, 'train_loss': 0.7120597084363302, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.64it/s]100%|██████████| 15/15 [00:01<00:00, 14.25it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6982, 'grad_norm': 0.12117672711610794, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.36it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.80it/s]                                              {'loss': 0.7364, 'grad_norm': 0.19030602276325226, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.80it/s]                                              {'loss': 0.689, 'grad_norm': 0.05605597048997879, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.80it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.6964, 'grad_norm': 0.10641639679670334, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.7201, 'grad_norm': 0.13699904084205627, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.97it/s] 40%|████      | 6/15 [00:00<00:00, 16.29it/s]                                              {'loss': 0.7261, 'grad_norm': 0.16588763892650604, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.29it/s]                                              {'loss': 0.7194, 'grad_norm': 0.15554890036582947, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.29it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.83it/s]                                              {'loss': 0.7121, 'grad_norm': 0.1380724310874939, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.83it/s]                                              {'loss': 0.7045, 'grad_norm': 0.0935603454709053, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.83it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.7193, 'grad_norm': 0.10768093913793564, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.711, 'grad_norm': 0.1493726223707199, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.26it/s] 80%|████████  | 12/15 [00:00<00:00, 15.74it/s]                                               {'loss': 0.7203, 'grad_norm': 0.1338721066713333, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.74it/s]                                               {'loss': 0.7052, 'grad_norm': 0.12482127547264099, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.74it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.7277, 'grad_norm': 0.15539248287677765, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.7005, 'grad_norm': 0.12808483839035034, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.70it/s]                                               {'train_runtime': 1.0418, 'train_samples_per_second': 407.937, 'train_steps_per_second': 14.398, 'train_loss': 0.7124039371808369, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.70it/s]100%|██████████| 15/15 [00:01<00:00, 14.40it/s]
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7211, 'grad_norm': 0.1632601022720337, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.36it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.82it/s]                                              {'loss': 0.7259, 'grad_norm': 0.1619042009115219, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.82it/s]                                              {'loss': 0.709, 'grad_norm': 0.07777664810419083, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.82it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.7255, 'grad_norm': 0.14959374070167542, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.7269, 'grad_norm': 0.147703155875206, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.44it/s] 40%|████      | 6/15 [00:00<00:00, 16.10it/s]                                              {'loss': 0.7119, 'grad_norm': 0.11775509268045425, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.10it/s]                                              {'loss': 0.7158, 'grad_norm': 0.14602065086364746, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.10it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.93it/s]                                              {'loss': 0.7083, 'grad_norm': 0.14909018576145172, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.93it/s]                                              {'loss': 0.7146, 'grad_norm': 0.13598819077014923, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.93it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.7117, 'grad_norm': 0.12260707467794418, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.7001, 'grad_norm': 0.11717825382947922, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.69it/s] 80%|████████  | 12/15 [00:00<00:00, 15.87it/s]                                               {'loss': 0.7393, 'grad_norm': 0.21804523468017578, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.87it/s]                                               {'loss': 0.723, 'grad_norm': 0.15282340347766876, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.87it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.715, 'grad_norm': 0.1440315693616867, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.7272, 'grad_norm': 0.15666767954826355, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.76it/s]                                               {'train_runtime': 1.038, 'train_samples_per_second': 409.433, 'train_steps_per_second': 14.451, 'train_loss': 0.718345566590627, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.76it/s]100%|██████████| 15/15 [00:01<00:00, 14.46it/s]
CLIENT:32
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7101, 'grad_norm': 0.11404670029878616, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.23it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.7319, 'grad_norm': 0.1869431585073471, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.7221, 'grad_norm': 0.15052220225334167, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.28it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.728, 'grad_norm': 0.16570720076560974, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.7312, 'grad_norm': 0.20052304863929749, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.51it/s] 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.6929, 'grad_norm': 0.09543291479349136, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.03it/s]                                              {'loss': 0.7316, 'grad_norm': 0.20456019043922424, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.03it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.7037, 'grad_norm': 0.10390474647283554, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.7258, 'grad_norm': 0.15916380286216736, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.63it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.723, 'grad_norm': 0.19686786830425262, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7067, 'grad_norm': 0.14998416602611542, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.12it/s] 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.7268, 'grad_norm': 0.12115904688835144, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.705, 'grad_norm': 0.12386313825845718, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.62it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.7188, 'grad_norm': 0.19500590860843658, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.7237, 'grad_norm': 0.18385756015777588, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.61it/s]                                               {'train_runtime': 1.0536, 'train_samples_per_second': 403.385, 'train_steps_per_second': 14.237, 'train_loss': 0.7187487483024597, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.61it/s]100%|██████████| 15/15 [00:01<00:00, 14.24it/s]
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7331, 'grad_norm': 0.17414076626300812, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.20it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.55it/s]                                              {'loss': 0.7378, 'grad_norm': 0.16873086988925934, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.55it/s]                                              {'loss': 0.7253, 'grad_norm': 0.1132998913526535, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.55it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.7197, 'grad_norm': 0.1455448418855667, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.7352, 'grad_norm': 0.13980674743652344, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.59it/s] 40%|████      | 6/15 [00:00<00:00, 16.57it/s]                                              {'loss': 0.7323, 'grad_norm': 0.22937920689582825, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.57it/s]                                              {'loss': 0.7295, 'grad_norm': 0.1468363255262375, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.57it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.7188, 'grad_norm': 0.14439840614795685, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.7338, 'grad_norm': 0.22665049135684967, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.68it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7304, 'grad_norm': 0.17135877907276154, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7259, 'grad_norm': 0.19392557442188263, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.12it/s] 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.7211, 'grad_norm': 0.13396963477134705, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.7339, 'grad_norm': 0.1997356414794922, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.62it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.7308, 'grad_norm': 0.16992805898189545, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.7281, 'grad_norm': 0.15026028454303741, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.61it/s]                                               {'train_runtime': 1.0516, 'train_samples_per_second': 404.144, 'train_steps_per_second': 14.264, 'train_loss': 0.7290549238522848, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.61it/s]100%|██████████| 15/15 [00:01<00:00, 14.27it/s]
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.721, 'grad_norm': 0.13172082602977753, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.11it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.43it/s]                                              {'loss': 0.6988, 'grad_norm': 0.07998418062925339, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.43it/s]                                              {'loss': 0.7315, 'grad_norm': 0.13373811542987823, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.43it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.86it/s]                                              {'loss': 0.7067, 'grad_norm': 0.05463870242238045, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.86it/s]                                              {'loss': 0.7251, 'grad_norm': 0.17406587302684784, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.86it/s] 40%|████      | 6/15 [00:00<00:00, 15.79it/s]                                              {'loss': 0.7032, 'grad_norm': 0.12350127846002579, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.79it/s]                                              {'loss': 0.7169, 'grad_norm': 0.08836326748132706, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.79it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.7156, 'grad_norm': 0.14352917671203613, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.7144, 'grad_norm': 0.11606205254793167, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.42it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.7155, 'grad_norm': 0.07602233439683914, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.7231, 'grad_norm': 0.14032037556171417, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.29it/s] 80%|████████  | 12/15 [00:00<00:00, 16.06it/s]                                               {'loss': 0.729, 'grad_norm': 0.15891309082508087, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.06it/s]                                               {'loss': 0.7372, 'grad_norm': 0.19627514481544495, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.06it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.7074, 'grad_norm': 0.08098775893449783, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.6952, 'grad_norm': 0.06238573044538498, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.87it/s]                                               {'train_runtime': 1.0481, 'train_samples_per_second': 405.501, 'train_steps_per_second': 14.312, 'train_loss': 0.71605357726415, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.87it/s]100%|██████████| 15/15 [00:01<00:00, 14.32it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7249, 'grad_norm': 0.19151145219802856, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.43it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.7407, 'grad_norm': 0.20782676339149475, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.7137, 'grad_norm': 0.1388861984014511, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.15it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.79it/s]                                              {'loss': 0.7266, 'grad_norm': 0.16835810244083405, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.79it/s]                                              {'loss': 0.7316, 'grad_norm': 0.2130628377199173, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.79it/s] 40%|████      | 6/15 [00:00<00:00, 15.82it/s]                                              {'loss': 0.7261, 'grad_norm': 0.1888805776834488, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.82it/s]                                              {'loss': 0.7496, 'grad_norm': 0.23577797412872314, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.82it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.7302, 'grad_norm': 0.21788029372692108, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.7216, 'grad_norm': 0.11665251851081848, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.57it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7286, 'grad_norm': 0.17188245058059692, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7182, 'grad_norm': 0.21668244898319244, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.12it/s] 80%|████████  | 12/15 [00:00<00:00, 15.61it/s]                                               {'loss': 0.7423, 'grad_norm': 0.20742371678352356, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.61it/s]                                               {'loss': 0.7247, 'grad_norm': 0.18903695046901703, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.61it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.7291, 'grad_norm': 0.1984660029411316, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.7391, 'grad_norm': 0.22810213267803192, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.98it/s]                                               {'train_runtime': 1.0474, 'train_samples_per_second': 405.769, 'train_steps_per_second': 14.321, 'train_loss': 0.7298088033994039, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.98it/s]100%|██████████| 15/15 [00:01<00:00, 14.33it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 36.97it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.56it/s] 33%|███▎      | 11/33 [00:00<00:00, 27.35it/s] 42%|████▏     | 14/33 [00:00<00:00, 26.27it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.60it/s] 61%|██████    | 20/33 [00:00<00:00, 25.17it/s] 70%|██████▉   | 23/33 [00:00<00:00, 24.97it/s] 79%|███████▉  | 26/33 [00:01<00:00, 24.84it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.09it/s] 97%|█████████▋| 32/33 [00:01<00:00, 24.97it/s]100%|██████████| 33/33 [00:01<00:00, 26.08it/s]
{'eval_loss': 0.71683669090271, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.31064237775647174, 'eval_runtime': 1.3062, 'eval_samples_per_second': 798.469, 'eval_steps_per_second': 25.263}
ROUND:7
CLIENT:71
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7019, 'grad_norm': 0.126055508852005, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.45it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.88it/s]                                              {'loss': 0.7006, 'grad_norm': 0.05965900421142578, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.88it/s]                                              {'loss': 0.7118, 'grad_norm': 0.10028774291276932, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.88it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.7223, 'grad_norm': 0.14940589666366577, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.6907, 'grad_norm': 0.05039017274975777, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.19it/s] 40%|████      | 6/15 [00:00<00:00, 16.00it/s]                                              {'loss': 0.7164, 'grad_norm': 0.13880088925361633, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.00it/s]                                              {'loss': 0.6981, 'grad_norm': 0.062006670981645584, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.00it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.7035, 'grad_norm': 0.11037299782037735, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.7085, 'grad_norm': 0.1076405718922615, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.57it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.7163, 'grad_norm': 0.10894240438938141, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.7017, 'grad_norm': 0.08574001491069794, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.22it/s] 80%|████████  | 12/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.6969, 'grad_norm': 0.07132133096456528, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.7199, 'grad_norm': 0.13650746643543243, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.85it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.7078, 'grad_norm': 0.055737100541591644, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.7183, 'grad_norm': 0.10117190331220627, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.93it/s]                                               {'train_runtime': 1.0441, 'train_samples_per_second': 407.047, 'train_steps_per_second': 14.366, 'train_loss': 0.7076612909634908, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.93it/s]100%|██████████| 15/15 [00:01<00:00, 14.37it/s]
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6971, 'grad_norm': 0.08758202195167542, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.17it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.7145, 'grad_norm': 0.1862480342388153, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.7254, 'grad_norm': 0.18449945747852325, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.32it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.7208, 'grad_norm': 0.12373387813568115, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.7355, 'grad_norm': 0.2132474035024643, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.56it/s] 40%|████      | 6/15 [00:00<00:00, 15.96it/s]                                              {'loss': 0.7178, 'grad_norm': 0.1336159110069275, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.96it/s]                                              {'loss': 0.7345, 'grad_norm': 0.22138062119483948, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.96it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.711, 'grad_norm': 0.12370790541172028, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.706, 'grad_norm': 0.08466050028800964, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.72it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.7193, 'grad_norm': 0.11472205817699432, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.7054, 'grad_norm': 0.17351555824279785, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.19it/s] 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.7391, 'grad_norm': 0.18757370114326477, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.7207, 'grad_norm': 0.132003054022789, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.62it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.7239, 'grad_norm': 0.16501447558403015, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.7193, 'grad_norm': 0.18631725013256073, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.19it/s]                                               {'train_runtime': 1.0308, 'train_samples_per_second': 412.311, 'train_steps_per_second': 14.552, 'train_loss': 0.7193674445152283, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.19it/s]100%|██████████| 15/15 [00:01<00:00, 14.56it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7086, 'grad_norm': 0.1086597889661789, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.02it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.80it/s]                                              {'loss': 0.7158, 'grad_norm': 0.18238486349582672, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.80it/s]                                              {'loss': 0.7335, 'grad_norm': 0.2204989492893219, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.80it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.7133, 'grad_norm': 0.11556194722652435, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.7341, 'grad_norm': 0.21556740999221802, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.20it/s] 40%|████      | 6/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.7257, 'grad_norm': 0.15952971577644348, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.7419, 'grad_norm': 0.20565292239189148, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.24it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.7078, 'grad_norm': 0.13752597570419312, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.7096, 'grad_norm': 0.15639886260032654, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.24it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.731, 'grad_norm': 0.17086143791675568, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.7119, 'grad_norm': 0.16116449236869812, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7134, 'grad_norm': 0.17185884714126587, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7081, 'grad_norm': 0.1261902004480362, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.38it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.7221, 'grad_norm': 0.2027428299188614, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.7351, 'grad_norm': 0.23167242109775543, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.47it/s]                                               {'train_runtime': 1.0791, 'train_samples_per_second': 393.841, 'train_steps_per_second': 13.9, 'train_loss': 0.7207868615786235, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.47it/s]100%|██████████| 15/15 [00:01<00:00, 13.90it/s]
CLIENT:10
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.743, 'grad_norm': 0.15097776055335999, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.13it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.48it/s]                                              {'loss': 0.7248, 'grad_norm': 0.10610897094011307, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.48it/s]                                              {'loss': 0.7263, 'grad_norm': 0.14068040251731873, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.48it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.7197, 'grad_norm': 0.11742982268333435, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.7192, 'grad_norm': 0.11987753212451935, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.66it/s] 40%|████      | 6/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.723, 'grad_norm': 0.1671799272298813, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.7137, 'grad_norm': 0.12317530065774918, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.23it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.7278, 'grad_norm': 0.12021785974502563, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.7314, 'grad_norm': 0.18315239250659943, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.27it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.718, 'grad_norm': 0.12193778902292252, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.7192, 'grad_norm': 0.17208515107631683, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.23it/s] 80%|████████  | 12/15 [00:00<00:00, 15.95it/s]                                               {'loss': 0.7178, 'grad_norm': 0.10046805441379547, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.95it/s]                                               {'loss': 0.7288, 'grad_norm': 0.15922078490257263, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.95it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.7175, 'grad_norm': 0.11533690243959427, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.7354, 'grad_norm': 0.1696123480796814, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.80it/s]                                               {'train_runtime': 1.0559, 'train_samples_per_second': 402.508, 'train_steps_per_second': 14.206, 'train_loss': 0.7243772784868876, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.80it/s]100%|██████████| 15/15 [00:01<00:00, 14.21it/s]
CLIENT:44
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7016, 'grad_norm': 0.0710161104798317, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.38it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.13it/s]                                              {'loss': 0.6872, 'grad_norm': 0.05459297075867653, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.13it/s]                                              {'loss': 0.7075, 'grad_norm': 0.07923237234354019, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.13it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.7034, 'grad_norm': 0.08412304520606995, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.6971, 'grad_norm': 0.06501933187246323, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.91it/s] 40%|████      | 6/15 [00:00<00:00, 15.61it/s]                                              {'loss': 0.6967, 'grad_norm': 0.05962614715099335, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.61it/s]                                              {'loss': 0.7329, 'grad_norm': 0.12480760365724564, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.61it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.44it/s]                                              {'loss': 0.686, 'grad_norm': 0.07975686341524124, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.44it/s]                                              {'loss': 0.7144, 'grad_norm': 0.12215346097946167, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.44it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.7001, 'grad_norm': 0.07713059335947037, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.7114, 'grad_norm': 0.10554318130016327, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.32it/s] 80%|████████  | 12/15 [00:00<00:00, 15.75it/s]                                               {'loss': 0.7093, 'grad_norm': 0.08961904048919678, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.75it/s]                                               {'loss': 0.6717, 'grad_norm': 0.06024845689535141, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.75it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.7084, 'grad_norm': 0.10531691461801529, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.7033, 'grad_norm': 0.10886134207248688, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.62it/s]                                               {'train_runtime': 1.0588, 'train_samples_per_second': 401.388, 'train_steps_per_second': 14.167, 'train_loss': 0.702079427242279, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.62it/s]100%|██████████| 15/15 [00:01<00:00, 14.17it/s]
CLIENT:34
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7014, 'grad_norm': 0.06516891717910767, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.24it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.89it/s]                                              {'loss': 0.7267, 'grad_norm': 0.1309347301721573, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.89it/s]                                              {'loss': 0.7351, 'grad_norm': 0.1937994360923767, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.89it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.7087, 'grad_norm': 0.08333399891853333, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.746, 'grad_norm': 0.22151660919189453, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.32it/s] 40%|████      | 6/15 [00:00<00:00, 15.84it/s]                                              {'loss': 0.6969, 'grad_norm': 0.0652615949511528, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.84it/s]                                              {'loss': 0.7333, 'grad_norm': 0.1885279268026352, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.84it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.7189, 'grad_norm': 0.12017226964235306, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.6886, 'grad_norm': 0.05898777395486832, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.58it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.7177, 'grad_norm': 0.14001061022281647, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.7101, 'grad_norm': 0.10673324763774872, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.31it/s] 80%|████████  | 12/15 [00:00<00:00, 15.67it/s]                                               {'loss': 0.7199, 'grad_norm': 0.11952737718820572, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.67it/s]                                               {'loss': 0.7093, 'grad_norm': 0.10985175520181656, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.67it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.7215, 'grad_norm': 0.1313718557357788, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.714, 'grad_norm': 0.14326801896095276, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.60it/s]                                               {'train_runtime': 1.0614, 'train_samples_per_second': 400.415, 'train_steps_per_second': 14.132, 'train_loss': 0.7165371934572856, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.60it/s]100%|██████████| 15/15 [00:01<00:00, 14.14it/s]
CLIENT:24
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.716, 'grad_norm': 0.131478950381279, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.47it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.17it/s]                                              {'loss': 0.7132, 'grad_norm': 0.09915239363908768, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.17it/s]                                              {'loss': 0.7267, 'grad_norm': 0.16526669263839722, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.17it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.7335, 'grad_norm': 0.14997868239879608, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.7194, 'grad_norm': 0.11095075309276581, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.37it/s] 40%|████      | 6/15 [00:00<00:00, 16.02it/s]                                              {'loss': 0.7024, 'grad_norm': 0.14147479832172394, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.02it/s]                                              {'loss': 0.7072, 'grad_norm': 0.11043402552604675, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.02it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.7152, 'grad_norm': 0.14200417697429657, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.7037, 'grad_norm': 0.11633364111185074, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.35it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.72, 'grad_norm': 0.14368268847465515, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.6999, 'grad_norm': 0.08801934123039246, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.95it/s] 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.7115, 'grad_norm': 0.1702824980020523, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.7276, 'grad_norm': 0.1690128892660141, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.46it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.7125, 'grad_norm': 0.12480171769857407, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.7051, 'grad_norm': 0.08051316440105438, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.53it/s]                                               {'train_runtime': 1.0637, 'train_samples_per_second': 399.543, 'train_steps_per_second': 14.102, 'train_loss': 0.7142422080039978, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.53it/s]100%|██████████| 15/15 [00:01<00:00, 14.11it/s]
CLIENT:98
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7329, 'grad_norm': 0.20498870313167572, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.13it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.50it/s]                                              {'loss': 0.7023, 'grad_norm': 0.08730633556842804, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.50it/s]                                              {'loss': 0.7243, 'grad_norm': 0.1964305341243744, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.50it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.82it/s]                                              {'loss': 0.7177, 'grad_norm': 0.18386690318584442, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.82it/s]                                              {'loss': 0.7079, 'grad_norm': 0.10921385139226913, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.82it/s] 40%|████      | 6/15 [00:00<00:00, 15.81it/s]                                              {'loss': 0.7187, 'grad_norm': 0.16354380548000336, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.81it/s]                                              {'loss': 0.7003, 'grad_norm': 0.09114769101142883, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.81it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.7141, 'grad_norm': 0.15445680916309357, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.7328, 'grad_norm': 0.24280668795108795, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.60it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.7289, 'grad_norm': 0.15796226263046265, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.7129, 'grad_norm': 0.16763989627361298, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.51it/s] 80%|████████  | 12/15 [00:00<00:00, 16.04it/s]                                               {'loss': 0.7248, 'grad_norm': 0.16312751173973083, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.04it/s]                                               {'loss': 0.7154, 'grad_norm': 0.15277324616909027, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.04it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.7141, 'grad_norm': 0.16932205855846405, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.7169, 'grad_norm': 0.1471634954214096, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.71it/s]                                               {'train_runtime': 1.0469, 'train_samples_per_second': 405.959, 'train_steps_per_second': 14.328, 'train_loss': 0.7176114797592164, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.71it/s]100%|██████████| 15/15 [00:01<00:00, 14.33it/s]
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7183, 'grad_norm': 0.10843753814697266, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.66it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.22it/s]                                              {'loss': 0.7105, 'grad_norm': 0.09912937879562378, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.22it/s]                                              {'loss': 0.6632, 'grad_norm': 0.10463624447584152, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.22it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.7031, 'grad_norm': 0.048779115080833435, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.7056, 'grad_norm': 0.07784303277730942, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.43it/s] 40%|████      | 6/15 [00:00<00:00, 15.39it/s]                                              {'loss': 0.7231, 'grad_norm': 0.11813587695360184, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.39it/s]                                              {'loss': 0.7109, 'grad_norm': 0.08344554156064987, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.39it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.6906, 'grad_norm': 0.058322690427303314, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.7047, 'grad_norm': 0.11744430661201477, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.28it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.6874, 'grad_norm': 0.051127951592206955, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.6931, 'grad_norm': 0.07082417607307434, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.84it/s] 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6994, 'grad_norm': 0.1267397403717041, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.7101, 'grad_norm': 0.08829452842473984, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.43it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6894, 'grad_norm': 0.04673714190721512, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.726, 'grad_norm': 0.1047644317150116, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]                                               {'train_runtime': 1.0724, 'train_samples_per_second': 396.323, 'train_steps_per_second': 13.988, 'train_loss': 0.7023589173952739, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]100%|██████████| 15/15 [00:01<00:00, 13.99it/s]
CLIENT:95
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.692, 'grad_norm': 0.08621132373809814, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.42it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.12it/s]                                              {'loss': 0.7137, 'grad_norm': 0.15883859992027283, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.12it/s]                                              {'loss': 0.7199, 'grad_norm': 0.15435965359210968, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.12it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.7294, 'grad_norm': 0.1798849254846573, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.6944, 'grad_norm': 0.09759600460529327, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.32it/s] 40%|████      | 6/15 [00:00<00:00, 16.32it/s]                                              {'loss': 0.7017, 'grad_norm': 0.1092296838760376, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.32it/s]                                              {'loss': 0.7136, 'grad_norm': 0.13130144774913788, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.32it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.7177, 'grad_norm': 0.13465319573879242, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.7036, 'grad_norm': 0.1285400092601776, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.78it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.17it/s]                                               {'loss': 0.6942, 'grad_norm': 0.09699118137359619, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.17it/s]                                               {'loss': 0.7036, 'grad_norm': 0.1678280532360077, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.17it/s] 80%|████████  | 12/15 [00:00<00:00, 15.82it/s]                                               {'loss': 0.715, 'grad_norm': 0.15268263220787048, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.82it/s]                                               {'loss': 0.6997, 'grad_norm': 0.0908571183681488, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.82it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.7076, 'grad_norm': 0.10555022954940796, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.7385, 'grad_norm': 0.2662924528121948, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.74it/s]                                               {'train_runtime': 1.047, 'train_samples_per_second': 405.924, 'train_steps_per_second': 14.327, 'train_loss': 0.7096480607986451, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.74it/s]100%|██████████| 15/15 [00:01<00:00, 14.33it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 33.26it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.80it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.54it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.83it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.27it/s] 61%|██████    | 20/33 [00:00<00:00, 25.02it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.19it/s] 79%|███████▉  | 26/33 [00:01<00:00, 24.93it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.11it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.16it/s]100%|██████████| 33/33 [00:01<00:00, 25.91it/s]
{'eval_loss': 0.7157673239707947, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.311601150527325, 'eval_runtime': 1.3145, 'eval_samples_per_second': 793.46, 'eval_steps_per_second': 25.105}
ROUND:8
CLIENT:2
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7132, 'grad_norm': 0.11291710287332535, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.94it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.60it/s]                                              {'loss': 0.714, 'grad_norm': 0.16457298398017883, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.60it/s]                                              {'loss': 0.7132, 'grad_norm': 0.14568984508514404, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.60it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.7239, 'grad_norm': 0.18313662707805634, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.7146, 'grad_norm': 0.10360941290855408, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.23it/s] 40%|████      | 6/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.7025, 'grad_norm': 0.0915461927652359, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.7116, 'grad_norm': 0.1154773086309433, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.26it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7161, 'grad_norm': 0.1186579018831253, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7139, 'grad_norm': 0.18534018099308014, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.25it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.7303, 'grad_norm': 0.201507106423378, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.7104, 'grad_norm': 0.0898207426071167, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.82it/s] 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7032, 'grad_norm': 0.09311816096305847, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7159, 'grad_norm': 0.13453294336795807, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.38it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.66it/s]                                               {'loss': 0.7153, 'grad_norm': 0.13108298182487488, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.66it/s]                                               {'loss': 0.7227, 'grad_norm': 0.14576658606529236, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.66it/s]                                               {'train_runtime': 1.0739, 'train_samples_per_second': 395.769, 'train_steps_per_second': 13.968, 'train_loss': 0.7147335807482401, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.66it/s]100%|██████████| 15/15 [00:01<00:00, 13.97it/s]
CLIENT:49
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7283, 'grad_norm': 0.15446698665618896, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.13it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.7168, 'grad_norm': 0.11060868203639984, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.7389, 'grad_norm': 0.22891941666603088, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.29it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.7203, 'grad_norm': 0.15776143968105316, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.7133, 'grad_norm': 0.14053112268447876, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.32it/s] 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.7267, 'grad_norm': 0.17337492108345032, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.7264, 'grad_norm': 0.1932488977909088, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.97it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.7206, 'grad_norm': 0.15419816970825195, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.7081, 'grad_norm': 0.1281813234090805, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.53it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.7217, 'grad_norm': 0.22008976340293884, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.7044, 'grad_norm': 0.12963198125362396, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.05it/s] 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.6977, 'grad_norm': 0.11469264328479767, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.7262, 'grad_norm': 0.14663133025169373, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.62it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.7135, 'grad_norm': 0.19549627602100372, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.7098, 'grad_norm': 0.1728106439113617, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.54it/s]                                               {'train_runtime': 1.0541, 'train_samples_per_second': 403.178, 'train_steps_per_second': 14.23, 'train_loss': 0.7181859612464905, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.54it/s]100%|██████████| 15/15 [00:01<00:00, 14.23it/s]
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7056, 'grad_norm': 0.10783781856298447, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.60it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.13it/s]                                              {'loss': 0.7119, 'grad_norm': 0.10387057811021805, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.13it/s]                                              {'loss': 0.7435, 'grad_norm': 0.25255778431892395, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.13it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.7352, 'grad_norm': 0.1906941831111908, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.7096, 'grad_norm': 0.14868168532848358, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.36it/s] 40%|████      | 6/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.7042, 'grad_norm': 0.11032869666814804, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.6938, 'grad_norm': 0.11864728480577469, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.30it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.7241, 'grad_norm': 0.16370956599712372, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.7163, 'grad_norm': 0.1838548481464386, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.74it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.7231, 'grad_norm': 0.13940025866031647, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.6909, 'grad_norm': 0.13583756983280182, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.20it/s] 80%|████████  | 12/15 [00:00<00:00, 15.79it/s]                                               {'loss': 0.7296, 'grad_norm': 0.17230063676834106, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.79it/s]                                               {'loss': 0.715, 'grad_norm': 0.12819810211658478, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.79it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.7161, 'grad_norm': 0.18071378767490387, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.7104, 'grad_norm': 0.13726097345352173, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.67it/s]                                               {'train_runtime': 1.0601, 'train_samples_per_second': 400.895, 'train_steps_per_second': 14.149, 'train_loss': 0.7152809977531434, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.67it/s]100%|██████████| 15/15 [00:01<00:00, 14.15it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7116, 'grad_norm': 0.10738760977983475, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.15it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.49it/s]                                              {'loss': 0.7208, 'grad_norm': 0.16852086782455444, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.49it/s]                                              {'loss': 0.7151, 'grad_norm': 0.17790813744068146, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.49it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.67it/s]                                              {'loss': 0.7133, 'grad_norm': 0.09234175086021423, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.67it/s]                                              {'loss': 0.7436, 'grad_norm': 0.25525712966918945, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.67it/s] 40%|████      | 6/15 [00:00<00:00, 15.60it/s]                                              {'loss': 0.7122, 'grad_norm': 0.12272419035434723, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.60it/s]                                              {'loss': 0.7176, 'grad_norm': 0.19550468027591705, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.60it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.17it/s]                                              {'loss': 0.7146, 'grad_norm': 0.13141849637031555, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.17it/s]                                              {'loss': 0.7118, 'grad_norm': 0.1432432383298874, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.17it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.7354, 'grad_norm': 0.15973708033561707, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.7042, 'grad_norm': 0.17275165021419525, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.7125, 'grad_norm': 0.1469382792711258, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.7205, 'grad_norm': 0.1632182002067566, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.37it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.7244, 'grad_norm': 0.17675156891345978, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.7065, 'grad_norm': 0.16025562584400177, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.80it/s]                                               {'train_runtime': 1.0618, 'train_samples_per_second': 400.261, 'train_steps_per_second': 14.127, 'train_loss': 0.7175995548566182, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.80it/s]100%|██████████| 15/15 [00:01<00:00, 14.13it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7078, 'grad_norm': 0.11199735850095749, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.84it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.7142, 'grad_norm': 0.18974994122982025, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.7315, 'grad_norm': 0.22975894808769226, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.09it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.7123, 'grad_norm': 0.11929658055305481, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.7322, 'grad_norm': 0.22329366207122803, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.31it/s] 40%|████      | 6/15 [00:00<00:00, 15.80it/s]                                              {'loss': 0.7243, 'grad_norm': 0.1657484471797943, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.80it/s]                                              {'loss': 0.74, 'grad_norm': 0.21422620117664337, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.80it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.7067, 'grad_norm': 0.1425677388906479, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.7082, 'grad_norm': 0.163102388381958, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.59it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.7295, 'grad_norm': 0.17770510911941528, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.7104, 'grad_norm': 0.16781958937644958, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.13it/s] 80%|████████  | 12/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.7119, 'grad_norm': 0.1786852478981018, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.7071, 'grad_norm': 0.13055852055549622, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.55it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.7202, 'grad_norm': 0.2112637162208557, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.733, 'grad_norm': 0.2411998063325882, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.72it/s]                                               {'train_runtime': 1.0528, 'train_samples_per_second': 403.704, 'train_steps_per_second': 14.248, 'train_loss': 0.7192961017290751, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.72it/s]100%|██████████| 15/15 [00:01<00:00, 14.25it/s]
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7264, 'grad_norm': 0.16278842091560364, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.61it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.7257, 'grad_norm': 0.15432997047901154, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.7257, 'grad_norm': 0.1371600180864334, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.99it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.7129, 'grad_norm': 0.1384313851594925, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.7267, 'grad_norm': 0.17057515680789948, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.31it/s] 40%|████      | 6/15 [00:00<00:00, 16.08it/s]                                              {'loss': 0.7248, 'grad_norm': 0.15352106094360352, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.08it/s]                                              {'loss': 0.7128, 'grad_norm': 0.10424730181694031, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.08it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.7161, 'grad_norm': 0.19565966725349426, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.7225, 'grad_norm': 0.18159238994121552, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.70it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.7118, 'grad_norm': 0.13222338259220123, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.725, 'grad_norm': 0.1596725434064865, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.13it/s] 80%|████████  | 12/15 [00:00<00:00, 15.75it/s]                                               {'loss': 0.7289, 'grad_norm': 0.2122068554162979, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.75it/s]                                               {'loss': 0.7198, 'grad_norm': 0.1884404718875885, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.75it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.7103, 'grad_norm': 0.13867317140102386, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.7306, 'grad_norm': 0.1592218428850174, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.67it/s]                                               {'train_runtime': 1.0493, 'train_samples_per_second': 405.044, 'train_steps_per_second': 14.296, 'train_loss': 0.7213424762090047, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.67it/s]100%|██████████| 15/15 [00:01<00:00, 14.30it/s]
CLIENT:87
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7167, 'grad_norm': 0.09958034008741379, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.24it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.90it/s]                                              {'loss': 0.7154, 'grad_norm': 0.1377687305212021, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.90it/s]                                              {'loss': 0.697, 'grad_norm': 0.06641624122858047, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.90it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6996, 'grad_norm': 0.07696729153394699, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.7151, 'grad_norm': 0.07954432815313339, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.13it/s] 40%|████      | 6/15 [00:00<00:00, 16.19it/s]                                              {'loss': 0.711, 'grad_norm': 0.14131009578704834, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.19it/s]                                              {'loss': 0.7136, 'grad_norm': 0.05991695448756218, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.19it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.7198, 'grad_norm': 0.13518333435058594, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.7029, 'grad_norm': 0.09555967897176743, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.68it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7126, 'grad_norm': 0.1147918775677681, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7004, 'grad_norm': 0.05341815575957298, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.12it/s] 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.7047, 'grad_norm': 0.13181337714195251, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.7061, 'grad_norm': 0.056812435388565063, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.56it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.6953, 'grad_norm': 0.05866248905658722, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.7404, 'grad_norm': 0.2725960314273834, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.63it/s]                                               {'train_runtime': 1.0535, 'train_samples_per_second': 403.41, 'train_steps_per_second': 14.238, 'train_loss': 0.7100369532903036, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.63it/s]100%|██████████| 15/15 [00:01<00:00, 14.24it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6961, 'grad_norm': 0.13030414283275604, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.70it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.88it/s]                                              {'loss': 0.7328, 'grad_norm': 0.20509390532970428, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.88it/s]                                              {'loss': 0.6886, 'grad_norm': 0.058447688817977905, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.88it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.03it/s]                                              {'loss': 0.6945, 'grad_norm': 0.1155669316649437, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.03it/s]                                              {'loss': 0.7176, 'grad_norm': 0.14753687381744385, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.03it/s] 40%|████      | 6/15 [00:00<00:00, 15.95it/s]                                              {'loss': 0.723, 'grad_norm': 0.1788494884967804, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.95it/s]                                              {'loss': 0.7164, 'grad_norm': 0.16836579144001007, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.95it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.26it/s]                                              {'loss': 0.7096, 'grad_norm': 0.14990262687206268, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.26it/s]                                              {'loss': 0.703, 'grad_norm': 0.09810180217027664, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.26it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.7173, 'grad_norm': 0.1164422482252121, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.7081, 'grad_norm': 0.16173529624938965, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.89it/s] 80%|████████  | 12/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.7179, 'grad_norm': 0.14472950994968414, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.7029, 'grad_norm': 0.13429078459739685, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.36it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.7247, 'grad_norm': 0.168859601020813, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6981, 'grad_norm': 0.13820573687553406, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.38it/s]                                               {'train_runtime': 1.0706, 'train_samples_per_second': 396.962, 'train_steps_per_second': 14.01, 'train_loss': 0.71003204981486, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 14.02it/s]
CLIENT:99
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/25 [00:00<?, ?it/s]                                      {'loss': 0.7157, 'grad_norm': 0.13998010754585266, 'learning_rate': 0.01, 'epoch': 0.2}
  4%|▍         | 1/25 [00:00<00:01, 12.32it/s]  8%|▊         | 2/25 [00:00<00:01, 12.73it/s]                                              {'loss': 0.7141, 'grad_norm': 0.18891768157482147, 'learning_rate': 0.0096, 'epoch': 0.4}
  8%|▊         | 2/25 [00:00<00:01, 12.73it/s]                                              {'loss': 0.7289, 'grad_norm': 0.18454575538635254, 'learning_rate': 0.0092, 'epoch': 0.6}
 12%|█▏        | 3/25 [00:00<00:01, 12.73it/s] 16%|█▌        | 4/25 [00:00<00:01, 14.24it/s]                                              {'loss': 0.7295, 'grad_norm': 0.20209620893001556, 'learning_rate': 0.0088, 'epoch': 0.8}
 16%|█▌        | 4/25 [00:00<00:01, 14.24it/s]                                              {'loss': 0.695, 'grad_norm': 0.10301908850669861, 'learning_rate': 0.0084, 'epoch': 1.0}
 20%|██        | 5/25 [00:00<00:01, 14.24it/s] 24%|██▍       | 6/25 [00:00<00:01, 16.28it/s]                                              {'loss': 0.7172, 'grad_norm': 0.15555663406848907, 'learning_rate': 0.008, 'epoch': 1.2}
 24%|██▍       | 6/25 [00:00<00:01, 16.28it/s]                                              {'loss': 0.7142, 'grad_norm': 0.15529781579971313, 'learning_rate': 0.0076, 'epoch': 1.4}
 28%|██▊       | 7/25 [00:00<00:01, 16.28it/s] 32%|███▏      | 8/25 [00:00<00:01, 14.75it/s]                                              {'loss': 0.7158, 'grad_norm': 0.18526165187358856, 'learning_rate': 0.0072, 'epoch': 1.6}
 32%|███▏      | 8/25 [00:00<00:01, 14.75it/s]                                              {'loss': 0.7355, 'grad_norm': 0.18611553311347961, 'learning_rate': 0.0068000000000000005, 'epoch': 1.8}
 36%|███▌      | 9/25 [00:00<00:01, 14.75it/s]                                              {'loss': 0.7233, 'grad_norm': 0.26920413970947266, 'learning_rate': 0.0064, 'epoch': 2.0}
 40%|████      | 10/25 [00:00<00:01, 14.75it/s] 44%|████▍     | 11/25 [00:00<00:00, 15.61it/s]                                               {'loss': 0.7154, 'grad_norm': 0.1838218867778778, 'learning_rate': 0.006, 'epoch': 2.2}
 44%|████▍     | 11/25 [00:00<00:00, 15.61it/s]                                               {'loss': 0.7089, 'grad_norm': 0.07902123034000397, 'learning_rate': 0.005600000000000001, 'epoch': 2.4}
 48%|████▊     | 12/25 [00:00<00:00, 15.61it/s] 52%|█████▏    | 13/25 [00:00<00:00, 14.70it/s]                                               {'loss': 0.7156, 'grad_norm': 0.182669997215271, 'learning_rate': 0.005200000000000001, 'epoch': 2.6}
 52%|█████▏    | 13/25 [00:00<00:00, 14.70it/s]                                               {'loss': 0.7396, 'grad_norm': 0.2269083559513092, 'learning_rate': 0.0048, 'epoch': 2.8}
 56%|█████▌    | 14/25 [00:00<00:00, 14.70it/s]                                               {'loss': 0.7532, 'grad_norm': 0.3369719684123993, 'learning_rate': 0.0044, 'epoch': 3.0}
 60%|██████    | 15/25 [00:00<00:00, 14.70it/s] 64%|██████▍   | 16/25 [00:01<00:00, 15.69it/s]                                               {'loss': 0.7134, 'grad_norm': 0.16999588906764984, 'learning_rate': 0.004, 'epoch': 3.2}
 64%|██████▍   | 16/25 [00:01<00:00, 15.69it/s]                                               {'loss': 0.7169, 'grad_norm': 0.18899840116500854, 'learning_rate': 0.0036, 'epoch': 3.4}
 68%|██████▊   | 17/25 [00:01<00:00, 15.69it/s] 72%|███████▏  | 18/25 [00:01<00:00, 14.77it/s]                                               {'loss': 0.7228, 'grad_norm': 0.21180762350559235, 'learning_rate': 0.0032, 'epoch': 3.6}
 72%|███████▏  | 18/25 [00:01<00:00, 14.77it/s]                                               {'loss': 0.7252, 'grad_norm': 0.21185167133808136, 'learning_rate': 0.0028000000000000004, 'epoch': 3.8}
 76%|███████▌  | 19/25 [00:01<00:00, 14.77it/s]                                               {'loss': 0.6756, 'grad_norm': 0.11498106271028519, 'learning_rate': 0.0024, 'epoch': 4.0}
 80%|████████  | 20/25 [00:01<00:00, 14.77it/s] 84%|████████▍ | 21/25 [00:01<00:00, 15.44it/s]                                               {'loss': 0.7214, 'grad_norm': 0.10176939517259598, 'learning_rate': 0.002, 'epoch': 4.2}
 84%|████████▍ | 21/25 [00:01<00:00, 15.44it/s]                                               {'loss': 0.7169, 'grad_norm': 0.19251537322998047, 'learning_rate': 0.0016, 'epoch': 4.4}
 88%|████████▊ | 22/25 [00:01<00:00, 15.44it/s] 92%|█████████▏| 23/25 [00:01<00:00, 14.63it/s]                                               {'loss': 0.7248, 'grad_norm': 0.24139763414859772, 'learning_rate': 0.0012, 'epoch': 4.6}
 92%|█████████▏| 23/25 [00:01<00:00, 14.63it/s]                                               {'loss': 0.7217, 'grad_norm': 0.2252245545387268, 'learning_rate': 0.0008, 'epoch': 4.8}
 96%|█████████▌| 24/25 [00:01<00:00, 14.63it/s]                                               {'loss': 0.7074, 'grad_norm': 0.21983328461647034, 'learning_rate': 0.0004, 'epoch': 5.0}
100%|██████████| 25/25 [00:01<00:00, 14.63it/s]                                               {'train_runtime': 1.704, 'train_samples_per_second': 399.069, 'train_steps_per_second': 14.672, 'train_loss': 0.7187135982513427, 'epoch': 5.0}
100%|██████████| 25/25 [00:01<00:00, 14.63it/s]100%|██████████| 25/25 [00:01<00:00, 14.67it/s]
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7106, 'grad_norm': 0.14312301576137543, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.60it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.51it/s]                                              {'loss': 0.7105, 'grad_norm': 0.08347297459840775, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.51it/s]                                              {'loss': 0.7316, 'grad_norm': 0.22502216696739197, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.51it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.721, 'grad_norm': 0.16150526702404022, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.7013, 'grad_norm': 0.10304787009954453, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.09it/s] 40%|████      | 6/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.694, 'grad_norm': 0.11613009870052338, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.6982, 'grad_norm': 0.10766932368278503, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.12it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.7187, 'grad_norm': 0.17708608508110046, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.711, 'grad_norm': 0.11860741674900055, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.32it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.68it/s]                                               {'loss': 0.7053, 'grad_norm': 0.12299130856990814, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.68it/s]                                               {'loss': 0.7041, 'grad_norm': 0.13836227357387543, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.68it/s] 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.7177, 'grad_norm': 0.16887150704860687, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.7152, 'grad_norm': 0.1545061469078064, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.26it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.7167, 'grad_norm': 0.1290220469236374, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.7139, 'grad_norm': 0.14451460540294647, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]                                               {'train_runtime': 1.0846, 'train_samples_per_second': 391.86, 'train_steps_per_second': 13.83, 'train_loss': 0.7113198359807332, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]100%|██████████| 15/15 [00:01<00:00, 13.84it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 39.09it/s] 24%|██▍       | 8/33 [00:00<00:00, 29.62it/s] 36%|███▋      | 12/33 [00:00<00:00, 26.90it/s] 45%|████▌     | 15/33 [00:00<00:00, 25.97it/s] 55%|█████▍    | 18/33 [00:00<00:00, 25.37it/s] 64%|██████▎   | 21/33 [00:00<00:00, 25.15it/s] 73%|███████▎  | 24/33 [00:00<00:00, 25.20it/s] 82%|████████▏ | 27/33 [00:01<00:00, 25.07it/s] 91%|█████████ | 30/33 [00:01<00:00, 25.12it/s]100%|██████████| 33/33 [00:01<00:00, 26.03it/s]100%|██████████| 33/33 [00:01<00:00, 26.21it/s]
{'eval_loss': 0.7142010927200317, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.311601150527325, 'eval_runtime': 1.3003, 'eval_samples_per_second': 802.102, 'eval_steps_per_second': 25.378}
ROUND:9
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7169, 'grad_norm': 0.14880558848381042, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.06it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.6969, 'grad_norm': 0.08685597032308578, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.7275, 'grad_norm': 0.15077854692935944, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.30it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.7063, 'grad_norm': 0.058519598096609116, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.7195, 'grad_norm': 0.19969546794891357, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.64it/s] 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.6994, 'grad_norm': 0.1406984180212021, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.7144, 'grad_norm': 0.0983540341258049, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.46it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.711, 'grad_norm': 0.16162991523742676, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.711, 'grad_norm': 0.13092955946922302, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.29it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.7137, 'grad_norm': 0.08548083901405334, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.7187, 'grad_norm': 0.1590295284986496, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.87it/s] 80%|████████  | 12/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.724, 'grad_norm': 0.18292266130447388, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.7307, 'grad_norm': 0.22557316720485687, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.34it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.7053, 'grad_norm': 0.0893300250172615, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.6946, 'grad_norm': 0.065799780189991, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]                                               {'train_runtime': 1.0751, 'train_samples_per_second': 395.318, 'train_steps_per_second': 13.952, 'train_loss': 0.7126626054445903, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]100%|██████████| 15/15 [00:01<00:00, 13.96it/s]
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.71, 'grad_norm': 0.16193872690200806, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.12it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.41it/s]                                              {'loss': 0.7095, 'grad_norm': 0.1537332534790039, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.41it/s]                                              {'loss': 0.7156, 'grad_norm': 0.1703750640153885, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.41it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.7293, 'grad_norm': 0.20033946633338928, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.6978, 'grad_norm': 0.10320322215557098, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.41it/s] 40%|████      | 6/15 [00:00<00:00, 16.36it/s]                                              {'loss': 0.7075, 'grad_norm': 0.17997869849205017, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.36it/s]                                              {'loss': 0.7115, 'grad_norm': 0.175772562623024, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.36it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.79it/s]                                              {'loss': 0.7148, 'grad_norm': 0.21139352023601532, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.79it/s]                                              {'loss': 0.7068, 'grad_norm': 0.08771920949220657, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.79it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.7164, 'grad_norm': 0.1435781568288803, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.701, 'grad_norm': 0.12127509713172913, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.20it/s] 80%|████████  | 12/15 [00:00<00:00, 15.64it/s]                                               {'loss': 0.7278, 'grad_norm': 0.24827207624912262, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.64it/s]                                               {'loss': 0.7107, 'grad_norm': 0.1469191312789917, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.64it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.7211, 'grad_norm': 0.17418430745601654, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.7222, 'grad_norm': 0.21368390321731567, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.60it/s]                                               {'train_runtime': 1.0511, 'train_samples_per_second': 404.352, 'train_steps_per_second': 14.271, 'train_loss': 0.7134678840637207, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.60it/s]100%|██████████| 15/15 [00:01<00:00, 14.28it/s]
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7064, 'grad_norm': 0.10584240406751633, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.85it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.6984, 'grad_norm': 0.11273830384016037, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.7241, 'grad_norm': 0.16182555258274078, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.01it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.69it/s]                                              {'loss': 0.6971, 'grad_norm': 0.09319856762886047, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.69it/s]                                              {'loss': 0.7114, 'grad_norm': 0.16196803748607635, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.69it/s] 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.7111, 'grad_norm': 0.12213656306266785, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.7115, 'grad_norm': 0.17154793441295624, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.45it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.7081, 'grad_norm': 0.0855732411146164, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.7107, 'grad_norm': 0.10948387533426285, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.29it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.7097, 'grad_norm': 0.12760990858078003, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.6959, 'grad_norm': 0.11117561161518097, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.86it/s] 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.7175, 'grad_norm': 0.15156039595603943, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.7055, 'grad_norm': 0.07520363479852676, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.37it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.7129, 'grad_norm': 0.1837022751569748, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.686, 'grad_norm': 0.13388195633888245, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.52it/s]                                               {'train_runtime': 1.0712, 'train_samples_per_second': 396.735, 'train_steps_per_second': 14.002, 'train_loss': 0.7071051279703776, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.52it/s]100%|██████████| 15/15 [00:01<00:00, 14.01it/s]
CLIENT:5
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7173, 'grad_norm': 0.17177215218544006, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.27it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.02it/s]                                              {'loss': 0.7152, 'grad_norm': 0.11002455651760101, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.02it/s]                                              {'loss': 0.6962, 'grad_norm': 0.06679143756628036, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.02it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6948, 'grad_norm': 0.061949245631694794, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.7268, 'grad_norm': 0.1896694302558899, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.13it/s] 40%|████      | 6/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.703, 'grad_norm': 0.12321266531944275, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.6975, 'grad_norm': 0.12544968724250793, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.63it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.84it/s]                                              {'loss': 0.6995, 'grad_norm': 0.09553918987512589, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.84it/s]                                              {'loss': 0.7102, 'grad_norm': 0.18019260466098785, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.84it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.7112, 'grad_norm': 0.10076705366373062, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.7007, 'grad_norm': 0.13274286687374115, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.03it/s] 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.7016, 'grad_norm': 0.14990411698818207, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.7085, 'grad_norm': 0.10730445384979248, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.51it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.50it/s]                                               {'loss': 0.7171, 'grad_norm': 0.1295190006494522, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.50it/s]                                               {'loss': 0.7258, 'grad_norm': 0.17259186506271362, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.50it/s]                                               {'train_runtime': 1.0639, 'train_samples_per_second': 399.484, 'train_steps_per_second': 14.099, 'train_loss': 0.70835356314977, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.50it/s]100%|██████████| 15/15 [00:01<00:00, 14.10it/s]
CLIENT:52
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7174, 'grad_norm': 0.14848867058753967, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.41it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.86it/s]                                              {'loss': 0.7155, 'grad_norm': 0.12313731014728546, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.86it/s]                                              {'loss': 0.6789, 'grad_norm': 0.07931513339281082, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.86it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.6959, 'grad_norm': 0.08826525509357452, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.7155, 'grad_norm': 0.13556641340255737, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.66it/s] 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.7161, 'grad_norm': 0.13580971956253052, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.7125, 'grad_norm': 0.14869190752506256, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.45it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.7171, 'grad_norm': 0.14702530205249786, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6985, 'grad_norm': 0.06310156732797623, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.32it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.90it/s]                                               {'loss': 0.702, 'grad_norm': 0.09880063682794571, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.90it/s]                                               {'loss': 0.7053, 'grad_norm': 0.12704113125801086, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.90it/s] 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7064, 'grad_norm': 0.1599997878074646, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7016, 'grad_norm': 0.06950628012418747, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.38it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.7087, 'grad_norm': 0.14762815833091736, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.7133, 'grad_norm': 0.21633268892765045, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.48it/s]                                               {'train_runtime': 1.085, 'train_samples_per_second': 391.703, 'train_steps_per_second': 13.825, 'train_loss': 0.7069748520851136, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]100%|██████████| 15/15 [00:01<00:00, 13.83it/s]
CLIENT:41
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7083, 'grad_norm': 0.13693030178546906, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.98it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.14it/s]                                              {'loss': 0.6978, 'grad_norm': 0.05517972260713577, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.14it/s]                                              {'loss': 0.7068, 'grad_norm': 0.08504150062799454, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.14it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.7055, 'grad_norm': 0.057464756071567535, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.7252, 'grad_norm': 0.16714100539684296, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.74it/s] 40%|████      | 6/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.6976, 'grad_norm': 0.09538356214761734, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.7201, 'grad_norm': 0.15883347392082214, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.51it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.7015, 'grad_norm': 0.05286793038249016, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.6977, 'grad_norm': 0.0629536509513855, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.70it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.7001, 'grad_norm': 0.0693589523434639, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.6998, 'grad_norm': 0.08476608246564865, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.88it/s] 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.702, 'grad_norm': 0.12182209640741348, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.7092, 'grad_norm': 0.10721505433320999, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.37it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.7107, 'grad_norm': 0.11177464574575424, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.7016, 'grad_norm': 0.06359390169382095, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.43it/s]                                               {'train_runtime': 1.0726, 'train_samples_per_second': 396.24, 'train_steps_per_second': 13.985, 'train_loss': 0.7055958032608032, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.43it/s]100%|██████████| 15/15 [00:01<00:00, 13.99it/s]
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7158, 'grad_norm': 0.1879405379295349, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.70it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.7208, 'grad_norm': 0.18513143062591553, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.7071, 'grad_norm': 0.08642725646495819, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.15it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.39it/s]                                              {'loss': 0.7207, 'grad_norm': 0.16973862051963806, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.39it/s]                                              {'loss': 0.7222, 'grad_norm': 0.1683064103126526, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.39it/s] 40%|████      | 6/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.7086, 'grad_norm': 0.13497716188430786, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.7111, 'grad_norm': 0.16527089476585388, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.31it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.7035, 'grad_norm': 0.1688009798526764, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.7105, 'grad_norm': 0.15678079426288605, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.22it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.7078, 'grad_norm': 0.13923382759094238, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.6964, 'grad_norm': 0.1334703266620636, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.80it/s] 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.732, 'grad_norm': 0.2492891103029251, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.718, 'grad_norm': 0.17626972496509552, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.7103, 'grad_norm': 0.16367076337337494, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.7221, 'grad_norm': 0.17985062301158905, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]                                               {'train_runtime': 1.0775, 'train_samples_per_second': 394.426, 'train_steps_per_second': 13.921, 'train_loss': 0.7137944976488749, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]100%|██████████| 15/15 [00:01<00:00, 13.93it/s]
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7161, 'grad_norm': 0.11827684938907623, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.10it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.78it/s]                                              {'loss': 0.7086, 'grad_norm': 0.10751697421073914, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.78it/s]                                              {'loss': 0.6651, 'grad_norm': 0.11480755358934402, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.78it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.21it/s]                                              {'loss': 0.7029, 'grad_norm': 0.05070468410849571, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.21it/s]                                              {'loss': 0.7042, 'grad_norm': 0.08260327577590942, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.21it/s] 40%|████      | 6/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.721, 'grad_norm': 0.12826572358608246, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.7095, 'grad_norm': 0.09003908932209015, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.15it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.6898, 'grad_norm': 0.06194755434989929, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.7025, 'grad_norm': 0.12586946785449982, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.13it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.06it/s]                                               {'loss': 0.6872, 'grad_norm': 0.05383114889264107, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.06it/s]                                               {'loss': 0.6919, 'grad_norm': 0.07583067566156387, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.06it/s] 80%|████████  | 12/15 [00:00<00:00, 16.09it/s]                                               {'loss': 0.697, 'grad_norm': 0.13710877299308777, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.09it/s]                                               {'loss': 0.7085, 'grad_norm': 0.09539465606212616, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.09it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.6892, 'grad_norm': 0.048725809901952744, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.724, 'grad_norm': 0.11242884397506714, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.86it/s]                                               {'train_runtime': 1.0568, 'train_samples_per_second': 402.171, 'train_steps_per_second': 14.194, 'train_loss': 0.7011647423108419, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.86it/s]100%|██████████| 15/15 [00:01<00:00, 14.20it/s]
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7276, 'grad_norm': 0.1995856910943985, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.60it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.09it/s]                                              {'loss': 0.7325, 'grad_norm': 0.19189441204071045, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.09it/s]                                              {'loss': 0.722, 'grad_norm': 0.12718673050403595, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.09it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.33it/s]                                              {'loss': 0.7151, 'grad_norm': 0.16507531702518463, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.33it/s]                                              {'loss': 0.7309, 'grad_norm': 0.15872697532176971, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.33it/s] 40%|████      | 6/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.7247, 'grad_norm': 0.26613038778305054, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.7248, 'grad_norm': 0.1678023785352707, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.23it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.7142, 'grad_norm': 0.1644323170185089, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.7262, 'grad_norm': 0.26109305024147034, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.18it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.7247, 'grad_norm': 0.19774416089057922, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.7194, 'grad_norm': 0.2212335169315338, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.81it/s] 80%|████████  | 12/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.717, 'grad_norm': 0.14999771118164062, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.7271, 'grad_norm': 0.23131738603115082, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.33it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.725, 'grad_norm': 0.19621355831623077, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.7233, 'grad_norm': 0.17000864446163177, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]                                               {'train_runtime': 1.0778, 'train_samples_per_second': 394.313, 'train_steps_per_second': 13.917, 'train_loss': 0.7236365079879761, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]100%|██████████| 15/15 [00:01<00:00, 13.92it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.733, 'grad_norm': 0.18504886329174042, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.28it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.92it/s]                                              {'loss': 0.7194, 'grad_norm': 0.09014058858156204, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.92it/s]                                              {'loss': 0.6808, 'grad_norm': 0.07545372098684311, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.92it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.7065, 'grad_norm': 0.10002565383911133, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.7148, 'grad_norm': 0.08074180781841278, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.48it/s] 40%|████      | 6/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.7095, 'grad_norm': 0.11655834317207336, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.7096, 'grad_norm': 0.11656567454338074, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.44it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.7079, 'grad_norm': 0.09424065053462982, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.85it/s]                                              {'loss': 0.7132, 'grad_norm': 0.10574016720056534, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.85it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.703, 'grad_norm': 0.0644708126783371, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.7031, 'grad_norm': 0.09771677851676941, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.40it/s] 80%|████████  | 12/15 [00:00<00:00, 15.78it/s]                                               {'loss': 0.712, 'grad_norm': 0.17695598304271698, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.78it/s]                                               {'loss': 0.7159, 'grad_norm': 0.13267649710178375, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.78it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.7073, 'grad_norm': 0.09270434081554413, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.7079, 'grad_norm': 0.09577035158872604, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.69it/s]                                               {'train_runtime': 1.0563, 'train_samples_per_second': 402.349, 'train_steps_per_second': 14.201, 'train_loss': 0.7095975677172343, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.69it/s]100%|██████████| 15/15 [00:01<00:00, 14.21it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 33.01it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.69it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.37it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.58it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.26it/s] 61%|██████    | 20/33 [00:00<00:00, 25.36it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.04it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.05it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.24it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.06it/s]100%|██████████| 33/33 [00:01<00:00, 25.88it/s]
{'eval_loss': 0.7129939794540405, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.311601150527325, 'eval_runtime': 1.3162, 'eval_samples_per_second': 792.428, 'eval_steps_per_second': 25.072}
ROUND:10
CLIENT:26
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7278, 'grad_norm': 0.20734897255897522, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.11it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.88it/s]                                              {'loss': 0.7058, 'grad_norm': 0.16150794923305511, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.88it/s]                                              {'loss': 0.7008, 'grad_norm': 0.07345068454742432, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.88it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.7186, 'grad_norm': 0.13602060079574585, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.7115, 'grad_norm': 0.12635479867458344, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.41it/s] 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.7072, 'grad_norm': 0.21914531290531158, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.7119, 'grad_norm': 0.1999165266752243, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.48it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.7051, 'grad_norm': 0.09735755622386932, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.708, 'grad_norm': 0.18047092854976654, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.27it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.7041, 'grad_norm': 0.12743204832077026, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.7105, 'grad_norm': 0.14817966520786285, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.63it/s] 80%|████████  | 12/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.7325, 'grad_norm': 0.2209968864917755, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.7098, 'grad_norm': 0.15547198057174683, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.24it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.31it/s]                                               {'loss': 0.7216, 'grad_norm': 0.17477823793888092, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.31it/s]                                               {'loss': 0.7201, 'grad_norm': 0.14419730007648468, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.31it/s]                                               {'train_runtime': 1.0826, 'train_samples_per_second': 392.584, 'train_steps_per_second': 13.856, 'train_loss': 0.7130300720532735, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.31it/s]100%|██████████| 15/15 [00:01<00:00, 13.86it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7094, 'grad_norm': 0.11682252585887909, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.36it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.91it/s]                                              {'loss': 0.7172, 'grad_norm': 0.18464264273643494, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.91it/s]                                              {'loss': 0.7114, 'grad_norm': 0.1940869688987732, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.91it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.7116, 'grad_norm': 0.09979739040136337, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.7379, 'grad_norm': 0.2826261520385742, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.23it/s] 40%|████      | 6/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.7097, 'grad_norm': 0.1334410160779953, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.7132, 'grad_norm': 0.2157052755355835, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.14it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.7118, 'grad_norm': 0.1426997035741806, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.7088, 'grad_norm': 0.1551695317029953, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.09it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.00it/s]                                               {'loss': 0.7319, 'grad_norm': 0.17543283104896545, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.00it/s]                                               {'loss': 0.7003, 'grad_norm': 0.1908070594072342, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.00it/s] 80%|████████  | 12/15 [00:00<00:00, 15.64it/s]                                               {'loss': 0.7095, 'grad_norm': 0.16021792590618134, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.64it/s]                                               {'loss': 0.7168, 'grad_norm': 0.1793382167816162, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.64it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.7205, 'grad_norm': 0.19451692700386047, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.703, 'grad_norm': 0.17674966156482697, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]                                               {'train_runtime': 1.079, 'train_samples_per_second': 393.865, 'train_steps_per_second': 13.901, 'train_loss': 0.714197023709615, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]100%|██████████| 15/15 [00:01<00:00, 13.91it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7056, 'grad_norm': 0.12166307866573334, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.66it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.81it/s]                                              {'loss': 0.7101, 'grad_norm': 0.2088148444890976, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.81it/s]                                              {'loss': 0.7266, 'grad_norm': 0.25161200761795044, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.81it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.10it/s]                                              {'loss': 0.7099, 'grad_norm': 0.12871822714805603, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.10it/s]                                              {'loss': 0.7272, 'grad_norm': 0.24805296957492828, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.10it/s] 40%|████      | 6/15 [00:00<00:00, 15.86it/s]                                              {'loss': 0.7208, 'grad_norm': 0.1807752549648285, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.86it/s]                                              {'loss': 0.7352, 'grad_norm': 0.23716552555561066, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.86it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.7036, 'grad_norm': 0.1559419482946396, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.7048, 'grad_norm': 0.17853087186813354, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.45it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.7255, 'grad_norm': 0.195574551820755, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.7067, 'grad_norm': 0.18506482243537903, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.84it/s] 80%|████████  | 12/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.7079, 'grad_norm': 0.19684338569641113, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.7042, 'grad_norm': 0.14160993695259094, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.36it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.46it/s]                                               {'loss': 0.7154, 'grad_norm': 0.2320888638496399, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.46it/s]                                               {'loss': 0.7275, 'grad_norm': 0.2660274803638458, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.46it/s]                                               {'train_runtime': 1.0616, 'train_samples_per_second': 400.342, 'train_steps_per_second': 14.13, 'train_loss': 0.7154051621754964, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.46it/s]100%|██████████| 15/15 [00:01<00:00, 14.13it/s]
CLIENT:86
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7154, 'grad_norm': 0.17655915021896362, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.51it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.7018, 'grad_norm': 0.1075645387172699, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.7333, 'grad_norm': 0.13651488721370697, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.04it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.33it/s]                                              {'loss': 0.719, 'grad_norm': 0.15072907507419586, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.33it/s]                                              {'loss': 0.7168, 'grad_norm': 0.12423592805862427, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.33it/s] 40%|████      | 6/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.6974, 'grad_norm': 0.1530056744813919, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.7208, 'grad_norm': 0.13271401822566986, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.23it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.6977, 'grad_norm': 0.0971105545759201, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.7176, 'grad_norm': 0.22847981750965118, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.13it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.77it/s]                                               {'loss': 0.7115, 'grad_norm': 0.1587732881307602, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.77it/s]                                               {'loss': 0.7013, 'grad_norm': 0.16755005717277527, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.77it/s] 80%|████████  | 12/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.6944, 'grad_norm': 0.08226577192544937, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.24it/s]                                               {'loss': 0.7158, 'grad_norm': 0.15298214554786682, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.24it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.7111, 'grad_norm': 0.15370945632457733, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.7141, 'grad_norm': 0.16254808008670807, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.86it/s]                                               {'train_runtime': 1.0643, 'train_samples_per_second': 399.305, 'train_steps_per_second': 14.093, 'train_loss': 0.7112050652503967, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.86it/s]100%|██████████| 15/15 [00:01<00:00, 14.10it/s]
CLIENT:76
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7094, 'grad_norm': 0.11941184848546982, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.22it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.7152, 'grad_norm': 0.11663666367530823, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.01it/s]                                              {'loss': 0.7084, 'grad_norm': 0.2351607382297516, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.01it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.7119, 'grad_norm': 0.11866036802530289, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.6936, 'grad_norm': 0.08238108456134796, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.63it/s] 40%|████      | 6/15 [00:00<00:00, 15.67it/s]                                              {'loss': 0.736, 'grad_norm': 0.29392144083976746, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.67it/s]                                              {'loss': 0.7048, 'grad_norm': 0.13936012983322144, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.67it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.6985, 'grad_norm': 0.1340656727552414, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.7161, 'grad_norm': 0.19434046745300293, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.37it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.7158, 'grad_norm': 0.18542806804180145, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.7047, 'grad_norm': 0.13820230960845947, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.88it/s] 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.7146, 'grad_norm': 0.10151798278093338, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.7014, 'grad_norm': 0.08864127099514008, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.54it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.7085, 'grad_norm': 0.15849077701568604, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.7135, 'grad_norm': 0.24730052053928375, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.54it/s]                                               {'train_runtime': 1.0696, 'train_samples_per_second': 397.35, 'train_steps_per_second': 14.024, 'train_loss': 0.7101556340853373, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.54it/s]100%|██████████| 15/15 [00:01<00:00, 14.03it/s]
CLIENT:14
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6907, 'grad_norm': 0.08287344872951508, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.19it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.91it/s]                                              {'loss': 0.712, 'grad_norm': 0.13918070495128632, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.91it/s]                                              {'loss': 0.7156, 'grad_norm': 0.147637277841568, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.91it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.26it/s]                                              {'loss': 0.7143, 'grad_norm': 0.11969307065010071, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.26it/s]                                              {'loss': 0.7029, 'grad_norm': 0.08780991286039352, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.26it/s] 40%|████      | 6/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.7102, 'grad_norm': 0.20025959610939026, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.7129, 'grad_norm': 0.15212386846542358, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.30it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.88it/s]                                              {'loss': 0.7059, 'grad_norm': 0.13242913782596588, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.88it/s]                                              {'loss': 0.6992, 'grad_norm': 0.10014992207288742, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.88it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.7069, 'grad_norm': 0.09939438104629517, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.696, 'grad_norm': 0.15752853453159332, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.60it/s] 80%|████████  | 12/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.6818, 'grad_norm': 0.12315458804368973, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7052, 'grad_norm': 0.09300187975168228, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.12it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.32it/s]                                               {'loss': 0.6852, 'grad_norm': 0.11862629652023315, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.32it/s]                                               {'loss': 0.7227, 'grad_norm': 0.23928776383399963, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.32it/s]                                               {'train_runtime': 1.0866, 'train_samples_per_second': 391.119, 'train_steps_per_second': 13.804, 'train_loss': 0.704108186562856, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.32it/s]100%|██████████| 15/15 [00:01<00:00, 13.81it/s]
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7258, 'grad_norm': 0.20850645005702972, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.07it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.7308, 'grad_norm': 0.1995217204093933, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.721, 'grad_norm': 0.13102634251117706, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.23it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.7137, 'grad_norm': 0.17114970088005066, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.7295, 'grad_norm': 0.16563670337200165, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.23it/s] 40%|████      | 6/15 [00:00<00:00, 16.01it/s]                                              {'loss': 0.7223, 'grad_norm': 0.2777058184146881, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.01it/s]                                              {'loss': 0.7233, 'grad_norm': 0.17587946355342865, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.01it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.7127, 'grad_norm': 0.170944482088089, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.7237, 'grad_norm': 0.27273982763290405, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.53it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.7229, 'grad_norm': 0.20614023506641388, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.7173, 'grad_norm': 0.2309735268354416, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.04it/s] 80%|████████  | 12/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.7157, 'grad_norm': 0.15428738296031952, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.7249, 'grad_norm': 0.24139505624771118, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.63it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.7232, 'grad_norm': 0.205442875623703, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.7218, 'grad_norm': 0.17621009051799774, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.55it/s]                                               {'train_runtime': 1.0531, 'train_samples_per_second': 403.581, 'train_steps_per_second': 14.244, 'train_loss': 0.7218931237856547, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.55it/s]100%|██████████| 15/15 [00:01<00:00, 14.25it/s]
CLIENT:48
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7266, 'grad_norm': 0.2310226559638977, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.71it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.84it/s]                                              {'loss': 0.7191, 'grad_norm': 0.21286238729953766, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.84it/s]                                              {'loss': 0.6914, 'grad_norm': 0.08132829517126083, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.84it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.10it/s]                                              {'loss': 0.7275, 'grad_norm': 0.21246138215065002, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.10it/s]                                              {'loss': 0.7146, 'grad_norm': 0.11893093585968018, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.10it/s] 40%|████      | 6/15 [00:00<00:00, 15.73it/s]                                              {'loss': 0.7213, 'grad_norm': 0.2257169932126999, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.73it/s]                                              {'loss': 0.7079, 'grad_norm': 0.16478946805000305, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.73it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.38it/s]                                              {'loss': 0.7134, 'grad_norm': 0.17447099089622498, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.38it/s]                                              {'loss': 0.7211, 'grad_norm': 0.22610299289226532, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.38it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.7036, 'grad_norm': 0.1435612142086029, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.7124, 'grad_norm': 0.23714427649974823, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.92it/s] 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7149, 'grad_norm': 0.18258748948574066, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7186, 'grad_norm': 0.218338280916214, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.38it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.7006, 'grad_norm': 0.14152050018310547, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.72, 'grad_norm': 0.2383061647415161, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]                                               {'train_runtime': 1.071, 'train_samples_per_second': 396.841, 'train_steps_per_second': 14.006, 'train_loss': 0.7142037868499755, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]100%|██████████| 15/15 [00:01<00:00, 14.01it/s]
CLIENT:71
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6983, 'grad_norm': 0.14118871092796326, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.03it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.6994, 'grad_norm': 0.06529387086629868, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.7093, 'grad_norm': 0.10922286659479141, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.33it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.7178, 'grad_norm': 0.1712225079536438, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.691, 'grad_norm': 0.0546099878847599, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.43it/s] 40%|████      | 6/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.7123, 'grad_norm': 0.15800821781158447, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.6969, 'grad_norm': 0.06618554890155792, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.28it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.7002, 'grad_norm': 0.12400984764099121, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.7055, 'grad_norm': 0.12170957773923874, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.90it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.7131, 'grad_norm': 0.1213950589299202, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.6994, 'grad_norm': 0.09513867646455765, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.26it/s] 80%|████████  | 12/15 [00:00<00:00, 15.71it/s]                                               {'loss': 0.6954, 'grad_norm': 0.0776916891336441, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.71it/s]                                               {'loss': 0.7157, 'grad_norm': 0.15690498054027557, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.71it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.7067, 'grad_norm': 0.058579444885253906, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.7157, 'grad_norm': 0.11227482557296753, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.64it/s]                                               {'train_runtime': 1.0574, 'train_samples_per_second': 401.935, 'train_steps_per_second': 14.186, 'train_loss': 0.7051207741101583, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.64it/s]100%|██████████| 15/15 [00:01<00:00, 14.19it/s]
CLIENT:67
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7071, 'grad_norm': 0.19570253789424896, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.25it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.95it/s]                                              {'loss': 0.7171, 'grad_norm': 0.21597491204738617, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.95it/s]                                              {'loss': 0.7076, 'grad_norm': 0.15582628548145294, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.95it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.7267, 'grad_norm': 0.23160237073898315, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.7179, 'grad_norm': 0.21607644855976105, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.48it/s] 40%|████      | 6/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.7027, 'grad_norm': 0.13274212181568146, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.51it/s]                                              {'loss': 0.7126, 'grad_norm': 0.15355776250362396, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.51it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.7214, 'grad_norm': 0.23655879497528076, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.7164, 'grad_norm': 0.235059455037117, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.24it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.716, 'grad_norm': 0.21786488592624664, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.7017, 'grad_norm': 0.1898319274187088, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.57it/s] 80%|████████  | 12/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.7254, 'grad_norm': 0.2281758040189743, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.7087, 'grad_norm': 0.19247286021709442, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.11it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.23it/s]                                               {'loss': 0.7098, 'grad_norm': 0.20968955755233765, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.23it/s]                                               {'loss': 0.7138, 'grad_norm': 0.24432013928890228, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.23it/s]                                               {'train_runtime': 1.0851, 'train_samples_per_second': 391.655, 'train_steps_per_second': 13.823, 'train_loss': 0.7136540373166402, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.23it/s]100%|██████████| 15/15 [00:01<00:00, 13.83it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 34.66it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.15it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.74it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.79it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.32it/s] 61%|██████    | 20/33 [00:00<00:00, 25.38it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.14it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.10it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.18it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.11it/s]100%|██████████| 33/33 [00:01<00:00, 26.01it/s]
{'eval_loss': 0.7113010287284851, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3087248322147651, 'eval_runtime': 1.3102, 'eval_samples_per_second': 796.087, 'eval_steps_per_second': 25.188}
ROUND:11
CLIENT:23
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7309, 'grad_norm': 0.22445014119148254, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.46it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.84it/s]                                              {'loss': 0.7121, 'grad_norm': 0.1319829374551773, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.84it/s]                                              {'loss': 0.7246, 'grad_norm': 0.16985486447811127, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.84it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7051, 'grad_norm': 0.12895506620407104, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7267, 'grad_norm': 0.23042123019695282, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.61it/s] 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.6985, 'grad_norm': 0.168991357088089, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.7071, 'grad_norm': 0.12150576710700989, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.46it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.7132, 'grad_norm': 0.17735719680786133, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.7191, 'grad_norm': 0.27681946754455566, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.29it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.7196, 'grad_norm': 0.1251991242170334, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.7133, 'grad_norm': 0.2199588418006897, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.18it/s] 80%|████████  | 12/15 [00:00<00:00, 15.80it/s]                                               {'loss': 0.6952, 'grad_norm': 0.18239405751228333, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.80it/s]                                               {'loss': 0.7152, 'grad_norm': 0.26103800535202026, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.80it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.7053, 'grad_norm': 0.09968559443950653, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.7132, 'grad_norm': 0.1925347000360489, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.41it/s]                                               {'train_runtime': 1.0708, 'train_samples_per_second': 396.891, 'train_steps_per_second': 14.008, 'train_loss': 0.71327250401179, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 14.01it/s]
CLIENT:81
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7037, 'grad_norm': 0.16599418222904205, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.86it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.03it/s]                                              {'loss': 0.7071, 'grad_norm': 0.1547355204820633, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.03it/s]                                              {'loss': 0.735, 'grad_norm': 0.28465530276298523, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.03it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.7279, 'grad_norm': 0.20222267508506775, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.7161, 'grad_norm': 0.21619060635566711, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.04it/s] 40%|████      | 6/15 [00:00<00:00, 15.74it/s]                                              {'loss': 0.699, 'grad_norm': 0.16196545958518982, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.74it/s]                                              {'loss': 0.7277, 'grad_norm': 0.24822096526622772, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.74it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.7164, 'grad_norm': 0.20668621361255646, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.7069, 'grad_norm': 0.16114270687103271, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.22it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.7143, 'grad_norm': 0.14792798459529877, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.719, 'grad_norm': 0.26903560757637024, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.84it/s] 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.7111, 'grad_norm': 0.19793063402175903, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.7162, 'grad_norm': 0.17564670741558075, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.7111, 'grad_norm': 0.20447880029678345, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.7158, 'grad_norm': 0.29194509983062744, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]                                               {'train_runtime': 1.0718, 'train_samples_per_second': 396.535, 'train_steps_per_second': 13.995, 'train_loss': 0.7151598493258159, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 14.00it/s]
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7058, 'grad_norm': 0.1647876352071762, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.99it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.25it/s]                                              {'loss': 0.7082, 'grad_norm': 0.0933677926659584, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.25it/s]                                              {'loss': 0.7239, 'grad_norm': 0.2602222263813019, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.25it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.38it/s]                                              {'loss': 0.7154, 'grad_norm': 0.18783313035964966, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.38it/s]                                              {'loss': 0.6981, 'grad_norm': 0.11651643365621567, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.38it/s] 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.6905, 'grad_norm': 0.13004107773303986, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.6946, 'grad_norm': 0.12239191681146622, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.22it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.7124, 'grad_norm': 0.20485694706439972, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.7073, 'grad_norm': 0.13326945900917053, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.13it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.77it/s]                                               {'loss': 0.7012, 'grad_norm': 0.1396804302930832, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.77it/s]                                               {'loss': 0.6994, 'grad_norm': 0.1578383445739746, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.77it/s] 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.7118, 'grad_norm': 0.19486702978610992, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.7097, 'grad_norm': 0.1800738275051117, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.26it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.35it/s]                                               {'loss': 0.7122, 'grad_norm': 0.14834964275360107, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.35it/s]                                               {'loss': 0.7092, 'grad_norm': 0.16737355291843414, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.35it/s]                                               {'train_runtime': 1.0806, 'train_samples_per_second': 393.287, 'train_steps_per_second': 13.881, 'train_loss': 0.7066478649775187, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.35it/s]100%|██████████| 15/15 [00:01<00:00, 13.89it/s]
CLIENT:34
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6995, 'grad_norm': 0.07278983294963837, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.37it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.98it/s]                                              {'loss': 0.7211, 'grad_norm': 0.15658771991729736, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.98it/s]                                              {'loss': 0.7265, 'grad_norm': 0.23426099121570587, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.98it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.7055, 'grad_norm': 0.09639950096607208, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.7358, 'grad_norm': 0.26984903216362, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.15it/s] 40%|████      | 6/15 [00:00<00:00, 15.71it/s]                                              {'loss': 0.6961, 'grad_norm': 0.07356944680213928, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.71it/s]                                              {'loss': 0.7245, 'grad_norm': 0.22973962128162384, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.71it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.7137, 'grad_norm': 0.14271214604377747, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.6888, 'grad_norm': 0.06823809444904327, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.42it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.7115, 'grad_norm': 0.16699373722076416, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.7057, 'grad_norm': 0.12752877175807953, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.95it/s] 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.7149, 'grad_norm': 0.14129413664340973, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.7045, 'grad_norm': 0.1286768764257431, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.40it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.7158, 'grad_norm': 0.15563040971755981, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.7078, 'grad_norm': 0.17045164108276367, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.39it/s]                                               {'train_runtime': 1.0704, 'train_samples_per_second': 397.046, 'train_steps_per_second': 14.013, 'train_loss': 0.7114473819732666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]100%|██████████| 15/15 [00:01<00:00, 14.02it/s]
CLIENT:62
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7189, 'grad_norm': 0.20280338823795319, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.31it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.7156, 'grad_norm': 0.16870461404323578, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.737, 'grad_norm': 0.3375031352043152, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.28it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.94it/s]                                              {'loss': 0.7288, 'grad_norm': 0.25487446784973145, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.94it/s]                                              {'loss': 0.709, 'grad_norm': 0.15925054252147675, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.94it/s] 40%|████      | 6/15 [00:00<00:00, 15.95it/s]                                              {'loss': 0.7159, 'grad_norm': 0.29209983348846436, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.95it/s]                                              {'loss': 0.7125, 'grad_norm': 0.22984404861927032, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.95it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.7223, 'grad_norm': 0.24363762140274048, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.706, 'grad_norm': 0.22329038381576538, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.57it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.7272, 'grad_norm': 0.22902393341064453, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.7111, 'grad_norm': 0.22443082928657532, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.10it/s] 80%|████████  | 12/15 [00:00<00:00, 15.96it/s]                                               {'loss': 0.7217, 'grad_norm': 0.28496822714805603, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.96it/s]                                               {'loss': 0.7209, 'grad_norm': 0.2417326718568802, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.96it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.7171, 'grad_norm': 0.20370757579803467, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.7399, 'grad_norm': 0.3238956034183502, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.62it/s]                                               {'train_runtime': 1.0453, 'train_samples_per_second': 406.566, 'train_steps_per_second': 14.349, 'train_loss': 0.7202636400858561, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.62it/s]100%|██████████| 15/15 [00:01<00:00, 14.35it/s]
CLIENT:46
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7023, 'grad_norm': 0.12180894613265991, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.14it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7046, 'grad_norm': 0.12261385470628738, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7412, 'grad_norm': 0.23359012603759766, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.25it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.706, 'grad_norm': 0.06882235407829285, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.7169, 'grad_norm': 0.23126372694969177, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.52it/s] 40%|████      | 6/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.7112, 'grad_norm': 0.1672200709581375, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.88it/s]                                              {'loss': 0.7187, 'grad_norm': 0.1832124888896942, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.88it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.46it/s]                                              {'loss': 0.7058, 'grad_norm': 0.17408393323421478, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.46it/s]                                              {'loss': 0.7028, 'grad_norm': 0.08636581897735596, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.46it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.7334, 'grad_norm': 0.16066768765449524, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6934, 'grad_norm': 0.10086303949356079, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.94it/s] 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.7023, 'grad_norm': 0.21318991482257843, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.7072, 'grad_norm': 0.1587359607219696, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.57it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.7224, 'grad_norm': 0.22083017230033875, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.7011, 'grad_norm': 0.07053044438362122, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.58it/s]                                               {'train_runtime': 1.0514, 'train_samples_per_second': 404.236, 'train_steps_per_second': 14.267, 'train_loss': 0.71129363377889, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.58it/s]100%|██████████| 15/15 [00:01<00:00, 14.27it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6918, 'grad_norm': 0.14924786984920502, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.08it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.21it/s]                                              {'loss': 0.7256, 'grad_norm': 0.2383454591035843, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.21it/s]                                              {'loss': 0.6881, 'grad_norm': 0.06308835744857788, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.21it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.6906, 'grad_norm': 0.13271170854568481, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.7125, 'grad_norm': 0.1702381670475006, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.17it/s] 40%|████      | 6/15 [00:00<00:00, 15.96it/s]                                              {'loss': 0.7169, 'grad_norm': 0.20593364536762238, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.96it/s]                                              {'loss': 0.7104, 'grad_norm': 0.1957131326198578, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.96it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.7043, 'grad_norm': 0.17203794419765472, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.7, 'grad_norm': 0.11018622666597366, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.58it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.7133, 'grad_norm': 0.13377812504768372, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.7023, 'grad_norm': 0.18831616640090942, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.23it/s] 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.7129, 'grad_norm': 0.16911913454532623, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.6983, 'grad_norm': 0.15253320336341858, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.56it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.7186, 'grad_norm': 0.19799405336380005, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.6934, 'grad_norm': 0.15726232528686523, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.53it/s]                                               {'train_runtime': 1.0557, 'train_samples_per_second': 402.568, 'train_steps_per_second': 14.208, 'train_loss': 0.705262569586436, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.53it/s]100%|██████████| 15/15 [00:01<00:00, 14.21it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7081, 'grad_norm': 0.12239652872085571, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.60it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.78it/s]                                              {'loss': 0.7149, 'grad_norm': 0.19400127232074738, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.78it/s]                                              {'loss': 0.7091, 'grad_norm': 0.20370465517044067, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.78it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.7105, 'grad_norm': 0.10370585322380066, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.7344, 'grad_norm': 0.2987719178199768, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.41it/s] 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.7082, 'grad_norm': 0.13913758099079132, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.7105, 'grad_norm': 0.22917203605175018, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.48it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.75it/s]                                              {'loss': 0.7101, 'grad_norm': 0.15010753273963928, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.75it/s]                                              {'loss': 0.707, 'grad_norm': 0.16317641735076904, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.75it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.7297, 'grad_norm': 0.1850622445344925, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.6979, 'grad_norm': 0.20097088813781738, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.33it/s] 80%|████████  | 12/15 [00:00<00:00, 15.80it/s]                                               {'loss': 0.7076, 'grad_norm': 0.16786690056324005, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.80it/s]                                               {'loss': 0.7146, 'grad_norm': 0.18874458968639374, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.80it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.7181, 'grad_norm': 0.20445111393928528, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.7009, 'grad_norm': 0.18575266003608704, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.44it/s]                                               {'train_runtime': 1.0663, 'train_samples_per_second': 398.585, 'train_steps_per_second': 14.068, 'train_loss': 0.7121100505193074, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]100%|██████████| 15/15 [00:01<00:00, 14.07it/s]
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7066, 'grad_norm': 0.1777946799993515, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.90it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.69it/s]                                              {'loss': 0.7064, 'grad_norm': 0.16874317824840546, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.69it/s]                                              {'loss': 0.7121, 'grad_norm': 0.18594540655612946, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.69it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.725, 'grad_norm': 0.2189941704273224, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6957, 'grad_norm': 0.11139625310897827, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.10it/s] 40%|████      | 6/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.7037, 'grad_norm': 0.19804520905017853, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.7077, 'grad_norm': 0.19297736883163452, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.09it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.7101, 'grad_norm': 0.23418597877025604, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.7055, 'grad_norm': 0.09242386370897293, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.08it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.7133, 'grad_norm': 0.15756011009216309, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.6985, 'grad_norm': 0.13228397071361542, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.73it/s] 80%|████████  | 12/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.7222, 'grad_norm': 0.2752816677093506, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.7075, 'grad_norm': 0.16225360333919525, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.23it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.33it/s]                                               {'loss': 0.7172, 'grad_norm': 0.19193783402442932, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.33it/s]                                               {'loss': 0.7175, 'grad_norm': 0.23414042592048645, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.33it/s]                                               {'train_runtime': 1.0855, 'train_samples_per_second': 391.52, 'train_steps_per_second': 13.818, 'train_loss': 0.7099240064620972, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.33it/s]100%|██████████| 15/15 [00:01<00:00, 13.82it/s]
CLIENT:11
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.711, 'grad_norm': 0.15601345896720886, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.05it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.19it/s]                                              {'loss': 0.7293, 'grad_norm': 0.2284027487039566, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.19it/s]                                              {'loss': 0.6958, 'grad_norm': 0.07930830866098404, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.19it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7204, 'grad_norm': 0.19738805294036865, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7305, 'grad_norm': 0.170058473944664, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.61it/s] 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.7122, 'grad_norm': 0.1116539016366005, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.7195, 'grad_norm': 0.17427752912044525, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.38it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.7159, 'grad_norm': 0.15249772369861603, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.7126, 'grad_norm': 0.13464082777500153, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.16it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.7176, 'grad_norm': 0.20009642839431763, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.75it/s]                                               {'loss': 0.71, 'grad_norm': 0.12739384174346924, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.75it/s] 80%|████████  | 12/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.7243, 'grad_norm': 0.1735410839319229, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.7151, 'grad_norm': 0.18357554078102112, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.39it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.7103, 'grad_norm': 0.10199590772390366, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.7298, 'grad_norm': 0.25826165080070496, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]                                               {'train_runtime': 1.0747, 'train_samples_per_second': 395.456, 'train_steps_per_second': 13.957, 'train_loss': 0.7169419725735983, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]100%|██████████| 15/15 [00:01<00:00, 13.96it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 32.97it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.80it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.30it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.43it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.31it/s] 61%|██████    | 20/33 [00:00<00:00, 25.29it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.05it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.21it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.04it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.21it/s]100%|██████████| 33/33 [00:01<00:00, 25.90it/s]
{'eval_loss': 0.7093712091445923, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.311601150527325, 'eval_runtime': 1.3151, 'eval_samples_per_second': 793.111, 'eval_steps_per_second': 25.094}
ROUND:12
CLIENT:80
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7094, 'grad_norm': 0.0830560252070427, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.08it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.24it/s]                                              {'loss': 0.6999, 'grad_norm': 0.11076853424310684, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.24it/s]                                              {'loss': 0.7232, 'grad_norm': 0.2565593719482422, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.24it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.69it/s]                                              {'loss': 0.7057, 'grad_norm': 0.11991208046674728, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.69it/s]                                              {'loss': 0.7081, 'grad_norm': 0.1821429431438446, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.69it/s] 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.6992, 'grad_norm': 0.08449780195951462, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.6986, 'grad_norm': 0.12425969541072845, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.48it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6937, 'grad_norm': 0.0909716933965683, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.7249, 'grad_norm': 0.21244636178016663, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.22it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.7071, 'grad_norm': 0.16588881611824036, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.6924, 'grad_norm': 0.0934106856584549, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.84it/s] 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.7366, 'grad_norm': 0.1978873908519745, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.7106, 'grad_norm': 0.218468576669693, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6932, 'grad_norm': 0.05771077796816826, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.7148, 'grad_norm': 0.21263830363750458, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.55it/s]                                               {'train_runtime': 1.0646, 'train_samples_per_second': 399.201, 'train_steps_per_second': 14.089, 'train_loss': 0.7078307390213012, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.55it/s]100%|██████████| 15/15 [00:01<00:00, 14.09it/s]
CLIENT:84
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7105, 'grad_norm': 0.1829163134098053, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.29it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.92it/s]                                              {'loss': 0.6998, 'grad_norm': 0.10430565476417542, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.92it/s]                                              {'loss': 0.6979, 'grad_norm': 0.12116282433271408, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.92it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.71it/s]                                              {'loss': 0.7133, 'grad_norm': 0.19100838899612427, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.71it/s]                                              {'loss': 0.6907, 'grad_norm': 0.08233097195625305, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.71it/s] 40%|████      | 6/15 [00:00<00:00, 16.05it/s]                                              {'loss': 0.707, 'grad_norm': 0.15661194920539856, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.05it/s]                                              {'loss': 0.7058, 'grad_norm': 0.10195881128311157, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.05it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.47it/s]                                              {'loss': 0.6993, 'grad_norm': 0.15033972263336182, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.47it/s]                                              {'loss': 0.7074, 'grad_norm': 0.18389186263084412, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.47it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.7024, 'grad_norm': 0.11122577637434006, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6861, 'grad_norm': 0.11557241529226303, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.94it/s] 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.733, 'grad_norm': 0.2476135641336441, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.7, 'grad_norm': 0.1773558109998703, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.57it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6919, 'grad_norm': 0.14778172969818115, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.7201, 'grad_norm': 0.14911793172359467, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.92it/s]                                               {'train_runtime': 1.0394, 'train_samples_per_second': 408.875, 'train_steps_per_second': 14.431, 'train_loss': 0.7043567697207133, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.92it/s]100%|██████████| 15/15 [00:01<00:00, 14.44it/s]
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6927, 'grad_norm': 0.10487890988588333, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.12it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.81it/s]                                              {'loss': 0.7032, 'grad_norm': 0.23705792427062988, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.81it/s]                                              {'loss': 0.7144, 'grad_norm': 0.2313210517168045, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.81it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.7134, 'grad_norm': 0.15705326199531555, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.7222, 'grad_norm': 0.27535301446914673, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.04it/s] 40%|████      | 6/15 [00:00<00:00, 16.28it/s]                                              {'loss': 0.7103, 'grad_norm': 0.16605305671691895, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.28it/s]                                              {'loss': 0.7204, 'grad_norm': 0.28514811396598816, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.28it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.38it/s]                                              {'loss': 0.7039, 'grad_norm': 0.1506870687007904, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.38it/s]                                              {'loss': 0.702, 'grad_norm': 0.10159412771463394, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.38it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.7124, 'grad_norm': 0.1450820118188858, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.6944, 'grad_norm': 0.22255432605743408, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.89it/s] 80%|████████  | 12/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.7273, 'grad_norm': 0.2463330328464508, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.7129, 'grad_norm': 0.16415300965309143, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.39it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.33it/s]                                               {'loss': 0.7133, 'grad_norm': 0.21502234041690826, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.33it/s]                                               {'loss': 0.7078, 'grad_norm': 0.23617085814476013, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.33it/s]                                               {'train_runtime': 1.0693, 'train_samples_per_second': 397.444, 'train_steps_per_second': 14.027, 'train_loss': 0.7100432594617208, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.33it/s]100%|██████████| 15/15 [00:01<00:00, 14.03it/s]
CLIENT:81
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7016, 'grad_norm': 0.17547878623008728, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.75it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.7051, 'grad_norm': 0.16269028186798096, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.7312, 'grad_norm': 0.30322280526161194, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.16it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.7252, 'grad_norm': 0.21434937417507172, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.7132, 'grad_norm': 0.23071274161338806, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.31it/s] 40%|████      | 6/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.6969, 'grad_norm': 0.17062662541866302, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.7243, 'grad_norm': 0.26220303773880005, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.19it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.7136, 'grad_norm': 0.21843966841697693, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.7049, 'grad_norm': 0.1707714945077896, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.10it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.7124, 'grad_norm': 0.1556594967842102, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.7152, 'grad_norm': 0.28499311208724976, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.58it/s] 80%|████████  | 12/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.7085, 'grad_norm': 0.21009643375873566, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.7138, 'grad_norm': 0.18350619077682495, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.33it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.7083, 'grad_norm': 0.2173415720462799, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.7118, 'grad_norm': 0.3091880679130554, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]                                               {'train_runtime': 1.0792, 'train_samples_per_second': 393.808, 'train_steps_per_second': 13.899, 'train_loss': 0.7124006430308024, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]100%|██████████| 15/15 [00:01<00:00, 13.90it/s]
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7043, 'grad_norm': 0.18999885022640228, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.66it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.12it/s]                                              {'loss': 0.7042, 'grad_norm': 0.17814913392066956, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.12it/s]                                              {'loss': 0.7098, 'grad_norm': 0.19629064202308655, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.12it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.722, 'grad_norm': 0.2323574721813202, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.6944, 'grad_norm': 0.11681760102510452, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.56it/s] 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.7011, 'grad_norm': 0.20903301239013672, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.7051, 'grad_norm': 0.20361007750034332, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.46it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.89it/s]                                              {'loss': 0.7069, 'grad_norm': 0.24793073534965515, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.89it/s]                                              {'loss': 0.7047, 'grad_norm': 0.09483855217695236, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.89it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.7112, 'grad_norm': 0.1666969656944275, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6968, 'grad_norm': 0.13931040465831757, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.43it/s] 80%|████████  | 12/15 [00:00<00:00, 15.82it/s]                                               {'loss': 0.7184, 'grad_norm': 0.29183322191238403, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.82it/s]                                               {'loss': 0.7054, 'grad_norm': 0.1715415120124817, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.82it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.7145, 'grad_norm': 0.20237639546394348, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.7144, 'grad_norm': 0.24703189730644226, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.69it/s]                                               {'train_runtime': 1.0477, 'train_samples_per_second': 405.66, 'train_steps_per_second': 14.317, 'train_loss': 0.7075438221295675, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.69it/s]100%|██████████| 15/15 [00:01<00:00, 14.32it/s]
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.706, 'grad_norm': 0.19851289689540863, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.78it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.7242, 'grad_norm': 0.25268280506134033, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.6965, 'grad_norm': 0.1314682513475418, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.97it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7109, 'grad_norm': 0.2570801079273224, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7183, 'grad_norm': 0.20482078194618225, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.61it/s] 40%|████      | 6/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.7108, 'grad_norm': 0.15214574337005615, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.7166, 'grad_norm': 0.2223198562860489, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.33it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.7225, 'grad_norm': 0.25185245275497437, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.7026, 'grad_norm': 0.16491761803627014, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.31it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.7094, 'grad_norm': 0.19351550936698914, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.7096, 'grad_norm': 0.1789666712284088, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.85it/s] 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.7254, 'grad_norm': 0.3404098153114319, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.7195, 'grad_norm': 0.2869425415992737, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.41it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.7238, 'grad_norm': 0.2100316882133484, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.711, 'grad_norm': 0.15728402137756348, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.47it/s]                                               {'train_runtime': 1.0652, 'train_samples_per_second': 398.971, 'train_steps_per_second': 14.081, 'train_loss': 0.713820747534434, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.47it/s]100%|██████████| 15/15 [00:01<00:00, 14.09it/s]
CLIENT:36
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7088, 'grad_norm': 0.19426919519901276, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.97it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.10it/s]                                              {'loss': 0.714, 'grad_norm': 0.18321722745895386, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.10it/s]                                              {'loss': 0.7093, 'grad_norm': 0.24952206015586853, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.10it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.65it/s]                                              {'loss': 0.7101, 'grad_norm': 0.22495517134666443, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.65it/s]                                              {'loss': 0.7207, 'grad_norm': 0.19053427875041962, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.65it/s] 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.6956, 'grad_norm': 0.1819031983613968, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.7273, 'grad_norm': 0.3146899938583374, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.38it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.7092, 'grad_norm': 0.1004183292388916, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.7088, 'grad_norm': 0.24175719916820526, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.20it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.7129, 'grad_norm': 0.27110594511032104, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.7018, 'grad_norm': 0.15910838544368744, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.74it/s] 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6993, 'grad_norm': 0.17254023253917694, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.7086, 'grad_norm': 0.17497064173221588, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.43it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.7072, 'grad_norm': 0.24024631083011627, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.7003, 'grad_norm': 0.23003962635993958, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.98it/s]                                               {'train_runtime': 1.0484, 'train_samples_per_second': 405.388, 'train_steps_per_second': 14.308, 'train_loss': 0.7089250802993774, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.98it/s]100%|██████████| 15/15 [00:01<00:00, 14.31it/s]
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7007, 'grad_norm': 0.12744154036045074, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.31it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.71it/s]                                              {'loss': 0.7072, 'grad_norm': 0.12263020128011703, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.71it/s]                                              {'loss': 0.7308, 'grad_norm': 0.3119153082370758, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.71it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.7255, 'grad_norm': 0.23609715700149536, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.7021, 'grad_norm': 0.1836278736591339, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.34it/s] 40%|████      | 6/15 [00:00<00:00, 15.81it/s]                                              {'loss': 0.6992, 'grad_norm': 0.13395944237709045, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.81it/s]                                              {'loss': 0.6882, 'grad_norm': 0.13965058326721191, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.81it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.52it/s]                                              {'loss': 0.7158, 'grad_norm': 0.2014181911945343, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.52it/s]                                              {'loss': 0.7069, 'grad_norm': 0.22640593349933624, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.52it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.716, 'grad_norm': 0.17106188833713531, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.6844, 'grad_norm': 0.16333480179309845, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.04it/s] 80%|████████  | 12/15 [00:00<00:00, 15.44it/s]                                               {'loss': 0.7208, 'grad_norm': 0.208896666765213, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.44it/s]                                               {'loss': 0.7085, 'grad_norm': 0.15819479525089264, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.44it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.7067, 'grad_norm': 0.21937809884548187, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.7037, 'grad_norm': 0.1658848077058792, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.83it/s]                                               {'train_runtime': 1.0519, 'train_samples_per_second': 404.046, 'train_steps_per_second': 14.26, 'train_loss': 0.7077615976333618, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.83it/s]100%|██████████| 15/15 [00:01<00:00, 14.27it/s]
CLIENT:69
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.701, 'grad_norm': 0.12967592477798462, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.54it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.91it/s]                                              {'loss': 0.705, 'grad_norm': 0.1717802882194519, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.91it/s]                                              {'loss': 0.7131, 'grad_norm': 0.17899668216705322, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.91it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.73it/s]                                              {'loss': 0.6987, 'grad_norm': 0.12770122289657593, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.73it/s]                                              {'loss': 0.7165, 'grad_norm': 0.17918913066387177, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.73it/s] 40%|████      | 6/15 [00:00<00:00, 15.62it/s]                                              {'loss': 0.7262, 'grad_norm': 0.23754912614822388, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.62it/s]                                              {'loss': 0.6991, 'grad_norm': 0.1647079735994339, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.62it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.6987, 'grad_norm': 0.14047113060951233, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.7102, 'grad_norm': 0.19918347895145416, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.35it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.7212, 'grad_norm': 0.20063334703445435, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.693, 'grad_norm': 0.15743301808834076, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.89it/s] 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.7008, 'grad_norm': 0.13522844016551971, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.7055, 'grad_norm': 0.13412746787071228, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.35it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.7043, 'grad_norm': 0.13353055715560913, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.7189, 'grad_norm': 0.2637275755405426, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.49it/s]                                               {'train_runtime': 1.0677, 'train_samples_per_second': 398.065, 'train_steps_per_second': 14.049, 'train_loss': 0.7074845433235168, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.49it/s]100%|██████████| 15/15 [00:01<00:00, 14.05it/s]
CLIENT:65
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7184, 'grad_norm': 0.11203320324420929, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.17it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.80it/s]                                              {'loss': 0.7194, 'grad_norm': 0.20769885182380676, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.80it/s]                                              {'loss': 0.7167, 'grad_norm': 0.08990822732448578, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.80it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.6947, 'grad_norm': 0.13229401409626007, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.7129, 'grad_norm': 0.16589120030403137, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.27it/s] 40%|████      | 6/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.701, 'grad_norm': 0.10955912619829178, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.7104, 'grad_norm': 0.12587638199329376, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.14it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.03it/s]                                              {'loss': 0.7059, 'grad_norm': 0.10139217227697372, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.03it/s]                                              {'loss': 0.7133, 'grad_norm': 0.21075916290283203, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.03it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.7153, 'grad_norm': 0.10586395859718323, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.7111, 'grad_norm': 0.12631762027740479, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.54it/s] 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.7093, 'grad_norm': 0.18764397501945496, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.7066, 'grad_norm': 0.15828844904899597, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.7148, 'grad_norm': 0.1380503624677658, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.7154, 'grad_norm': 0.12863808870315552, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]                                               {'train_runtime': 1.0868, 'train_samples_per_second': 391.059, 'train_steps_per_second': 13.802, 'train_loss': 0.7110170245170593, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]100%|██████████| 15/15 [00:01<00:00, 13.81it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 33.53it/s] 24%|██▍       | 8/33 [00:00<00:00, 26.84it/s] 33%|███▎      | 11/33 [00:00<00:00, 25.77it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.16it/s] 52%|█████▏    | 17/33 [00:00<00:00, 24.78it/s] 61%|██████    | 20/33 [00:00<00:00, 24.69it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.01it/s] 79%|███████▉  | 26/33 [00:01<00:00, 24.79it/s] 88%|████████▊ | 29/33 [00:01<00:00, 24.80it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.24it/s]100%|██████████| 33/33 [00:01<00:00, 25.66it/s]
{'eval_loss': 0.707298994064331, 'eval_model_preparation_time': 0.0026, 'eval_acc': 0.31639501438159157, 'eval_runtime': 1.331, 'eval_samples_per_second': 783.627, 'eval_steps_per_second': 24.794}
ROUND:13
CLIENT:14
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6882, 'grad_norm': 0.09125088900327682, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.09it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.27it/s]                                              {'loss': 0.7066, 'grad_norm': 0.16080030798912048, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.27it/s]                                              {'loss': 0.7099, 'grad_norm': 0.17332634329795837, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.27it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.7096, 'grad_norm': 0.1414778083562851, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.7002, 'grad_norm': 0.09741517901420593, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.32it/s] 40%|████      | 6/15 [00:00<00:00, 16.09it/s]                                              {'loss': 0.7022, 'grad_norm': 0.23103578388690948, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.09it/s]                                              {'loss': 0.7067, 'grad_norm': 0.178993359208107, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.09it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.7006, 'grad_norm': 0.15508000552654266, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.696, 'grad_norm': 0.11258870363235474, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.64it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7033, 'grad_norm': 0.1147681474685669, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.6896, 'grad_norm': 0.18372945487499237, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.12it/s] 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.6774, 'grad_norm': 0.14009928703308105, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.702, 'grad_norm': 0.10363401472568512, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.62it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.6808, 'grad_norm': 0.13555467128753662, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.7127, 'grad_norm': 0.28107091784477234, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.67it/s]                                               {'train_runtime': 1.0538, 'train_samples_per_second': 403.313, 'train_steps_per_second': 14.235, 'train_loss': 0.6990651766459147, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.67it/s]100%|██████████| 15/15 [00:01<00:00, 14.24it/s]
CLIENT:74
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7015, 'grad_norm': 0.09526971727609634, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.86it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.56it/s]                                              {'loss': 0.7055, 'grad_norm': 0.19852657616138458, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.56it/s]                                              {'loss': 0.7142, 'grad_norm': 0.2496650367975235, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.56it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.7175, 'grad_norm': 0.20517519116401672, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.6891, 'grad_norm': 0.07696398347616196, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.91it/s] 40%|████      | 6/15 [00:00<00:00, 15.92it/s]                                              {'loss': 0.7159, 'grad_norm': 0.29766646027565, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.92it/s]                                              {'loss': 0.7015, 'grad_norm': 0.14387670159339905, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.92it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.7034, 'grad_norm': 0.23851539194583893, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.7055, 'grad_norm': 0.1542445868253708, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.53it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.692, 'grad_norm': 0.15049666166305542, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.7035, 'grad_norm': 0.2204265296459198, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.02it/s] 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.7179, 'grad_norm': 0.14864985644817352, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.62it/s]                                               {'loss': 0.7064, 'grad_norm': 0.08616252988576889, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.62it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.7092, 'grad_norm': 0.2665233612060547, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.6987, 'grad_norm': 0.2051892876625061, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.60it/s]                                               {'train_runtime': 1.0615, 'train_samples_per_second': 400.395, 'train_steps_per_second': 14.132, 'train_loss': 0.7054649353027344, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.60it/s]100%|██████████| 15/15 [00:01<00:00, 14.14it/s]
CLIENT:15
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7151, 'grad_norm': 0.1763276308774948, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.82it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.57it/s]                                              {'loss': 0.7011, 'grad_norm': 0.1618032157421112, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.57it/s]                                              {'loss': 0.6889, 'grad_norm': 0.11145416647195816, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.57it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.95it/s]                                              {'loss': 0.7117, 'grad_norm': 0.16088612377643585, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.95it/s]                                              {'loss': 0.7092, 'grad_norm': 0.14108726382255554, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.95it/s] 40%|████      | 6/15 [00:00<00:00, 16.30it/s]                                              {'loss': 0.7031, 'grad_norm': 0.14811524748802185, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.30it/s]                                              {'loss': 0.7061, 'grad_norm': 0.10342191904783249, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.30it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.7052, 'grad_norm': 0.18666206300258636, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.6926, 'grad_norm': 0.17242048680782318, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.64it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.7039, 'grad_norm': 0.1605646163225174, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.6978, 'grad_norm': 0.0689915269613266, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.10it/s] 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.7227, 'grad_norm': 0.27322104573249817, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.7039, 'grad_norm': 0.16710373759269714, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.54it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.6967, 'grad_norm': 0.17206956446170807, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.7011, 'grad_norm': 0.13851596415042877, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.56it/s]                                               {'train_runtime': 1.0597, 'train_samples_per_second': 401.069, 'train_steps_per_second': 14.155, 'train_loss': 0.7039312680562337, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.56it/s]100%|██████████| 15/15 [00:01<00:00, 14.16it/s]
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7016, 'grad_norm': 0.1258578896522522, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.28it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.73it/s]                                              {'loss': 0.693, 'grad_norm': 0.1403866857290268, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.73it/s]                                              {'loss': 0.7161, 'grad_norm': 0.19792981445789337, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.73it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.6931, 'grad_norm': 0.10720515996217728, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.7031, 'grad_norm': 0.19674210250377655, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.12it/s] 40%|████      | 6/15 [00:00<00:00, 15.66it/s]                                              {'loss': 0.7053, 'grad_norm': 0.14577363431453705, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.66it/s]                                              {'loss': 0.7027, 'grad_norm': 0.21098582446575165, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.66it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.7042, 'grad_norm': 0.10043110698461533, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.28it/s]                                              {'loss': 0.7058, 'grad_norm': 0.12785150110721588, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.28it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.7034, 'grad_norm': 0.154195174574852, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.73it/s]                                               {'loss': 0.6906, 'grad_norm': 0.13153912127017975, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.73it/s] 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.71, 'grad_norm': 0.18382272124290466, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.7025, 'grad_norm': 0.08834382146596909, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.35it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.15it/s]                                               {'loss': 0.7031, 'grad_norm': 0.22513838112354279, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.15it/s]                                               {'loss': 0.6796, 'grad_norm': 0.15786699950695038, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.15it/s]                                               {'train_runtime': 1.0782, 'train_samples_per_second': 394.185, 'train_steps_per_second': 13.912, 'train_loss': 0.700945774714152, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.15it/s]100%|██████████| 15/15 [00:01<00:00, 13.92it/s]
CLIENT:32
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7006, 'grad_norm': 0.1518339216709137, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.46it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.82it/s]                                              {'loss': 0.7152, 'grad_norm': 0.26028114557266235, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.82it/s]                                              {'loss': 0.7088, 'grad_norm': 0.21118608117103577, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.82it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.82it/s]                                              {'loss': 0.7133, 'grad_norm': 0.22676582634449005, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.82it/s]                                              {'loss': 0.7125, 'grad_norm': 0.2853473126888275, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.82it/s] 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.6865, 'grad_norm': 0.12343785911798477, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.7124, 'grad_norm': 0.29037168622016907, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.50it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.6949, 'grad_norm': 0.13544520735740662, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7114, 'grad_norm': 0.22278448939323425, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.25it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.704, 'grad_norm': 0.28513652086257935, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6926, 'grad_norm': 0.21155919134616852, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.7165, 'grad_norm': 0.16403307020664215, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6944, 'grad_norm': 0.1648642122745514, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.43it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.7, 'grad_norm': 0.27938878536224365, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.7066, 'grad_norm': 0.25254184007644653, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.45it/s]                                               {'train_runtime': 1.0703, 'train_samples_per_second': 397.098, 'train_steps_per_second': 14.015, 'train_loss': 0.7046441555023193, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 14.02it/s]
CLIENT:59
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6963, 'grad_norm': 0.18893761932849884, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.06it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.75it/s]                                              {'loss': 0.7088, 'grad_norm': 0.1294955611228943, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.75it/s]                                              {'loss': 0.6953, 'grad_norm': 0.11248587816953659, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.75it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.12it/s]                                              {'loss': 0.7015, 'grad_norm': 0.07893820106983185, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.12it/s]                                              {'loss': 0.7022, 'grad_norm': 0.15097735822200775, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.12it/s] 40%|████      | 6/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.7143, 'grad_norm': 0.2671932876110077, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.7212, 'grad_norm': 0.23971658945083618, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.09it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.6971, 'grad_norm': 0.07912237197160721, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.692, 'grad_norm': 0.11852333694696426, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.08it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.6985, 'grad_norm': 0.09660738706588745, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.7115, 'grad_norm': 0.25629252195358276, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.62it/s] 80%|████████  | 12/15 [00:00<00:00, 15.64it/s]                                               {'loss': 0.6931, 'grad_norm': 0.09556041657924652, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.64it/s]                                               {'loss': 0.7147, 'grad_norm': 0.23913879692554474, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.64it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.6782, 'grad_norm': 0.07759872078895569, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.6883, 'grad_norm': 0.1326390504837036, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.86it/s]                                               {'train_runtime': 1.0699, 'train_samples_per_second': 397.224, 'train_steps_per_second': 14.02, 'train_loss': 0.7008593559265137, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.86it/s]100%|██████████| 15/15 [00:01<00:00, 14.02it/s]
CLIENT:91
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7038, 'grad_norm': 0.16674891114234924, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.88it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.03it/s]                                              {'loss': 0.7066, 'grad_norm': 0.18642516434192657, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.03it/s]                                              {'loss': 0.726, 'grad_norm': 0.2635491192340851, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.03it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.7059, 'grad_norm': 0.19760748744010925, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.7113, 'grad_norm': 0.2007679045200348, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.21it/s] 40%|████      | 6/15 [00:00<00:00, 15.70it/s]                                              {'loss': 0.7059, 'grad_norm': 0.19142962992191315, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.70it/s]                                              {'loss': 0.6993, 'grad_norm': 0.15958313643932343, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.70it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.39it/s]                                              {'loss': 0.7112, 'grad_norm': 0.19797006249427795, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.39it/s]                                              {'loss': 0.7065, 'grad_norm': 0.24013373255729675, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.39it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.7106, 'grad_norm': 0.21829591691493988, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.6974, 'grad_norm': 0.16157706081867218, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.89it/s] 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.7094, 'grad_norm': 0.2629980444908142, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.7099, 'grad_norm': 0.196074441075325, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.59it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.7064, 'grad_norm': 0.19029147922992706, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.7271, 'grad_norm': 0.24335888028144836, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.59it/s]                                               {'train_runtime': 1.0591, 'train_samples_per_second': 401.302, 'train_steps_per_second': 14.164, 'train_loss': 0.7091467142105102, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.59it/s]100%|██████████| 15/15 [00:01<00:00, 14.17it/s]
CLIENT:10
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7316, 'grad_norm': 0.20116440951824188, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.16it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.35it/s]                                              {'loss': 0.7174, 'grad_norm': 0.13498610258102417, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.35it/s]                                              {'loss': 0.7157, 'grad_norm': 0.1849260926246643, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.35it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.7111, 'grad_norm': 0.1540556401014328, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.7104, 'grad_norm': 0.15782985091209412, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.58it/s] 40%|████      | 6/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.7102, 'grad_norm': 0.22210811078548431, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.7043, 'grad_norm': 0.16291652619838715, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.40it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.719, 'grad_norm': 0.15869279205799103, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.7169, 'grad_norm': 0.24742214381694794, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.22it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.7087, 'grad_norm': 0.16341684758663177, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.7053, 'grad_norm': 0.2359953373670578, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.82it/s] 80%|████████  | 12/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.7112, 'grad_norm': 0.12454035878181458, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.7161, 'grad_norm': 0.21962079405784607, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.21it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.7086, 'grad_norm': 0.15320836007595062, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.7219, 'grad_norm': 0.23109769821166992, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]                                               {'train_runtime': 1.0847, 'train_samples_per_second': 391.804, 'train_steps_per_second': 13.828, 'train_loss': 0.7139017343521118, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]100%|██████████| 15/15 [00:01<00:00, 13.83it/s]
CLIENT:50
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7127, 'grad_norm': 0.2190953493118286, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.08it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.7076, 'grad_norm': 0.2040662318468094, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.7073, 'grad_norm': 0.2215307354927063, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.30it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.7236, 'grad_norm': 0.28067833185195923, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.7062, 'grad_norm': 0.21968995034694672, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.49it/s] 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.6915, 'grad_norm': 0.14986076951026917, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.7052, 'grad_norm': 0.3112141788005829, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.29it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.7075, 'grad_norm': 0.16338308155536652, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.7099, 'grad_norm': 0.20810136198997498, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.18it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.715, 'grad_norm': 0.248222216963768, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.7063, 'grad_norm': 0.2163088470697403, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.76it/s] 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.7122, 'grad_norm': 0.20704787969589233, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.7159, 'grad_norm': 0.30864202976226807, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.7061, 'grad_norm': 0.11890502274036407, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.709, 'grad_norm': 0.2954207956790924, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]                                               {'train_runtime': 1.0778, 'train_samples_per_second': 394.321, 'train_steps_per_second': 13.917, 'train_loss': 0.7090729991594951, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]100%|██████████| 15/15 [00:01<00:00, 13.92it/s]
CLIENT:53
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7046, 'grad_norm': 0.18705043196678162, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.99it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.62it/s]                                              {'loss': 0.6928, 'grad_norm': 0.08996568620204926, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.62it/s]                                              {'loss': 0.7229, 'grad_norm': 0.197424054145813, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.62it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7014, 'grad_norm': 0.08873393386602402, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7104, 'grad_norm': 0.24667344987392426, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.25it/s] 40%|████      | 6/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.7017, 'grad_norm': 0.14561951160430908, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.7183, 'grad_norm': 0.147317036986351, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.17it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.6992, 'grad_norm': 0.21880251169204712, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.6938, 'grad_norm': 0.08980602771043777, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.13it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.7185, 'grad_norm': 0.17403557896614075, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.7, 'grad_norm': 0.12886033952236176, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.74it/s] 80%|████████  | 12/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.7049, 'grad_norm': 0.20247676968574524, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.7121, 'grad_norm': 0.1955139935016632, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.21it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.7062, 'grad_norm': 0.1918441653251648, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.6905, 'grad_norm': 0.08896572142839432, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]                                               {'train_runtime': 1.0792, 'train_samples_per_second': 393.806, 'train_steps_per_second': 13.899, 'train_loss': 0.7051383892695109, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]100%|██████████| 15/15 [00:01<00:00, 13.90it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 36.78it/s] 24%|██▍       | 8/33 [00:00<00:00, 29.13it/s] 36%|███▋      | 12/33 [00:00<00:00, 26.62it/s] 45%|████▌     | 15/33 [00:00<00:00, 25.78it/s] 55%|█████▍    | 18/33 [00:00<00:00, 25.47it/s] 64%|██████▎   | 21/33 [00:00<00:00, 25.46it/s] 73%|███████▎  | 24/33 [00:00<00:00, 25.24it/s] 82%|████████▏ | 27/33 [00:01<00:00, 25.23it/s] 91%|█████████ | 30/33 [00:01<00:00, 25.13it/s]100%|██████████| 33/33 [00:01<00:00, 26.09it/s]100%|██████████| 33/33 [00:01<00:00, 26.18it/s]
{'eval_loss': 0.7051599621772766, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3211888782358581, 'eval_runtime': 1.3023, 'eval_samples_per_second': 800.885, 'eval_steps_per_second': 25.34}
ROUND:14
CLIENT:72
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.706, 'grad_norm': 0.12806624174118042, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.58it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.80it/s]                                              {'loss': 0.7132, 'grad_norm': 0.25690758228302, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.80it/s]                                              {'loss': 0.7075, 'grad_norm': 0.2998060882091522, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.80it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.7128, 'grad_norm': 0.22620369493961334, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.7098, 'grad_norm': 0.28677940368652344, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.43it/s] 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.6903, 'grad_norm': 0.09937387704849243, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.7023, 'grad_norm': 0.09698138386011124, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.50it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.708, 'grad_norm': 0.27877116203308105, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.7118, 'grad_norm': 0.3566403090953827, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.31it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.7111, 'grad_norm': 0.24673685431480408, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.7084, 'grad_norm': 0.21239326894283295, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.18it/s] 80%|████████  | 12/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.7064, 'grad_norm': 0.21001514792442322, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.7021, 'grad_norm': 0.257315069437027, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.85it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.7084, 'grad_norm': 0.20494085550308228, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.7054, 'grad_norm': 0.25023239850997925, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.44it/s]                                               {'train_runtime': 1.0694, 'train_samples_per_second': 397.425, 'train_steps_per_second': 14.027, 'train_loss': 0.7069005966186523, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]100%|██████████| 15/15 [00:01<00:00, 14.03it/s]
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6903, 'grad_norm': 0.11267075687646866, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.75it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.6968, 'grad_norm': 0.2630896270275116, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.7082, 'grad_norm': 0.25787651538848877, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.15it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.7092, 'grad_norm': 0.1730828434228897, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.7144, 'grad_norm': 0.3100986182689667, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.29it/s] 40%|████      | 6/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.706, 'grad_norm': 0.18204154074192047, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.7123, 'grad_norm': 0.32060155272483826, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.16it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.6999, 'grad_norm': 0.16585735976696014, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.6997, 'grad_norm': 0.11098738759756088, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.77it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.7083, 'grad_norm': 0.16251112520694733, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6881, 'grad_norm': 0.24942658841609955, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.31it/s] 80%|████████  | 12/15 [00:00<00:00, 15.78it/s]                                               {'loss': 0.7204, 'grad_norm': 0.2769705653190613, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.78it/s]                                               {'loss': 0.7085, 'grad_norm': 0.1786973476409912, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.78it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.7072, 'grad_norm': 0.23918884992599487, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.7011, 'grad_norm': 0.2683733105659485, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.69it/s]                                               {'train_runtime': 1.0587, 'train_samples_per_second': 401.452, 'train_steps_per_second': 14.169, 'train_loss': 0.704703668753306, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.69it/s]100%|██████████| 15/15 [00:01<00:00, 14.17it/s]
CLIENT:58
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7057, 'grad_norm': 0.23567157983779907, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.55it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.05it/s]                                              {'loss': 0.7027, 'grad_norm': 0.31779053807258606, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.05it/s]                                              {'loss': 0.6889, 'grad_norm': 0.1529766470193863, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.05it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.7161, 'grad_norm': 0.22713710367679596, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.7102, 'grad_norm': 0.30426302552223206, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.31it/s] 40%|████      | 6/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.7091, 'grad_norm': 0.23430249094963074, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.71, 'grad_norm': 0.28337201476097107, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.15it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.6937, 'grad_norm': 0.31150081753730774, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.7087, 'grad_norm': 0.15228691697120667, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.37it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.705, 'grad_norm': 0.15801310539245605, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.7079, 'grad_norm': 0.28766536712646484, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.86it/s] 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.7142, 'grad_norm': 0.4236312508583069, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.7085, 'grad_norm': 0.35421499609947205, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.37it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.7065, 'grad_norm': 0.29507532715797424, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.42it/s]                                               {'loss': 0.6898, 'grad_norm': 0.0973392203450203, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]                                               {'train_runtime': 1.0772, 'train_samples_per_second': 394.551, 'train_steps_per_second': 13.925, 'train_loss': 0.7051372090975444, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.42it/s]100%|██████████| 15/15 [00:01<00:00, 13.93it/s]
CLIENT:2
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7052, 'grad_norm': 0.1502150148153305, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.20it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.79it/s]                                              {'loss': 0.7007, 'grad_norm': 0.221061572432518, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.79it/s]                                              {'loss': 0.7016, 'grad_norm': 0.20119328796863556, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.79it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.7085, 'grad_norm': 0.2487749457359314, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.04it/s]                                              {'loss': 0.707, 'grad_norm': 0.13315126299858093, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.04it/s] 40%|████      | 6/15 [00:00<00:00, 16.32it/s]                                              {'loss': 0.6962, 'grad_norm': 0.1191994920372963, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.32it/s]                                              {'loss': 0.7025, 'grad_norm': 0.15675151348114014, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.32it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.7063, 'grad_norm': 0.16355134546756744, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.6984, 'grad_norm': 0.25243276357650757, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.64it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7125, 'grad_norm': 0.28523319959640503, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7039, 'grad_norm': 0.11890846490859985, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.12it/s] 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.6971, 'grad_norm': 0.11675203591585159, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.7047, 'grad_norm': 0.1820502132177353, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.53it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.7045, 'grad_norm': 0.17950953543186188, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.7107, 'grad_norm': 0.19994927942752838, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.58it/s]                                               {'train_runtime': 1.0579, 'train_samples_per_second': 401.748, 'train_steps_per_second': 14.179, 'train_loss': 0.7039825836817424, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.58it/s]100%|██████████| 15/15 [00:01<00:00, 14.18it/s]
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7024, 'grad_norm': 0.25762197375297546, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.85it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.02it/s]                                              {'loss': 0.6947, 'grad_norm': 0.09487121552228928, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.02it/s]                                              {'loss': 0.7007, 'grad_norm': 0.13320153951644897, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.02it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.7112, 'grad_norm': 0.11344218999147415, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.7124, 'grad_norm': 0.21962212026119232, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.15it/s] 40%|████      | 6/15 [00:00<00:00, 15.86it/s]                                              {'loss': 0.6877, 'grad_norm': 0.1378951072692871, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.86it/s]                                              {'loss': 0.7003, 'grad_norm': 0.20707815885543823, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.86it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.6987, 'grad_norm': 0.13932225108146667, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.6977, 'grad_norm': 0.16924113035202026, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.24it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.7171, 'grad_norm': 0.34592944383621216, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.695, 'grad_norm': 0.06925424933433533, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.80it/s] 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.7055, 'grad_norm': 0.1441466361284256, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.705, 'grad_norm': 0.22417554259300232, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.32it/s]                                               {'loss': 0.6952, 'grad_norm': 0.15140405297279358, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.32it/s]                                               {'loss': 0.6885, 'grad_norm': 0.13795959949493408, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.32it/s]                                               {'train_runtime': 1.0729, 'train_samples_per_second': 396.133, 'train_steps_per_second': 13.981, 'train_loss': 0.7008109450340271, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.32it/s]100%|██████████| 15/15 [00:01<00:00, 13.99it/s]
CLIENT:84
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7058, 'grad_norm': 0.20063713192939758, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.62it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.14it/s]                                              {'loss': 0.6973, 'grad_norm': 0.11319713294506073, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.14it/s]                                              {'loss': 0.695, 'grad_norm': 0.13179278373718262, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.14it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.7082, 'grad_norm': 0.21423472464084625, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.6891, 'grad_norm': 0.08778099715709686, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.37it/s] 40%|████      | 6/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.703, 'grad_norm': 0.17092759907245636, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.7034, 'grad_norm': 0.11143846064805984, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.28it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.6953, 'grad_norm': 0.16498854756355286, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.7025, 'grad_norm': 0.20436440408229828, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.18it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.77it/s]                                               {'loss': 0.6997, 'grad_norm': 0.12165483832359314, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.77it/s]                                               {'loss': 0.6832, 'grad_norm': 0.12683044373989105, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.77it/s] 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.7261, 'grad_norm': 0.2779998183250427, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.6952, 'grad_norm': 0.1959003210067749, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.26it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.33it/s]                                               {'loss': 0.6881, 'grad_norm': 0.16211926937103271, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.33it/s]                                               {'loss': 0.7164, 'grad_norm': 0.16391567885875702, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.33it/s]                                               {'train_runtime': 1.0803, 'train_samples_per_second': 393.4, 'train_steps_per_second': 13.885, 'train_loss': 0.7005468169848125, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.33it/s]100%|██████████| 15/15 [00:01<00:00, 13.89it/s]
CLIENT:54
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7144, 'grad_norm': 0.23532573878765106, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.14it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.96it/s]                                              {'loss': 0.7042, 'grad_norm': 0.12234736233949661, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.96it/s]                                              {'loss': 0.6954, 'grad_norm': 0.1470915526151657, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.96it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.7009, 'grad_norm': 0.1956794559955597, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.7093, 'grad_norm': 0.10055810958147049, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.23it/s] 40%|████      | 6/15 [00:00<00:00, 15.90it/s]                                              {'loss': 0.7096, 'grad_norm': 0.2437090277671814, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.90it/s]                                              {'loss': 0.7042, 'grad_norm': 0.26576441526412964, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.90it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.33it/s]                                              {'loss': 0.6936, 'grad_norm': 0.12692765891551971, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.33it/s]                                              {'loss': 0.6972, 'grad_norm': 0.12535756826400757, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.33it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6972, 'grad_norm': 0.1857132762670517, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.7005, 'grad_norm': 0.15940634906291962, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.92it/s] 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.7053, 'grad_norm': 0.22213374078273773, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.6979, 'grad_norm': 0.22645875811576843, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.37it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.7051, 'grad_norm': 0.20137211680412292, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.6857, 'grad_norm': 0.1034516841173172, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.36it/s]                                               {'train_runtime': 1.0686, 'train_samples_per_second': 397.708, 'train_steps_per_second': 14.037, 'train_loss': 0.7013533393541972, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]100%|██████████| 15/15 [00:01<00:00, 14.04it/s]
CLIENT:75
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6942, 'grad_norm': 0.09404164552688599, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.04it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.715, 'grad_norm': 0.25376516580581665, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.30it/s]                                              {'loss': 0.7077, 'grad_norm': 0.1896532028913498, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.30it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.7064, 'grad_norm': 0.12400805205106735, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.7205, 'grad_norm': 0.26591259241104126, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.66it/s] 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.6978, 'grad_norm': 0.1660432070493698, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.7086, 'grad_norm': 0.26557087898254395, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.50it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.82it/s]                                              {'loss': 0.6853, 'grad_norm': 0.15476752817630768, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.82it/s]                                              {'loss': 0.6997, 'grad_norm': 0.14015214145183563, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.82it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.6998, 'grad_norm': 0.1376739740371704, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.6938, 'grad_norm': 0.16336286067962646, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.35it/s] 80%|████████  | 12/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.6967, 'grad_norm': 0.27332603931427, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.707, 'grad_norm': 0.14290373027324677, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.66it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.706, 'grad_norm': 0.22635340690612793, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.7082, 'grad_norm': 0.16586104035377502, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.65it/s]                                               {'train_runtime': 1.0571, 'train_samples_per_second': 402.051, 'train_steps_per_second': 14.19, 'train_loss': 0.7031186699867249, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.65it/s]100%|██████████| 15/15 [00:01<00:00, 14.19it/s]
CLIENT:28
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7053, 'grad_norm': 0.1452207714319229, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.12it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.7033, 'grad_norm': 0.2211916595697403, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.7134, 'grad_norm': 0.250426322221756, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.33it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.47it/s]                                              {'loss': 0.7127, 'grad_norm': 0.1711568683385849, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.47it/s]                                              {'loss': 0.7007, 'grad_norm': 0.19714638590812683, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.47it/s] 40%|████      | 6/15 [00:00<00:00, 15.27it/s]                                              {'loss': 0.7151, 'grad_norm': 0.24106010794639587, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.27it/s]                                              {'loss': 0.7179, 'grad_norm': 0.1986311823129654, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.27it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.6968, 'grad_norm': 0.2748030722141266, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.6923, 'grad_norm': 0.11235564202070236, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.78it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.7063, 'grad_norm': 0.1883252114057541, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.7034, 'grad_norm': 0.2549755573272705, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.91it/s] 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.7172, 'grad_norm': 0.15929020941257477, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.714, 'grad_norm': 0.20676285028457642, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.40it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.7119, 'grad_norm': 0.3165476322174072, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.7048, 'grad_norm': 0.10926184058189392, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]                                               {'train_runtime': 1.071, 'train_samples_per_second': 396.813, 'train_steps_per_second': 14.005, 'train_loss': 0.7076810836791992, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]100%|██████████| 15/15 [00:01<00:00, 14.01it/s]
CLIENT:40
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7003, 'grad_norm': 0.19245271384716034, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.93it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.08it/s]                                              {'loss': 0.6907, 'grad_norm': 0.1609230786561966, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.08it/s]                                              {'loss': 0.7092, 'grad_norm': 0.24983176589012146, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.08it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.709, 'grad_norm': 0.1777963489294052, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.7154, 'grad_norm': 0.2094377726316452, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.66it/s] 40%|████      | 6/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.6951, 'grad_norm': 0.16903989017009735, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.6873, 'grad_norm': 0.19030076265335083, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.41it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.7036, 'grad_norm': 0.1771547943353653, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.7071, 'grad_norm': 0.2517094612121582, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.23it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.7134, 'grad_norm': 0.29315897822380066, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6993, 'grad_norm': 0.06223509460687637, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.71it/s] 80%|████████  | 12/15 [00:00<00:00, 15.42it/s]                                               {'loss': 0.7195, 'grad_norm': 0.2747984826564789, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.42it/s]                                               {'loss': 0.6864, 'grad_norm': 0.20013758540153503, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.42it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.97it/s]                                               {'loss': 0.6882, 'grad_norm': 0.21331092715263367, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.97it/s]                                               {'loss': 0.6961, 'grad_norm': 0.17176483571529388, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.97it/s]                                               {'train_runtime': 1.0552, 'train_samples_per_second': 402.765, 'train_steps_per_second': 14.215, 'train_loss': 0.7013629674911499, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.97it/s]100%|██████████| 15/15 [00:01<00:00, 14.22it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 34.18it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.60it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.12it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.35it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.10it/s] 61%|██████    | 20/33 [00:00<00:00, 25.19it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.04it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.05it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.06it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.14it/s]100%|██████████| 33/33 [00:01<00:00, 25.84it/s]
{'eval_loss': 0.7026345133781433, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.3480345158197507, 'eval_runtime': 1.3183, 'eval_samples_per_second': 791.16, 'eval_steps_per_second': 25.032}
ROUND:15
CLIENT:2
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7033, 'grad_norm': 0.15581999719142914, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.98it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.46it/s]                                              {'loss': 0.6975, 'grad_norm': 0.23537175357341766, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.46it/s]                                              {'loss': 0.6987, 'grad_norm': 0.21417322754859924, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.46it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.7048, 'grad_norm': 0.2616124153137207, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.7052, 'grad_norm': 0.13839054107666016, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.53it/s] 40%|████      | 6/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.6948, 'grad_norm': 0.12362701445817947, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.7003, 'grad_norm': 0.163435161113739, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.36it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.7038, 'grad_norm': 0.17468947172164917, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6946, 'grad_norm': 0.2682487368583679, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.22it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.7081, 'grad_norm': 0.30321890115737915, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.7023, 'grad_norm': 0.12567543983459473, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.82it/s] 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6957, 'grad_norm': 0.12156414985656738, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.702, 'grad_norm': 0.19325871765613556, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.32it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.7018, 'grad_norm': 0.18730947375297546, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.7077, 'grad_norm': 0.21032805740833282, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.43it/s]                                               {'train_runtime': 1.073, 'train_samples_per_second': 396.088, 'train_steps_per_second': 13.98, 'train_loss': 0.7013853907585144, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.43it/s]100%|██████████| 15/15 [00:01<00:00, 13.98it/s]
CLIENT:49
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7126, 'grad_norm': 0.22459086775779724, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.83it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.7063, 'grad_norm': 0.15359964966773987, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.715, 'grad_norm': 0.33090391755104065, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.06it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.07it/s]                                              {'loss': 0.7042, 'grad_norm': 0.22579030692577362, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.07it/s]                                              {'loss': 0.6989, 'grad_norm': 0.20245744287967682, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.07it/s] 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.7092, 'grad_norm': 0.2486008256673813, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.7055, 'grad_norm': 0.28383055329322815, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.97it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7047, 'grad_norm': 0.21486610174179077, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.6953, 'grad_norm': 0.1828078180551529, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.61it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.6975, 'grad_norm': 0.3252579867839813, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.6911, 'grad_norm': 0.18347914516925812, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.38it/s] 80%|████████  | 12/15 [00:00<00:00, 15.90it/s]                                               {'loss': 0.6882, 'grad_norm': 0.14930112659931183, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.90it/s]                                               {'loss': 0.7106, 'grad_norm': 0.2136196792125702, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.90it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.693, 'grad_norm': 0.2760327160358429, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.6917, 'grad_norm': 0.249136283993721, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.63it/s]                                               {'train_runtime': 1.0505, 'train_samples_per_second': 404.576, 'train_steps_per_second': 14.279, 'train_loss': 0.7015839179356893, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.63it/s]100%|██████████| 15/15 [00:01<00:00, 14.28it/s]
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6957, 'grad_norm': 0.14558079838752747, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.39it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.07it/s]                                              {'loss': 0.7024, 'grad_norm': 0.1393381506204605, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.07it/s]                                              {'loss': 0.7171, 'grad_norm': 0.37144044041633606, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.07it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.715, 'grad_norm': 0.27855589985847473, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.90it/s]                                              {'loss': 0.694, 'grad_norm': 0.22059012949466705, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.90it/s] 40%|████      | 6/15 [00:00<00:00, 16.18it/s]                                              {'loss': 0.6939, 'grad_norm': 0.1580199897289276, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.18it/s]                                              {'loss': 0.6823, 'grad_norm': 0.16139450669288635, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.18it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.7069, 'grad_norm': 0.23767127096652985, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.6968, 'grad_norm': 0.2687286138534546, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.40it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.7084, 'grad_norm': 0.20345889031887054, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.6776, 'grad_norm': 0.18714168667793274, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.95it/s] 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.7114, 'grad_norm': 0.24702532589435577, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.7015, 'grad_norm': 0.18452708423137665, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.43it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6967, 'grad_norm': 0.2576964199542999, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6967, 'grad_norm': 0.18941476941108704, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.45it/s]                                               {'train_runtime': 1.0661, 'train_samples_per_second': 398.637, 'train_steps_per_second': 14.07, 'train_loss': 0.6997477253278096, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 14.07it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7016, 'grad_norm': 0.14647215604782104, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.26it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.03it/s]                                              {'loss': 0.7035, 'grad_norm': 0.24272851645946503, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.03it/s]                                              {'loss': 0.6974, 'grad_norm': 0.24776746332645416, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.03it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7053, 'grad_norm': 0.12354876846075058, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7161, 'grad_norm': 0.376219242811203, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.61it/s] 40%|████      | 6/15 [00:00<00:00, 15.64it/s]                                              {'loss': 0.7006, 'grad_norm': 0.16759803891181946, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.64it/s]                                              {'loss': 0.6963, 'grad_norm': 0.2898412346839905, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.64it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.7015, 'grad_norm': 0.18105216324329376, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.6976, 'grad_norm': 0.2008584588766098, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.42it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.7184, 'grad_norm': 0.2295493185520172, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.6854, 'grad_norm': 0.25098517537117004, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.26it/s] 80%|████████  | 12/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.6982, 'grad_norm': 0.20350633561611176, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.7031, 'grad_norm': 0.2337290346622467, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.85it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.7055, 'grad_norm': 0.2536466121673584, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.6897, 'grad_norm': 0.23207950592041016, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.44it/s]                                               {'train_runtime': 1.0658, 'train_samples_per_second': 398.775, 'train_steps_per_second': 14.074, 'train_loss': 0.7013481815656026, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]100%|██████████| 15/15 [00:01<00:00, 14.08it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6978, 'grad_norm': 0.14982281625270844, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.15it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.6947, 'grad_norm': 0.27683037519454956, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.7076, 'grad_norm': 0.331633061170578, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.24it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.701, 'grad_norm': 0.16077543795108795, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.7082, 'grad_norm': 0.32515671849250793, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.20it/s] 40%|████      | 6/15 [00:00<00:00, 15.95it/s]                                              {'loss': 0.7075, 'grad_norm': 0.2365819662809372, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.95it/s]                                              {'loss': 0.7168, 'grad_norm': 0.31587353348731995, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.95it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.6919, 'grad_norm': 0.2015431970357895, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.6915, 'grad_norm': 0.2392328679561615, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.51it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.7106, 'grad_norm': 0.25326672196388245, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.20it/s]                                               {'loss': 0.6921, 'grad_norm': 0.24674634635448456, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.20it/s] 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6927, 'grad_norm': 0.25927290320396423, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6938, 'grad_norm': 0.1765230894088745, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.54it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6972, 'grad_norm': 0.3036518692970276, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.7064, 'grad_norm': 0.3563072383403778, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.55it/s]                                               {'train_runtime': 1.0551, 'train_samples_per_second': 402.798, 'train_steps_per_second': 14.216, 'train_loss': 0.7006478031476339, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.55it/s]100%|██████████| 15/15 [00:01<00:00, 14.22it/s]
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7102, 'grad_norm': 0.23278069496154785, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.09it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.80it/s]                                              {'loss': 0.71, 'grad_norm': 0.22170905768871307, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.80it/s]                                              {'loss': 0.7123, 'grad_norm': 0.19735591113567352, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.80it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.6992, 'grad_norm': 0.19584009051322937, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.709, 'grad_norm': 0.239760622382164, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.19it/s] 40%|████      | 6/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.7096, 'grad_norm': 0.21768973767757416, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.7032, 'grad_norm': 0.13934285938739777, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.17it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.6952, 'grad_norm': 0.2843301296234131, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.7038, 'grad_norm': 0.2625595033168793, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.91it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.698, 'grad_norm': 0.1918579638004303, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.7083, 'grad_norm': 0.22751200199127197, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.39it/s] 80%|████████  | 12/15 [00:00<00:00, 15.75it/s]                                               {'loss': 0.7059, 'grad_norm': 0.3142586946487427, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.75it/s]                                               {'loss': 0.6995, 'grad_norm': 0.27010199427604675, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.75it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6962, 'grad_norm': 0.19246692955493927, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.7145, 'grad_norm': 0.226339653134346, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.55it/s]                                               {'train_runtime': 1.0626, 'train_samples_per_second': 399.977, 'train_steps_per_second': 14.117, 'train_loss': 0.7049921313921611, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.55it/s]100%|██████████| 15/15 [00:01<00:00, 14.12it/s]
CLIENT:87
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7077, 'grad_norm': 0.13998563587665558, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.09it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.76it/s]                                              {'loss': 0.7014, 'grad_norm': 0.2000643014907837, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.76it/s]                                              {'loss': 0.6962, 'grad_norm': 0.0822855532169342, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.76it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.6936, 'grad_norm': 0.10053170472383499, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.7085, 'grad_norm': 0.10376747697591782, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.15it/s] 40%|████      | 6/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.6974, 'grad_norm': 0.20370224118232727, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.7115, 'grad_norm': 0.07160935550928116, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.11it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.07it/s]                                              {'loss': 0.7063, 'grad_norm': 0.1881246417760849, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.07it/s]                                              {'loss': 0.6945, 'grad_norm': 0.1261533945798874, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.07it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.7015, 'grad_norm': 0.15745322406291962, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6981, 'grad_norm': 0.06649016588926315, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.71it/s] 80%|████████  | 12/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.6931, 'grad_norm': 0.18011724948883057, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.7029, 'grad_norm': 0.07085099071264267, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.21it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.6936, 'grad_norm': 0.07335297763347626, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.7112, 'grad_norm': 0.39680495858192444, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]                                               {'train_runtime': 1.0836, 'train_samples_per_second': 392.216, 'train_steps_per_second': 13.843, 'train_loss': 0.7011784195899964, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]100%|██████████| 15/15 [00:01<00:00, 13.85it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6833, 'grad_norm': 0.1855248212814331, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.95it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.66it/s]                                              {'loss': 0.7115, 'grad_norm': 0.29943910241127014, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.66it/s]                                              {'loss': 0.6872, 'grad_norm': 0.07209663838148117, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.66it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.99it/s]                                              {'loss': 0.6831, 'grad_norm': 0.16093973815441132, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.99it/s]                                              {'loss': 0.7025, 'grad_norm': 0.20989638566970825, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.99it/s] 40%|████      | 6/15 [00:00<00:00, 16.02it/s]                                              {'loss': 0.7047, 'grad_norm': 0.2573750615119934, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.02it/s]                                              {'loss': 0.6985, 'grad_norm': 0.24681340157985687, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.02it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.6942, 'grad_norm': 0.20922115445137024, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.6945, 'grad_norm': 0.13183538615703583, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.60it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.7055, 'grad_norm': 0.1626526117324829, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.6908, 'grad_norm': 0.23440520465373993, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.12it/s] 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.7028, 'grad_norm': 0.2141265720129013, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.6894, 'grad_norm': 0.18855255842208862, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.53it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.7062, 'grad_norm': 0.251252681016922, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.6842, 'grad_norm': 0.19351106882095337, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.56it/s]                                               {'train_runtime': 1.0608, 'train_samples_per_second': 400.653, 'train_steps_per_second': 14.141, 'train_loss': 0.6959062218666077, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.56it/s]100%|██████████| 15/15 [00:01<00:00, 14.15it/s]
CLIENT:99
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/25 [00:00<?, ?it/s]                                      {'loss': 0.702, 'grad_norm': 0.1967747062444687, 'learning_rate': 0.01, 'epoch': 0.2}
  4%|▍         | 1/25 [00:00<00:01, 13.05it/s]  8%|▊         | 2/25 [00:00<00:01, 12.98it/s]                                              {'loss': 0.6945, 'grad_norm': 0.27290230989456177, 'learning_rate': 0.0096, 'epoch': 0.4}
  8%|▊         | 2/25 [00:00<00:01, 12.98it/s]                                              {'loss': 0.7095, 'grad_norm': 0.2713918685913086, 'learning_rate': 0.0092, 'epoch': 0.6}
 12%|█▏        | 3/25 [00:00<00:01, 12.98it/s] 16%|█▌        | 4/25 [00:00<00:01, 13.19it/s]                                              {'loss': 0.7081, 'grad_norm': 0.2936941087245941, 'learning_rate': 0.0088, 'epoch': 0.8}
 16%|█▌        | 4/25 [00:00<00:01, 13.19it/s]                                              {'loss': 0.694, 'grad_norm': 0.12855340540409088, 'learning_rate': 0.0084, 'epoch': 1.0}
 20%|██        | 5/25 [00:00<00:01, 13.19it/s] 24%|██▍       | 6/25 [00:00<00:01, 15.55it/s]                                              {'loss': 0.701, 'grad_norm': 0.22182387113571167, 'learning_rate': 0.008, 'epoch': 1.2}
 24%|██▍       | 6/25 [00:00<00:01, 15.55it/s]                                              {'loss': 0.698, 'grad_norm': 0.22146713733673096, 'learning_rate': 0.0076, 'epoch': 1.4}
 28%|██▊       | 7/25 [00:00<00:01, 15.55it/s] 32%|███▏      | 8/25 [00:00<00:01, 14.42it/s]                                              {'loss': 0.6959, 'grad_norm': 0.26707586646080017, 'learning_rate': 0.0072, 'epoch': 1.6}
 32%|███▏      | 8/25 [00:00<00:01, 14.42it/s]                                              {'loss': 0.7154, 'grad_norm': 0.2724585235118866, 'learning_rate': 0.0068000000000000005, 'epoch': 1.8}
 36%|███▌      | 9/25 [00:00<00:01, 14.42it/s]                                              {'loss': 0.695, 'grad_norm': 0.3973073363304138, 'learning_rate': 0.0064, 'epoch': 2.0}
 40%|████      | 10/25 [00:00<00:01, 14.42it/s] 44%|████▍     | 11/25 [00:00<00:00, 15.51it/s]                                               {'loss': 0.6953, 'grad_norm': 0.2668142318725586, 'learning_rate': 0.006, 'epoch': 2.2}
 44%|████▍     | 11/25 [00:00<00:00, 15.51it/s]                                               {'loss': 0.7025, 'grad_norm': 0.1010453999042511, 'learning_rate': 0.005600000000000001, 'epoch': 2.4}
 48%|████▊     | 12/25 [00:00<00:00, 15.51it/s] 52%|█████▏    | 13/25 [00:00<00:00, 14.63it/s]                                               {'loss': 0.6954, 'grad_norm': 0.2684193551540375, 'learning_rate': 0.005200000000000001, 'epoch': 2.6}
 52%|█████▏    | 13/25 [00:00<00:00, 14.63it/s]                                               {'loss': 0.7136, 'grad_norm': 0.3407853841781616, 'learning_rate': 0.0048, 'epoch': 2.8}
 56%|█████▌    | 14/25 [00:00<00:00, 14.63it/s]                                               {'loss': 0.7161, 'grad_norm': 0.49463388323783875, 'learning_rate': 0.0044, 'epoch': 3.0}
 60%|██████    | 15/25 [00:00<00:00, 14.63it/s] 64%|██████▍   | 16/25 [00:01<00:00, 15.55it/s]                                               {'loss': 0.6943, 'grad_norm': 0.24763892590999603, 'learning_rate': 0.004, 'epoch': 3.2}
 64%|██████▍   | 16/25 [00:01<00:00, 15.55it/s]                                               {'loss': 0.6954, 'grad_norm': 0.28044232726097107, 'learning_rate': 0.0036, 'epoch': 3.4}
 68%|██████▊   | 17/25 [00:01<00:00, 15.55it/s] 72%|███████▏  | 18/25 [00:01<00:00, 14.73it/s]                                               {'loss': 0.6984, 'grad_norm': 0.3143981993198395, 'learning_rate': 0.0032, 'epoch': 3.6}
 72%|███████▏  | 18/25 [00:01<00:00, 14.73it/s]                                               {'loss': 0.7008, 'grad_norm': 0.31291601061820984, 'learning_rate': 0.0028000000000000004, 'epoch': 3.8}
 76%|███████▌  | 19/25 [00:01<00:00, 14.73it/s]                                               {'loss': 0.6842, 'grad_norm': 0.17387406527996063, 'learning_rate': 0.0024, 'epoch': 4.0}
 80%|████████  | 20/25 [00:01<00:00, 14.73it/s] 84%|████████▍ | 21/25 [00:01<00:00, 15.50it/s]                                               {'loss': 0.7114, 'grad_norm': 0.1477360725402832, 'learning_rate': 0.002, 'epoch': 4.2}
 84%|████████▍ | 21/25 [00:01<00:00, 15.50it/s]                                               {'loss': 0.6947, 'grad_norm': 0.28587329387664795, 'learning_rate': 0.0016, 'epoch': 4.4}
 88%|████████▊ | 22/25 [00:01<00:00, 15.50it/s] 92%|█████████▏| 23/25 [00:01<00:00, 14.65it/s]                                               {'loss': 0.6963, 'grad_norm': 0.36145591735839844, 'learning_rate': 0.0012, 'epoch': 4.6}
 92%|█████████▏| 23/25 [00:01<00:00, 14.65it/s]                                               {'loss': 0.6959, 'grad_norm': 0.32799965143203735, 'learning_rate': 0.0008, 'epoch': 4.8}
 96%|█████████▌| 24/25 [00:01<00:00, 14.65it/s]                                               {'loss': 0.685, 'grad_norm': 0.3219502866268158, 'learning_rate': 0.0004, 'epoch': 5.0}
100%|██████████| 25/25 [00:01<00:00, 14.65it/s]                                               {'train_runtime': 1.7163, 'train_samples_per_second': 396.192, 'train_steps_per_second': 14.566, 'train_loss': 0.6997153234481811, 'epoch': 5.0}
100%|██████████| 25/25 [00:01<00:00, 14.65it/s]100%|██████████| 25/25 [00:01<00:00, 14.57it/s]
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6964, 'grad_norm': 0.2075522392988205, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.51it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.02it/s]                                              {'loss': 0.7037, 'grad_norm': 0.11228340864181519, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.02it/s]                                              {'loss': 0.7085, 'grad_norm': 0.32958176732063293, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.02it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.7043, 'grad_norm': 0.23519502580165863, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.6918, 'grad_norm': 0.1419583559036255, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.74it/s] 40%|████      | 6/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.684, 'grad_norm': 0.15281300246715546, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.6879, 'grad_norm': 0.14799907803535461, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.40it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.6999, 'grad_norm': 0.25679105520248413, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.7001, 'grad_norm': 0.16372068226337433, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.30it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.6932, 'grad_norm': 0.1729043871164322, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.6902, 'grad_norm': 0.19437721371650696, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.86it/s] 80%|████████  | 12/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.7002, 'grad_norm': 0.24168315529823303, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.6986, 'grad_norm': 0.2254149168729782, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.52it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.7034, 'grad_norm': 0.18340946733951569, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.6998, 'grad_norm': 0.212835893034935, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.51it/s]                                               {'train_runtime': 1.0687, 'train_samples_per_second': 397.682, 'train_steps_per_second': 14.036, 'train_loss': 0.6974764625231425, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.51it/s]100%|██████████| 15/15 [00:01<00:00, 14.04it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 35.68it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.68it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.23it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.54it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.13it/s] 61%|██████    | 20/33 [00:00<00:00, 24.88it/s] 70%|██████▉   | 23/33 [00:00<00:00, 24.67it/s] 79%|███████▉  | 26/33 [00:01<00:00, 24.90it/s] 88%|████████▊ | 29/33 [00:01<00:00, 24.99it/s] 97%|█████████▋| 32/33 [00:01<00:00, 24.93it/s]100%|██████████| 33/33 [00:01<00:00, 25.76it/s]
{'eval_loss': 0.6993983387947083, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.39789069990412274, 'eval_runtime': 1.3222, 'eval_samples_per_second': 788.839, 'eval_steps_per_second': 24.958}
ROUND:16
CLIENT:7
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7017, 'grad_norm': 0.19683195650577545, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.42it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.84it/s]                                              {'loss': 0.7061, 'grad_norm': 0.1254328042268753, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.84it/s]                                              {'loss': 0.7089, 'grad_norm': 0.27728626132011414, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.84it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.6908, 'grad_norm': 0.22332879900932312, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.7111, 'grad_norm': 0.2313314825296402, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.76it/s] 40%|████      | 6/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.6844, 'grad_norm': 0.11526773124933243, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.7004, 'grad_norm': 0.2345089316368103, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.56it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.6994, 'grad_norm': 0.17494888603687286, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.37it/s]                                              {'loss': 0.7009, 'grad_norm': 0.20384810864925385, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.37it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6934, 'grad_norm': 0.2017912119626999, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6998, 'grad_norm': 0.1770637184381485, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.92it/s] 80%|████████  | 12/15 [00:00<00:00, 15.45it/s]                                               {'loss': 0.7045, 'grad_norm': 0.22033657133579254, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.45it/s]                                               {'loss': 0.7002, 'grad_norm': 0.13774707913398743, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.45it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.6965, 'grad_norm': 0.2717381715774536, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.6892, 'grad_norm': 0.21247796714305878, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.43it/s]                                               {'train_runtime': 1.0684, 'train_samples_per_second': 397.805, 'train_steps_per_second': 14.04, 'train_loss': 0.6991674224535624, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.43it/s]100%|██████████| 15/15 [00:01<00:00, 14.04it/s]
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7011, 'grad_norm': 0.22370290756225586, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.49it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.02it/s]                                              {'loss': 0.6987, 'grad_norm': 0.21460753679275513, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.02it/s]                                              {'loss': 0.6858, 'grad_norm': 0.10530336201190948, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.02it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.7076, 'grad_norm': 0.2668547034263611, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.6947, 'grad_norm': 0.14174096286296844, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.49it/s] 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.6911, 'grad_norm': 0.14766347408294678, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.6925, 'grad_norm': 0.21872439980506897, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.38it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.7057, 'grad_norm': 0.13985061645507812, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6982, 'grad_norm': 0.23953869938850403, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.22it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.7066, 'grad_norm': 0.2579600512981415, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6985, 'grad_norm': 0.126382514834404, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.76it/s] 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.712, 'grad_norm': 0.20291852951049805, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6933, 'grad_norm': 0.19701507687568665, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.29it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6943, 'grad_norm': 0.12275712937116623, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.7031, 'grad_norm': 0.3467027246952057, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]                                               {'train_runtime': 1.0767, 'train_samples_per_second': 394.721, 'train_steps_per_second': 13.931, 'train_loss': 0.6988783955574036, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 13.94it/s]
CLIENT:71
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6856, 'grad_norm': 0.1872839629650116, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.58it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.07it/s]                                              {'loss': 0.6955, 'grad_norm': 0.08046022057533264, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.07it/s]                                              {'loss': 0.7008, 'grad_norm': 0.14190888404846191, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.07it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.7013, 'grad_norm': 0.24207903444766998, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.6928, 'grad_norm': 0.07600492984056473, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.25it/s] 40%|████      | 6/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6975, 'grad_norm': 0.2191738784313202, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6932, 'grad_norm': 0.07858016341924667, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.16it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.6887, 'grad_norm': 0.1654880791902542, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.6945, 'grad_norm': 0.16968396306037903, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.14it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.7019, 'grad_norm': 0.16386429965496063, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6913, 'grad_norm': 0.1248573362827301, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.27it/s]                                               {'loss': 0.6907, 'grad_norm': 0.09628859162330627, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.27it/s]                                               {'loss': 0.7007, 'grad_norm': 0.2124071568250656, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.27it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.7035, 'grad_norm': 0.06784983724355698, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.7068, 'grad_norm': 0.14574116468429565, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]                                               {'train_runtime': 1.0783, 'train_samples_per_second': 394.13, 'train_steps_per_second': 13.91, 'train_loss': 0.6963104923566182, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 13.91it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6802, 'grad_norm': 0.19588741660118103, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.93it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.29it/s]                                              {'loss': 0.7061, 'grad_norm': 0.3198786675930023, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.29it/s]                                              {'loss': 0.6871, 'grad_norm': 0.07474376261234283, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.29it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.6803, 'grad_norm': 0.17291389405727386, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.6988, 'grad_norm': 0.22569388151168823, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.31it/s] 40%|████      | 6/15 [00:00<00:00, 16.23it/s]                                              {'loss': 0.7001, 'grad_norm': 0.2777429521083832, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.23it/s]                                              {'loss': 0.694, 'grad_norm': 0.26564404368400574, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.23it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.65it/s]                                              {'loss': 0.6904, 'grad_norm': 0.222101092338562, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.65it/s]                                              {'loss': 0.6924, 'grad_norm': 0.13808253407478333, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.65it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.7025, 'grad_norm': 0.1740061342716217, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.6864, 'grad_norm': 0.2530179023742676, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.10it/s] 80%|████████  | 12/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.699, 'grad_norm': 0.22592173516750336, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.50it/s]                                               {'loss': 0.686, 'grad_norm': 0.20085421204566956, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.50it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.7016, 'grad_norm': 0.266611248254776, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.6807, 'grad_norm': 0.20683501660823822, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.58it/s]                                               {'train_runtime': 1.0572, 'train_samples_per_second': 402.022, 'train_steps_per_second': 14.189, 'train_loss': 0.6923690597216289, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.58it/s]100%|██████████| 15/15 [00:01<00:00, 14.19it/s]
CLIENT:47
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7001, 'grad_norm': 0.22839368879795074, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.96it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.7069, 'grad_norm': 0.24897265434265137, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.7008, 'grad_norm': 0.14968793094158173, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.23it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7011, 'grad_norm': 0.1083071231842041, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.7085, 'grad_norm': 0.3388631343841553, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.61it/s] 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.6932, 'grad_norm': 0.18977315723896027, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.6988, 'grad_norm': 0.1946207731962204, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.46it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.6877, 'grad_norm': 0.23283372819423676, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.6986, 'grad_norm': 0.22362181544303894, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.30it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.7146, 'grad_norm': 0.17220178246498108, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.6986, 'grad_norm': 0.28619256615638733, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.18it/s] 80%|████████  | 12/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.7117, 'grad_norm': 0.18974752724170685, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.6943, 'grad_norm': 0.2524842321872711, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.69it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.6967, 'grad_norm': 0.182595893740654, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.6897, 'grad_norm': 0.24768254160881042, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.43it/s]                                               {'train_runtime': 1.0692, 'train_samples_per_second': 397.477, 'train_steps_per_second': 14.029, 'train_loss': 0.7000887989997864, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.43it/s]100%|██████████| 15/15 [00:01<00:00, 14.03it/s]
CLIENT:29
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6894, 'grad_norm': 0.16244085133075714, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.53it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.99it/s]                                              {'loss': 0.7007, 'grad_norm': 0.1646912544965744, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.99it/s]                                              {'loss': 0.7046, 'grad_norm': 0.2254834771156311, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.99it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.699, 'grad_norm': 0.16810506582260132, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.7043, 'grad_norm': 0.13913482427597046, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.41it/s] 40%|████      | 6/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.706, 'grad_norm': 0.2355165034532547, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.7066, 'grad_norm': 0.2501774728298187, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.53it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.7007, 'grad_norm': 0.17790749669075012, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.6988, 'grad_norm': 0.09186572581529617, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.36it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.7076, 'grad_norm': 0.22744624316692352, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.95it/s]                                               {'loss': 0.6912, 'grad_norm': 0.11961572617292404, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.95it/s] 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.6867, 'grad_norm': 0.16563457250595093, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.7021, 'grad_norm': 0.13874176144599915, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.51it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.6889, 'grad_norm': 0.22690485417842865, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.6908, 'grad_norm': 0.20327605307102203, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.51it/s]                                               {'train_runtime': 1.0638, 'train_samples_per_second': 399.519, 'train_steps_per_second': 14.101, 'train_loss': 0.6984958330790202, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.51it/s]100%|██████████| 15/15 [00:01<00:00, 14.11it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7009, 'grad_norm': 0.27390530705451965, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.34it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.7068, 'grad_norm': 0.2916147708892822, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.6881, 'grad_norm': 0.18588896095752716, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.28it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.93it/s]                                              {'loss': 0.7007, 'grad_norm': 0.16856206953525543, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.93it/s]                                              {'loss': 0.7102, 'grad_norm': 0.356610506772995, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.93it/s] 40%|████      | 6/15 [00:00<00:00, 16.13it/s]                                              {'loss': 0.7034, 'grad_norm': 0.27857428789138794, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.13it/s]                                              {'loss': 0.6891, 'grad_norm': 0.24087880551815033, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.13it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.6943, 'grad_norm': 0.21338051557540894, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.7017, 'grad_norm': 0.3821090757846832, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.68it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.7022, 'grad_norm': 0.2979085147380829, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.6927, 'grad_norm': 0.15939924120903015, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.46it/s] 80%|████████  | 12/15 [00:00<00:00, 15.67it/s]                                               {'loss': 0.7016, 'grad_norm': 0.3813491761684418, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.67it/s]                                               {'loss': 0.6961, 'grad_norm': 0.32467710971832275, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.67it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.6969, 'grad_norm': 0.2106567919254303, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.7168, 'grad_norm': 0.23227094113826752, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.58it/s]                                               {'train_runtime': 1.0429, 'train_samples_per_second': 407.531, 'train_steps_per_second': 14.383, 'train_loss': 0.700109871228536, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.58it/s]100%|██████████| 15/15 [00:01<00:00, 14.39it/s]
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.705, 'grad_norm': 0.30397123098373413, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.29it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.59it/s]                                              {'loss': 0.7116, 'grad_norm': 0.2761235535144806, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.59it/s]                                              {'loss': 0.7094, 'grad_norm': 0.17529094219207764, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.59it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.6972, 'grad_norm': 0.236691415309906, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.7134, 'grad_norm': 0.23136068880558014, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.49it/s] 40%|████      | 6/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.6934, 'grad_norm': 0.4089893102645874, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.7058, 'grad_norm': 0.24450860917568207, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.36it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.89it/s]                                              {'loss': 0.6954, 'grad_norm': 0.23794081807136536, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.89it/s]                                              {'loss': 0.6954, 'grad_norm': 0.38969242572784424, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.89it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.7016, 'grad_norm': 0.2879944443702698, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.6932, 'grad_norm': 0.32874956727027893, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.37it/s] 80%|████████  | 12/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.7008, 'grad_norm': 0.20750053226947784, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.6992, 'grad_norm': 0.3463671803474426, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.15it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.00it/s]                                               {'loss': 0.7014, 'grad_norm': 0.29911914467811584, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.00it/s]                                               {'loss': 0.705, 'grad_norm': 0.23532569408416748, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.00it/s]                                               {'train_runtime': 1.1024, 'train_samples_per_second': 385.525, 'train_steps_per_second': 13.607, 'train_loss': 0.7018756747245789, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.00it/s]100%|██████████| 15/15 [00:01<00:00, 13.61it/s]
CLIENT:50
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7021, 'grad_norm': 0.26347795128822327, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.18it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.89it/s]                                              {'loss': 0.6978, 'grad_norm': 0.24401094019412994, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.89it/s]                                              {'loss': 0.6968, 'grad_norm': 0.2589181065559387, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.89it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.7096, 'grad_norm': 0.34028393030166626, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.6953, 'grad_norm': 0.26324301958084106, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.41it/s] 40%|████      | 6/15 [00:00<00:00, 15.43it/s]                                              {'loss': 0.6851, 'grad_norm': 0.17649149894714355, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.43it/s]                                              {'loss': 0.6892, 'grad_norm': 0.37832796573638916, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.43it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.05it/s]                                              {'loss': 0.6996, 'grad_norm': 0.1905437409877777, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.05it/s]                                              {'loss': 0.6996, 'grad_norm': 0.24813039600849152, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.05it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.7022, 'grad_norm': 0.30138519406318665, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.6952, 'grad_norm': 0.25759822130203247, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.65it/s] 80%|████████  | 12/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.7019, 'grad_norm': 0.25348684191703796, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.6998, 'grad_norm': 0.3675752878189087, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.22it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.32it/s]                                               {'loss': 0.7003, 'grad_norm': 0.14753450453281403, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.32it/s]                                               {'loss': 0.6938, 'grad_norm': 0.35079044103622437, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.32it/s]                                               {'train_runtime': 1.0786, 'train_samples_per_second': 394.038, 'train_steps_per_second': 13.907, 'train_loss': 0.6978770057360332, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.32it/s]100%|██████████| 15/15 [00:01<00:00, 13.91it/s]
CLIENT:9
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6992, 'grad_norm': 0.24786680936813354, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.83it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.701, 'grad_norm': 0.2067357748746872, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.6951, 'grad_norm': 0.2755803167819977, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.97it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.67it/s]                                              {'loss': 0.6893, 'grad_norm': 0.22841985523700714, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.67it/s]                                              {'loss': 0.7032, 'grad_norm': 0.2935318946838379, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.67it/s] 40%|████      | 6/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.7085, 'grad_norm': 0.22562238574028015, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.7142, 'grad_norm': 0.2522795498371124, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.41it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.6965, 'grad_norm': 0.2686043083667755, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.7034, 'grad_norm': 0.20132553577423096, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.48it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.7169, 'grad_norm': 0.28260767459869385, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.708, 'grad_norm': 0.18529881536960602, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.91it/s] 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.6908, 'grad_norm': 0.27674955129623413, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.6922, 'grad_norm': 0.26171109080314636, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.37it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.6877, 'grad_norm': 0.21375501155853271, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.6992, 'grad_norm': 0.33869969844818115, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.43it/s]                                               {'train_runtime': 1.0721, 'train_samples_per_second': 396.421, 'train_steps_per_second': 13.991, 'train_loss': 0.7003561615943908, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.43it/s]100%|██████████| 15/15 [00:01<00:00, 14.00it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 32.84it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.44it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.30it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.52it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.06it/s] 61%|██████    | 20/33 [00:00<00:00, 25.17it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.15it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.08it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.14it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.06it/s]100%|██████████| 33/33 [00:01<00:00, 25.82it/s]
{'eval_loss': 0.6962301135063171, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.44966442953020136, 'eval_runtime': 1.319, 'eval_samples_per_second': 790.751, 'eval_steps_per_second': 25.019}
ROUND:17
CLIENT:20
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6972, 'grad_norm': 0.28202855587005615, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.12it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.695, 'grad_norm': 0.09952767938375473, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.7054, 'grad_norm': 0.2320486456155777, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.33it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.6957, 'grad_norm': 0.08714382350444794, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.6861, 'grad_norm': 0.2892776131629944, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.77it/s] 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.6858, 'grad_norm': 0.1610044687986374, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.6864, 'grad_norm': 0.2235567420721054, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.59it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.7002, 'grad_norm': 0.1095273420214653, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.6906, 'grad_norm': 0.27435213327407837, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.41it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.7071, 'grad_norm': 0.21002702414989471, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6889, 'grad_norm': 0.17882658541202545, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.13it/s] 80%|████████  | 12/15 [00:00<00:00, 16.14it/s]                                               {'loss': 0.6957, 'grad_norm': 0.23786203563213348, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.14it/s]                                               {'loss': 0.7026, 'grad_norm': 0.35004374384880066, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.14it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6878, 'grad_norm': 0.16143964231014252, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6878, 'grad_norm': 0.11411243677139282, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.92it/s]                                               {'train_runtime': 1.0488, 'train_samples_per_second': 405.208, 'train_steps_per_second': 14.301, 'train_loss': 0.6941493272781372, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.92it/s]100%|██████████| 15/15 [00:01<00:00, 14.31it/s]
CLIENT:10
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7183, 'grad_norm': 0.25215819478034973, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.35it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.94it/s]                                              {'loss': 0.7092, 'grad_norm': 0.16182608902454376, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.94it/s]                                              {'loss': 0.7033, 'grad_norm': 0.23509037494659424, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.94it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.7012, 'grad_norm': 0.18840602040290833, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6999, 'grad_norm': 0.19598832726478577, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.22it/s] 40%|████      | 6/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.6952, 'grad_norm': 0.2824030816555023, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.6936, 'grad_norm': 0.19713835418224335, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.14it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.7088, 'grad_norm': 0.19756446778774261, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6996, 'grad_norm': 0.3132327198982239, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.10it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6976, 'grad_norm': 0.20586766302585602, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6885, 'grad_norm': 0.2957332730293274, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.71it/s] 80%|████████  | 12/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.7041, 'grad_norm': 0.14685507118701935, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.7008, 'grad_norm': 0.2777724862098694, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.69it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.6981, 'grad_norm': 0.18934336304664612, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.7057, 'grad_norm': 0.292301744222641, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]                                               {'train_runtime': 1.0782, 'train_samples_per_second': 394.185, 'train_steps_per_second': 13.912, 'train_loss': 0.7015949924786885, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]100%|██████████| 15/15 [00:01<00:00, 13.92it/s]
CLIENT:96
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6981, 'grad_norm': 0.3171072006225586, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.90it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.7012, 'grad_norm': 0.21134169399738312, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.7025, 'grad_norm': 0.32720279693603516, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.16it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.7047, 'grad_norm': 0.2716445326805115, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.6974, 'grad_norm': 0.289884477853775, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.74it/s] 40%|████      | 6/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.6992, 'grad_norm': 0.36216452717781067, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.6815, 'grad_norm': 0.15895035862922668, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.53it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.26it/s]                                              {'loss': 0.6786, 'grad_norm': 0.39895758032798767, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.26it/s]                                              {'loss': 0.6854, 'grad_norm': 0.35340413451194763, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.26it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.7068, 'grad_norm': 0.313604474067688, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.6836, 'grad_norm': 0.1888716220855713, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.81it/s] 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6913, 'grad_norm': 0.4493095576763153, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6964, 'grad_norm': 0.37427425384521484, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.31it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.6898, 'grad_norm': 0.219329833984375, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.6925, 'grad_norm': 0.3960769772529602, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.34it/s]                                               {'train_runtime': 1.0735, 'train_samples_per_second': 395.888, 'train_steps_per_second': 13.973, 'train_loss': 0.6939363916714986, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.34it/s]100%|██████████| 15/15 [00:01<00:00, 13.98it/s]
CLIENT:16
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6814, 'grad_norm': 0.1601504683494568, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.93it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.64it/s]                                              {'loss': 0.6779, 'grad_norm': 0.2983253598213196, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.64it/s]                                              {'loss': 0.7027, 'grad_norm': 0.2560862898826599, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.64it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.6895, 'grad_norm': 0.1796754151582718, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.6797, 'grad_norm': 0.24514128267765045, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.13it/s] 40%|████      | 6/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.6762, 'grad_norm': 0.27624937891960144, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.6951, 'grad_norm': 0.19171355664730072, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.09it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.07it/s]                                              {'loss': 0.6788, 'grad_norm': 0.25372791290283203, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.07it/s]                                              {'loss': 0.6894, 'grad_norm': 0.28907153010368347, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.07it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.7043, 'grad_norm': 0.23873288929462433, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.6782, 'grad_norm': 0.2665666937828064, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.69it/s] 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.703, 'grad_norm': 0.23613828420639038, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.6902, 'grad_norm': 0.2751825153827667, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6921, 'grad_norm': 0.22690291702747345, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6729, 'grad_norm': 0.25933152437210083, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]                                               {'train_runtime': 1.0833, 'train_samples_per_second': 392.31, 'train_steps_per_second': 13.846, 'train_loss': 0.687435781955719, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 13.85it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6964, 'grad_norm': 0.2908632159233093, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.94it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.702, 'grad_norm': 0.31032249331474304, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.16it/s]                                              {'loss': 0.6853, 'grad_norm': 0.19709151983261108, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.16it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.698, 'grad_norm': 0.176573246717453, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.7042, 'grad_norm': 0.37862271070480347, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.29it/s] 40%|████      | 6/15 [00:00<00:00, 16.24it/s]                                              {'loss': 0.6989, 'grad_norm': 0.29284989833831787, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.24it/s]                                              {'loss': 0.685, 'grad_norm': 0.2549056112766266, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.24it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.6908, 'grad_norm': 0.2239656299352646, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.695, 'grad_norm': 0.4096086323261261, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.76it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.697, 'grad_norm': 0.3204233646392822, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.6902, 'grad_norm': 0.163515105843544, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.23it/s] 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.695, 'grad_norm': 0.404453843832016, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.6904, 'grad_norm': 0.3431726098060608, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.59it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.6933, 'grad_norm': 0.22086042165756226, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.7129, 'grad_norm': 0.2462318241596222, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.59it/s]                                               {'train_runtime': 1.0543, 'train_samples_per_second': 403.1, 'train_steps_per_second': 14.227, 'train_loss': 0.6956242005030314, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.59it/s]100%|██████████| 15/15 [00:01<00:00, 14.23it/s]
CLIENT:24
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6951, 'grad_norm': 0.2160346806049347, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.12it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.17it/s]                                              {'loss': 0.6981, 'grad_norm': 0.16112573444843292, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.17it/s]                                              {'loss': 0.6998, 'grad_norm': 0.27729326486587524, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.17it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.7087, 'grad_norm': 0.24834294617176056, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.7023, 'grad_norm': 0.17467990517616272, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.40it/s] 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.6799, 'grad_norm': 0.23689596354961395, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.6895, 'grad_norm': 0.17761297523975372, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.29it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6917, 'grad_norm': 0.228949174284935, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.685, 'grad_norm': 0.19601181149482727, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.06it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.6958, 'grad_norm': 0.24053996801376343, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.6869, 'grad_norm': 0.1351308375597, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.70it/s] 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.6815, 'grad_norm': 0.2963116466999054, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.6974, 'grad_norm': 0.2923068404197693, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.37it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.6917, 'grad_norm': 0.20443449914455414, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.43it/s]                                               {'loss': 0.6957, 'grad_norm': 0.11820845305919647, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.43it/s]                                               {'train_runtime': 1.0736, 'train_samples_per_second': 395.856, 'train_steps_per_second': 13.971, 'train_loss': 0.6932606140772501, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.43it/s]100%|██████████| 15/15 [00:01<00:00, 13.98it/s]
CLIENT:53
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6917, 'grad_norm': 0.24868112802505493, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.86it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.13it/s]                                              {'loss': 0.6881, 'grad_norm': 0.10588252544403076, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.13it/s]                                              {'loss': 0.7099, 'grad_norm': 0.25057515501976013, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.13it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.65it/s]                                              {'loss': 0.6966, 'grad_norm': 0.10736449807882309, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.65it/s]                                              {'loss': 0.6932, 'grad_norm': 0.31783685088157654, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.65it/s] 40%|████      | 6/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.6933, 'grad_norm': 0.17502231895923615, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.7084, 'grad_norm': 0.18416337668895721, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.47it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.79it/s]                                              {'loss': 0.6838, 'grad_norm': 0.27895835041999817, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.79it/s]                                              {'loss': 0.6899, 'grad_norm': 0.10541755706071854, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.79it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.7066, 'grad_norm': 0.21991650760173798, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.6914, 'grad_norm': 0.16837328672409058, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.16it/s] 80%|████████  | 12/15 [00:00<00:00, 15.77it/s]                                               {'loss': 0.6914, 'grad_norm': 0.25211766362190247, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.77it/s]                                               {'loss': 0.6985, 'grad_norm': 0.2457950860261917, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.77it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.6931, 'grad_norm': 0.23688198626041412, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.687, 'grad_norm': 0.10471230000257492, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.47it/s]                                               {'train_runtime': 1.0703, 'train_samples_per_second': 397.072, 'train_steps_per_second': 14.014, 'train_loss': 0.6948583841323852, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.47it/s]100%|██████████| 15/15 [00:01<00:00, 14.02it/s]
CLIENT:97
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6892, 'grad_norm': 0.1511734277009964, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.87it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.64it/s]                                              {'loss': 0.6854, 'grad_norm': 0.17878364026546478, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.64it/s]                                              {'loss': 0.7039, 'grad_norm': 0.15064729750156403, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.64it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.7097, 'grad_norm': 0.15206053853034973, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.24it/s]                                              {'loss': 0.6921, 'grad_norm': 0.12492666393518448, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.24it/s] 40%|████      | 6/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.6871, 'grad_norm': 0.21177154779434204, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.7, 'grad_norm': 0.14353279769420624, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.21it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.6869, 'grad_norm': 0.16706474125385284, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.6907, 'grad_norm': 0.15426263213157654, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.16it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.6937, 'grad_norm': 0.07734289020299911, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.6921, 'grad_norm': 0.24318954348564148, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.10it/s] 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.6814, 'grad_norm': 0.3390882611274719, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.6933, 'grad_norm': 0.07164232432842255, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.57it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.19it/s]                                               {'loss': 0.6927, 'grad_norm': 0.2588314712047577, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.19it/s]                                               {'loss': 0.7099, 'grad_norm': 0.24896885454654694, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.19it/s]                                               {'train_runtime': 1.091, 'train_samples_per_second': 389.569, 'train_steps_per_second': 13.749, 'train_loss': 0.6938751379648844, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.19it/s]100%|██████████| 15/15 [00:01<00:00, 13.75it/s]
CLIENT:41
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6917, 'grad_norm': 0.20031055808067322, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.90it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.61it/s]                                              {'loss': 0.6938, 'grad_norm': 0.06904740631580353, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.61it/s]                                              {'loss': 0.6985, 'grad_norm': 0.12262561917304993, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.61it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.96it/s]                                              {'loss': 0.708, 'grad_norm': 0.09348592162132263, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.96it/s]                                              {'loss': 0.7036, 'grad_norm': 0.25363048911094666, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.96it/s] 40%|████      | 6/15 [00:00<00:00, 16.27it/s]                                              {'loss': 0.6892, 'grad_norm': 0.12844514846801758, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.27it/s]                                              {'loss': 0.6995, 'grad_norm': 0.23598456382751465, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.27it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.6992, 'grad_norm': 0.06924524903297424, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.698, 'grad_norm': 0.09116793423891068, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.62it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.6949, 'grad_norm': 0.08989327400922775, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.6915, 'grad_norm': 0.11169765144586563, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.07it/s] 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.6879, 'grad_norm': 0.1804247498512268, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.6969, 'grad_norm': 0.15180909633636475, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.47it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.6975, 'grad_norm': 0.15532854199409485, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.7029, 'grad_norm': 0.09140098094940186, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.56it/s]                                               {'train_runtime': 1.0619, 'train_samples_per_second': 400.227, 'train_steps_per_second': 14.126, 'train_loss': 0.6968632102012634, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.56it/s]100%|██████████| 15/15 [00:01<00:00, 14.13it/s]
CLIENT:47
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6964, 'grad_norm': 0.24187085032463074, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.36it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.09it/s]                                              {'loss': 0.7027, 'grad_norm': 0.2647469639778137, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.09it/s]                                              {'loss': 0.6986, 'grad_norm': 0.1579935997724533, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.09it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.88it/s]                                              {'loss': 0.6997, 'grad_norm': 0.11158730089664459, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.88it/s]                                              {'loss': 0.7028, 'grad_norm': 0.3571367859840393, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.88it/s] 40%|████      | 6/15 [00:00<00:00, 16.28it/s]                                              {'loss': 0.6902, 'grad_norm': 0.19634847342967987, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.28it/s]                                              {'loss': 0.6957, 'grad_norm': 0.2047441154718399, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.28it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.07it/s]                                              {'loss': 0.6839, 'grad_norm': 0.2455066293478012, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.07it/s]                                              {'loss': 0.695, 'grad_norm': 0.23511704802513123, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.07it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.42it/s]                                               {'loss': 0.7118, 'grad_norm': 0.1806350201368332, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.42it/s]                                               {'loss': 0.6937, 'grad_norm': 0.3041608929634094, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.42it/s] 80%|████████  | 12/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.7087, 'grad_norm': 0.20411896705627441, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.85it/s]                                               {'loss': 0.6899, 'grad_norm': 0.267157644033432, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.85it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6937, 'grad_norm': 0.18860526382923126, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6856, 'grad_norm': 0.26226139068603516, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.71it/s]                                               {'train_runtime': 1.0466, 'train_samples_per_second': 406.063, 'train_steps_per_second': 14.332, 'train_loss': 0.6965544939041137, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.71it/s]100%|██████████| 15/15 [00:01<00:00, 14.34it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 34.32it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.10it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.55it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.68it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.22it/s] 61%|██████    | 20/33 [00:00<00:00, 24.94it/s] 70%|██████▉   | 23/33 [00:00<00:00, 24.69it/s] 79%|███████▉  | 26/33 [00:01<00:00, 24.90it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.01it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.03it/s]100%|██████████| 33/33 [00:01<00:00, 25.83it/s]
{'eval_loss': 0.6929733753204346, 'eval_model_preparation_time': 0.0022, 'eval_acc': 0.5273250239693192, 'eval_runtime': 1.3187, 'eval_samples_per_second': 790.959, 'eval_steps_per_second': 25.026}
ROUND:18
CLIENT:83
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6979, 'grad_norm': 0.3470163345336914, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.05it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.93it/s]                                              {'loss': 0.6934, 'grad_norm': 0.240603506565094, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.93it/s]                                              {'loss': 0.6817, 'grad_norm': 0.2052038460969925, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.93it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.6913, 'grad_norm': 0.28713804483413696, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.6935, 'grad_norm': 0.2292819321155548, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.51it/s] 40%|████      | 6/15 [00:00<00:00, 15.62it/s]                                              {'loss': 0.6769, 'grad_norm': 0.35749784111976624, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.62it/s]                                              {'loss': 0.6953, 'grad_norm': 0.2800869345664978, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.62it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.44it/s]                                              {'loss': 0.6784, 'grad_norm': 0.30813169479370117, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.44it/s]                                              {'loss': 0.6873, 'grad_norm': 0.22130542993545532, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.44it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.6958, 'grad_norm': 0.2518065273761749, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.6812, 'grad_norm': 0.363656610250473, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.33it/s] 80%|████████  | 12/15 [00:00<00:00, 15.68it/s]                                               {'loss': 0.6809, 'grad_norm': 0.2617238461971283, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.68it/s]                                               {'loss': 0.6822, 'grad_norm': 0.42566972970962524, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.68it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.6818, 'grad_norm': 0.1804816722869873, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.6801, 'grad_norm': 0.24844025075435638, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.60it/s]                                               {'train_runtime': 1.0643, 'train_samples_per_second': 399.342, 'train_steps_per_second': 14.094, 'train_loss': 0.6865147233009339, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.60it/s]100%|██████████| 15/15 [00:01<00:00, 14.10it/s]
CLIENT:1
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6899, 'grad_norm': 0.08487103134393692, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.53it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.05it/s]                                              {'loss': 0.6895, 'grad_norm': 0.16445818543434143, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.05it/s]                                              {'loss': 0.6983, 'grad_norm': 0.32359784841537476, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.05it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.6936, 'grad_norm': 0.08847463876008987, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.6919, 'grad_norm': 0.14904549717903137, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.78it/s] 40%|████      | 6/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.7046, 'grad_norm': 0.2015683352947235, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.6903, 'grad_norm': 0.15715542435646057, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.44it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6953, 'grad_norm': 0.15479788184165955, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.7128, 'grad_norm': 0.09986502677202225, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.32it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.7076, 'grad_norm': 0.14184540510177612, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.6943, 'grad_norm': 0.1471090167760849, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.91it/s] 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.6748, 'grad_norm': 0.13790678977966309, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.7046, 'grad_norm': 0.07668735086917877, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.46it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.97it/s]                                               {'loss': 0.7044, 'grad_norm': 0.1463364064693451, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.97it/s]                                               {'loss': 0.7027, 'grad_norm': 0.3191361427307129, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.97it/s]                                               {'train_runtime': 1.0554, 'train_samples_per_second': 402.694, 'train_steps_per_second': 14.213, 'train_loss': 0.6969594319661458, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.97it/s]100%|██████████| 15/15 [00:01<00:00, 14.22it/s]
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.684, 'grad_norm': 0.32845601439476013, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.37it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.95it/s]                                              {'loss': 0.6894, 'grad_norm': 0.10466931760311127, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.95it/s]                                              {'loss': 0.6924, 'grad_norm': 0.15620945394039154, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.95it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.7038, 'grad_norm': 0.13754621148109436, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.15it/s]                                              {'loss': 0.6966, 'grad_norm': 0.2771320939064026, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.15it/s] 40%|████      | 6/15 [00:00<00:00, 16.31it/s]                                              {'loss': 0.6793, 'grad_norm': 0.16257339715957642, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.31it/s]                                              {'loss': 0.6855, 'grad_norm': 0.2652114927768707, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.31it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.6894, 'grad_norm': 0.17026929557323456, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.6866, 'grad_norm': 0.21654903888702393, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.71it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6909, 'grad_norm': 0.45399710536003113, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6956, 'grad_norm': 0.08458222448825836, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.13it/s] 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.6965, 'grad_norm': 0.17005884647369385, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.6882, 'grad_norm': 0.2864519953727722, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.56it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.6852, 'grad_norm': 0.1787356734275818, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.6801, 'grad_norm': 0.16256576776504517, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.59it/s]                                               {'train_runtime': 1.0551, 'train_samples_per_second': 402.818, 'train_steps_per_second': 14.217, 'train_loss': 0.6895588477452596, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.59it/s]100%|██████████| 15/15 [00:01<00:00, 14.22it/s]
CLIENT:9
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6911, 'grad_norm': 0.2769010663032532, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.53it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.6942, 'grad_norm': 0.22823184728622437, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.6858, 'grad_norm': 0.3141286373138428, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.04it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.6815, 'grad_norm': 0.25982439517974854, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.6931, 'grad_norm': 0.32837390899658203, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.25it/s] 40%|████      | 6/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.701, 'grad_norm': 0.25415077805519104, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.7056, 'grad_norm': 0.2805927097797394, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.13it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.6872, 'grad_norm': 0.29995617270469666, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.6967, 'grad_norm': 0.22917823493480682, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.08it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.7068, 'grad_norm': 0.31690695881843567, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.7017, 'grad_norm': 0.2076183557510376, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.98it/s] 80%|████████  | 12/15 [00:00<00:00, 15.86it/s]                                               {'loss': 0.681, 'grad_norm': 0.3145197927951813, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.86it/s]                                               {'loss': 0.6828, 'grad_norm': 0.3030257523059845, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.86it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.6805, 'grad_norm': 0.2371445745229721, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.6871, 'grad_norm': 0.3839446008205414, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.64it/s]                                               {'train_runtime': 1.0684, 'train_samples_per_second': 397.802, 'train_steps_per_second': 14.04, 'train_loss': 0.6917357802391052, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.64it/s]100%|██████████| 15/15 [00:01<00:00, 14.04it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6945, 'grad_norm': 0.16703279316425323, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.34it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.6904, 'grad_norm': 0.28880348801612854, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.6846, 'grad_norm': 0.2893002927303314, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.28it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.70it/s]                                              {'loss': 0.6996, 'grad_norm': 0.141940176486969, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.70it/s]                                              {'loss': 0.6953, 'grad_norm': 0.4514479339122772, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.70it/s] 40%|████      | 6/15 [00:00<00:00, 16.10it/s]                                              {'loss': 0.692, 'grad_norm': 0.19598588347434998, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.10it/s]                                              {'loss': 0.68, 'grad_norm': 0.3496531844139099, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.10it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.6922, 'grad_norm': 0.20841838419437408, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.687, 'grad_norm': 0.23678569495677948, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.68it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.7058, 'grad_norm': 0.27119287848472595, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.6713, 'grad_norm': 0.3043024241924286, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.02it/s] 80%|████████  | 12/15 [00:00<00:00, 15.65it/s]                                               {'loss': 0.6877, 'grad_norm': 0.2335183173418045, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.65it/s]                                               {'loss': 0.6905, 'grad_norm': 0.2687171399593353, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.65it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.6914, 'grad_norm': 0.3018900752067566, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.86it/s]                                               {'loss': 0.677, 'grad_norm': 0.2800665497779846, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.86it/s]                                               {'train_runtime': 1.0416, 'train_samples_per_second': 408.033, 'train_steps_per_second': 14.401, 'train_loss': 0.6892901539802552, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.86it/s]100%|██████████| 15/15 [00:01<00:00, 14.41it/s]
CLIENT:28
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6955, 'grad_norm': 0.18324270844459534, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.21it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6877, 'grad_norm': 0.2770458161830902, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6953, 'grad_norm': 0.32422885298728943, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.22it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.7011, 'grad_norm': 0.20710210502147675, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.6867, 'grad_norm': 0.23880037665367126, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.46it/s] 40%|████      | 6/15 [00:00<00:00, 15.84it/s]                                              {'loss': 0.6987, 'grad_norm': 0.2984931766986847, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.84it/s]                                              {'loss': 0.7038, 'grad_norm': 0.2414725124835968, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.84it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.6764, 'grad_norm': 0.3517906069755554, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.6869, 'grad_norm': 0.12939365208148956, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.42it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.6929, 'grad_norm': 0.23552128672599792, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.6844, 'grad_norm': 0.3225163519382477, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.93it/s] 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.7067, 'grad_norm': 0.19190935790538788, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.6991, 'grad_norm': 0.2566276788711548, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.56it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6878, 'grad_norm': 0.40548771619796753, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.7, 'grad_norm': 0.11909306049346924, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.55it/s]                                               {'train_runtime': 1.0564, 'train_samples_per_second': 402.3, 'train_steps_per_second': 14.199, 'train_loss': 0.693537704149882, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.55it/s]100%|██████████| 15/15 [00:01<00:00, 14.20it/s]
CLIENT:96
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.693, 'grad_norm': 0.33492985367774963, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.50it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.87it/s]                                              {'loss': 0.698, 'grad_norm': 0.22227652370929718, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.87it/s]                                              {'loss': 0.6971, 'grad_norm': 0.3495081961154938, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.87it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.7002, 'grad_norm': 0.29175126552581787, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.6926, 'grad_norm': 0.30348658561706543, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.53it/s] 40%|████      | 6/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.6931, 'grad_norm': 0.3854762017726898, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.52it/s]                                              {'loss': 0.6791, 'grad_norm': 0.1656099259853363, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.52it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.94it/s]                                              {'loss': 0.6717, 'grad_norm': 0.42526230216026306, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.94it/s]                                              {'loss': 0.6794, 'grad_norm': 0.3721674084663391, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.94it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.7014, 'grad_norm': 0.33517131209373474, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6806, 'grad_norm': 0.19660726189613342, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.32it/s] 80%|████████  | 12/15 [00:00<00:00, 15.87it/s]                                               {'loss': 0.6836, 'grad_norm': 0.4735272228717804, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.87it/s]                                               {'loss': 0.6898, 'grad_norm': 0.4065968692302704, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.87it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6861, 'grad_norm': 0.22575941681861877, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6857, 'grad_norm': 0.4198138117790222, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.71it/s]                                               {'train_runtime': 1.0536, 'train_samples_per_second': 403.392, 'train_steps_per_second': 14.237, 'train_loss': 0.6887636343638103, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.71it/s]100%|██████████| 15/15 [00:01<00:00, 14.24it/s]
CLIENT:29
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6843, 'grad_norm': 0.18100041151046753, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.96it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.28it/s]                                              {'loss': 0.6957, 'grad_norm': 0.17776432633399963, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.28it/s]                                              {'loss': 0.6975, 'grad_norm': 0.24904762208461761, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.28it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.6938, 'grad_norm': 0.1817878633737564, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.7001, 'grad_norm': 0.15100820362567902, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.64it/s] 40%|████      | 6/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.6983, 'grad_norm': 0.25907576084136963, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.6982, 'grad_norm': 0.28171998262405396, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.47it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6951, 'grad_norm': 0.18809209764003754, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.697, 'grad_norm': 0.09791646897792816, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.22it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.6998, 'grad_norm': 0.25378233194351196, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.81it/s]                                               {'loss': 0.6879, 'grad_norm': 0.12647411227226257, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.81it/s] 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.682, 'grad_norm': 0.17695319652557373, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6979, 'grad_norm': 0.14783595502376556, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.32it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.6812, 'grad_norm': 0.2539884150028229, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.6844, 'grad_norm': 0.22133183479309082, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.63it/s]                                               {'train_runtime': 1.0683, 'train_samples_per_second': 397.839, 'train_steps_per_second': 14.041, 'train_loss': 0.6928747614224752, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.63it/s]100%|██████████| 15/15 [00:01<00:00, 14.05it/s]
CLIENT:86
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.69, 'grad_norm': 0.2778943181037903, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.46it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.6882, 'grad_norm': 0.15938742458820343, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.04it/s]                                              {'loss': 0.7143, 'grad_norm': 0.21702657639980316, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.04it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.6983, 'grad_norm': 0.22558248043060303, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.74it/s]                                              {'loss': 0.7005, 'grad_norm': 0.17879869043827057, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.74it/s] 40%|████      | 6/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.6767, 'grad_norm': 0.23156338930130005, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.19it/s]                                              {'loss': 0.7028, 'grad_norm': 0.1951969712972641, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.19it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.686, 'grad_norm': 0.1316470354795456, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.99it/s]                                              {'loss': 0.6831, 'grad_norm': 0.3705282509326935, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.99it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6883, 'grad_norm': 0.25001227855682373, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.677, 'grad_norm': 0.24883560836315155, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.55it/s] 80%|████████  | 12/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.6887, 'grad_norm': 0.10949345678091049, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.6937, 'grad_norm': 0.23294313251972198, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.15it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.23it/s]                                               {'loss': 0.6896, 'grad_norm': 0.2291046530008316, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.23it/s]                                               {'loss': 0.6925, 'grad_norm': 0.23119015991687775, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.23it/s]                                               {'train_runtime': 1.0893, 'train_samples_per_second': 390.144, 'train_steps_per_second': 13.77, 'train_loss': 0.691306988398234, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.23it/s]100%|██████████| 15/15 [00:01<00:00, 13.77it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6917, 'grad_norm': 0.3037123680114746, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.08it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.28it/s]                                              {'loss': 0.697, 'grad_norm': 0.3284340798854828, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.28it/s]                                              {'loss': 0.6824, 'grad_norm': 0.19997212290763855, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.28it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.6952, 'grad_norm': 0.18544067442417145, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.6979, 'grad_norm': 0.4022473394870758, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.66it/s] 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.6942, 'grad_norm': 0.3104199171066284, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.6808, 'grad_norm': 0.26695773005485535, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.38it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6873, 'grad_norm': 0.23655273020267487, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6881, 'grad_norm': 0.43490728735923767, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.22it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6916, 'grad_norm': 0.3402562737464905, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6876, 'grad_norm': 0.1734590083360672, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.82it/s] 80%|████████  | 12/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.6882, 'grad_norm': 0.424927681684494, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.6846, 'grad_norm': 0.35964640974998474, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.28it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6896, 'grad_norm': 0.2313992977142334, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.7089, 'grad_norm': 0.25767913460731506, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]                                               {'train_runtime': 1.0764, 'train_samples_per_second': 394.84, 'train_steps_per_second': 13.936, 'train_loss': 0.6910192807515462, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 13.94it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 35.36it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.28it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.66it/s] 42%|████▏     | 14/33 [00:00<00:00, 26.22it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.74it/s] 61%|██████    | 20/33 [00:00<00:00, 25.45it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.39it/s] 79%|███████▉  | 26/33 [00:00<00:00, 25.19it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.24it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.20it/s]100%|██████████| 33/33 [00:01<00:00, 26.17it/s]
{'eval_loss': 0.6890529990196228, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.5992329817833174, 'eval_runtime': 1.3026, 'eval_samples_per_second': 800.676, 'eval_steps_per_second': 25.333}
ROUND:19
CLIENT:83
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6915, 'grad_norm': 0.3689556121826172, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.62it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.93it/s]                                              {'loss': 0.689, 'grad_norm': 0.2572014331817627, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.93it/s]                                              {'loss': 0.6781, 'grad_norm': 0.2142205834388733, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.93it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.686, 'grad_norm': 0.3019658029079437, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.6893, 'grad_norm': 0.23696710169315338, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.40it/s] 40%|████      | 6/15 [00:00<00:00, 15.85it/s]                                              {'loss': 0.6701, 'grad_norm': 0.3832820653915405, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.85it/s]                                              {'loss': 0.6899, 'grad_norm': 0.298061341047287, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.85it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.6725, 'grad_norm': 0.32460054755210876, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.6833, 'grad_norm': 0.23699520528316498, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.57it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.06it/s]                                               {'loss': 0.691, 'grad_norm': 0.27474334836006165, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.06it/s]                                               {'loss': 0.674, 'grad_norm': 0.39162448048591614, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.06it/s] 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.676, 'grad_norm': 0.28369638323783875, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.6738, 'grad_norm': 0.45399975776672363, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.48it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.6786, 'grad_norm': 0.18587888777256012, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.6754, 'grad_norm': 0.25985968112945557, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.02it/s]                                               {'train_runtime': 1.0425, 'train_samples_per_second': 407.664, 'train_steps_per_second': 14.388, 'train_loss': 0.6812422355016072, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.02it/s]100%|██████████| 15/15 [00:01<00:00, 14.39it/s]
CLIENT:1
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6905, 'grad_norm': 0.098152294754982, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.66it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.01it/s]                                              {'loss': 0.6867, 'grad_norm': 0.17267915606498718, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.01it/s]                                              {'loss': 0.6924, 'grad_norm': 0.34341099858283997, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.01it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.6926, 'grad_norm': 0.09082306921482086, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.6894, 'grad_norm': 0.15575452148914337, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.66it/s] 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.7012, 'grad_norm': 0.20595461130142212, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.6876, 'grad_norm': 0.16692668199539185, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.38it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.6926, 'grad_norm': 0.15932604670524597, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.7124, 'grad_norm': 0.10575029253959656, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.20it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.7053, 'grad_norm': 0.14013327658176422, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6919, 'grad_norm': 0.15522940456867218, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.76it/s] 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.673, 'grad_norm': 0.1416105479001999, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.705, 'grad_norm': 0.08061116933822632, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.41it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.32it/s]                                               {'loss': 0.7021, 'grad_norm': 0.14937680959701538, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.32it/s]                                               {'loss': 0.6969, 'grad_norm': 0.333217054605484, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.32it/s]                                               {'train_runtime': 1.0753, 'train_samples_per_second': 395.22, 'train_steps_per_second': 13.949, 'train_loss': 0.6946504433949788, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.32it/s]100%|██████████| 15/15 [00:01<00:00, 13.95it/s]
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6779, 'grad_norm': 0.3497384190559387, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.15it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.31it/s]                                              {'loss': 0.6879, 'grad_norm': 0.10998353362083435, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.31it/s]                                              {'loss': 0.6899, 'grad_norm': 0.16018564999103546, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.31it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.7015, 'grad_norm': 0.14137135446071625, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.6915, 'grad_norm': 0.2926490306854248, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.59it/s] 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.6766, 'grad_norm': 0.17248652875423431, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.6806, 'grad_norm': 0.28097060322761536, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.38it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.6864, 'grad_norm': 0.1780720353126526, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.6829, 'grad_norm': 0.23189829289913177, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.19it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6821, 'grad_norm': 0.4842119514942169, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.696, 'grad_norm': 0.09249790757894516, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.27it/s]                                               {'loss': 0.6937, 'grad_norm': 0.18032336235046387, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.27it/s]                                               {'loss': 0.6828, 'grad_norm': 0.3083765208721161, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.27it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.6821, 'grad_norm': 0.18615522980690002, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.6775, 'grad_norm': 0.16686059534549713, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]                                               {'train_runtime': 1.077, 'train_samples_per_second': 394.601, 'train_steps_per_second': 13.927, 'train_loss': 0.6859600027402242, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]100%|██████████| 15/15 [00:01<00:00, 13.93it/s]
CLIENT:9
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6861, 'grad_norm': 0.2918560802936554, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.62it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.13it/s]                                              {'loss': 0.6901, 'grad_norm': 0.24115481972694397, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.13it/s]                                              {'loss': 0.68, 'grad_norm': 0.33755168318748474, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.13it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.6766, 'grad_norm': 0.27955400943756104, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.6868, 'grad_norm': 0.3516082167625427, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.30it/s] 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.6964, 'grad_norm': 0.2657621204853058, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.22it/s]                                              {'loss': 0.7002, 'grad_norm': 0.295026957988739, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.22it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.6814, 'grad_norm': 0.32025137543678284, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.6925, 'grad_norm': 0.2421720325946808, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.16it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.7006, 'grad_norm': 0.3374117314815521, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.6978, 'grad_norm': 0.21703198552131653, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.80it/s] 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.675, 'grad_norm': 0.33402055501937866, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6769, 'grad_norm': 0.3230380415916443, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.29it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.6761, 'grad_norm': 0.24682380259037018, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.6796, 'grad_norm': 0.40920165181159973, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.87it/s]                                               {'train_runtime': 1.0646, 'train_samples_per_second': 399.217, 'train_steps_per_second': 14.09, 'train_loss': 0.6864018440246582, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.87it/s]100%|██████████| 15/15 [00:01<00:00, 14.09it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6918, 'grad_norm': 0.1708521693944931, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.73it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6851, 'grad_norm': 0.31032803654670715, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6794, 'grad_norm': 0.30702945590019226, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.10it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.6973, 'grad_norm': 0.14576800167560577, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.6867, 'grad_norm': 0.47948047518730164, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.40it/s] 40%|████      | 6/15 [00:00<00:00, 16.09it/s]                                              {'loss': 0.6886, 'grad_norm': 0.20246858894824982, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.09it/s]                                              {'loss': 0.6733, 'grad_norm': 0.3752064108848572, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.09it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.6885, 'grad_norm': 0.21544694900512695, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.6827, 'grad_norm': 0.2521592080593109, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.70it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.7007, 'grad_norm': 0.2810502350330353, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.6655, 'grad_norm': 0.311812162399292, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.11it/s] 80%|████████  | 12/15 [00:00<00:00, 15.61it/s]                                               {'loss': 0.6836, 'grad_norm': 0.24593430757522583, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.61it/s]                                               {'loss': 0.6854, 'grad_norm': 0.2841118574142456, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.61it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.66it/s]                                               {'loss': 0.6856, 'grad_norm': 0.31775325536727905, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.66it/s]                                               {'loss': 0.6719, 'grad_norm': 0.28551483154296875, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.66it/s]                                               {'train_runtime': 1.0509, 'train_samples_per_second': 404.422, 'train_steps_per_second': 14.274, 'train_loss': 0.6844014326731364, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.66it/s]100%|██████████| 15/15 [00:01<00:00, 14.28it/s]
CLIENT:28
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6923, 'grad_norm': 0.18913939595222473, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.70it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.84it/s]                                              {'loss': 0.6826, 'grad_norm': 0.29545214772224426, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.84it/s]                                              {'loss': 0.6893, 'grad_norm': 0.3444851040840149, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.84it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.00it/s]                                              {'loss': 0.6974, 'grad_norm': 0.217903271317482, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.00it/s]                                              {'loss': 0.6822, 'grad_norm': 0.252760648727417, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.00it/s] 40%|████      | 6/15 [00:00<00:00, 16.25it/s]                                              {'loss': 0.6935, 'grad_norm': 0.30999407172203064, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.25it/s]                                              {'loss': 0.6994, 'grad_norm': 0.25216343998908997, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.25it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.6696, 'grad_norm': 0.378414124250412, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.6853, 'grad_norm': 0.13221709430217743, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.66it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.6886, 'grad_norm': 0.24531272053718567, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.6783, 'grad_norm': 0.335936039686203, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.89it/s] 80%|████████  | 12/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.7034, 'grad_norm': 0.20210930705070496, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.6943, 'grad_norm': 0.2675837278366089, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.39it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.68, 'grad_norm': 0.42739221453666687, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.6988, 'grad_norm': 0.12262284010648727, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.40it/s]                                               {'train_runtime': 1.0696, 'train_samples_per_second': 397.341, 'train_steps_per_second': 14.024, 'train_loss': 0.6889984965324402, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]100%|██████████| 15/15 [00:01<00:00, 14.03it/s]
CLIENT:96
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6868, 'grad_norm': 0.3567814230918884, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.28it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.79it/s]                                              {'loss': 0.6939, 'grad_norm': 0.23875173926353455, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.79it/s]                                              {'loss': 0.6906, 'grad_norm': 0.3724663555622101, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.79it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.98it/s]                                              {'loss': 0.6947, 'grad_norm': 0.3147520124912262, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.98it/s]                                              {'loss': 0.6868, 'grad_norm': 0.3236405849456787, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.98it/s] 40%|████      | 6/15 [00:00<00:00, 16.11it/s]                                              {'loss': 0.6857, 'grad_norm': 0.41611260175704956, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.11it/s]                                              {'loss': 0.6762, 'grad_norm': 0.17410193383693695, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.11it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.6633, 'grad_norm': 0.4556097090244293, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.6721, 'grad_norm': 0.3937455713748932, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.55it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.6947, 'grad_norm': 0.3596179485321045, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.6769, 'grad_norm': 0.2063044160604477, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.98it/s] 80%|████████  | 12/15 [00:00<00:00, 15.60it/s]                                               {'loss': 0.6741, 'grad_norm': 0.5039036273956299, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.60it/s]                                               {'loss': 0.6816, 'grad_norm': 0.43758031725883484, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.60it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6818, 'grad_norm': 0.23799282312393188, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6773, 'grad_norm': 0.4422948956489563, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.76it/s]                                               {'train_runtime': 1.0457, 'train_samples_per_second': 406.409, 'train_steps_per_second': 14.344, 'train_loss': 0.6824321905771892, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.76it/s]100%|██████████| 15/15 [00:01<00:00, 14.35it/s]
CLIENT:29
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6811, 'grad_norm': 0.19023500382900238, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.61it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.93it/s]                                              {'loss': 0.6926, 'grad_norm': 0.1886487603187561, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.93it/s]                                              {'loss': 0.6932, 'grad_norm': 0.2632943093776703, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.93it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.6907, 'grad_norm': 0.18890608847141266, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.6975, 'grad_norm': 0.16132153570652008, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.31it/s] 40%|████      | 6/15 [00:00<00:00, 15.82it/s]                                              {'loss': 0.6935, 'grad_norm': 0.2740216553211212, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.82it/s]                                              {'loss': 0.6929, 'grad_norm': 0.2949827313423157, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.82it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.6918, 'grad_norm': 0.19207289814949036, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.696, 'grad_norm': 0.10141873359680176, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.48it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.695, 'grad_norm': 0.26951906085014343, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.686, 'grad_norm': 0.12974557280540466, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.15it/s] 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6792, 'grad_norm': 0.18542605638504028, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6955, 'grad_norm': 0.15266460180282593, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.54it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.6763, 'grad_norm': 0.27514490485191345, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.6805, 'grad_norm': 0.23185887932777405, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.52it/s]                                               {'train_runtime': 1.0597, 'train_samples_per_second': 401.04, 'train_steps_per_second': 14.154, 'train_loss': 0.6894551634788513, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.52it/s]100%|██████████| 15/15 [00:01<00:00, 14.16it/s]
CLIENT:86
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6849, 'grad_norm': 0.2936813533306122, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.34it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.91it/s]                                              {'loss': 0.6856, 'grad_norm': 0.16643628478050232, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.91it/s]                                              {'loss': 0.7104, 'grad_norm': 0.23552438616752625, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.91it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6943, 'grad_norm': 0.23613382875919342, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6974, 'grad_norm': 0.18663406372070312, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.23it/s] 40%|████      | 6/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6726, 'grad_norm': 0.2399500012397766, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6994, 'grad_norm': 0.20550920069217682, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.13it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.6839, 'grad_norm': 0.13635219633579254, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.6761, 'grad_norm': 0.39089488983154297, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.77it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6836, 'grad_norm': 0.2629278600215912, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6724, 'grad_norm': 0.26286581158638, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.32it/s] 80%|████████  | 12/15 [00:00<00:00, 15.73it/s]                                               {'loss': 0.6879, 'grad_norm': 0.1133621484041214, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.73it/s]                                               {'loss': 0.6894, 'grad_norm': 0.24109506607055664, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.73it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.6854, 'grad_norm': 0.23829513788223267, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.6885, 'grad_norm': 0.23739922046661377, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.67it/s]                                               {'train_runtime': 1.0588, 'train_samples_per_second': 401.38, 'train_steps_per_second': 14.166, 'train_loss': 0.6874548037846883, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.67it/s]100%|██████████| 15/15 [00:01<00:00, 14.17it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6861, 'grad_norm': 0.3235146105289459, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.23it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.83it/s]                                              {'loss': 0.6909, 'grad_norm': 0.34983691573143005, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.83it/s]                                              {'loss': 0.6789, 'grad_norm': 0.2072134017944336, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.83it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.692, 'grad_norm': 0.1885153204202652, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6903, 'grad_norm': 0.42842766642570496, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.23it/s] 40%|████      | 6/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.6884, 'grad_norm': 0.33340713381767273, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.11it/s]                                              {'loss': 0.6758, 'grad_norm': 0.2763294577598572, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.11it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6831, 'grad_norm': 0.24874918162822723, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6796, 'grad_norm': 0.4694977104663849, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.06it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.6851, 'grad_norm': 0.3586844205856323, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.6846, 'grad_norm': 0.17871642112731934, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.70it/s] 80%|████████  | 12/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.68, 'grad_norm': 0.4473355710506439, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.34it/s]                                               {'loss': 0.6776, 'grad_norm': 0.38242191076278687, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.34it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.6851, 'grad_norm': 0.2522691488265991, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.7041, 'grad_norm': 0.2747938334941864, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.87it/s]                                               {'train_runtime': 1.0719, 'train_samples_per_second': 396.483, 'train_steps_per_second': 13.994, 'train_loss': 0.6854363600413005, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.87it/s]100%|██████████| 15/15 [00:01<00:00, 14.00it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 32.98it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.70it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.24it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.40it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.09it/s] 61%|██████    | 20/33 [00:00<00:00, 25.24it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.00it/s] 79%|███████▉  | 26/33 [00:01<00:00, 24.99it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.20it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.07it/s]100%|██████████| 33/33 [00:01<00:00, 25.82it/s]
{'eval_loss': 0.684709906578064, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.62607861936721, 'eval_runtime': 1.3196, 'eval_samples_per_second': 790.411, 'eval_steps_per_second': 25.008}
ROUND:20
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6745, 'grad_norm': 0.30762118101119995, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.28it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.682, 'grad_norm': 0.41996368765830994, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.97it/s]                                              {'loss': 0.6795, 'grad_norm': 0.1842801421880722, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.97it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.6676, 'grad_norm': 0.41775035858154297, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.6842, 'grad_norm': 0.3229731619358063, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.50it/s] 40%|████      | 6/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.6887, 'grad_norm': 0.2210519015789032, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.56it/s]                                              {'loss': 0.6789, 'grad_norm': 0.3578661382198334, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.56it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.73it/s]                                              {'loss': 0.6795, 'grad_norm': 0.3990655541419983, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.73it/s]                                              {'loss': 0.6764, 'grad_norm': 0.2587609887123108, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.73it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.6792, 'grad_norm': 0.27261197566986084, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.6793, 'grad_norm': 0.2964586317539215, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.07it/s] 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.665, 'grad_norm': 0.58278888463974, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.6693, 'grad_norm': 0.46359312534332275, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.47it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.6879, 'grad_norm': 0.33535537123680115, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.6869, 'grad_norm': 0.22488363087177277, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.40it/s]                                               {'train_runtime': 1.07, 'train_samples_per_second': 397.185, 'train_steps_per_second': 14.018, 'train_loss': 0.6785810867945353, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]100%|██████████| 15/15 [00:01<00:00, 14.02it/s]
CLIENT:24
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.683, 'grad_norm': 0.2533511519432068, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.68it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.85it/s]                                              {'loss': 0.69, 'grad_norm': 0.17502805590629578, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.85it/s]                                              {'loss': 0.6839, 'grad_norm': 0.3395567238330841, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.85it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.6945, 'grad_norm': 0.2970848083496094, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.693, 'grad_norm': 0.2040025293827057, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.55it/s] 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.6667, 'grad_norm': 0.2827194333076477, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.6798, 'grad_norm': 0.20101913809776306, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.46it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.6787, 'grad_norm': 0.25876784324645996, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.6741, 'grad_norm': 0.24150289595127106, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.29it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.6817, 'grad_norm': 0.2842274010181427, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.6801, 'grad_norm': 0.14796555042266846, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.89it/s] 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.6644, 'grad_norm': 0.3419387936592102, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.6797, 'grad_norm': 0.3597937822341919, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.46it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.50it/s]                                               {'loss': 0.6802, 'grad_norm': 0.22884991765022278, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.50it/s]                                               {'loss': 0.6912, 'grad_norm': 0.1325085461139679, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.50it/s]                                               {'train_runtime': 1.0717, 'train_samples_per_second': 396.565, 'train_steps_per_second': 13.996, 'train_loss': 0.6813970446586609, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.50it/s]100%|██████████| 15/15 [00:01<00:00, 14.00it/s]
CLIENT:64
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6805, 'grad_norm': 0.19412600994110107, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.47it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.08it/s]                                              {'loss': 0.6773, 'grad_norm': 0.4348600506782532, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.08it/s]                                              {'loss': 0.6902, 'grad_norm': 0.18546640872955322, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.08it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.73it/s]                                              {'loss': 0.6848, 'grad_norm': 0.08749419450759888, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.73it/s]                                              {'loss': 0.6786, 'grad_norm': 0.40248429775238037, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.73it/s] 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.6797, 'grad_norm': 0.2893202304840088, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.46it/s]                                              {'loss': 0.6929, 'grad_norm': 0.14471718668937683, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.46it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6821, 'grad_norm': 0.30168747901916504, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6711, 'grad_norm': 0.4667717218399048, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.31it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.6902, 'grad_norm': 0.19013629853725433, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.6574, 'grad_norm': 0.416692316532135, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.23it/s] 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.674, 'grad_norm': 0.2331061214208603, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.57it/s]                                               {'loss': 0.6928, 'grad_norm': 0.20301374793052673, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.57it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6834, 'grad_norm': 0.2706568241119385, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.677, 'grad_norm': 0.415103942155838, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.48it/s]                                               {'train_runtime': 1.0672, 'train_samples_per_second': 398.244, 'train_steps_per_second': 14.056, 'train_loss': 0.6808061798413595, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]100%|██████████| 15/15 [00:01<00:00, 14.06it/s]
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6739, 'grad_norm': 0.295452356338501, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.87it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6761, 'grad_norm': 0.28205427527427673, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6801, 'grad_norm': 0.2895413637161255, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.06it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6832, 'grad_norm': 0.3891567587852478, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6777, 'grad_norm': 0.16798211634159088, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.13it/s] 40%|████      | 6/15 [00:00<00:00, 15.90it/s]                                              {'loss': 0.667, 'grad_norm': 0.3244975805282593, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.90it/s]                                              {'loss': 0.6712, 'grad_norm': 0.324048787355423, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.90it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.6638, 'grad_norm': 0.41445764899253845, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.6955, 'grad_norm': 0.12190677225589752, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.58it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.6843, 'grad_norm': 0.25583651661872864, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.6751, 'grad_norm': 0.21521447598934174, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.02it/s] 80%|████████  | 12/15 [00:00<00:00, 15.58it/s]                                               {'loss': 0.6683, 'grad_norm': 0.47598519921302795, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.58it/s]                                               {'loss': 0.6774, 'grad_norm': 0.2741108238697052, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.58it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.68, 'grad_norm': 0.3328087627887726, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.93it/s]                                               {'loss': 0.672, 'grad_norm': 0.40251657366752625, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.93it/s]                                               {'train_runtime': 1.0462, 'train_samples_per_second': 406.237, 'train_steps_per_second': 14.338, 'train_loss': 0.6763641119003296, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.93it/s]100%|██████████| 15/15 [00:01<00:00, 14.34it/s]
CLIENT:97
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6815, 'grad_norm': 0.1669226884841919, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.00it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.68it/s]                                              {'loss': 0.6757, 'grad_norm': 0.2103092223405838, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.68it/s]                                              {'loss': 0.6969, 'grad_norm': 0.16961833834648132, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.68it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.96it/s]                                              {'loss': 0.7017, 'grad_norm': 0.18040312826633453, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.96it/s]                                              {'loss': 0.6861, 'grad_norm': 0.13673356175422668, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.96it/s] 40%|████      | 6/15 [00:00<00:00, 16.18it/s]                                              {'loss': 0.6763, 'grad_norm': 0.23838673532009125, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.18it/s]                                              {'loss': 0.6932, 'grad_norm': 0.15757839381694794, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.18it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.6781, 'grad_norm': 0.1825273633003235, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.6831, 'grad_norm': 0.17201416194438934, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.59it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.6961, 'grad_norm': 0.10134577006101608, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.6782, 'grad_norm': 0.2868298590183258, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.03it/s] 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6618, 'grad_norm': 0.4092850387096405, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6922, 'grad_norm': 0.07902377843856812, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.43it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.6783, 'grad_norm': 0.2899779677391052, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.51it/s]                                               {'loss': 0.6958, 'grad_norm': 0.2913106679916382, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.51it/s]                                               {'train_runtime': 1.0564, 'train_samples_per_second': 402.298, 'train_steps_per_second': 14.199, 'train_loss': 0.6849986394246419, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.51it/s]100%|██████████| 15/15 [00:01<00:00, 14.20it/s]
CLIENT:29
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6776, 'grad_norm': 0.19956693053245544, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.89it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.63it/s]                                              {'loss': 0.6893, 'grad_norm': 0.19484513998031616, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.63it/s]                                              {'loss': 0.6884, 'grad_norm': 0.2761407196521759, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.63it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6873, 'grad_norm': 0.19684772193431854, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6948, 'grad_norm': 0.16361376643180847, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.23it/s] 40%|████      | 6/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.6883, 'grad_norm': 0.28361380100250244, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.17it/s]                                              {'loss': 0.6872, 'grad_norm': 0.303754061460495, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.17it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6882, 'grad_norm': 0.20107349753379822, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.695, 'grad_norm': 0.10569870471954346, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.10it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.01it/s]                                               {'loss': 0.6898, 'grad_norm': 0.2834242284297943, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.01it/s]                                               {'loss': 0.684, 'grad_norm': 0.13100147247314453, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.01it/s] 80%|████████  | 12/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.6761, 'grad_norm': 0.19376133382320404, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.6929, 'grad_norm': 0.16112112998962402, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.39it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.6709, 'grad_norm': 0.2912241816520691, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.6762, 'grad_norm': 0.24367548525333405, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]                                               {'train_runtime': 1.0805, 'train_samples_per_second': 393.338, 'train_steps_per_second': 13.883, 'train_loss': 0.6857264955838521, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]100%|██████████| 15/15 [00:01<00:00, 13.89it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6702, 'grad_norm': 0.40803492069244385, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.44it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.10it/s]                                              {'loss': 0.6777, 'grad_norm': 0.46839088201522827, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.10it/s]                                              {'loss': 0.6771, 'grad_norm': 0.265182226896286, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.10it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.54it/s]                                              {'loss': 0.6762, 'grad_norm': 0.36655840277671814, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.54it/s]                                              {'loss': 0.6655, 'grad_norm': 0.4780815839767456, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.54it/s] 40%|████      | 6/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.6701, 'grad_norm': 0.3862611651420593, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.6753, 'grad_norm': 0.5261289477348328, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.53it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.6614, 'grad_norm': 0.47950077056884766, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.689, 'grad_norm': 0.2251303791999817, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.34it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.6742, 'grad_norm': 0.38332971930503845, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.07it/s]                                               {'loss': 0.6465, 'grad_norm': 0.503028392791748, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.07it/s] 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6742, 'grad_norm': 0.47890704870224, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6651, 'grad_norm': 0.413360595703125, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.43it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6651, 'grad_norm': 0.43753352761268616, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6645, 'grad_norm': 0.5065546035766602, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.48it/s]                                               {'train_runtime': 1.0703, 'train_samples_per_second': 397.089, 'train_steps_per_second': 14.015, 'train_loss': 0.6701342066129049, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]100%|██████████| 15/15 [00:01<00:00, 14.02it/s]
CLIENT:70
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6692, 'grad_norm': 0.38212132453918457, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.96it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.18it/s]                                              {'loss': 0.6757, 'grad_norm': 0.19401244819164276, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.18it/s]                                              {'loss': 0.6784, 'grad_norm': 0.32647818326950073, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.18it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.6889, 'grad_norm': 0.2510510981082916, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.6736, 'grad_norm': 0.28381288051605225, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.66it/s] 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.6831, 'grad_norm': 0.3517325520515442, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.6601, 'grad_norm': 0.4766108989715576, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.45it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.6896, 'grad_norm': 0.19398696720600128, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.6706, 'grad_norm': 0.22952114045619965, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.71it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6637, 'grad_norm': 0.4548628032207489, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6735, 'grad_norm': 0.19815436005592346, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.92it/s] 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6856, 'grad_norm': 0.23856286704540253, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6674, 'grad_norm': 0.5077388286590576, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.41it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6809, 'grad_norm': 0.11145295947790146, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6638, 'grad_norm': 0.35826852917671204, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.41it/s]                                               {'train_runtime': 1.0716, 'train_samples_per_second': 396.603, 'train_steps_per_second': 13.998, 'train_loss': 0.6749328414599101, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 14.00it/s]
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6825, 'grad_norm': 0.18542981147766113, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.41it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.65it/s]                                              {'loss': 0.6904, 'grad_norm': 0.1673194319009781, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.65it/s]                                              {'loss': 0.6794, 'grad_norm': 0.5073349475860596, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.65it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.6866, 'grad_norm': 0.37163764238357544, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.6723, 'grad_norm': 0.2819305658340454, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.91it/s] 40%|████      | 6/15 [00:00<00:00, 15.79it/s]                                              {'loss': 0.6796, 'grad_norm': 0.1898280829191208, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.79it/s]                                              {'loss': 0.6671, 'grad_norm': 0.20029473304748535, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.79it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.6831, 'grad_norm': 0.3097132742404938, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.6692, 'grad_norm': 0.3579024076461792, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.51it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.688, 'grad_norm': 0.26530805230140686, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6594, 'grad_norm': 0.24417248368263245, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6863, 'grad_norm': 0.3241863548755646, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6834, 'grad_norm': 0.2289903610944748, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.32it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.11it/s]                                               {'loss': 0.6694, 'grad_norm': 0.3511463403701782, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.11it/s]                                               {'loss': 0.6786, 'grad_norm': 0.23852291703224182, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.11it/s]                                               {'train_runtime': 1.0838, 'train_samples_per_second': 392.141, 'train_steps_per_second': 13.84, 'train_loss': 0.6783492565155029, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.11it/s]100%|██████████| 15/15 [00:01<00:00, 13.84it/s]
CLIENT:11
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6832, 'grad_norm': 0.24943745136260986, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.92it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.63it/s]                                              {'loss': 0.6866, 'grad_norm': 0.3898625075817108, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.63it/s]                                              {'loss': 0.689, 'grad_norm': 0.1081789955496788, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.63it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.6838, 'grad_norm': 0.33058714866638184, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.6994, 'grad_norm': 0.282958060503006, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.16it/s] 40%|████      | 6/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6961, 'grad_norm': 0.18877974152565002, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6879, 'grad_norm': 0.2884596884250641, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.20it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.84it/s]                                              {'loss': 0.6886, 'grad_norm': 0.2428009957075119, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.84it/s]                                              {'loss': 0.6905, 'grad_norm': 0.19991108775138855, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.84it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.17it/s]                                               {'loss': 0.6785, 'grad_norm': 0.3537929952144623, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.17it/s]                                               {'loss': 0.6884, 'grad_norm': 0.1851906180381775, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.17it/s] 80%|████████  | 12/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.6937, 'grad_norm': 0.28615236282348633, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.6815, 'grad_norm': 0.284473717212677, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.55it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6946, 'grad_norm': 0.13938817381858826, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6794, 'grad_norm': 0.4510260224342346, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]                                               {'train_runtime': 1.0728, 'train_samples_per_second': 396.145, 'train_steps_per_second': 13.982, 'train_loss': 0.6880895256996155, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 13.99it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 35.83it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.33it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.67it/s] 42%|████▏     | 14/33 [00:00<00:00, 26.19it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.75it/s] 61%|██████    | 20/33 [00:00<00:00, 25.14it/s] 70%|██████▉   | 23/33 [00:00<00:00, 24.78it/s] 79%|███████▉  | 26/33 [00:01<00:00, 24.59it/s] 88%|████████▊ | 29/33 [00:01<00:00, 24.50it/s] 97%|█████████▋| 32/33 [00:01<00:00, 24.52it/s]100%|██████████| 33/33 [00:01<00:00, 25.76it/s]
{'eval_loss': 0.679774820804596, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6663470757430489, 'eval_runtime': 1.3224, 'eval_samples_per_second': 788.735, 'eval_steps_per_second': 24.955}
ROUND:21
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6677, 'grad_norm': 0.3188486397266388, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.04it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.27it/s]                                              {'loss': 0.6701, 'grad_norm': 0.30058610439300537, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.27it/s]                                              {'loss': 0.6742, 'grad_norm': 0.29633137583732605, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.27it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.6749, 'grad_norm': 0.4102463126182556, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.6746, 'grad_norm': 0.17113877832889557, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.33it/s] 40%|████      | 6/15 [00:00<00:00, 16.24it/s]                                              {'loss': 0.6601, 'grad_norm': 0.34744927287101746, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.24it/s]                                              {'loss': 0.664, 'grad_norm': 0.3533819019794464, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.24it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.6547, 'grad_norm': 0.4392346441745758, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.6939, 'grad_norm': 0.12731394171714783, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.72it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.6789, 'grad_norm': 0.26961466670036316, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.6707, 'grad_norm': 0.2300114780664444, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.18it/s] 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.6575, 'grad_norm': 0.5210428237915039, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.6715, 'grad_norm': 0.2899926006793976, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.56it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.6727, 'grad_norm': 0.34972938895225525, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.6631, 'grad_norm': 0.44209951162338257, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.61it/s]                                               {'train_runtime': 1.0544, 'train_samples_per_second': 403.075, 'train_steps_per_second': 14.226, 'train_loss': 0.6698963721593221, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.61it/s]100%|██████████| 15/15 [00:01<00:00, 14.23it/s]
CLIENT:67
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6598, 'grad_norm': 0.363547682762146, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.09it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.75it/s]                                              {'loss': 0.6624, 'grad_norm': 0.4440986216068268, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.75it/s]                                              {'loss': 0.6715, 'grad_norm': 0.27865415811538696, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.75it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.12it/s]                                              {'loss': 0.6665, 'grad_norm': 0.4559151828289032, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.12it/s]                                              {'loss': 0.6619, 'grad_norm': 0.42744511365890503, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.12it/s] 40%|████      | 6/15 [00:00<00:00, 15.06it/s]                                              {'loss': 0.6745, 'grad_norm': 0.2298160344362259, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.06it/s]                                              {'loss': 0.6756, 'grad_norm': 0.2941279709339142, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.06it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.01it/s]                                              {'loss': 0.659, 'grad_norm': 0.46635767817497253, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.01it/s]                                              {'loss': 0.653, 'grad_norm': 0.48385873436927795, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.01it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6568, 'grad_norm': 0.4450533390045166, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6523, 'grad_norm': 0.3593303859233856, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.92it/s] 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.666, 'grad_norm': 0.43598294258117676, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.6577, 'grad_norm': 0.36934322118759155, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.59it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.28it/s]                                               {'loss': 0.6544, 'grad_norm': 0.3871649503707886, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.28it/s]                                               {'loss': 0.6467, 'grad_norm': 0.5055623650550842, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.28it/s]                                               {'train_runtime': 1.0848, 'train_samples_per_second': 391.792, 'train_steps_per_second': 13.828, 'train_loss': 0.6612034598986308, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.28it/s]100%|██████████| 15/15 [00:01<00:00, 13.83it/s]
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6755, 'grad_norm': 0.2519448697566986, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.99it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.6781, 'grad_norm': 0.34104907512664795, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.6761, 'grad_norm': 0.40016183257102966, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.08it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.667, 'grad_norm': 0.33259889483451843, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.6871, 'grad_norm': 0.22001929581165314, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.40it/s] 40%|████      | 6/15 [00:00<00:00, 15.80it/s]                                              {'loss': 0.6658, 'grad_norm': 0.4201871156692505, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.80it/s]                                              {'loss': 0.6756, 'grad_norm': 0.2989443838596344, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.80it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.6773, 'grad_norm': 0.28512394428253174, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.6614, 'grad_norm': 0.37996307015419006, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.40it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.6728, 'grad_norm': 0.36185669898986816, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.667, 'grad_norm': 0.2524747848510742, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.88it/s] 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.6747, 'grad_norm': 0.4226733148097992, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.6639, 'grad_norm': 0.3720077574253082, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.51it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.6651, 'grad_norm': 0.23371052742004395, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.6699, 'grad_norm': 0.33627209067344666, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.52it/s]                                               {'train_runtime': 1.061, 'train_samples_per_second': 400.561, 'train_steps_per_second': 14.137, 'train_loss': 0.6718145132064819, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.52it/s]100%|██████████| 15/15 [00:01<00:00, 14.14it/s]
CLIENT:64
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6767, 'grad_norm': 0.20569878816604614, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.11it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.6682, 'grad_norm': 0.4605112075805664, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.687, 'grad_norm': 0.1886463463306427, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.33it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.44it/s]                                              {'loss': 0.6838, 'grad_norm': 0.09189445525407791, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.44it/s]                                              {'loss': 0.6702, 'grad_norm': 0.41185542941093445, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.44it/s] 40%|████      | 6/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.6738, 'grad_norm': 0.3018484115600586, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.6902, 'grad_norm': 0.15188248455524445, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.23it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.6758, 'grad_norm': 0.3156513571739197, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.661, 'grad_norm': 0.5030269622802734, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.76it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6865, 'grad_norm': 0.19513148069381714, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6483, 'grad_norm': 0.44448474049568176, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.92it/s] 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6694, 'grad_norm': 0.24211707711219788, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6887, 'grad_norm': 0.21164080500602722, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.41it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.6782, 'grad_norm': 0.28207799792289734, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.6679, 'grad_norm': 0.4451281428337097, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]                                               {'train_runtime': 1.0751, 'train_samples_per_second': 395.313, 'train_steps_per_second': 13.952, 'train_loss': 0.6750451644261678, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]100%|██████████| 15/15 [00:01<00:00, 13.96it/s]
CLIENT:96
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6717, 'grad_norm': 0.40941330790519714, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.32it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.98it/s]                                              {'loss': 0.6843, 'grad_norm': 0.26395276188850403, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.98it/s]                                              {'loss': 0.6747, 'grad_norm': 0.4215794503688812, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.98it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.83it/s]                                              {'loss': 0.6811, 'grad_norm': 0.3630779981613159, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.83it/s]                                              {'loss': 0.6728, 'grad_norm': 0.3649938404560089, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.83it/s] 40%|████      | 6/15 [00:00<00:00, 15.49it/s]                                              {'loss': 0.6676, 'grad_norm': 0.47550997138023376, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.49it/s]                                              {'loss': 0.6696, 'grad_norm': 0.18736466765403748, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.49it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.6428, 'grad_norm': 0.5174244046211243, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.30it/s]                                              {'loss': 0.6543, 'grad_norm': 0.45007020235061646, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.30it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.90it/s]                                               {'loss': 0.6787, 'grad_norm': 0.4120842218399048, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.90it/s]                                               {'loss': 0.6684, 'grad_norm': 0.2241314798593521, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.90it/s] 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.6512, 'grad_norm': 0.5675431489944458, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.6617, 'grad_norm': 0.49284958839416504, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.49it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.6715, 'grad_norm': 0.2613127827644348, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.64it/s]                                               {'loss': 0.6575, 'grad_norm': 0.49157392978668213, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.64it/s]                                               {'train_runtime': 1.0635, 'train_samples_per_second': 399.62, 'train_steps_per_second': 14.104, 'train_loss': 0.6671927293141683, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.64it/s]100%|██████████| 15/15 [00:01<00:00, 14.11it/s]
CLIENT:83
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6759, 'grad_norm': 0.41716164350509644, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.41it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.78it/s]                                              {'loss': 0.6786, 'grad_norm': 0.2865584194660187, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.78it/s]                                              {'loss': 0.6697, 'grad_norm': 0.23854729533195496, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.78it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.81it/s]                                              {'loss': 0.6731, 'grad_norm': 0.33950304985046387, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.81it/s]                                              {'loss': 0.6797, 'grad_norm': 0.2563621997833252, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.81it/s] 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.6537, 'grad_norm': 0.4308009445667267, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.6772, 'grad_norm': 0.32999297976493835, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.45it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.6586, 'grad_norm': 0.3587809205055237, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.6739, 'grad_norm': 0.2606698274612427, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.20it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6796, 'grad_norm': 0.2902725636959076, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6565, 'grad_norm': 0.45124807953834534, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.71it/s] 80%|████████  | 12/15 [00:00<00:00, 15.42it/s]                                               {'loss': 0.6642, 'grad_norm': 0.3092215657234192, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.42it/s]                                               {'loss': 0.6536, 'grad_norm': 0.5104336738586426, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.42it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.46it/s]                                               {'loss': 0.6713, 'grad_norm': 0.20099496841430664, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.46it/s]                                               {'loss': 0.6647, 'grad_norm': 0.2789594829082489, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.46it/s]                                               {'train_runtime': 1.0718, 'train_samples_per_second': 396.529, 'train_steps_per_second': 13.995, 'train_loss': 0.6686891754468282, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.46it/s]100%|██████████| 15/15 [00:01<00:00, 14.00it/s]
CLIENT:98
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.667, 'grad_norm': 0.4869198501110077, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.90it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.6824, 'grad_norm': 0.14004574716091156, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.6665, 'grad_norm': 0.39837512373924255, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.23it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.6597, 'grad_norm': 0.39289164543151855, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.30it/s]                                              {'loss': 0.6782, 'grad_norm': 0.2035226970911026, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.30it/s] 40%|████      | 6/15 [00:00<00:00, 16.21it/s]                                              {'loss': 0.6687, 'grad_norm': 0.3498237431049347, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.21it/s]                                              {'loss': 0.6763, 'grad_norm': 0.15261566638946533, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.21it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.47it/s]                                              {'loss': 0.6658, 'grad_norm': 0.3244541883468628, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.47it/s]                                              {'loss': 0.6504, 'grad_norm': 0.5684306025505066, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.47it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.6779, 'grad_norm': 0.35421305894851685, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.659, 'grad_norm': 0.34514427185058594, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.96it/s] 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.6726, 'grad_norm': 0.34379488229751587, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.56it/s]                                               {'loss': 0.6659, 'grad_norm': 0.3421042859554291, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.56it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.6612, 'grad_norm': 0.34027257561683655, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.6705, 'grad_norm': 0.31490933895111084, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.58it/s]                                               {'train_runtime': 1.0653, 'train_samples_per_second': 398.933, 'train_steps_per_second': 14.08, 'train_loss': 0.6681341250737508, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.58it/s]100%|██████████| 15/15 [00:01<00:00, 14.08it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6611, 'grad_norm': 0.2536475956439972, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.05it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.20it/s]                                              {'loss': 0.6721, 'grad_norm': 0.4384169578552246, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.20it/s]                                              {'loss': 0.6885, 'grad_norm': 0.1242341622710228, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.20it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.6633, 'grad_norm': 0.21911843121051788, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.66it/s]                                              {'loss': 0.6756, 'grad_norm': 0.29144376516342163, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.66it/s] 40%|████      | 6/15 [00:00<00:00, 15.43it/s]                                              {'loss': 0.6715, 'grad_norm': 0.364400178194046, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.43it/s]                                              {'loss': 0.6652, 'grad_norm': 0.3676951825618744, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.43it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.668, 'grad_norm': 0.28183239698410034, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6813, 'grad_norm': 0.1531926691532135, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.23it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6854, 'grad_norm': 0.21898166835308075, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6593, 'grad_norm': 0.32471075654029846, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.76it/s] 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.6764, 'grad_norm': 0.29136449098587036, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.6657, 'grad_norm': 0.2615566849708557, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.38it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6731, 'grad_norm': 0.34556472301483154, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6599, 'grad_norm': 0.2655608654022217, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]                                               {'train_runtime': 1.0698, 'train_samples_per_second': 397.257, 'train_steps_per_second': 14.021, 'train_loss': 0.6711010893185934, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 14.03it/s]
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6784, 'grad_norm': 0.3002427816390991, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.88it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.6778, 'grad_norm': 0.27082690596580505, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.6789, 'grad_norm': 0.12280842661857605, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.23it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.38it/s]                                              {'loss': 0.6793, 'grad_norm': 0.3611812889575958, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.38it/s]                                              {'loss': 0.682, 'grad_norm': 0.16072878241539001, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.38it/s] 40%|████      | 6/15 [00:00<00:00, 15.91it/s]                                              {'loss': 0.6794, 'grad_norm': 0.17084819078445435, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.91it/s]                                              {'loss': 0.671, 'grad_norm': 0.26628032326698303, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.91it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.6932, 'grad_norm': 0.16072116792201996, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.6735, 'grad_norm': 0.3182522654533386, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.51it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.6795, 'grad_norm': 0.33759427070617676, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.6882, 'grad_norm': 0.13493357598781586, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.88it/s] 80%|████████  | 12/15 [00:00<00:00, 15.44it/s]                                               {'loss': 0.6927, 'grad_norm': 0.2560003399848938, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.44it/s]                                               {'loss': 0.6733, 'grad_norm': 0.2493126094341278, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.44it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.684, 'grad_norm': 0.1449527144432068, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6675, 'grad_norm': 0.4396037757396698, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.55it/s]                                               {'train_runtime': 1.0661, 'train_samples_per_second': 398.653, 'train_steps_per_second': 14.07, 'train_loss': 0.6799076477686564, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.55it/s]100%|██████████| 15/15 [00:01<00:00, 14.07it/s]
CLIENT:15
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6859, 'grad_norm': 0.2738356292247772, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.22it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.96it/s]                                              {'loss': 0.6748, 'grad_norm': 0.24282385408878326, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.96it/s]                                              {'loss': 0.6758, 'grad_norm': 0.14714446663856506, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.96it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6849, 'grad_norm': 0.24390849471092224, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6868, 'grad_norm': 0.21255697309970856, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.20it/s] 40%|████      | 6/15 [00:00<00:00, 16.34it/s]                                              {'loss': 0.68, 'grad_norm': 0.22975407540798187, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.34it/s]                                              {'loss': 0.6922, 'grad_norm': 0.1336968094110489, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.34it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.6739, 'grad_norm': 0.27737730741500854, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.6641, 'grad_norm': 0.26002153754234314, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.64it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.6776, 'grad_norm': 0.2357797771692276, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.6932, 'grad_norm': 0.09729567170143127, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.83it/s] 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6744, 'grad_norm': 0.433302104473114, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6761, 'grad_norm': 0.2433779388666153, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.29it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.33it/s]                                               {'loss': 0.6672, 'grad_norm': 0.2673506438732147, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.33it/s]                                               {'loss': 0.6803, 'grad_norm': 0.20354855060577393, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.33it/s]                                               {'train_runtime': 1.0677, 'train_samples_per_second': 398.061, 'train_steps_per_second': 14.049, 'train_loss': 0.6791499813397726, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.33it/s]100%|██████████| 15/15 [00:01<00:00, 14.05it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 32.92it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.41it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.36it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.50it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.25it/s] 61%|██████    | 20/33 [00:00<00:00, 25.46it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.01it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.07it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.19it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.08it/s]100%|██████████| 33/33 [00:01<00:00, 25.86it/s]
{'eval_loss': 0.6740578413009644, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6864813039309684, 'eval_runtime': 1.3171, 'eval_samples_per_second': 791.894, 'eval_steps_per_second': 25.055}
ROUND:22
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6773, 'grad_norm': 0.2060629278421402, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.63it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.77it/s]                                              {'loss': 0.6502, 'grad_norm': 0.407105028629303, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.77it/s]                                              {'loss': 0.6521, 'grad_norm': 0.5085616707801819, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.77it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.6773, 'grad_norm': 0.21948954463005066, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.6529, 'grad_norm': 0.5088562965393066, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.41it/s] 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.6698, 'grad_norm': 0.34163179993629456, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.6627, 'grad_norm': 0.48867008090019226, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.50it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6594, 'grad_norm': 0.2936972379684448, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6534, 'grad_norm': 0.3352212905883789, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.31it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.6687, 'grad_norm': 0.36915066838264465, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.6507, 'grad_norm': 0.3693794906139374, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.87it/s] 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.6479, 'grad_norm': 0.4053681492805481, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.6679, 'grad_norm': 0.22180429100990295, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.49it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.00it/s]                                               {'loss': 0.6457, 'grad_norm': 0.4518430531024933, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.00it/s]                                               {'loss': 0.6442, 'grad_norm': 0.5625153183937073, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.00it/s]                                               {'train_runtime': 1.0562, 'train_samples_per_second': 402.368, 'train_steps_per_second': 14.201, 'train_loss': 0.6586777488390605, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.00it/s]100%|██████████| 15/15 [00:01<00:00, 14.21it/s]
CLIENT:62
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6654, 'grad_norm': 0.39607036113739014, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.90it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.03it/s]                                              {'loss': 0.6729, 'grad_norm': 0.30468669533729553, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.03it/s]                                              {'loss': 0.6465, 'grad_norm': 0.6476491689682007, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.03it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.69it/s]                                              {'loss': 0.658, 'grad_norm': 0.5105463266372681, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.69it/s]                                              {'loss': 0.6704, 'grad_norm': 0.2530948519706726, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.69it/s] 40%|████      | 6/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.6322, 'grad_norm': 0.5973628759384155, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.6501, 'grad_norm': 0.4224961996078491, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.42it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.6527, 'grad_norm': 0.4830824136734009, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.6472, 'grad_norm': 0.3784768581390381, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.20it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.6663, 'grad_norm': 0.402846097946167, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.74it/s]                                               {'loss': 0.6483, 'grad_norm': 0.4183512032032013, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.74it/s] 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6357, 'grad_norm': 0.588932991027832, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6538, 'grad_norm': 0.4349839687347412, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.43it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6605, 'grad_norm': 0.38012221455574036, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6429, 'grad_norm': 0.6707499027252197, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.92it/s]                                               {'train_runtime': 1.0653, 'train_samples_per_second': 398.95, 'train_steps_per_second': 14.081, 'train_loss': 0.6535272598266602, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.92it/s]100%|██████████| 15/15 [00:01<00:00, 14.09it/s]
CLIENT:26
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.668, 'grad_norm': 0.4233603775501251, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.96it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.44it/s]                                              {'loss': 0.6636, 'grad_norm': 0.2766771912574768, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.44it/s]                                              {'loss': 0.6934, 'grad_norm': 0.11358784139156342, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.44it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.84it/s]                                              {'loss': 0.6807, 'grad_norm': 0.2705126106739044, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.84it/s]                                              {'loss': 0.6799, 'grad_norm': 0.21086843311786652, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.84it/s] 40%|████      | 6/15 [00:00<00:00, 15.75it/s]                                              {'loss': 0.6438, 'grad_norm': 0.4438934028148651, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.75it/s]                                              {'loss': 0.6532, 'grad_norm': 0.39252969622612, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.75it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.682, 'grad_norm': 0.15522673726081848, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.50it/s]                                              {'loss': 0.6567, 'grad_norm': 0.36939072608947754, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.50it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.668, 'grad_norm': 0.2578355669975281, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.6728, 'grad_norm': 0.24808089435100555, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.98it/s] 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6642, 'grad_norm': 0.4739786386489868, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6669, 'grad_norm': 0.2776052951812744, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.54it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.671, 'grad_norm': 0.31826701760292053, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.683, 'grad_norm': 0.2550908923149109, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.91it/s]                                               {'train_runtime': 1.0534, 'train_samples_per_second': 403.469, 'train_steps_per_second': 14.24, 'train_loss': 0.6698125600814819, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.91it/s]100%|██████████| 15/15 [00:01<00:00, 14.24it/s]
CLIENT:41
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6714, 'grad_norm': 0.24558791518211365, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.31it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.06it/s]                                              {'loss': 0.6921, 'grad_norm': 0.09294765442609787, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.06it/s]                                              {'loss': 0.6897, 'grad_norm': 0.14934656023979187, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.06it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.7162, 'grad_norm': 0.17262879014015198, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.6768, 'grad_norm': 0.31715986132621765, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.72it/s] 40%|████      | 6/15 [00:00<00:00, 15.58it/s]                                              {'loss': 0.6805, 'grad_norm': 0.13863126933574677, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.58it/s]                                              {'loss': 0.6745, 'grad_norm': 0.3012045919895172, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.58it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.6983, 'grad_norm': 0.0955478847026825, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.7007, 'grad_norm': 0.1338774710893631, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.36it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.6911, 'grad_norm': 0.10321937501430511, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.91it/s]                                               {'loss': 0.6838, 'grad_norm': 0.1243152990937233, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.91it/s] 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.6709, 'grad_norm': 0.2248554229736328, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.6835, 'grad_norm': 0.18170133233070374, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.53it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.6825, 'grad_norm': 0.20198719203472137, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.7076, 'grad_norm': 0.14771990478038788, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.99it/s]                                               {'train_runtime': 1.0575, 'train_samples_per_second': 401.887, 'train_steps_per_second': 14.184, 'train_loss': 0.6879783312479655, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.99it/s]100%|██████████| 15/15 [00:01<00:00, 14.19it/s]
CLIENT:35
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6723, 'grad_norm': 0.38837048411369324, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.93it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6877, 'grad_norm': 0.20882517099380493, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6775, 'grad_norm': 0.17700281739234924, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.10it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.6766, 'grad_norm': 0.28714290261268616, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.6673, 'grad_norm': 0.25892192125320435, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.24it/s] 40%|████      | 6/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.6768, 'grad_norm': 0.1648227870464325, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.89it/s]                                              {'loss': 0.6821, 'grad_norm': 0.21957583725452423, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.89it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.6697, 'grad_norm': 0.2983796000480652, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.671, 'grad_norm': 0.25239941477775574, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.58it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.6673, 'grad_norm': 0.25846850872039795, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.05it/s]                                               {'loss': 0.6682, 'grad_norm': 0.15625832974910736, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.05it/s] 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.6651, 'grad_norm': 0.4827953577041626, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.51it/s]                                               {'loss': 0.672, 'grad_norm': 0.18429717421531677, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.51it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.6726, 'grad_norm': 0.262874960899353, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.6604, 'grad_norm': 0.3677583634853363, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.53it/s]                                               {'train_runtime': 1.0575, 'train_samples_per_second': 401.883, 'train_steps_per_second': 14.184, 'train_loss': 0.672443147500356, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.53it/s]100%|██████████| 15/15 [00:01<00:00, 14.19it/s]
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6714, 'grad_norm': 0.3185688555240631, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.68it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6718, 'grad_norm': 0.2783846855163574, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.06it/s]                                              {'loss': 0.6776, 'grad_norm': 0.12761276960372925, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.06it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.6709, 'grad_norm': 0.3851211667060852, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.6789, 'grad_norm': 0.16093245148658752, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.72it/s] 40%|████      | 6/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.6766, 'grad_norm': 0.17375528812408447, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.47it/s]                                              {'loss': 0.665, 'grad_norm': 0.2805858850479126, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.47it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.69, 'grad_norm': 0.1657121479511261, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.6662, 'grad_norm': 0.33524906635284424, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.29it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.6715, 'grad_norm': 0.3656129240989685, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.89it/s]                                               {'loss': 0.6858, 'grad_norm': 0.1373242735862732, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.89it/s] 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.6873, 'grad_norm': 0.2600325644016266, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.6679, 'grad_norm': 0.2499130517244339, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.35it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6812, 'grad_norm': 0.15446694195270538, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6573, 'grad_norm': 0.4604138731956482, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.45it/s]                                               {'train_runtime': 1.0699, 'train_samples_per_second': 397.217, 'train_steps_per_second': 14.019, 'train_loss': 0.6746166586875916, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 14.02it/s]
CLIENT:36
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6644, 'grad_norm': 0.35289546847343445, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.31it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.02it/s]                                              {'loss': 0.6727, 'grad_norm': 0.31445643305778503, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.02it/s]                                              {'loss': 0.6504, 'grad_norm': 0.456612229347229, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.02it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.6543, 'grad_norm': 0.430277943611145, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.6772, 'grad_norm': 0.31520411372184753, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.61it/s] 40%|████      | 6/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.6551, 'grad_norm': 0.2986229360103607, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.63it/s]                                              {'loss': 0.6478, 'grad_norm': 0.5854461193084717, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.63it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6917, 'grad_norm': 0.14475834369659424, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6483, 'grad_norm': 0.44489914178848267, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.32it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.645, 'grad_norm': 0.48632416129112244, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.6664, 'grad_norm': 0.25236913561820984, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.83it/s] 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.6595, 'grad_norm': 0.297099232673645, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.53it/s]                                               {'loss': 0.6698, 'grad_norm': 0.26681482791900635, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.53it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6469, 'grad_norm': 0.4410305917263031, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6434, 'grad_norm': 0.40503954887390137, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.76it/s]                                               {'train_runtime': 1.0623, 'train_samples_per_second': 400.086, 'train_steps_per_second': 14.121, 'train_loss': 0.6595219850540162, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.76it/s]100%|██████████| 15/15 [00:01<00:00, 14.13it/s]
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6754, 'grad_norm': 0.1457192748785019, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.32it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.25it/s]                                              {'loss': 0.6477, 'grad_norm': 0.42442095279693604, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.25it/s]                                              {'loss': 0.6601, 'grad_norm': 0.4190135896205902, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.25it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.14it/s]                                              {'loss': 0.678, 'grad_norm': 0.2713565528392792, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.14it/s]                                              {'loss': 0.6535, 'grad_norm': 0.505765438079834, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.14it/s] 40%|████      | 6/15 [00:00<00:00, 16.37it/s]                                              {'loss': 0.675, 'grad_norm': 0.251072496175766, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.37it/s]                                              {'loss': 0.6481, 'grad_norm': 0.5459522008895874, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.37it/s] 53%|█████▎    | 8/15 [00:00<00:00, 15.05it/s]                                              {'loss': 0.6719, 'grad_norm': 0.21956320106983185, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 15.05it/s]                                              {'loss': 0.6857, 'grad_norm': 0.1395377367734909, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 15.05it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.6787, 'grad_norm': 0.24969175457954407, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.69it/s]                                               {'loss': 0.6405, 'grad_norm': 0.37045684456825256, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.69it/s] 80%|████████  | 12/15 [00:00<00:00, 15.99it/s]                                               {'loss': 0.668, 'grad_norm': 0.43612074851989746, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.99it/s]                                               {'loss': 0.6795, 'grad_norm': 0.24273166060447693, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.99it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.6602, 'grad_norm': 0.3930012285709381, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.6502, 'grad_norm': 0.42941713333129883, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.54it/s]                                               {'train_runtime': 1.037, 'train_samples_per_second': 409.823, 'train_steps_per_second': 14.464, 'train_loss': 0.6648403247197469, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.54it/s]100%|██████████| 15/15 [00:01<00:00, 14.47it/s]
CLIENT:77
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.677, 'grad_norm': 0.252848744392395, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.29it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.25it/s]                                              {'loss': 0.6919, 'grad_norm': 0.15685179829597473, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.25it/s]                                              {'loss': 0.6808, 'grad_norm': 0.22004710137844086, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.25it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.91it/s]                                              {'loss': 0.6909, 'grad_norm': 0.10661828517913818, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.91it/s]                                              {'loss': 0.6876, 'grad_norm': 0.20094874501228333, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.91it/s] 40%|████      | 6/15 [00:00<00:00, 16.17it/s]                                              {'loss': 0.6609, 'grad_norm': 0.3792518377304077, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.17it/s]                                              {'loss': 0.6813, 'grad_norm': 0.24933578073978424, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.17it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.6818, 'grad_norm': 0.16300074756145477, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.6814, 'grad_norm': 0.23064522445201874, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.61it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.6904, 'grad_norm': 0.17202816903591156, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.6661, 'grad_norm': 0.3200208842754364, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.38it/s] 80%|████████  | 12/15 [00:00<00:00, 15.97it/s]                                               {'loss': 0.6966, 'grad_norm': 0.16510368883609772, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.97it/s]                                               {'loss': 0.6732, 'grad_norm': 0.31079092621803284, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.97it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.7056, 'grad_norm': 0.09681899845600128, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.6924, 'grad_norm': 0.33056706190109253, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.54it/s]                                               {'train_runtime': 1.0435, 'train_samples_per_second': 407.301, 'train_steps_per_second': 14.375, 'train_loss': 0.6838743487993876, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.54it/s]100%|██████████| 15/15 [00:01<00:00, 14.38it/s]
CLIENT:21
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6586, 'grad_norm': 0.3323007822036743, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.29it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.6883, 'grad_norm': 0.19381408393383026, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.6614, 'grad_norm': 0.45869770646095276, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.26it/s] 27%|██▋       | 4/15 [00:00<00:00, 16.00it/s]                                              {'loss': 0.6728, 'grad_norm': 0.30466046929359436, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 16.00it/s]                                              {'loss': 0.6732, 'grad_norm': 0.3593674600124359, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 16.00it/s] 40%|████      | 6/15 [00:00<00:00, 16.17it/s]                                              {'loss': 0.6764, 'grad_norm': 0.24413567781448364, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.17it/s]                                              {'loss': 0.6619, 'grad_norm': 0.5266041159629822, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.17it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.6873, 'grad_norm': 0.128866046667099, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.6607, 'grad_norm': 0.4329601228237152, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.63it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.6743, 'grad_norm': 0.242601677775383, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.6715, 'grad_norm': 0.3018556237220764, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.11it/s] 80%|████████  | 12/15 [00:00<00:00, 16.05it/s]                                               {'loss': 0.6628, 'grad_norm': 0.41785168647766113, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.05it/s]                                               {'loss': 0.6617, 'grad_norm': 0.3430413603782654, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.05it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.6671, 'grad_norm': 0.2301705777645111, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.663, 'grad_norm': 0.4247843325138092, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.58it/s]                                               {'train_runtime': 1.0447, 'train_samples_per_second': 406.83, 'train_steps_per_second': 14.359, 'train_loss': 0.6694024324417114, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.58it/s]100%|██████████| 15/15 [00:01<00:00, 14.36it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 33.10it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.81it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.27it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.54it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.10it/s] 61%|██████    | 20/33 [00:00<00:00, 25.20it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.16it/s] 79%|███████▉  | 26/33 [00:01<00:00, 24.96it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.15it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.13it/s]100%|██████████| 33/33 [00:01<00:00, 25.87it/s]
{'eval_loss': 0.6684598326683044, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3168, 'eval_samples_per_second': 792.051, 'eval_steps_per_second': 25.06}
ROUND:23
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6501, 'grad_norm': 0.2708141505718231, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.98it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.12it/s]                                              {'loss': 0.6513, 'grad_norm': 0.5036419034004211, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.12it/s]                                              {'loss': 0.6916, 'grad_norm': 0.14969581365585327, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.12it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.99it/s]                                              {'loss': 0.6539, 'grad_norm': 0.22782132029533386, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.99it/s]                                              {'loss': 0.6624, 'grad_norm': 0.31166625022888184, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.99it/s] 40%|████      | 6/15 [00:00<00:00, 15.71it/s]                                              {'loss': 0.6551, 'grad_norm': 0.39455530047416687, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.71it/s]                                              {'loss': 0.6481, 'grad_norm': 0.3947125971317291, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.71it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.26it/s]                                              {'loss': 0.6552, 'grad_norm': 0.3113553524017334, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.26it/s]                                              {'loss': 0.6759, 'grad_norm': 0.15748219192028046, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.26it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.6759, 'grad_norm': 0.2297409176826477, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.6442, 'grad_norm': 0.35064029693603516, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.85it/s] 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6633, 'grad_norm': 0.32347598671913147, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6542, 'grad_norm': 0.26700833439826965, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.31it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.6569, 'grad_norm': 0.3767273426055908, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.34it/s]                                               {'loss': 0.6483, 'grad_norm': 0.2789715826511383, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.34it/s]                                               {'train_runtime': 1.0738, 'train_samples_per_second': 395.784, 'train_steps_per_second': 13.969, 'train_loss': 0.6590991775194804, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.34it/s]100%|██████████| 15/15 [00:01<00:00, 13.97it/s]
CLIENT:54
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6607, 'grad_norm': 0.414504736661911, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.20it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.89it/s]                                              {'loss': 0.6815, 'grad_norm': 0.16877637803554535, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.89it/s]                                              {'loss': 0.6701, 'grad_norm': 0.19754399359226227, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.89it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.6579, 'grad_norm': 0.3210535943508148, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.6941, 'grad_norm': 0.1588861495256424, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.35it/s] 40%|████      | 6/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.657, 'grad_norm': 0.41448667645454407, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.32it/s]                                              {'loss': 0.6449, 'grad_norm': 0.4263455271720886, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.32it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.90it/s]                                              {'loss': 0.6702, 'grad_norm': 0.17570431530475616, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.90it/s]                                              {'loss': 0.6784, 'grad_norm': 0.20031388103961945, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.90it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.6573, 'grad_norm': 0.29148414731025696, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.59it/s]                                               {'loss': 0.6688, 'grad_norm': 0.2372216284275055, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.59it/s] 80%|████████  | 12/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.655, 'grad_norm': 0.3568393886089325, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6487, 'grad_norm': 0.33806902170181274, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.13it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.30it/s]                                               {'loss': 0.6613, 'grad_norm': 0.3216705024242401, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.30it/s]                                               {'loss': 0.6698, 'grad_norm': 0.15537451207637787, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.30it/s]                                               {'train_runtime': 1.0897, 'train_samples_per_second': 390.03, 'train_steps_per_second': 13.766, 'train_loss': 0.6650479873021443, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.30it/s]100%|██████████| 15/15 [00:01<00:00, 13.77it/s]
CLIENT:27
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6441, 'grad_norm': 0.5764450430870056, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.41it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.06it/s]                                              {'loss': 0.6514, 'grad_norm': 0.3230014741420746, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.06it/s]                                              {'loss': 0.6672, 'grad_norm': 0.2239989936351776, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.06it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6748, 'grad_norm': 0.21789304912090302, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6596, 'grad_norm': 0.39942988753318787, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.16it/s] 40%|████      | 6/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.6352, 'grad_norm': 0.5798967480659485, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.6671, 'grad_norm': 0.32980504631996155, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.53it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.6375, 'grad_norm': 0.45191690325737, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.6411, 'grad_norm': 0.40216153860092163, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.70it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.6553, 'grad_norm': 0.40361717343330383, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.96it/s]                                               {'loss': 0.6405, 'grad_norm': 0.40275290608406067, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.96it/s] 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.639, 'grad_norm': 0.3758879005908966, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6469, 'grad_norm': 0.44255101680755615, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.41it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6546, 'grad_norm': 0.3274310231208801, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6492, 'grad_norm': 0.4425748288631439, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.41it/s]                                               {'train_runtime': 1.0682, 'train_samples_per_second': 397.855, 'train_steps_per_second': 14.042, 'train_loss': 0.6508902311325073, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 14.05it/s]
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6533, 'grad_norm': 0.34417545795440674, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.27it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.6564, 'grad_norm': 0.32493487000465393, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.77it/s]                                              {'loss': 0.6608, 'grad_norm': 0.3325461745262146, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.77it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.83it/s]                                              {'loss': 0.6553, 'grad_norm': 0.4636685252189636, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.83it/s]                                              {'loss': 0.6669, 'grad_norm': 0.20266065001487732, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.83it/s] 40%|████      | 6/15 [00:00<00:00, 16.04it/s]                                              {'loss': 0.6439, 'grad_norm': 0.3710898756980896, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.04it/s]                                              {'loss': 0.6473, 'grad_norm': 0.39540573954582214, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.04it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.6331, 'grad_norm': 0.4963560402393341, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.51it/s]                                              {'loss': 0.6913, 'grad_norm': 0.13500863313674927, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.51it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.6663, 'grad_norm': 0.29076334834098816, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.28it/s]                                               {'loss': 0.6606, 'grad_norm': 0.24863764643669128, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.28it/s] 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.6319, 'grad_norm': 0.5989593267440796, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.6582, 'grad_norm': 0.3008866608142853, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.46it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.6564, 'grad_norm': 0.38186565041542053, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.6415, 'grad_norm': 0.5075770020484924, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.49it/s]                                               {'train_runtime': 1.05, 'train_samples_per_second': 404.75, 'train_steps_per_second': 14.285, 'train_loss': 0.6548857808113098, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.49it/s]100%|██████████| 15/15 [00:01<00:00, 14.29it/s]
CLIENT:39
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6634, 'grad_norm': 0.50020432472229, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.81it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.04it/s]                                              {'loss': 0.6826, 'grad_norm': 0.1450558304786682, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.04it/s]                                              {'loss': 0.6755, 'grad_norm': 0.2662086486816406, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.04it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.49it/s]                                              {'loss': 0.6847, 'grad_norm': 0.1963527500629425, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.49it/s]                                              {'loss': 0.6607, 'grad_norm': 0.3084781765937805, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.49it/s] 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.6396, 'grad_norm': 0.33890265226364136, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.6911, 'grad_norm': 0.12829335033893585, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.97it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.6617, 'grad_norm': 0.37806224822998047, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.6445, 'grad_norm': 0.42228466272354126, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.57it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6619, 'grad_norm': 0.30455148220062256, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6547, 'grad_norm': 0.37048017978668213, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.13it/s] 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.6512, 'grad_norm': 0.3020884096622467, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.59it/s]                                               {'loss': 0.6515, 'grad_norm': 0.466155081987381, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.59it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.6747, 'grad_norm': 0.22209642827510834, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.683, 'grad_norm': 0.16911599040031433, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.53it/s]                                               {'train_runtime': 1.0553, 'train_samples_per_second': 402.724, 'train_steps_per_second': 14.214, 'train_loss': 0.6653969327608744, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.53it/s]100%|██████████| 15/15 [00:01<00:00, 14.22it/s]
CLIENT:65
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6931, 'grad_norm': 0.1888185441493988, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.22it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.91it/s]                                              {'loss': 0.6635, 'grad_norm': 0.38193362951278687, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.91it/s]                                              {'loss': 0.7066, 'grad_norm': 0.13923852145671844, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.91it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.6634, 'grad_norm': 0.20019175112247467, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.6708, 'grad_norm': 0.2790684700012207, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.42it/s] 40%|████      | 6/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.6801, 'grad_norm': 0.17465825378894806, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.40it/s]                                              {'loss': 0.6825, 'grad_norm': 0.18942195177078247, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.40it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.86it/s]                                              {'loss': 0.6865, 'grad_norm': 0.14067335426807404, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.86it/s]                                              {'loss': 0.6574, 'grad_norm': 0.37727662920951843, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.86it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.6911, 'grad_norm': 0.1602477878332138, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.6823, 'grad_norm': 0.19364289939403534, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.21it/s] 80%|████████  | 12/15 [00:00<00:00, 15.61it/s]                                               {'loss': 0.6611, 'grad_norm': 0.3121826946735382, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.61it/s]                                               {'loss': 0.6667, 'grad_norm': 0.25598880648612976, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.61it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.6803, 'grad_norm': 0.23811738193035126, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.6913, 'grad_norm': 0.183669313788414, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.54it/s]                                               {'train_runtime': 1.0646, 'train_samples_per_second': 399.211, 'train_steps_per_second': 14.09, 'train_loss': 0.678432019551595, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.54it/s]100%|██████████| 15/15 [00:01<00:00, 14.09it/s]
CLIENT:7
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6716, 'grad_norm': 0.25553640723228455, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.02it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.69it/s]                                              {'loss': 0.6903, 'grad_norm': 0.15794572234153748, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.69it/s]                                              {'loss': 0.6633, 'grad_norm': 0.40155383944511414, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.69it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.6566, 'grad_norm': 0.3025595247745514, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.6743, 'grad_norm': 0.3162692189216614, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.27it/s] 40%|████      | 6/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6754, 'grad_norm': 0.15469980239868164, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6616, 'grad_norm': 0.34239906072616577, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.16it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.6739, 'grad_norm': 0.22171494364738464, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.6706, 'grad_norm': 0.2654654085636139, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.09it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.6625, 'grad_norm': 0.2680448293685913, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.6764, 'grad_norm': 0.20948690176010132, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.72it/s] 80%|████████  | 12/15 [00:00<00:00, 15.27it/s]                                               {'loss': 0.671, 'grad_norm': 0.3003893196582794, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.27it/s]                                               {'loss': 0.6805, 'grad_norm': 0.17863605916500092, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.27it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6528, 'grad_norm': 0.3740095794200897, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6578, 'grad_norm': 0.26573365926742554, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.82it/s]                                               {'train_runtime': 1.0722, 'train_samples_per_second': 396.371, 'train_steps_per_second': 13.99, 'train_loss': 0.6692287802696228, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.82it/s]100%|██████████| 15/15 [00:01<00:00, 13.99it/s]
CLIENT:26
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6586, 'grad_norm': 0.45235946774482727, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.08it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.17it/s]                                              {'loss': 0.6576, 'grad_norm': 0.2863940894603729, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.17it/s]                                              {'loss': 0.6936, 'grad_norm': 0.13072624802589417, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.17it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6751, 'grad_norm': 0.2806131839752197, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6758, 'grad_norm': 0.21043714880943298, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.20it/s] 40%|████      | 6/15 [00:00<00:00, 15.96it/s]                                              {'loss': 0.6344, 'grad_norm': 0.4518507719039917, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.96it/s]                                              {'loss': 0.6447, 'grad_norm': 0.4067729413509369, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.96it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.6794, 'grad_norm': 0.14744427800178528, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.649, 'grad_norm': 0.3730286955833435, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.64it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.09it/s]                                               {'loss': 0.6632, 'grad_norm': 0.2487976998090744, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.09it/s]                                               {'loss': 0.6681, 'grad_norm': 0.2513951063156128, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.09it/s] 80%|████████  | 12/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.6539, 'grad_norm': 0.4901656210422516, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.661, 'grad_norm': 0.2948036789894104, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.52it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.6642, 'grad_norm': 0.3290044963359833, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.678, 'grad_norm': 0.2626277208328247, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.99it/s]                                               {'train_runtime': 1.0455, 'train_samples_per_second': 406.497, 'train_steps_per_second': 14.347, 'train_loss': 0.6637794176737467, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.99it/s]100%|██████████| 15/15 [00:01<00:00, 14.35it/s]
CLIENT:24
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6662, 'grad_norm': 0.2834506034851074, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.43it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.75it/s]                                              {'loss': 0.6795, 'grad_norm': 0.20085042715072632, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.75it/s]                                              {'loss': 0.6614, 'grad_norm': 0.3762095272541046, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.75it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.94it/s]                                              {'loss': 0.674, 'grad_norm': 0.3367856740951538, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.94it/s]                                              {'loss': 0.68, 'grad_norm': 0.24952878057956696, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.94it/s] 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.6475, 'grad_norm': 0.3409426510334015, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.59it/s]                                              {'loss': 0.6665, 'grad_norm': 0.22076304256916046, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.59it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.6612, 'grad_norm': 0.2865925133228302, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.6586, 'grad_norm': 0.270550400018692, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.35it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6623, 'grad_norm': 0.31719690561294556, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.672, 'grad_norm': 0.15665370225906372, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.92it/s] 80%|████████  | 12/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.6407, 'grad_norm': 0.3872554898262024, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.654, 'grad_norm': 0.4186766743659973, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.36it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.665, 'grad_norm': 0.25452426075935364, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6855, 'grad_norm': 0.17463676631450653, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.38it/s]                                               {'train_runtime': 1.0695, 'train_samples_per_second': 397.389, 'train_steps_per_second': 14.025, 'train_loss': 0.6649651606877645, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 14.03it/s]
CLIENT:60
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6653, 'grad_norm': 0.20852795243263245, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.66it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.14it/s]                                              {'loss': 0.6708, 'grad_norm': 0.1668744683265686, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.14it/s]                                              {'loss': 0.6709, 'grad_norm': 0.23590514063835144, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.14it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.6685, 'grad_norm': 0.3016606867313385, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.6803, 'grad_norm': 0.11284033209085464, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.55it/s] 40%|████      | 6/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.6923, 'grad_norm': 0.19326499104499817, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.6754, 'grad_norm': 0.10142200440168381, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.42it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.17it/s]                                              {'loss': 0.667, 'grad_norm': 0.24710655212402344, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.17it/s]                                              {'loss': 0.6705, 'grad_norm': 0.22128190100193024, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.17it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.6492, 'grad_norm': 0.4108559787273407, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.6772, 'grad_norm': 0.11902016401290894, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.10it/s] 80%|████████  | 12/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.6665, 'grad_norm': 0.18600152432918549, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.7007, 'grad_norm': 0.1349489986896515, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.63it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.6614, 'grad_norm': 0.22305138409137726, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.6338, 'grad_norm': 0.5121102333068848, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.57it/s]                                               {'train_runtime': 1.0676, 'train_samples_per_second': 398.095, 'train_steps_per_second': 14.05, 'train_loss': 0.6699831128120423, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.57it/s]100%|██████████| 15/15 [00:01<00:00, 14.05it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 34.65it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.89it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.48it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.61it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.24it/s] 61%|██████    | 20/33 [00:00<00:00, 25.29it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.10it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.12it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.11it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.11it/s]100%|██████████| 33/33 [00:01<00:00, 25.95it/s]
{'eval_loss': 0.663073718547821, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.313, 'eval_samples_per_second': 794.335, 'eval_steps_per_second': 25.132}
ROUND:24
CLIENT:38
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6705, 'grad_norm': 0.2593241035938263, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.29it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.03it/s]                                              {'loss': 0.669, 'grad_norm': 0.28539085388183594, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.03it/s]                                              {'loss': 0.6539, 'grad_norm': 0.41919034719467163, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.03it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.6453, 'grad_norm': 0.4477378726005554, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.6763, 'grad_norm': 0.18216685950756073, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.70it/s] 40%|████      | 6/15 [00:00<00:00, 15.66it/s]                                              {'loss': 0.654, 'grad_norm': 0.2553325891494751, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.66it/s]                                              {'loss': 0.6607, 'grad_norm': 0.35610076785087585, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.66it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.6463, 'grad_norm': 0.33218374848365784, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.6713, 'grad_norm': 0.24669842422008514, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.35it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.6589, 'grad_norm': 0.24167868494987488, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.6436, 'grad_norm': 0.37556350231170654, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.88it/s] 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6537, 'grad_norm': 0.30526623129844666, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6668, 'grad_norm': 0.18464411795139313, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.54it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6526, 'grad_norm': 0.2079693228006363, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6146, 'grad_norm': 0.6553563475608826, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.82it/s]                                               {'train_runtime': 1.0598, 'train_samples_per_second': 401.007, 'train_steps_per_second': 14.153, 'train_loss': 0.6558248122533162, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.82it/s]100%|██████████| 15/15 [00:01<00:00, 14.16it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6694, 'grad_norm': 0.22052375972270966, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.17it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.37it/s]                                              {'loss': 0.6328, 'grad_norm': 0.44118383526802063, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.37it/s]                                              {'loss': 0.6297, 'grad_norm': 0.5656247138977051, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.37it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.6684, 'grad_norm': 0.2386314868927002, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.6303, 'grad_norm': 0.5376293659210205, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.59it/s] 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.655, 'grad_norm': 0.3801098167896271, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.6403, 'grad_norm': 0.551551342010498, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.38it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.6472, 'grad_norm': 0.2967289686203003, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.19it/s]                                              {'loss': 0.6401, 'grad_norm': 0.3292541801929474, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.19it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6522, 'grad_norm': 0.4056089520454407, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6345, 'grad_norm': 0.3957315981388092, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.6296, 'grad_norm': 0.4387454688549042, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.6597, 'grad_norm': 0.21805809438228607, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6253, 'grad_norm': 0.5003674030303955, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6184, 'grad_norm': 0.6210021376609802, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]                                               {'train_runtime': 1.0755, 'train_samples_per_second': 395.156, 'train_steps_per_second': 13.947, 'train_loss': 0.6421974698702494, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 13.95it/s]
CLIENT:32
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6616, 'grad_norm': 0.2388114184141159, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.99it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.6413, 'grad_norm': 0.4902523159980774, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.08it/s]                                              {'loss': 0.6519, 'grad_norm': 0.3603798449039459, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.08it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.6534, 'grad_norm': 0.37912318110466003, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.6272, 'grad_norm': 0.5534325242042542, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.36it/s] 40%|████      | 6/15 [00:00<00:00, 15.81it/s]                                              {'loss': 0.662, 'grad_norm': 0.19112525880336761, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.81it/s]                                              {'loss': 0.624, 'grad_norm': 0.5724784135818481, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.81it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.44it/s]                                              {'loss': 0.6651, 'grad_norm': 0.1744552105665207, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.44it/s]                                              {'loss': 0.6477, 'grad_norm': 0.4439494013786316, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.44it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.6211, 'grad_norm': 0.490423321723938, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.6349, 'grad_norm': 0.33835527300834656, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.98it/s] 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6751, 'grad_norm': 0.26970618963241577, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6578, 'grad_norm': 0.22277098894119263, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.54it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.6178, 'grad_norm': 0.4987820088863373, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.6309, 'grad_norm': 0.4682634174823761, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.54it/s]                                               {'train_runtime': 1.0579, 'train_samples_per_second': 401.737, 'train_steps_per_second': 14.179, 'train_loss': 0.6447861870129903, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.54it/s]100%|██████████| 15/15 [00:01<00:00, 14.18it/s]
CLIENT:50
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6498, 'grad_norm': 0.4214470386505127, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.85it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.6496, 'grad_norm': 0.3883494734764099, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.6467, 'grad_norm': 0.3860175609588623, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.13it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.64, 'grad_norm': 0.5280304551124573, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.644, 'grad_norm': 0.3910124897956848, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.70it/s] 40%|████      | 6/15 [00:00<00:00, 16.28it/s]                                              {'loss': 0.657, 'grad_norm': 0.25578343868255615, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.28it/s]                                              {'loss': 0.609, 'grad_norm': 0.6183328628540039, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.28it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.665, 'grad_norm': 0.24681825935840607, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.78it/s]                                              {'loss': 0.6504, 'grad_norm': 0.3664410412311554, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.78it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.6387, 'grad_norm': 0.4795847237110138, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.36it/s]                                               {'loss': 0.6433, 'grad_norm': 0.37073326110839844, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.36it/s] 80%|████████  | 12/15 [00:00<00:00, 15.68it/s]                                               {'loss': 0.6505, 'grad_norm': 0.39491358399391174, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.68it/s]                                               {'loss': 0.6192, 'grad_norm': 0.6047958731651306, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.68it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6755, 'grad_norm': 0.18272610008716583, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.55it/s]                                               {'loss': 0.6224, 'grad_norm': 0.51386958360672, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.55it/s]                                               {'train_runtime': 1.0526, 'train_samples_per_second': 403.759, 'train_steps_per_second': 14.25, 'train_loss': 0.6440819462140401, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.55it/s]100%|██████████| 15/15 [00:01<00:00, 14.26it/s]
CLIENT:10
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6752, 'grad_norm': 0.3884804844856262, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.37it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.93it/s]                                              {'loss': 0.6871, 'grad_norm': 0.1934358775615692, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.93it/s]                                              {'loss': 0.6644, 'grad_norm': 0.33427780866622925, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.93it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.6718, 'grad_norm': 0.25433897972106934, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.6684, 'grad_norm': 0.25919291377067566, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.43it/s] 40%|████      | 6/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.6479, 'grad_norm': 0.4180432856082916, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.6612, 'grad_norm': 0.2785133421421051, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.33it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.6774, 'grad_norm': 0.27196869254112244, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.6458, 'grad_norm': 0.4610401690006256, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.18it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6637, 'grad_norm': 0.2660379409790039, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6375, 'grad_norm': 0.41232430934906006, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.6851, 'grad_norm': 0.18678408861160278, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.6543, 'grad_norm': 0.3851584792137146, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.97it/s]                                               {'loss': 0.6688, 'grad_norm': 0.22091470658779144, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.97it/s]                                               {'loss': 0.657, 'grad_norm': 0.4095705449581146, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.97it/s]                                               {'train_runtime': 1.0572, 'train_samples_per_second': 402.018, 'train_steps_per_second': 14.189, 'train_loss': 0.6643729527791341, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.97it/s]100%|██████████| 15/15 [00:01<00:00, 14.19it/s]
CLIENT:94
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6582, 'grad_norm': 0.2674129009246826, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.11it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.31it/s]                                              {'loss': 0.6613, 'grad_norm': 0.28990793228149414, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.31it/s]                                              {'loss': 0.6419, 'grad_norm': 0.5393968224525452, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.31it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.6623, 'grad_norm': 0.3707101047039032, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.649, 'grad_norm': 0.3797464072704315, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.58it/s] 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.6502, 'grad_norm': 0.3002258837223053, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.38it/s]                                              {'loss': 0.6227, 'grad_norm': 0.5571539998054504, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.38it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.6555, 'grad_norm': 0.36486053466796875, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.6955, 'grad_norm': 0.19933457672595978, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.18it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6529, 'grad_norm': 0.33358585834503174, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6537, 'grad_norm': 0.241399884223938, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.76it/s] 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6087, 'grad_norm': 0.5385052561759949, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.645, 'grad_norm': 0.32062196731567383, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.29it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.6486, 'grad_norm': 0.35660216212272644, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.6321, 'grad_norm': 0.45334264636039734, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]                                               {'train_runtime': 1.0748, 'train_samples_per_second': 395.407, 'train_steps_per_second': 13.956, 'train_loss': 0.6491676052411397, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]100%|██████████| 15/15 [00:01<00:00, 13.96it/s]
CLIENT:19
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6624, 'grad_norm': 0.31240975856781006, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.73it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.85it/s]                                              {'loss': 0.6661, 'grad_norm': 0.2846709191799164, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.85it/s]                                              {'loss': 0.6365, 'grad_norm': 0.5105088353157043, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.85it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.6547, 'grad_norm': 0.371660977602005, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.6467, 'grad_norm': 0.4903380870819092, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.55it/s] 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.67, 'grad_norm': 0.18686483800411224, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.6441, 'grad_norm': 0.44410982728004456, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.48it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6613, 'grad_norm': 0.3616221845149994, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6552, 'grad_norm': 0.2898622453212738, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.31it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.6461, 'grad_norm': 0.4789264500141144, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.19it/s]                                               {'loss': 0.6287, 'grad_norm': 0.42858025431632996, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.19it/s] 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.6735, 'grad_norm': 0.15901458263397217, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.6461, 'grad_norm': 0.3291751742362976, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.47it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.6318, 'grad_norm': 0.44175300002098083, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.47it/s]                                               {'loss': 0.6525, 'grad_norm': 0.38205403089523315, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.47it/s]                                               {'train_runtime': 1.0702, 'train_samples_per_second': 397.113, 'train_steps_per_second': 14.016, 'train_loss': 0.6516995628674825, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.47it/s]100%|██████████| 15/15 [00:01<00:00, 14.02it/s]
CLIENT:44
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6837, 'grad_norm': 0.11818446964025497, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.30it/s] 13%|█▎        | 2/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.6827, 'grad_norm': 0.11742407828569412, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.6862, 'grad_norm': 0.1455872654914856, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 15.31it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.92it/s]                                              {'loss': 0.6792, 'grad_norm': 0.12928061187267303, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.92it/s]                                              {'loss': 0.6827, 'grad_norm': 0.1051093265414238, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.92it/s] 40%|████      | 6/15 [00:00<00:00, 16.35it/s]                                              {'loss': 0.7126, 'grad_norm': 0.21317671239376068, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.35it/s]                                              {'loss': 0.6807, 'grad_norm': 0.30139750242233276, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.35it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.7292, 'grad_norm': 0.3464116156101227, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.6665, 'grad_norm': 0.2970919609069824, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.71it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.6785, 'grad_norm': 0.12776483595371246, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.6738, 'grad_norm': 0.18537254631519318, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.12it/s] 80%|████████  | 12/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.7485, 'grad_norm': 0.32078129053115845, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.7047, 'grad_norm': 0.3142474591732025, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.52it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6706, 'grad_norm': 0.19469672441482544, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6674, 'grad_norm': 0.2066689133644104, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.45it/s]                                               {'train_runtime': 1.05, 'train_samples_per_second': 404.769, 'train_steps_per_second': 14.286, 'train_loss': 0.6898061235745748, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 14.29it/s]
CLIENT:86
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6511, 'grad_norm': 0.36092087626457214, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.25it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.69it/s]                                              {'loss': 0.6696, 'grad_norm': 0.1959417313337326, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.69it/s]                                              {'loss': 0.6849, 'grad_norm': 0.27216339111328125, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.69it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.6688, 'grad_norm': 0.27137815952301025, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.91it/s]                                              {'loss': 0.6783, 'grad_norm': 0.2062656283378601, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.91it/s] 40%|████      | 6/15 [00:00<00:00, 15.77it/s]                                              {'loss': 0.6475, 'grad_norm': 0.27908915281295776, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.77it/s]                                              {'loss': 0.6776, 'grad_norm': 0.24497604370117188, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.77it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.46it/s]                                              {'loss': 0.6727, 'grad_norm': 0.13553722202777863, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.46it/s]                                              {'loss': 0.6292, 'grad_norm': 0.5246303677558899, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.46it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.01it/s]                                               {'loss': 0.6538, 'grad_norm': 0.31029775738716125, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.01it/s]                                               {'loss': 0.6429, 'grad_norm': 0.30994099378585815, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.01it/s] 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6871, 'grad_norm': 0.13740748167037964, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.663, 'grad_norm': 0.2668808102607727, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.54it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.6605, 'grad_norm': 0.2763283848762512, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.54it/s]                                               {'loss': 0.6676, 'grad_norm': 0.2541314661502838, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.54it/s]                                               {'train_runtime': 1.0609, 'train_samples_per_second': 400.599, 'train_steps_per_second': 14.139, 'train_loss': 0.6636404395103455, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.54it/s]100%|██████████| 15/15 [00:01<00:00, 14.14it/s]
CLIENT:23
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6525, 'grad_norm': 0.47383585572242737, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.87it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.62it/s]                                              {'loss': 0.6755, 'grad_norm': 0.20307180285453796, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.62it/s]                                              {'loss': 0.6673, 'grad_norm': 0.34197208285331726, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.62it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.26it/s]                                              {'loss': 0.6658, 'grad_norm': 0.24492329359054565, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.26it/s]                                              {'loss': 0.6472, 'grad_norm': 0.47520485520362854, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.26it/s] 40%|████      | 6/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6443, 'grad_norm': 0.3034069538116455, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6734, 'grad_norm': 0.17904633283615112, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.20it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.88it/s]                                              {'loss': 0.6527, 'grad_norm': 0.34861576557159424, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.88it/s]                                              {'loss': 0.6177, 'grad_norm': 0.6010130047798157, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.88it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.6811, 'grad_norm': 0.2315947562456131, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.6389, 'grad_norm': 0.42293551564216614, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.38it/s] 80%|████████  | 12/15 [00:00<00:00, 15.68it/s]                                               {'loss': 0.6374, 'grad_norm': 0.3242919147014618, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.68it/s]                                               {'loss': 0.6171, 'grad_norm': 0.5646628737449646, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.68it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.6836, 'grad_norm': 0.1461266577243805, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.61it/s]                                               {'loss': 0.6513, 'grad_norm': 0.3426104485988617, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.61it/s]                                               {'train_runtime': 1.0623, 'train_samples_per_second': 400.085, 'train_steps_per_second': 14.121, 'train_loss': 0.6537073135375977, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.61it/s]100%|██████████| 15/15 [00:01<00:00, 14.13it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 34.72it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.25it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.43it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.59it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.43it/s] 61%|██████    | 20/33 [00:00<00:00, 25.37it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.13it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.28it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.05it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.21it/s]100%|██████████| 33/33 [00:01<00:00, 26.03it/s]
{'eval_loss': 0.6569350361824036, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3091, 'eval_samples_per_second': 796.731, 'eval_steps_per_second': 25.208}
ROUND:25
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.653, 'grad_norm': 0.31732746958732605, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.89it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.10it/s]                                              {'loss': 0.6447, 'grad_norm': 0.4109700620174408, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.10it/s]                                              {'loss': 0.6379, 'grad_norm': 0.4643043279647827, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.10it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.67it/s]                                              {'loss': 0.6345, 'grad_norm': 0.40844300389289856, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.67it/s]                                              {'loss': 0.6678, 'grad_norm': 0.24635322391986847, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.67it/s] 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.6274, 'grad_norm': 0.4557715654373169, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.647, 'grad_norm': 0.35248079895973206, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.45it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6494, 'grad_norm': 0.3475980758666992, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6243, 'grad_norm': 0.4330533742904663, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.31it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6367, 'grad_norm': 0.4816104769706726, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.92it/s]                                               {'loss': 0.6451, 'grad_norm': 0.2541651427745819, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.92it/s] 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.6357, 'grad_norm': 0.44464796781539917, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.628, 'grad_norm': 0.4082487225532532, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.38it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.6469, 'grad_norm': 0.204642653465271, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.6385, 'grad_norm': 0.37948885560035706, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.99it/s]                                               {'train_runtime': 1.055, 'train_samples_per_second': 402.835, 'train_steps_per_second': 14.218, 'train_loss': 0.6411083022753398, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.99it/s]100%|██████████| 15/15 [00:01<00:00, 14.22it/s]
CLIENT:65
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6861, 'grad_norm': 0.191344752907753, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.90it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.65it/s]                                              {'loss': 0.6465, 'grad_norm': 0.39109957218170166, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.65it/s]                                              {'loss': 0.7058, 'grad_norm': 0.16710540652275085, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.65it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.6559, 'grad_norm': 0.19519956409931183, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.18it/s]                                              {'loss': 0.6595, 'grad_norm': 0.2795855700969696, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.18it/s] 40%|████      | 6/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.6757, 'grad_norm': 0.16721408069133759, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.676, 'grad_norm': 0.1990000307559967, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.14it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.6831, 'grad_norm': 0.14005827903747559, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.6413, 'grad_norm': 0.414599746465683, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.11it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6858, 'grad_norm': 0.14670048654079437, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6755, 'grad_norm': 0.19865480065345764, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.71it/s] 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.6478, 'grad_norm': 0.34319186210632324, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.26it/s]                                               {'loss': 0.6564, 'grad_norm': 0.2701322138309479, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.26it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.6713, 'grad_norm': 0.25206664204597473, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.6867, 'grad_norm': 0.17897628247737885, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]                                               {'train_runtime': 1.0838, 'train_samples_per_second': 392.149, 'train_steps_per_second': 13.841, 'train_loss': 0.6702353596687317, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]100%|██████████| 15/15 [00:01<00:00, 13.84it/s]
CLIENT:95
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.664, 'grad_norm': 0.14900794625282288, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.01it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.69it/s]                                              {'loss': 0.6364, 'grad_norm': 0.3531784117221832, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.69it/s]                                              {'loss': 0.6467, 'grad_norm': 0.3644011616706848, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.69it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.6351, 'grad_norm': 0.48362863063812256, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.6571, 'grad_norm': 0.16941732168197632, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.43it/s] 40%|████      | 6/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.6632, 'grad_norm': 0.19438916444778442, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.6522, 'grad_norm': 0.31651571393013, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.37it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.6532, 'grad_norm': 0.30013707280158997, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.6501, 'grad_norm': 0.22817540168762207, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.20it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.661, 'grad_norm': 0.15133455395698547, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6194, 'grad_norm': 0.38780200481414795, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6414, 'grad_norm': 0.3467637002468109, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6723, 'grad_norm': 0.15770302712917328, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.31it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.6685, 'grad_norm': 0.1770252138376236, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.5883, 'grad_norm': 0.7245746850967407, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]                                               {'train_runtime': 1.0802, 'train_samples_per_second': 393.434, 'train_steps_per_second': 13.886, 'train_loss': 0.6472513318061829, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]100%|██████████| 15/15 [00:01<00:00, 13.89it/s]
CLIENT:84
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6498, 'grad_norm': 0.3145174980163574, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.98it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.6715, 'grad_norm': 0.17028595507144928, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.667, 'grad_norm': 0.18304531276226044, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.14it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.6456, 'grad_norm': 0.36760836839675903, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.14it/s]                                              {'loss': 0.6798, 'grad_norm': 0.12516812980175018, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.14it/s] 40%|████      | 6/15 [00:00<00:00, 15.92it/s]                                              {'loss': 0.656, 'grad_norm': 0.3175112307071686, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.92it/s]                                              {'loss': 0.6817, 'grad_norm': 0.12441466003656387, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.92it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.54it/s]                                              {'loss': 0.6498, 'grad_norm': 0.2555534541606903, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.54it/s]                                              {'loss': 0.645, 'grad_norm': 0.3667799234390259, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.54it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.6747, 'grad_norm': 0.16922815144062042, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.6563, 'grad_norm': 0.16110263764858246, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.04it/s] 80%|████████  | 12/15 [00:00<00:00, 15.58it/s]                                               {'loss': 0.6383, 'grad_norm': 0.5579750537872314, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.58it/s]                                               {'loss': 0.6431, 'grad_norm': 0.2779802978038788, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.58it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.06it/s]                                               {'loss': 0.6488, 'grad_norm': 0.1864987462759018, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.06it/s]                                               {'loss': 0.6752, 'grad_norm': 0.2854650020599365, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.06it/s]                                               {'train_runtime': 1.0484, 'train_samples_per_second': 405.39, 'train_steps_per_second': 14.308, 'train_loss': 0.6588416576385498, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.06it/s]100%|██████████| 15/15 [00:01<00:00, 14.31it/s]
CLIENT:61
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6818, 'grad_norm': 0.11642239987850189, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.15it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.46it/s]                                              {'loss': 0.6714, 'grad_norm': 0.24845698475837708, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.46it/s]                                              {'loss': 0.6133, 'grad_norm': 0.6839914917945862, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.46it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.6432, 'grad_norm': 0.4254956841468811, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.6544, 'grad_norm': 0.33364337682724, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.55it/s] 40%|████      | 6/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.7144, 'grad_norm': 0.2963269352912903, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.44it/s]                                              {'loss': 0.6431, 'grad_norm': 0.3704819083213806, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.44it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.21it/s]                                              {'loss': 0.6483, 'grad_norm': 0.34666746854782104, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.21it/s]                                              {'loss': 0.7145, 'grad_norm': 0.18700017035007477, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.21it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.77it/s]                                               {'loss': 0.6396, 'grad_norm': 0.40488559007644653, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.77it/s]                                               {'loss': 0.694, 'grad_norm': 0.11993208527565002, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.77it/s] 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6307, 'grad_norm': 0.3820330798625946, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.6729, 'grad_norm': 0.1948883980512619, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.29it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.6454, 'grad_norm': 0.3206925094127655, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.6613, 'grad_norm': 0.36882179975509644, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]                                               {'train_runtime': 1.0765, 'train_samples_per_second': 394.789, 'train_steps_per_second': 13.934, 'train_loss': 0.6618818998336792, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]100%|██████████| 15/15 [00:01<00:00, 13.94it/s]
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6518, 'grad_norm': 0.374342143535614, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.93it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.70it/s]                                              {'loss': 0.653, 'grad_norm': 0.36898374557495117, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.70it/s]                                              {'loss': 0.663, 'grad_norm': 0.31518739461898804, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.70it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.07it/s]                                              {'loss': 0.6503, 'grad_norm': 0.30119627714157104, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.07it/s]                                              {'loss': 0.6488, 'grad_norm': 0.3607982397079468, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.07it/s] 40%|████      | 6/15 [00:00<00:00, 15.69it/s]                                              {'loss': 0.6549, 'grad_norm': 0.3355700373649597, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.69it/s]                                              {'loss': 0.677, 'grad_norm': 0.14723336696624756, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.69it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.6179, 'grad_norm': 0.4898572564125061, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.43it/s]                                              {'loss': 0.6327, 'grad_norm': 0.45880740880966187, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.43it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.6491, 'grad_norm': 0.3084549903869629, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.87it/s]                                               {'loss': 0.651, 'grad_norm': 0.3431849479675293, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.87it/s] 80%|████████  | 12/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.6245, 'grad_norm': 0.4776493012905121, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.52it/s]                                               {'loss': 0.6281, 'grad_norm': 0.429216206073761, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.52it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.6528, 'grad_norm': 0.24329833686351776, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.6502, 'grad_norm': 0.4269670844078064, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.72it/s]                                               {'train_runtime': 1.0642, 'train_samples_per_second': 399.37, 'train_steps_per_second': 14.095, 'train_loss': 0.6469978411992391, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.72it/s]100%|██████████| 15/15 [00:01<00:00, 14.10it/s]
CLIENT:30
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6799, 'grad_norm': 0.1352221518754959, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.12it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.84it/s]                                              {'loss': 0.6618, 'grad_norm': 0.32158327102661133, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.84it/s]                                              {'loss': 0.6377, 'grad_norm': 0.4307807683944702, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.84it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6552, 'grad_norm': 0.2954329550266266, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.31it/s]                                              {'loss': 0.6711, 'grad_norm': 0.22046774625778198, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.31it/s] 40%|████      | 6/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.6428, 'grad_norm': 0.3236326575279236, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.31it/s]                                              {'loss': 0.6686, 'grad_norm': 0.15565045177936554, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.31it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.89it/s]                                              {'loss': 0.6501, 'grad_norm': 0.3662373423576355, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.89it/s]                                              {'loss': 0.6528, 'grad_norm': 0.2799580991268158, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.89it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.6528, 'grad_norm': 0.27787482738494873, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.692, 'grad_norm': 0.163373664021492, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.60it/s] 80%|████████  | 12/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.6009, 'grad_norm': 0.6773601770401001, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.6494, 'grad_norm': 0.27582982182502747, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.11it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.30it/s]                                               {'loss': 0.6431, 'grad_norm': 0.3113665282726288, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.30it/s]                                               {'loss': 0.672, 'grad_norm': 0.16521933674812317, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.30it/s]                                               {'train_runtime': 1.0872, 'train_samples_per_second': 390.898, 'train_steps_per_second': 13.796, 'train_loss': 0.6553600033124288, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.30it/s]100%|██████████| 15/15 [00:01<00:00, 13.80it/s]
CLIENT:69
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6637, 'grad_norm': 0.19823487102985382, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.73it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.14it/s]                                              {'loss': 0.6458, 'grad_norm': 0.3251592814922333, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.14it/s]                                              {'loss': 0.6544, 'grad_norm': 0.32312098145484924, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.14it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.6623, 'grad_norm': 0.18945036828517914, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.23it/s]                                              {'loss': 0.6572, 'grad_norm': 0.31112754344940186, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.23it/s] 40%|████      | 6/15 [00:00<00:00, 16.14it/s]                                              {'loss': 0.6364, 'grad_norm': 0.532961905002594, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.14it/s]                                              {'loss': 0.6456, 'grad_norm': 0.26814699172973633, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.14it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.6598, 'grad_norm': 0.2116699069738388, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.6451, 'grad_norm': 0.32062017917633057, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.59it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.6472, 'grad_norm': 0.4089638888835907, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.04it/s]                                               {'loss': 0.6461, 'grad_norm': 0.25067734718322754, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.04it/s] 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.6716, 'grad_norm': 0.17458432912826538, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.6671, 'grad_norm': 0.18499381840229034, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.46it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.6622, 'grad_norm': 0.2190122753381729, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.6233, 'grad_norm': 0.49675339460372925, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.37it/s]                                               {'train_runtime': 1.0609, 'train_samples_per_second': 400.609, 'train_steps_per_second': 14.139, 'train_loss': 0.6525117476781209, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]100%|██████████| 15/15 [00:01<00:00, 14.14it/s]
CLIENT:35
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6457, 'grad_norm': 0.4495680332183838, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.01it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.29it/s]                                              {'loss': 0.6751, 'grad_norm': 0.22011545300483704, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.29it/s]                                              {'loss': 0.6697, 'grad_norm': 0.17992238700389862, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.29it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.6574, 'grad_norm': 0.3295598328113556, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.651, 'grad_norm': 0.2626855671405792, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.40it/s] 40%|████      | 6/15 [00:00<00:00, 15.25it/s]                                              {'loss': 0.6683, 'grad_norm': 0.18211571872234344, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.25it/s]                                              {'loss': 0.6696, 'grad_norm': 0.2097846120595932, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.25it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.6499, 'grad_norm': 0.33474189043045044, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.15it/s]                                              {'loss': 0.656, 'grad_norm': 0.26942571997642517, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.15it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.6518, 'grad_norm': 0.24241040647029877, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.6614, 'grad_norm': 0.149513840675354, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.03it/s] 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.6328, 'grad_norm': 0.5150647163391113, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.40it/s]                                               {'loss': 0.6616, 'grad_norm': 0.19064733386039734, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.40it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.6565, 'grad_norm': 0.2551456689834595, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.72it/s]                                               {'loss': 0.6375, 'grad_norm': 0.39108511805534363, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.72it/s]                                               {'train_runtime': 1.0652, 'train_samples_per_second': 398.977, 'train_steps_per_second': 14.082, 'train_loss': 0.6562845627466838, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.72it/s]100%|██████████| 15/15 [00:01<00:00, 14.09it/s]
CLIENT:1
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.7024, 'grad_norm': 0.2281305342912674, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.29it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.6661, 'grad_norm': 0.17055128514766693, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.6432, 'grad_norm': 0.44349202513694763, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.68it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6912, 'grad_norm': 0.13919536769390106, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6706, 'grad_norm': 0.1659347116947174, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.13it/s] 40%|████      | 6/15 [00:00<00:00, 15.73it/s]                                              {'loss': 0.6774, 'grad_norm': 0.23036569356918335, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.73it/s]                                              {'loss': 0.6692, 'grad_norm': 0.1593615859746933, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.73it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.6728, 'grad_norm': 0.19412626326084137, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.40it/s]                                              {'loss': 0.7165, 'grad_norm': 0.18520550429821014, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.40it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6917, 'grad_norm': 0.1436287760734558, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6751, 'grad_norm': 0.15112179517745972, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.94it/s] 80%|████████  | 12/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.6637, 'grad_norm': 0.18377386033535004, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.39it/s]                                               {'loss': 0.7114, 'grad_norm': 0.1504606455564499, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.39it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6869, 'grad_norm': 0.15436458587646484, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6505, 'grad_norm': 0.42838627099990845, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.38it/s]                                               {'train_runtime': 1.0684, 'train_samples_per_second': 397.78, 'train_steps_per_second': 14.039, 'train_loss': 0.6792426149050395, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.38it/s]100%|██████████| 15/15 [00:01<00:00, 14.04it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 33.63it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.90it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.29it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.54it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.57it/s] 61%|██████    | 20/33 [00:00<00:00, 25.22it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.26it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.18it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.07it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.25it/s]100%|██████████| 33/33 [00:01<00:00, 25.98it/s]
{'eval_loss': 0.6520239114761353, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.311, 'eval_samples_per_second': 795.582, 'eval_steps_per_second': 25.172}
ROUND:26
CLIENT:23
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6322, 'grad_norm': 0.4904833734035492, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.07it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.82it/s]                                              {'loss': 0.6687, 'grad_norm': 0.20069169998168945, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.82it/s]                                              {'loss': 0.653, 'grad_norm': 0.3776392340660095, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.82it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6571, 'grad_norm': 0.2212582528591156, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6272, 'grad_norm': 0.4860682189464569, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.32it/s] 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.633, 'grad_norm': 0.3070383071899414, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.6678, 'grad_norm': 0.1810707300901413, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.29it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.96it/s]                                              {'loss': 0.6387, 'grad_norm': 0.3484695553779602, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.96it/s]                                              {'loss': 0.593, 'grad_norm': 0.6079619526863098, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.96it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.6735, 'grad_norm': 0.2379092574119568, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.6224, 'grad_norm': 0.41659998893737793, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.69it/s] 80%|████████  | 12/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.6245, 'grad_norm': 0.3324996829032898, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.22it/s]                                               {'loss': 0.5934, 'grad_norm': 0.590414822101593, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.22it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.29it/s]                                               {'loss': 0.6821, 'grad_norm': 0.1571834832429886, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.29it/s]                                               {'loss': 0.6383, 'grad_norm': 0.33764883875846863, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.29it/s]                                               {'train_runtime': 1.0864, 'train_samples_per_second': 391.204, 'train_steps_per_second': 13.807, 'train_loss': 0.64033016761144, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.29it/s]100%|██████████| 15/15 [00:01<00:00, 13.81it/s]
CLIENT:81
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6312, 'grad_norm': 0.3926919400691986, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.16it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.99it/s]                                              {'loss': 0.6495, 'grad_norm': 0.2580586075782776, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.99it/s]                                              {'loss': 0.6011, 'grad_norm': 0.7530630230903625, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.99it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.6376, 'grad_norm': 0.4299643337726593, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.6229, 'grad_norm': 0.42126595973968506, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.63it/s] 40%|████      | 6/15 [00:00<00:00, 15.65it/s]                                              {'loss': 0.6326, 'grad_norm': 0.3739347755908966, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.65it/s]                                              {'loss': 0.6137, 'grad_norm': 0.5212668776512146, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.65it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.6247, 'grad_norm': 0.44737541675567627, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.35it/s]                                              {'loss': 0.642, 'grad_norm': 0.30117952823638916, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.35it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.6567, 'grad_norm': 0.24347425997257233, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.5895, 'grad_norm': 0.6379956603050232, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.18it/s] 80%|████████  | 12/15 [00:00<00:00, 15.80it/s]                                               {'loss': 0.6271, 'grad_norm': 0.3690553903579712, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.80it/s]                                               {'loss': 0.6436, 'grad_norm': 0.31345421075820923, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.80it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.6204, 'grad_norm': 0.38670557737350464, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.5814, 'grad_norm': 0.6019255518913269, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.65it/s]                                               {'train_runtime': 1.0618, 'train_samples_per_second': 400.275, 'train_steps_per_second': 14.127, 'train_loss': 0.6249226450920105, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.65it/s]100%|██████████| 15/15 [00:01<00:00, 14.13it/s]
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6363, 'grad_norm': 0.3537233769893646, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.31it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.75it/s]                                              {'loss': 0.6838, 'grad_norm': 0.15896441042423248, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.75it/s]                                              {'loss': 0.6121, 'grad_norm': 0.5489078164100647, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.75it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.85it/s]                                              {'loss': 0.6333, 'grad_norm': 0.40778255462646484, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.85it/s]                                              {'loss': 0.6597, 'grad_norm': 0.18241368234157562, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.85it/s] 40%|████      | 6/15 [00:00<00:00, 16.27it/s]                                              {'loss': 0.6526, 'grad_norm': 0.21517598628997803, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.27it/s]                                              {'loss': 0.6559, 'grad_norm': 0.16438932716846466, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.27it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.6219, 'grad_norm': 0.43913277983665466, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.68it/s]                                              {'loss': 0.6599, 'grad_norm': 0.24696770310401917, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.68it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.6506, 'grad_norm': 0.2135157585144043, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.12it/s]                                               {'loss': 0.6385, 'grad_norm': 0.2706151604652405, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.12it/s] 80%|████████  | 12/15 [00:00<00:00, 16.02it/s]                                               {'loss': 0.6336, 'grad_norm': 0.35475581884384155, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 16.02it/s]                                               {'loss': 0.6356, 'grad_norm': 0.3416520953178406, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 16.02it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6541, 'grad_norm': 0.2677859961986542, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6485, 'grad_norm': 0.261791467666626, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.76it/s]                                               {'train_runtime': 1.0353, 'train_samples_per_second': 410.496, 'train_steps_per_second': 14.488, 'train_loss': 0.6450916687647502, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.76it/s]100%|██████████| 15/15 [00:01<00:00, 14.49it/s]
CLIENT:34
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6923, 'grad_norm': 0.14833731949329376, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.11it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.66, 'grad_norm': 0.29415369033813477, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6311, 'grad_norm': 0.47638195753097534, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.22it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6743, 'grad_norm': 0.1512359231710434, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6181, 'grad_norm': 0.5814973711967468, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.13it/s] 40%|████      | 6/15 [00:00<00:00, 15.94it/s]                                              {'loss': 0.6979, 'grad_norm': 0.21346120536327362, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.94it/s]                                              {'loss': 0.6239, 'grad_norm': 0.5002864003181458, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.94it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.6607, 'grad_norm': 0.26371270418167114, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.53it/s]                                              {'loss': 0.7123, 'grad_norm': 0.2997312545776367, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.53it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.6486, 'grad_norm': 0.2770368754863739, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.03it/s]                                               {'loss': 0.6606, 'grad_norm': 0.21077510714530945, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.03it/s] 80%|████████  | 12/15 [00:00<00:00, 15.60it/s]                                               {'loss': 0.6661, 'grad_norm': 0.23841743171215057, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.60it/s]                                               {'loss': 0.6574, 'grad_norm': 0.20919574797153473, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.60it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.6561, 'grad_norm': 0.2592242956161499, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.6479, 'grad_norm': 0.27045589685440063, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.56it/s]                                               {'train_runtime': 1.0558, 'train_samples_per_second': 402.533, 'train_steps_per_second': 14.207, 'train_loss': 0.660481862227122, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.56it/s]100%|██████████| 15/15 [00:01<00:00, 14.21it/s]
CLIENT:62
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6313, 'grad_norm': 0.4242366552352905, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.02it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.66it/s]                                              {'loss': 0.6474, 'grad_norm': 0.3185200095176697, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.66it/s]                                              {'loss': 0.5864, 'grad_norm': 0.7733263373374939, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.66it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.6125, 'grad_norm': 0.5739554762840271, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.6498, 'grad_norm': 0.258093923330307, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.27it/s] 40%|████      | 6/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.5783, 'grad_norm': 0.673258900642395, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.20it/s]                                              {'loss': 0.6133, 'grad_norm': 0.4584074318408966, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.20it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.6074, 'grad_norm': 0.5615275502204895, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.6165, 'grad_norm': 0.3890557289123535, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.16it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6312, 'grad_norm': 0.4502864181995392, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.79it/s]                                               {'loss': 0.6112, 'grad_norm': 0.4485148787498474, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.79it/s] 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.5817, 'grad_norm': 0.6402610540390015, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.29it/s]                                               {'loss': 0.615, 'grad_norm': 0.4529366195201874, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.29it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.6273, 'grad_norm': 0.4134283661842346, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.5794, 'grad_norm': 0.7977261543273926, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.52it/s]                                               {'train_runtime': 1.0788, 'train_samples_per_second': 393.974, 'train_steps_per_second': 13.905, 'train_loss': 0.6125726501146952, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.52it/s]100%|██████████| 15/15 [00:01<00:00, 13.91it/s]
CLIENT:46
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6638, 'grad_norm': 0.18758781254291534, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.91it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.25it/s]                                              {'loss': 0.662, 'grad_norm': 0.19633731245994568, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.25it/s]                                              {'loss': 0.6445, 'grad_norm': 0.48424917459487915, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.25it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.7052, 'grad_norm': 0.14147037267684937, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.34it/s]                                              {'loss': 0.6199, 'grad_norm': 0.46854352951049805, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.34it/s] 40%|████      | 6/15 [00:00<00:00, 16.30it/s]                                              {'loss': 0.6432, 'grad_norm': 0.3385290503501892, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.30it/s]                                              {'loss': 0.6431, 'grad_norm': 0.3424309194087982, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.30it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.6397, 'grad_norm': 0.303376168012619, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.76it/s]                                              {'loss': 0.6886, 'grad_norm': 0.1522294133901596, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.76it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.6674, 'grad_norm': 0.32611772418022156, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.18it/s]                                               {'loss': 0.6666, 'grad_norm': 0.14089705049991608, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.18it/s] 80%|████████  | 12/15 [00:00<00:00, 15.61it/s]                                               {'loss': 0.6136, 'grad_norm': 0.4345420002937317, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.61it/s]                                               {'loss': 0.6442, 'grad_norm': 0.2701621353626251, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.61it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.6325, 'grad_norm': 0.43285253643989563, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.65it/s]                                               {'loss': 0.7114, 'grad_norm': 0.21521931886672974, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.65it/s]                                               {'train_runtime': 1.0522, 'train_samples_per_second': 403.916, 'train_steps_per_second': 14.256, 'train_loss': 0.6563822468121846, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.65it/s]100%|██████████| 15/15 [00:01<00:00, 14.26it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6355, 'grad_norm': 0.2605828046798706, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.09it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.74it/s]                                              {'loss': 0.6177, 'grad_norm': 0.5822290182113647, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.74it/s]                                              {'loss': 0.6983, 'grad_norm': 0.1964208334684372, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.74it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6413, 'grad_norm': 0.22228221595287323, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6433, 'grad_norm': 0.3308604955673218, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.23it/s] 40%|████      | 6/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.631, 'grad_norm': 0.4217435121536255, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6227, 'grad_norm': 0.4136755168437958, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.13it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6363, 'grad_norm': 0.33071601390838623, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6696, 'grad_norm': 0.16414298117160797, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.10it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.663, 'grad_norm': 0.21476277709007263, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6214, 'grad_norm': 0.38581299781799316, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.71it/s] 80%|████████  | 12/15 [00:00<00:00, 15.68it/s]                                               {'loss': 0.6447, 'grad_norm': 0.3220180571079254, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.68it/s]                                               {'loss': 0.6394, 'grad_norm': 0.2460232526063919, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.68it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.634, 'grad_norm': 0.3821478486061096, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6329, 'grad_norm': 0.2608817517757416, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]                                               {'train_runtime': 1.0788, 'train_samples_per_second': 393.951, 'train_steps_per_second': 13.904, 'train_loss': 0.6420677900314331, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 13.91it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6669, 'grad_norm': 0.19258059561252594, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.70it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.81it/s]                                              {'loss': 0.6313, 'grad_norm': 0.4136945307254791, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.81it/s]                                              {'loss': 0.6303, 'grad_norm': 0.38845980167388916, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.81it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.6786, 'grad_norm': 0.15009771287441254, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.6011, 'grad_norm': 0.6535326242446899, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.09it/s] 40%|████      | 6/15 [00:00<00:00, 15.69it/s]                                              {'loss': 0.6597, 'grad_norm': 0.22826607525348663, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.69it/s]                                              {'loss': 0.6073, 'grad_norm': 0.4890386462211609, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.69it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.6581, 'grad_norm': 0.23450152575969696, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.36it/s]                                              {'loss': 0.6405, 'grad_norm': 0.3303240239620209, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.36it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6521, 'grad_norm': 0.3556079864501953, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6139, 'grad_norm': 0.36539241671562195, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.94it/s] 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6474, 'grad_norm': 0.28517478704452515, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.43it/s]                                               {'loss': 0.6387, 'grad_norm': 0.33999231457710266, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.43it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.634, 'grad_norm': 0.3789491057395935, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6311, 'grad_norm': 0.30532515048980713, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.45it/s]                                               {'train_runtime': 1.0702, 'train_samples_per_second': 397.127, 'train_steps_per_second': 14.016, 'train_loss': 0.6394151449203491, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 14.02it/s]
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6319, 'grad_norm': 0.36207011342048645, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.12it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.58it/s]                                              {'loss': 0.6368, 'grad_norm': 0.33400413393974304, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.58it/s]                                              {'loss': 0.6416, 'grad_norm': 0.3152233958244324, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.58it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.6251, 'grad_norm': 0.531238853931427, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.60it/s]                                              {'loss': 0.6552, 'grad_norm': 0.23911364376544952, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.60it/s] 40%|████      | 6/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.6216, 'grad_norm': 0.3659572899341583, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.26it/s]                                              {'loss': 0.6222, 'grad_norm': 0.41356027126312256, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.26it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.07it/s]                                              {'loss': 0.6007, 'grad_norm': 0.5376935005187988, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.07it/s]                                              {'loss': 0.6888, 'grad_norm': 0.17362312972545624, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.07it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.6491, 'grad_norm': 0.2745153307914734, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.49it/s]                                               {'loss': 0.6475, 'grad_norm': 0.25267818570137024, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.49it/s] 80%|████████  | 12/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.5932, 'grad_norm': 0.6445697546005249, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.10it/s]                                               {'loss': 0.64, 'grad_norm': 0.3034110367298126, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.10it/s] 93%|█████████▎| 14/15 [00:00<00:00, 13.94it/s]                                               {'loss': 0.6339, 'grad_norm': 0.37480464577674866, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 13.94it/s]                                               {'loss': 0.6103, 'grad_norm': 0.5099403858184814, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.94it/s]                                               {'train_runtime': 1.1022, 'train_samples_per_second': 385.603, 'train_steps_per_second': 13.61, 'train_loss': 0.6331891139348348, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 13.94it/s]100%|██████████| 15/15 [00:01<00:00, 13.61it/s]
CLIENT:11
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6522, 'grad_norm': 0.261889785528183, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.31it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.91it/s]                                              {'loss': 0.6287, 'grad_norm': 0.49070432782173157, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.91it/s]                                              {'loss': 0.6874, 'grad_norm': 0.15489013493061066, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.91it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.46it/s]                                              {'loss': 0.6356, 'grad_norm': 0.43714413046836853, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.46it/s]                                              {'loss': 0.6584, 'grad_norm': 0.3696279525756836, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.46it/s] 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.677, 'grad_norm': 0.27258163690567017, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.65, 'grad_norm': 0.3240077495574951, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.48it/s] 53%|█████▎    | 8/15 [00:00<00:00, 13.85it/s]                                              {'loss': 0.6543, 'grad_norm': 0.33398693799972534, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 13.85it/s]                                              {'loss': 0.667, 'grad_norm': 0.23424389958381653, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 13.85it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6281, 'grad_norm': 0.41176843643188477, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.38it/s]                                               {'loss': 0.6687, 'grad_norm': 0.1835382729768753, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.38it/s] 80%|████████  | 12/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.6559, 'grad_norm': 0.34602856636047363, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.6446, 'grad_norm': 0.29654520750045776, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.11it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.10it/s]                                               {'loss': 0.6791, 'grad_norm': 0.15254901349544525, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.10it/s]                                               {'loss': 0.613, 'grad_norm': 0.5734111666679382, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.10it/s]                                               {'train_runtime': 1.1013, 'train_samples_per_second': 385.9, 'train_steps_per_second': 13.62, 'train_loss': 0.6533337036768595, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.10it/s]100%|██████████| 15/15 [00:01<00:00, 13.63it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 36.58it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.02it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.36it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.52it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.47it/s] 61%|██████    | 20/33 [00:00<00:00, 25.30it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.13it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.25it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.05it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.23it/s]100%|██████████| 33/33 [00:01<00:00, 26.05it/s]
{'eval_loss': 0.6458919048309326, 'eval_model_preparation_time': 0.0026, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3121, 'eval_samples_per_second': 794.896, 'eval_steps_per_second': 25.15}
ROUND:27
CLIENT:65
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6798, 'grad_norm': 0.19717255234718323, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.66it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.86it/s]                                              {'loss': 0.6299, 'grad_norm': 0.40565767884254456, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.86it/s]                                              {'loss': 0.7053, 'grad_norm': 0.19702905416488647, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.86it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.6505, 'grad_norm': 0.1836988478899002, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.56it/s]                                              {'loss': 0.6491, 'grad_norm': 0.2719650864601135, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.56it/s] 40%|████      | 6/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.6736, 'grad_norm': 0.17104791104793549, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.6705, 'grad_norm': 0.18213559687137604, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.37it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.79it/s]                                              {'loss': 0.6819, 'grad_norm': 0.16233909130096436, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.79it/s]                                              {'loss': 0.6251, 'grad_norm': 0.4296511709690094, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.79it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.97it/s]                                               {'loss': 0.682, 'grad_norm': 0.14652326703071594, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.97it/s]                                               {'loss': 0.6696, 'grad_norm': 0.1993030309677124, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.97it/s] 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.6353, 'grad_norm': 0.3308109939098358, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.6465, 'grad_norm': 0.26426950097084045, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.46it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6628, 'grad_norm': 0.2685929238796234, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.48it/s]                                               {'loss': 0.6836, 'grad_norm': 0.1812731772661209, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.48it/s]                                               {'train_runtime': 1.0694, 'train_samples_per_second': 397.404, 'train_steps_per_second': 14.026, 'train_loss': 0.6630418221155803, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.48it/s]100%|██████████| 15/15 [00:01<00:00, 14.03it/s]
CLIENT:21
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6215, 'grad_norm': 0.35751211643218994, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.74it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.88it/s]                                              {'loss': 0.6692, 'grad_norm': 0.21187806129455566, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.88it/s]                                              {'loss': 0.609, 'grad_norm': 0.5261306762695312, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.88it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.52it/s]                                              {'loss': 0.6388, 'grad_norm': 0.32695645093917847, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.52it/s]                                              {'loss': 0.6315, 'grad_norm': 0.43340975046157837, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.52it/s] 40%|████      | 6/15 [00:00<00:00, 15.49it/s]                                              {'loss': 0.6546, 'grad_norm': 0.23671957850456238, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.49it/s]                                              {'loss': 0.5975, 'grad_norm': 0.6430246233940125, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.49it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.6806, 'grad_norm': 0.12977145612239838, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.29it/s]                                              {'loss': 0.6138, 'grad_norm': 0.4428212344646454, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.29it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.6496, 'grad_norm': 0.24897019565105438, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.83it/s]                                               {'loss': 0.6401, 'grad_norm': 0.3262862265110016, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.83it/s] 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.6167, 'grad_norm': 0.4351649880409241, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.6232, 'grad_norm': 0.37886103987693787, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.49it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.6471, 'grad_norm': 0.20612235367298126, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.6167, 'grad_norm': 0.44052818417549133, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.69it/s]                                               {'train_runtime': 1.0677, 'train_samples_per_second': 398.046, 'train_steps_per_second': 14.049, 'train_loss': 0.6339928468068441, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.69it/s]100%|██████████| 15/15 [00:01<00:00, 14.05it/s]
CLIENT:87
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6732, 'grad_norm': 0.19083422422409058, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.37it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.82it/s]                                              {'loss': 0.6413, 'grad_norm': 0.27819588780403137, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.82it/s]                                              {'loss': 0.7106, 'grad_norm': 0.24148745834827423, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.82it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.6744, 'grad_norm': 0.14854513108730316, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.09it/s]                                              {'loss': 0.6901, 'grad_norm': 0.13827437162399292, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.09it/s] 40%|████      | 6/15 [00:00<00:00, 15.82it/s]                                              {'loss': 0.6393, 'grad_norm': 0.3322656750679016, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.82it/s]                                              {'loss': 0.7152, 'grad_norm': 0.187082439661026, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.82it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.648, 'grad_norm': 0.3070794939994812, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.25it/s]                                              {'loss': 0.6696, 'grad_norm': 0.1727191060781479, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.25it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.6604, 'grad_norm': 0.20105183124542236, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.7048, 'grad_norm': 0.19327868521213531, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.85it/s] 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.6485, 'grad_norm': 0.28940433263778687, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.38it/s]                                               {'loss': 0.7016, 'grad_norm': 0.14772218465805054, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.38it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.6989, 'grad_norm': 0.23665188252925873, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.39it/s]                                               {'loss': 0.5736, 'grad_norm': 0.7461560964584351, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.39it/s]                                               {'train_runtime': 1.0697, 'train_samples_per_second': 397.317, 'train_steps_per_second': 14.023, 'train_loss': 0.6699630618095398, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.39it/s]100%|██████████| 15/15 [00:01<00:00, 14.03it/s]
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6403, 'grad_norm': 0.33684206008911133, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.07it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.31it/s]                                              {'loss': 0.6269, 'grad_norm': 0.44064706563949585, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.31it/s]                                              {'loss': 0.6195, 'grad_norm': 0.4235702157020569, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.31it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.6171, 'grad_norm': 0.4288393557071686, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.6591, 'grad_norm': 0.23722155392169952, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.35it/s] 40%|████      | 6/15 [00:00<00:00, 16.27it/s]                                              {'loss': 0.6086, 'grad_norm': 0.45835646986961365, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.27it/s]                                              {'loss': 0.6329, 'grad_norm': 0.33335983753204346, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.27it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.80it/s]                                              {'loss': 0.6355, 'grad_norm': 0.3452237546443939, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.80it/s]                                              {'loss': 0.6069, 'grad_norm': 0.4391889274120331, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.80it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.6186, 'grad_norm': 0.4463583827018738, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.23it/s]                                               {'loss': 0.6357, 'grad_norm': 0.2591592073440552, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.23it/s] 80%|████████  | 12/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.6179, 'grad_norm': 0.4261198937892914, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.66it/s]                                               {'loss': 0.6116, 'grad_norm': 0.39237862825393677, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.66it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.6409, 'grad_norm': 0.1947598159313202, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.62it/s]                                               {'loss': 0.6237, 'grad_norm': 0.39060840010643005, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.62it/s]                                               {'train_runtime': 1.0493, 'train_samples_per_second': 405.017, 'train_steps_per_second': 14.295, 'train_loss': 0.6263443509737651, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.62it/s]100%|██████████| 15/15 [00:01<00:00, 14.30it/s]
CLIENT:46
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6613, 'grad_norm': 0.19021320343017578, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.00it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.70it/s]                                              {'loss': 0.6581, 'grad_norm': 0.20220540463924408, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.70it/s]                                              {'loss': 0.6334, 'grad_norm': 0.4699721336364746, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.70it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.7074, 'grad_norm': 0.16949769854545593, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.609, 'grad_norm': 0.4680023193359375, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.14it/s] 40%|████      | 6/15 [00:00<00:00, 15.08it/s]                                              {'loss': 0.6357, 'grad_norm': 0.35377225279808044, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.08it/s]                                              {'loss': 0.6356, 'grad_norm': 0.3272760212421417, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.08it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.05it/s]                                              {'loss': 0.6331, 'grad_norm': 0.30138978362083435, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.05it/s]                                              {'loss': 0.6898, 'grad_norm': 0.16937582194805145, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.05it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.6605, 'grad_norm': 0.34712639451026917, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.67it/s]                                               {'loss': 0.6653, 'grad_norm': 0.14361140131950378, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.67it/s] 80%|████████  | 12/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.6041, 'grad_norm': 0.42460721731185913, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.6385, 'grad_norm': 0.27069568634033203, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.21it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.6228, 'grad_norm': 0.4442440867424011, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.7146, 'grad_norm': 0.23372305929660797, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]                                               {'train_runtime': 1.0862, 'train_samples_per_second': 391.258, 'train_steps_per_second': 13.809, 'train_loss': 0.6512844284375509, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]100%|██████████| 15/15 [00:01<00:00, 13.81it/s]
CLIENT:74
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6809, 'grad_norm': 0.13326305150985718, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.28it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.87it/s]                                              {'loss': 0.6259, 'grad_norm': 0.3876308500766754, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.87it/s]                                              {'loss': 0.6133, 'grad_norm': 0.504119873046875, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.87it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6277, 'grad_norm': 0.44407960772514343, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6802, 'grad_norm': 0.14661338925361633, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.23it/s] 40%|████      | 6/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.593, 'grad_norm': 0.617655873298645, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6545, 'grad_norm': 0.1916261911392212, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.13it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6021, 'grad_norm': 0.4811623692512512, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.10it/s]                                              {'loss': 0.6655, 'grad_norm': 0.21938461065292358, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.10it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.6401, 'grad_norm': 0.23475946485996246, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.70it/s]                                               {'loss': 0.6186, 'grad_norm': 0.38141533732414246, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.70it/s] 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6752, 'grad_norm': 0.244005024433136, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6901, 'grad_norm': 0.13792309165000916, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.32it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.5949, 'grad_norm': 0.5265747904777527, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.85it/s]                                               {'loss': 0.6283, 'grad_norm': 0.2804524302482605, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.85it/s]                                               {'train_runtime': 1.0691, 'train_samples_per_second': 397.517, 'train_steps_per_second': 14.03, 'train_loss': 0.6393555323282878, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.85it/s]100%|██████████| 15/15 [00:01<00:00, 14.03it/s]
CLIENT:44
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6819, 'grad_norm': 0.1281125247478485, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.86it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.6874, 'grad_norm': 0.16994284093379974, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.23it/s]                                              {'loss': 0.6834, 'grad_norm': 0.15263010561466217, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.23it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.6751, 'grad_norm': 0.1376936435699463, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.34it/s]                                              {'loss': 0.6841, 'grad_norm': 0.15066881477832794, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.34it/s] 40%|████      | 6/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.724, 'grad_norm': 0.30598974227905273, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.24it/s]                                              {'loss': 0.6632, 'grad_norm': 0.3065345585346222, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.24it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.7531, 'grad_norm': 0.44786956906318665, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.20it/s]                                              {'loss': 0.6488, 'grad_norm': 0.31854790449142456, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.20it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.6752, 'grad_norm': 0.13238638639450073, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.6651, 'grad_norm': 0.17846696078777313, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.84it/s] 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.7692, 'grad_norm': 0.4338715970516205, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.725, 'grad_norm': 0.40938135981559753, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.31it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.6629, 'grad_norm': 0.17009027302265167, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.36it/s]                                               {'loss': 0.6573, 'grad_norm': 0.23127196729183197, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]                                               {'train_runtime': 1.0746, 'train_samples_per_second': 395.485, 'train_steps_per_second': 13.958, 'train_loss': 0.6903755187988281, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.36it/s]100%|██████████| 15/15 [00:01<00:00, 13.96it/s]
CLIENT:60
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.651, 'grad_norm': 0.20091216266155243, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.13it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.39it/s]                                              {'loss': 0.6614, 'grad_norm': 0.1552559733390808, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.39it/s]                                              {'loss': 0.6585, 'grad_norm': 0.19688978791236877, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.39it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.6448, 'grad_norm': 0.3065163791179657, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.6771, 'grad_norm': 0.13135144114494324, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.37it/s] 40%|████      | 6/15 [00:00<00:00, 16.23it/s]                                              {'loss': 0.6863, 'grad_norm': 0.3144659996032715, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.23it/s]                                              {'loss': 0.677, 'grad_norm': 0.1494121551513672, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.23it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.69it/s]                                              {'loss': 0.6484, 'grad_norm': 0.23387446999549866, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.69it/s]                                              {'loss': 0.6569, 'grad_norm': 0.21404704451560974, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.69it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.6133, 'grad_norm': 0.47070425748825073, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.15it/s]                                               {'loss': 0.6873, 'grad_norm': 0.22759146988391876, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.15it/s] 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6557, 'grad_norm': 0.1760132908821106, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.7128, 'grad_norm': 0.2493581622838974, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.54it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.6452, 'grad_norm': 0.23915953934192657, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.56it/s]                                               {'loss': 0.5915, 'grad_norm': 0.5222118496894836, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.56it/s]                                               {'train_runtime': 1.0516, 'train_samples_per_second': 404.129, 'train_steps_per_second': 14.263, 'train_loss': 0.6578093687693278, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.56it/s]100%|██████████| 15/15 [00:01<00:00, 14.27it/s]
CLIENT:72
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6727, 'grad_norm': 0.2543933093547821, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.12it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.34it/s]                                              {'loss': 0.6142, 'grad_norm': 0.4963741600513458, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.34it/s]                                              {'loss': 0.5976, 'grad_norm': 0.5420917272567749, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.34it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.6286, 'grad_norm': 0.43173977732658386, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.37it/s]                                              {'loss': 0.6031, 'grad_norm': 0.5105997323989868, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.37it/s] 40%|████      | 6/15 [00:00<00:00, 16.24it/s]                                              {'loss': 0.6858, 'grad_norm': 0.20188960433006287, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.24it/s]                                              {'loss': 0.6913, 'grad_norm': 0.17283353209495544, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.24it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.6064, 'grad_norm': 0.49217668175697327, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.5696, 'grad_norm': 0.7335930466651917, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.64it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6193, 'grad_norm': 0.4312905967235565, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6299, 'grad_norm': 0.39800316095352173, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.13it/s] 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6354, 'grad_norm': 0.3118109107017517, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.54it/s]                                               {'loss': 0.6054, 'grad_norm': 0.4117845892906189, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.54it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.642, 'grad_norm': 0.2950064241886139, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.60it/s]                                               {'loss': 0.6305, 'grad_norm': 0.2882247567176819, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.60it/s]                                               {'train_runtime': 1.0517, 'train_samples_per_second': 404.109, 'train_steps_per_second': 14.263, 'train_loss': 0.6287866473197937, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.60it/s]100%|██████████| 15/15 [00:01<00:00, 14.27it/s]
CLIENT:5
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6227, 'grad_norm': 0.4022558033466339, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.02it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.6623, 'grad_norm': 0.2310446947813034, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.33it/s]                                              {'loss': 0.7021, 'grad_norm': 0.2256794571876526, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.33it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.6877, 'grad_norm': 0.15694674849510193, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.36it/s]                                              {'loss': 0.6197, 'grad_norm': 0.5194100737571716, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.36it/s] 40%|████      | 6/15 [00:00<00:00, 16.45it/s]                                              {'loss': 0.6524, 'grad_norm': 0.2504862844944, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.45it/s]                                              {'loss': 0.6412, 'grad_norm': 0.20077447593212128, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.45it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.6693, 'grad_norm': 0.1621512472629547, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.6107, 'grad_norm': 0.43322688341140747, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.57it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.09it/s]                                               {'loss': 0.6709, 'grad_norm': 0.15145528316497803, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.09it/s]                                               {'loss': 0.64, 'grad_norm': 0.20505419373512268, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.09it/s] 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.6204, 'grad_norm': 0.384884238243103, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.49it/s]                                               {'loss': 0.6709, 'grad_norm': 0.15787231922149658, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.49it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.6533, 'grad_norm': 0.25795140862464905, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.6339, 'grad_norm': 0.4260651171207428, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.57it/s]                                               {'train_runtime': 1.0531, 'train_samples_per_second': 403.561, 'train_steps_per_second': 14.243, 'train_loss': 0.6504976272583007, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.57it/s]100%|██████████| 15/15 [00:01<00:00, 14.25it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 34.85it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.27it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.64it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.81it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.37it/s] 61%|██████    | 20/33 [00:00<00:00, 25.42it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.14it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.20it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.15it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.12it/s]100%|██████████| 33/33 [00:01<00:00, 26.04it/s]
{'eval_loss': 0.6420372724533081, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3087, 'eval_samples_per_second': 796.958, 'eval_steps_per_second': 25.215}
ROUND:28
CLIENT:67
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6021, 'grad_norm': 0.4104951322078705, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.91it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.89it/s]                                              {'loss': 0.5883, 'grad_norm': 0.5735135078430176, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.89it/s]                                              {'loss': 0.6292, 'grad_norm': 0.2962563931941986, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.89it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.47it/s]                                              {'loss': 0.5867, 'grad_norm': 0.5779911279678345, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.47it/s]                                              {'loss': 0.5906, 'grad_norm': 0.49108606576919556, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.47it/s] 40%|████      | 6/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.6451, 'grad_norm': 0.22786474227905273, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.53it/s]                                              {'loss': 0.6359, 'grad_norm': 0.2753669321537018, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.53it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.578, 'grad_norm': 0.5934600234031677, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.55it/s]                                              {'loss': 0.5751, 'grad_norm': 0.5589065551757812, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.55it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.00it/s]                                               {'loss': 0.5855, 'grad_norm': 0.4697892963886261, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.00it/s]                                               {'loss': 0.5946, 'grad_norm': 0.4176194965839386, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.00it/s] 80%|████████  | 12/15 [00:00<00:00, 15.61it/s]                                               {'loss': 0.5931, 'grad_norm': 0.5234808325767517, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.61it/s]                                               {'loss': 0.5989, 'grad_norm': 0.4012305438518524, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.61it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.5939, 'grad_norm': 0.40950700640678406, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.5664, 'grad_norm': 0.5522181987762451, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.58it/s]                                               {'train_runtime': 1.0647, 'train_samples_per_second': 399.188, 'train_steps_per_second': 14.089, 'train_loss': 0.597567621866862, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.58it/s]100%|██████████| 15/15 [00:01<00:00, 14.09it/s]
CLIENT:14
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6731, 'grad_norm': 0.1352098435163498, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.59it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.08it/s]                                              {'loss': 0.6464, 'grad_norm': 0.2587721645832062, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.08it/s]                                              {'loss': 0.6388, 'grad_norm': 0.3317956328392029, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.08it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6582, 'grad_norm': 0.2349173128604889, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6792, 'grad_norm': 0.14911194145679474, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.32it/s] 40%|████      | 6/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.6127, 'grad_norm': 0.3956266939640045, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.21it/s]                                              {'loss': 0.6399, 'grad_norm': 0.22271336615085602, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.21it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.6462, 'grad_norm': 0.22983457148075104, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.11it/s]                                              {'loss': 0.6679, 'grad_norm': 0.18147142231464386, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.11it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6697, 'grad_norm': 0.14544394612312317, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.71it/s]                                               {'loss': 0.6247, 'grad_norm': 0.25864294171333313, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.71it/s] 80%|████████  | 12/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.6358, 'grad_norm': 0.18623898923397064, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.25it/s]                                               {'loss': 0.6796, 'grad_norm': 0.14891935884952545, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.25it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.6373, 'grad_norm': 0.1942780762910843, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.5953, 'grad_norm': 0.4949761927127838, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]                                               {'train_runtime': 1.0821, 'train_samples_per_second': 392.738, 'train_steps_per_second': 13.861, 'train_loss': 0.6469799478848776, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]100%|██████████| 15/15 [00:01<00:00, 13.87it/s]
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6002, 'grad_norm': 0.4449506402015686, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.65it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.85it/s]                                              {'loss': 0.6076, 'grad_norm': 0.4674559533596039, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.85it/s]                                              {'loss': 0.6856, 'grad_norm': 0.16863539814949036, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.85it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.6239, 'grad_norm': 0.3543466627597809, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.12it/s]                                              {'loss': 0.6238, 'grad_norm': 0.4257969856262207, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.12it/s] 40%|████      | 6/15 [00:00<00:00, 15.74it/s]                                              {'loss': 0.6452, 'grad_norm': 0.3145233392715454, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.74it/s]                                              {'loss': 0.6201, 'grad_norm': 0.3015417158603668, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.74it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.6107, 'grad_norm': 0.36194702982902527, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.48it/s]                                              {'loss': 0.6201, 'grad_norm': 0.3808535933494568, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.48it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6375, 'grad_norm': 0.24754764139652252, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.94it/s]                                               {'loss': 0.6237, 'grad_norm': 0.27151596546173096, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.94it/s] 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.579, 'grad_norm': 0.617798924446106, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6231, 'grad_norm': 0.3288515508174896, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.41it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6138, 'grad_norm': 0.42000606656074524, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6245, 'grad_norm': 0.3465055525302887, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.41it/s]                                               {'train_runtime': 1.07, 'train_samples_per_second': 397.209, 'train_steps_per_second': 14.019, 'train_loss': 0.6225763559341431, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 14.02it/s]
CLIENT:86
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6223, 'grad_norm': 0.3583897352218628, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.74it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.85it/s]                                              {'loss': 0.658, 'grad_norm': 0.2048657089471817, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.85it/s]                                              {'loss': 0.666, 'grad_norm': 0.25822219252586365, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.85it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.6489, 'grad_norm': 0.2598245441913605, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.57it/s]                                              {'loss': 0.666, 'grad_norm': 0.20033225417137146, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.57it/s] 40%|████      | 6/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.629, 'grad_norm': 0.29126882553100586, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.6616, 'grad_norm': 0.2013450711965561, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.42it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.6646, 'grad_norm': 0.15413013100624084, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.42it/s]                                              {'loss': 0.5877, 'grad_norm': 0.5435307025909424, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.42it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.6309, 'grad_norm': 0.29751598834991455, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.99it/s]                                               {'loss': 0.6213, 'grad_norm': 0.3087078928947449, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.99it/s] 80%|████████  | 12/15 [00:00<00:00, 15.42it/s]                                               {'loss': 0.6936, 'grad_norm': 0.21877646446228027, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.42it/s]                                               {'loss': 0.6445, 'grad_norm': 0.2523626983165741, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.42it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6446, 'grad_norm': 0.261875182390213, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.6541, 'grad_norm': 0.2401529848575592, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]                                               {'train_runtime': 1.0708, 'train_samples_per_second': 396.906, 'train_steps_per_second': 14.008, 'train_loss': 0.6462143818537395, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 14.01it/s]
CLIENT:13
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6153, 'grad_norm': 0.38214632868766785, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.85it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.98it/s]                                              {'loss': 0.6097, 'grad_norm': 0.46207723021507263, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.98it/s]                                              {'loss': 0.5743, 'grad_norm': 0.6682142615318298, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.98it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.5843, 'grad_norm': 0.5786288380622864, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.64it/s]                                              {'loss': 0.6004, 'grad_norm': 0.4669746458530426, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.64it/s] 40%|████      | 6/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.6225, 'grad_norm': 0.34814369678497314, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.35it/s]                                              {'loss': 0.5889, 'grad_norm': 0.5221350789070129, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.35it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.5856, 'grad_norm': 0.5647754073143005, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.6212, 'grad_norm': 0.32432082295417786, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.27it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.6514, 'grad_norm': 0.20949029922485352, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.5734, 'grad_norm': 0.5153160095214844, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.84it/s] 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.5566, 'grad_norm': 0.730926513671875, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.47it/s]                                               {'loss': 0.6289, 'grad_norm': 0.25938737392425537, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.47it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.5328, 'grad_norm': 0.7948333024978638, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.98it/s]                                               {'loss': 0.6217, 'grad_norm': 0.2757667303085327, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.98it/s]                                               {'train_runtime': 1.0574, 'train_samples_per_second': 401.912, 'train_steps_per_second': 14.185, 'train_loss': 0.5978136380513509, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.98it/s]100%|██████████| 15/15 [00:01<00:00, 14.19it/s]
CLIENT:18
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6051, 'grad_norm': 0.5386764407157898, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.15it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.40it/s]                                              {'loss': 0.6379, 'grad_norm': 0.31022465229034424, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.40it/s]                                              {'loss': 0.6648, 'grad_norm': 0.17759978771209717, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.40it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.6023, 'grad_norm': 0.4959602355957031, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.45it/s]                                              {'loss': 0.6523, 'grad_norm': 0.18500864505767822, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.45it/s] 40%|████      | 6/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.6443, 'grad_norm': 0.258600652217865, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.28it/s]                                              {'loss': 0.6751, 'grad_norm': 0.13690677285194397, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.28it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.6312, 'grad_norm': 0.27274084091186523, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.16it/s]                                              {'loss': 0.5464, 'grad_norm': 0.8071492314338684, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.16it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6103, 'grad_norm': 0.4914694130420685, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.76it/s]                                               {'loss': 0.6282, 'grad_norm': 0.29458218812942505, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.76it/s] 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.6591, 'grad_norm': 0.22240400314331055, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.30it/s]                                               {'loss': 0.6208, 'grad_norm': 0.3589952290058136, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.30it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.665, 'grad_norm': 0.18563786149024963, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.40it/s]                                               {'loss': 0.5872, 'grad_norm': 0.5101158618927002, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]                                               {'train_runtime': 1.0774, 'train_samples_per_second': 394.475, 'train_steps_per_second': 13.923, 'train_loss': 0.628668216864268, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.40it/s]100%|██████████| 15/15 [00:01<00:00, 13.93it/s]
CLIENT:30
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6762, 'grad_norm': 0.14205943048000336, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.91it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.17it/s]                                              {'loss': 0.6432, 'grad_norm': 0.3304193317890167, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.17it/s]                                              {'loss': 0.6123, 'grad_norm': 0.48643001914024353, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.17it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.27it/s]                                              {'loss': 0.6394, 'grad_norm': 0.2892721891403198, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.27it/s]                                              {'loss': 0.6597, 'grad_norm': 0.2392387092113495, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.27it/s] 40%|████      | 6/15 [00:00<00:00, 16.73it/s]                                              {'loss': 0.6258, 'grad_norm': 0.31629180908203125, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 16.73it/s]                                              {'loss': 0.6628, 'grad_norm': 0.17692196369171143, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 16.73it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.6285, 'grad_norm': 0.37036874890327454, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.71it/s]                                              {'loss': 0.6391, 'grad_norm': 0.26544445753097534, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.71it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.17it/s]                                               {'loss': 0.6383, 'grad_norm': 0.28720584511756897, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.17it/s]                                               {'loss': 0.6912, 'grad_norm': 0.1995488405227661, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.17it/s] 80%|████████  | 12/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.5592, 'grad_norm': 0.7522881627082825, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.55it/s]                                               {'loss': 0.6361, 'grad_norm': 0.2593286335468292, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.55it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.6255, 'grad_norm': 0.3692206144332886, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.52it/s]                                               {'loss': 0.6695, 'grad_norm': 0.17712466418743134, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.52it/s]                                               {'train_runtime': 1.0554, 'train_samples_per_second': 402.68, 'train_steps_per_second': 14.212, 'train_loss': 0.6404600183169047, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.52it/s]100%|██████████| 15/15 [00:01<00:00, 14.22it/s]
CLIENT:36
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6219, 'grad_norm': 0.3475970923900604, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.09it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.73it/s]                                              {'loss': 0.6323, 'grad_norm': 0.36147838830947876, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.73it/s]                                              {'loss': 0.5886, 'grad_norm': 0.5147972106933594, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.73it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.5974, 'grad_norm': 0.46526843309402466, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.41it/s]                                              {'loss': 0.6387, 'grad_norm': 0.3300889730453491, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.41it/s] 40%|████      | 6/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.6217, 'grad_norm': 0.2870214283466339, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.33it/s]                                              {'loss': 0.5682, 'grad_norm': 0.7010493874549866, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.33it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.17it/s]                                              {'loss': 0.684, 'grad_norm': 0.19593189656734467, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.17it/s]                                              {'loss': 0.5915, 'grad_norm': 0.4519643187522888, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.17it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.5801, 'grad_norm': 0.566816508769989, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.11it/s]                                               {'loss': 0.639, 'grad_norm': 0.2293757200241089, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.11it/s] 80%|████████  | 12/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.6277, 'grad_norm': 0.27856582403182983, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.6431, 'grad_norm': 0.2243392914533615, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.63it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.5906, 'grad_norm': 0.4545055627822876, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.58it/s]                                               {'loss': 0.5886, 'grad_norm': 0.47634798288345337, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.58it/s]                                               {'train_runtime': 1.0693, 'train_samples_per_second': 397.457, 'train_steps_per_second': 14.028, 'train_loss': 0.6142289360364278, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.58it/s]100%|██████████| 15/15 [00:01<00:00, 14.03it/s]
CLIENT:52
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6348, 'grad_norm': 0.34214842319488525, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 11.91it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.62it/s]                                              {'loss': 0.6558, 'grad_norm': 0.19025608897209167, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.62it/s]                                              {'loss': 0.6805, 'grad_norm': 0.18408294022083282, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.62it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.6639, 'grad_norm': 0.14045630395412445, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.6428, 'grad_norm': 0.27397313714027405, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.14it/s] 40%|████      | 6/15 [00:00<00:00, 15.08it/s]                                              {'loss': 0.6508, 'grad_norm': 0.2266317903995514, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.08it/s]                                              {'loss': 0.6371, 'grad_norm': 0.27537721395492554, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.08it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.01it/s]                                              {'loss': 0.6455, 'grad_norm': 0.281526654958725, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.01it/s]                                              {'loss': 0.7083, 'grad_norm': 0.2490362524986267, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.01it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.6705, 'grad_norm': 0.15141941606998444, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.63it/s]                                               {'loss': 0.6377, 'grad_norm': 0.2519812285900116, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.63it/s] 80%|████████  | 12/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.624, 'grad_norm': 0.33260732889175415, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.33it/s]                                               {'loss': 0.6881, 'grad_norm': 0.1677808314561844, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.33it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6316, 'grad_norm': 0.2877400517463684, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.5996, 'grad_norm': 0.4185352325439453, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]                                               {'train_runtime': 1.0843, 'train_samples_per_second': 391.968, 'train_steps_per_second': 13.834, 'train_loss': 0.6513944943745931, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 13.84it/s]
CLIENT:95
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6597, 'grad_norm': 0.15633226931095123, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.15it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.84it/s]                                              {'loss': 0.616, 'grad_norm': 0.39372313022613525, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.84it/s]                                              {'loss': 0.628, 'grad_norm': 0.34955862164497375, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.84it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6077, 'grad_norm': 0.4627249836921692, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.32it/s]                                              {'loss': 0.6503, 'grad_norm': 0.15815909206867218, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.32it/s] 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.6563, 'grad_norm': 0.20916792750358582, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.29it/s]                                              {'loss': 0.6365, 'grad_norm': 0.2954638600349426, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.29it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.6378, 'grad_norm': 0.2866153120994568, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.72it/s]                                              {'loss': 0.6396, 'grad_norm': 0.26312991976737976, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.72it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.659, 'grad_norm': 0.1528739035129547, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.5994, 'grad_norm': 0.35268524289131165, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.16it/s] 80%|████████  | 12/15 [00:00<00:00, 15.58it/s]                                               {'loss': 0.6239, 'grad_norm': 0.3470064699649811, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.58it/s]                                               {'loss': 0.6698, 'grad_norm': 0.17543067038059235, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.58it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.6616, 'grad_norm': 0.196642205119133, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.57it/s]                                               {'loss': 0.5463, 'grad_norm': 0.7505733966827393, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.57it/s]                                               {'train_runtime': 1.0659, 'train_samples_per_second': 398.732, 'train_steps_per_second': 14.073, 'train_loss': 0.6327787240346273, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.57it/s]100%|██████████| 15/15 [00:01<00:00, 14.08it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 32.91it/s] 24%|██▍       | 8/33 [00:00<00:00, 27.60it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.20it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.34it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.27it/s] 61%|██████    | 20/33 [00:00<00:00, 25.41it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.04it/s] 79%|███████▉  | 26/33 [00:01<00:00, 25.00it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.22it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.18it/s]100%|██████████| 33/33 [00:01<00:00, 25.87it/s]
{'eval_loss': 0.6369798183441162, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3168, 'eval_samples_per_second': 792.097, 'eval_steps_per_second': 25.062}
ROUND:29
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6239, 'grad_norm': 0.35392138361930847, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.01it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.73it/s]                                              {'loss': 0.6332, 'grad_norm': 0.2975122630596161, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.73it/s]                                              {'loss': 0.678, 'grad_norm': 0.21705879271030426, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.73it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.04it/s]                                              {'loss': 0.6095, 'grad_norm': 0.4618467092514038, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.04it/s]                                              {'loss': 0.6612, 'grad_norm': 0.14995019137859344, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.04it/s] 40%|████      | 6/15 [00:00<00:00, 15.00it/s]                                              {'loss': 0.6637, 'grad_norm': 0.18119877576828003, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.00it/s]                                              {'loss': 0.6227, 'grad_norm': 0.325615793466568, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.00it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.03it/s]                                              {'loss': 0.6745, 'grad_norm': 0.16239577531814575, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.03it/s]                                              {'loss': 0.6187, 'grad_norm': 0.34371012449264526, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.03it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.6217, 'grad_norm': 0.3238186240196228, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.69it/s]                                               {'loss': 0.6749, 'grad_norm': 0.2186807096004486, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.69it/s] 80%|████████  | 12/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.6568, 'grad_norm': 0.2525113821029663, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.21it/s]                                               {'loss': 0.6393, 'grad_norm': 0.19050288200378418, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.21it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.32it/s]                                               {'loss': 0.6653, 'grad_norm': 0.15726789832115173, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.32it/s]                                               {'loss': 0.5928, 'grad_norm': 0.48924556374549866, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.32it/s]                                               {'train_runtime': 1.0891, 'train_samples_per_second': 390.219, 'train_steps_per_second': 13.772, 'train_loss': 0.6424284855524699, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.32it/s]100%|██████████| 15/15 [00:01<00:00, 13.78it/s]
CLIENT:99
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/25 [00:00<?, ?it/s]                                      {'loss': 0.6311, 'grad_norm': 0.2781470715999603, 'learning_rate': 0.01, 'epoch': 0.2}
  4%|▍         | 1/25 [00:00<00:01, 14.22it/s]  8%|▊         | 2/25 [00:00<00:01, 14.96it/s]                                              {'loss': 0.5776, 'grad_norm': 0.5140781998634338, 'learning_rate': 0.0096, 'epoch': 0.4}
  8%|▊         | 2/25 [00:00<00:01, 14.96it/s]                                              {'loss': 0.5983, 'grad_norm': 0.4653107821941376, 'learning_rate': 0.0092, 'epoch': 0.6}
 12%|█▏        | 3/25 [00:00<00:01, 14.96it/s] 16%|█▌        | 4/25 [00:00<00:01, 13.74it/s]                                              {'loss': 0.5954, 'grad_norm': 0.45459306240081787, 'learning_rate': 0.0088, 'epoch': 0.8}
 16%|█▌        | 4/25 [00:00<00:01, 13.74it/s]                                              {'loss': 0.7152, 'grad_norm': 0.3792397975921631, 'learning_rate': 0.0084, 'epoch': 1.0}
 20%|██        | 5/25 [00:00<00:01, 13.74it/s] 24%|██▍       | 6/25 [00:00<00:01, 15.68it/s]                                              {'loss': 0.6163, 'grad_norm': 0.31073880195617676, 'learning_rate': 0.008, 'epoch': 1.2}
 24%|██▍       | 6/25 [00:00<00:01, 15.68it/s]                                              {'loss': 0.6193, 'grad_norm': 0.280070424079895, 'learning_rate': 0.0076, 'epoch': 1.4}
 28%|██▊       | 7/25 [00:00<00:01, 15.68it/s] 32%|███▏      | 8/25 [00:00<00:01, 14.24it/s]                                              {'loss': 0.5933, 'grad_norm': 0.4013780355453491, 'learning_rate': 0.0072, 'epoch': 1.6}
 32%|███▏      | 8/25 [00:00<00:01, 14.24it/s]                                              {'loss': 0.6012, 'grad_norm': 0.47802919149398804, 'learning_rate': 0.0068000000000000005, 'epoch': 1.8}
 36%|███▌      | 9/25 [00:00<00:01, 14.24it/s]                                              {'loss': 0.5446, 'grad_norm': 0.5984973907470703, 'learning_rate': 0.0064, 'epoch': 2.0}
 40%|████      | 10/25 [00:00<00:01, 14.24it/s] 44%|████▍     | 11/25 [00:00<00:00, 15.27it/s]                                               {'loss': 0.5988, 'grad_norm': 0.4082781970500946, 'learning_rate': 0.006, 'epoch': 2.2}
 44%|████▍     | 11/25 [00:00<00:00, 15.27it/s]                                               {'loss': 0.6921, 'grad_norm': 0.19602985680103302, 'learning_rate': 0.005600000000000001, 'epoch': 2.4}
 48%|████▊     | 12/25 [00:00<00:00, 15.27it/s] 52%|█████▏    | 13/25 [00:00<00:00, 14.25it/s]                                               {'loss': 0.5861, 'grad_norm': 0.4573074281215668, 'learning_rate': 0.005200000000000001, 'epoch': 2.6}
 52%|█████▏    | 13/25 [00:00<00:00, 14.25it/s]                                               {'loss': 0.5643, 'grad_norm': 0.6310006380081177, 'learning_rate': 0.0048, 'epoch': 2.8}
 56%|█████▌    | 14/25 [00:00<00:00, 14.25it/s]                                               {'loss': 0.499, 'grad_norm': 1.001320242881775, 'learning_rate': 0.0044, 'epoch': 3.0}
 60%|██████    | 15/25 [00:00<00:00, 14.25it/s] 64%|██████▍   | 16/25 [00:01<00:00, 15.14it/s]                                               {'loss': 0.5993, 'grad_norm': 0.35458365082740784, 'learning_rate': 0.004, 'epoch': 3.2}
 64%|██████▍   | 16/25 [00:01<00:00, 15.14it/s]                                               {'loss': 0.5816, 'grad_norm': 0.43509024381637573, 'learning_rate': 0.0036, 'epoch': 3.4}
 68%|██████▊   | 17/25 [00:01<00:00, 15.14it/s] 72%|███████▏  | 18/25 [00:01<00:00, 14.28it/s]                                               {'loss': 0.5755, 'grad_norm': 0.42899519205093384, 'learning_rate': 0.0032, 'epoch': 3.6}
 72%|███████▏  | 18/25 [00:01<00:00, 14.28it/s]                                               {'loss': 0.5613, 'grad_norm': 0.5795378684997559, 'learning_rate': 0.0028000000000000004, 'epoch': 3.8}
 76%|███████▌  | 19/25 [00:01<00:00, 14.28it/s] 80%|████████  | 20/25 [00:01<00:00, 15.53it/s]                                               {'loss': 0.7819, 'grad_norm': 0.7315273284912109, 'learning_rate': 0.0024, 'epoch': 4.0}
 80%|████████  | 20/25 [00:01<00:00, 15.53it/s]                                               {'loss': 0.6641, 'grad_norm': 0.2920536696910858, 'learning_rate': 0.002, 'epoch': 4.2}
 84%|████████▍ | 21/25 [00:01<00:00, 15.53it/s] 88%|████████▊ | 22/25 [00:01<00:00, 14.54it/s]                                               {'loss': 0.5875, 'grad_norm': 0.378033846616745, 'learning_rate': 0.0016, 'epoch': 4.4}
 88%|████████▊ | 22/25 [00:01<00:00, 14.54it/s]                                               {'loss': 0.5448, 'grad_norm': 0.55057293176651, 'learning_rate': 0.0012, 'epoch': 4.6}
 92%|█████████▏| 23/25 [00:01<00:00, 14.54it/s] 96%|█████████▌| 24/25 [00:01<00:00, 14.01it/s]                                               {'loss': 0.5632, 'grad_norm': 0.5318901538848877, 'learning_rate': 0.0008, 'epoch': 4.8}
 96%|█████████▌| 24/25 [00:01<00:00, 14.01it/s]                                               {'loss': 0.5717, 'grad_norm': 0.45729202032089233, 'learning_rate': 0.0004, 'epoch': 5.0}
100%|██████████| 25/25 [00:01<00:00, 14.01it/s]                                               {'train_runtime': 1.7475, 'train_samples_per_second': 389.118, 'train_steps_per_second': 14.306, 'train_loss': 0.6025456643104553, 'epoch': 5.0}
100%|██████████| 25/25 [00:01<00:00, 14.01it/s]100%|██████████| 25/25 [00:01<00:00, 14.31it/s]
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6491, 'grad_norm': 0.2595517933368683, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.84it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.02it/s]                                              {'loss': 0.6646, 'grad_norm': 0.14610984921455383, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.02it/s]                                              {'loss': 0.7685, 'grad_norm': 0.6102223992347717, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.02it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.7225, 'grad_norm': 0.24472574889659882, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.63it/s]                                              {'loss': 0.6812, 'grad_norm': 0.1418790966272354, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.63it/s] 40%|████      | 6/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.6537, 'grad_norm': 0.2927323281764984, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.42it/s]                                              {'loss': 0.6849, 'grad_norm': 0.16747939586639404, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.42it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6803, 'grad_norm': 0.17441636323928833, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.22it/s]                                              {'loss': 0.6401, 'grad_norm': 0.24812838435173035, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.22it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.6997, 'grad_norm': 0.2613890767097473, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.80it/s]                                               {'loss': 0.676, 'grad_norm': 0.14711369574069977, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.80it/s] 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.6315, 'grad_norm': 0.22258727252483368, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.35it/s]                                               {'loss': 0.6648, 'grad_norm': 0.2014041244983673, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.35it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.7041, 'grad_norm': 0.27992668747901917, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.88it/s]                                               {'loss': 0.6786, 'grad_norm': 0.16483670473098755, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.88it/s]                                               {'train_runtime': 1.0668, 'train_samples_per_second': 398.371, 'train_steps_per_second': 14.06, 'train_loss': 0.6799754341443379, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.88it/s]100%|██████████| 15/15 [00:01<00:00, 14.07it/s]
CLIENT:61
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6854, 'grad_norm': 0.20051324367523193, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.34it/s] 13%|█▎        | 2/15 [00:00<00:01, 12.94it/s]                                              {'loss': 0.6547, 'grad_norm': 0.2048514038324356, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:01, 12.94it/s]                                              {'loss': 0.5561, 'grad_norm': 0.754454493522644, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 12.94it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6093, 'grad_norm': 0.4500519931316376, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.23it/s]                                              {'loss': 0.6291, 'grad_norm': 0.33848291635513306, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.23it/s] 40%|████      | 6/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.7384, 'grad_norm': 0.42437678575515747, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6153, 'grad_norm': 0.34491559863090515, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.16it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.6227, 'grad_norm': 0.33444344997406006, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.14it/s]                                              {'loss': 0.7259, 'grad_norm': 0.30416420102119446, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.14it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.6102, 'grad_norm': 0.3724019527435303, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.02it/s]                                               {'loss': 0.6942, 'grad_norm': 0.1566830426454544, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.02it/s] 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.6045, 'grad_norm': 0.3482216000556946, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.46it/s]                                               {'loss': 0.6605, 'grad_norm': 0.24474601447582245, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.46it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.6268, 'grad_norm': 0.270233690738678, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.44it/s]                                               {'loss': 0.6363, 'grad_norm': 0.33967506885528564, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]                                               {'train_runtime': 1.0797, 'train_samples_per_second': 393.639, 'train_steps_per_second': 13.893, 'train_loss': 0.6446337660153707, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.44it/s]100%|██████████| 15/15 [00:01<00:00, 13.90it/s]
CLIENT:66
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6277, 'grad_norm': 0.2602976858615875, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.65it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.95it/s]                                              {'loss': 0.6626, 'grad_norm': 0.14857786893844604, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.95it/s]                                              {'loss': 0.6373, 'grad_norm': 0.3371432423591614, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.95it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6797, 'grad_norm': 0.1471814215183258, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.16it/s]                                              {'loss': 0.6332, 'grad_norm': 0.29979047179222107, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.16it/s] 40%|████      | 6/15 [00:00<00:00, 15.73it/s]                                              {'loss': 0.6252, 'grad_norm': 0.2541143298149109, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.73it/s]                                              {'loss': 0.6203, 'grad_norm': 0.3182670772075653, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.73it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.6869, 'grad_norm': 0.16765931248664856, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.49it/s]                                              {'loss': 0.5975, 'grad_norm': 0.42027905583381653, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.49it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.06it/s]                                               {'loss': 0.6714, 'grad_norm': 0.14555154740810394, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.06it/s]                                               {'loss': 0.6574, 'grad_norm': 0.15206031501293182, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.06it/s] 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.5518, 'grad_norm': 0.5927427411079407, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.48it/s]                                               {'loss': 0.6403, 'grad_norm': 0.20870254933834076, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.48it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.6452, 'grad_norm': 0.19422370195388794, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.53it/s]                                               {'loss': 0.6546, 'grad_norm': 0.2607041001319885, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.53it/s]                                               {'train_runtime': 1.0554, 'train_samples_per_second': 402.699, 'train_steps_per_second': 14.213, 'train_loss': 0.6393948634465535, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.53it/s]100%|██████████| 15/15 [00:01<00:00, 14.22it/s]
CLIENT:36
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6151, 'grad_norm': 0.32527241110801697, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.03it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.6254, 'grad_norm': 0.33118364214897156, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.13it/s]                                              {'loss': 0.5776, 'grad_norm': 0.5266461372375488, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.13it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.5875, 'grad_norm': 0.49012476205825806, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.50it/s]                                              {'loss': 0.6327, 'grad_norm': 0.296694815158844, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.50it/s] 40%|████      | 6/15 [00:00<00:00, 15.96it/s]                                              {'loss': 0.6168, 'grad_norm': 0.2706647217273712, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.96it/s]                                              {'loss': 0.5535, 'grad_norm': 0.6972112655639648, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.96it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.6837, 'grad_norm': 0.22832168638706207, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.58it/s]                                              {'loss': 0.5827, 'grad_norm': 0.4568324685096741, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.58it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.5683, 'grad_norm': 0.5764675140380859, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.13it/s]                                               {'loss': 0.6353, 'grad_norm': 0.21992167830467224, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.13it/s] 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.623, 'grad_norm': 0.2761649191379547, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.41it/s]                                               {'loss': 0.6395, 'grad_norm': 0.21709835529327393, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.41it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.5814, 'grad_norm': 0.45275288820266724, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.45it/s]                                               {'loss': 0.5794, 'grad_norm': 0.4801015853881836, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 14.45it/s]                                               {'train_runtime': 1.0588, 'train_samples_per_second': 401.413, 'train_steps_per_second': 14.168, 'train_loss': 0.6067950844764709, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.45it/s]100%|██████████| 15/15 [00:01<00:00, 14.17it/s]
CLIENT:23
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6034, 'grad_norm': 0.4756915867328644, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 12.91it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.10it/s]                                              {'loss': 0.6619, 'grad_norm': 0.18139052391052246, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.10it/s]                                              {'loss': 0.6329, 'grad_norm': 0.3444827198982239, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.10it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.6477, 'grad_norm': 0.19792424142360687, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.5997, 'grad_norm': 0.4606962502002716, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.61it/s] 40%|████      | 6/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.6166, 'grad_norm': 0.3058789074420929, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.41it/s]                                              {'loss': 0.6619, 'grad_norm': 0.15256370604038239, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.41it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.6208, 'grad_norm': 0.30393773317337036, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.59it/s]                                              {'loss': 0.5568, 'grad_norm': 0.6416617631912231, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.59it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.6647, 'grad_norm': 0.22603926062583923, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.84it/s]                                               {'loss': 0.6006, 'grad_norm': 0.4047747254371643, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.84it/s] 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.6061, 'grad_norm': 0.3448968529701233, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.32it/s]                                               {'loss': 0.5585, 'grad_norm': 0.5914167761802673, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.32it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.6842, 'grad_norm': 0.2126132994890213, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.37it/s]                                               {'loss': 0.6213, 'grad_norm': 0.30873045325279236, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]                                               {'train_runtime': 1.0741, 'train_samples_per_second': 395.666, 'train_steps_per_second': 13.965, 'train_loss': 0.62248162428538, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.37it/s]100%|██████████| 15/15 [00:01<00:00, 13.97it/s]
CLIENT:83
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.5933, 'grad_norm': 0.5287505984306335, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.01it/s] 13%|█▎        | 2/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.6263, 'grad_norm': 0.3511258065700531, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 14.09it/s]                                              {'loss': 0.6302, 'grad_norm': 0.26851406693458557, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 14.09it/s] 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6051, 'grad_norm': 0.42965972423553467, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 15.13it/s]                                              {'loss': 0.6367, 'grad_norm': 0.281038761138916, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 15.13it/s] 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.5694, 'grad_norm': 0.5380253791809082, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.97it/s]                                              {'loss': 0.6187, 'grad_norm': 0.324077308177948, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.97it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.5843, 'grad_norm': 0.48691457509994507, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.61it/s]                                              {'loss': 0.6315, 'grad_norm': 0.2689831852912903, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.61it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.6307, 'grad_norm': 0.2568506598472595, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.37it/s]                                               {'loss': 0.5661, 'grad_norm': 0.545706570148468, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.37it/s] 80%|████████  | 12/15 [00:00<00:00, 15.84it/s]                                               {'loss': 0.614, 'grad_norm': 0.32120123505592346, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.84it/s]                                               {'loss': 0.5535, 'grad_norm': 0.593996524810791, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.84it/s] 93%|█████████▎| 14/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.6488, 'grad_norm': 0.16565988957881927, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 15.08it/s]                                               {'loss': 0.6134, 'grad_norm': 0.32676565647125244, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:00<00:00, 15.08it/s]                                               {'train_runtime': 1.0377, 'train_samples_per_second': 409.578, 'train_steps_per_second': 14.456, 'train_loss': 0.6081496516863505, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 15.08it/s]100%|██████████| 15/15 [00:01<00:00, 14.46it/s]
CLIENT:77
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6412, 'grad_norm': 0.27112075686454773, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:00, 14.28it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.97it/s]                                              {'loss': 0.6796, 'grad_norm': 0.14716345071792603, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.97it/s]                                              {'loss': 0.6617, 'grad_norm': 0.18966418504714966, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.97it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.6899, 'grad_norm': 0.14903128147125244, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.70it/s]                                              {'loss': 0.6661, 'grad_norm': 0.17218023538589478, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.70it/s] 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.6109, 'grad_norm': 0.3692190647125244, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.45it/s]                                              {'loss': 0.6527, 'grad_norm': 0.22863009572029114, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.45it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.26it/s]                                              {'loss': 0.669, 'grad_norm': 0.1515933722257614, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.26it/s]                                              {'loss': 0.6505, 'grad_norm': 0.2687254250049591, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.26it/s] 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6754, 'grad_norm': 0.1537223607301712, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 14.82it/s]                                               {'loss': 0.6257, 'grad_norm': 0.3072557747364044, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 14.82it/s] 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6932, 'grad_norm': 0.19433867931365967, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.31it/s]                                               {'loss': 0.6292, 'grad_norm': 0.30431613326072693, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.31it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.7132, 'grad_norm': 0.21824930608272552, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.41it/s]                                               {'loss': 0.6427, 'grad_norm': 0.3976309895515442, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]                                               {'train_runtime': 1.0726, 'train_samples_per_second': 396.243, 'train_steps_per_second': 13.985, 'train_loss': 0.6600695967674255, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.41it/s]100%|██████████| 15/15 [00:01<00:00, 13.99it/s]
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/15 [00:00<?, ?it/s]                                      {'loss': 0.6295, 'grad_norm': 0.3084937632083893, 'learning_rate': 0.01, 'epoch': 0.33}
  7%|▋         | 1/15 [00:00<00:01, 13.01it/s] 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.6116, 'grad_norm': 0.41084927320480347, 'learning_rate': 0.009333333333333334, 'epoch': 0.67}
 13%|█▎        | 2/15 [00:00<00:00, 13.15it/s]                                              {'loss': 0.6059, 'grad_norm': 0.3774700164794922, 'learning_rate': 0.008666666666666668, 'epoch': 1.0}
 20%|██        | 3/15 [00:00<00:00, 13.15it/s] 27%|██▋       | 4/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.6014, 'grad_norm': 0.4431935250759125, 'learning_rate': 0.008, 'epoch': 1.33}
 27%|██▋       | 4/15 [00:00<00:00, 14.62it/s]                                              {'loss': 0.6522, 'grad_norm': 0.24088360369205475, 'learning_rate': 0.007333333333333333, 'epoch': 1.67}
 33%|███▎      | 5/15 [00:00<00:00, 14.62it/s] 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.5932, 'grad_norm': 0.42393746972084045, 'learning_rate': 0.006666666666666666, 'epoch': 2.0}
 40%|████      | 6/15 [00:00<00:00, 15.48it/s]                                              {'loss': 0.622, 'grad_norm': 0.3240165412425995, 'learning_rate': 0.006, 'epoch': 2.33}
 47%|████▋     | 7/15 [00:00<00:00, 15.48it/s] 53%|█████▎    | 8/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.6241, 'grad_norm': 0.33884865045547485, 'learning_rate': 0.005333333333333333, 'epoch': 2.67}
 53%|█████▎    | 8/15 [00:00<00:00, 14.27it/s]                                              {'loss': 0.5927, 'grad_norm': 0.3996371626853943, 'learning_rate': 0.004666666666666667, 'epoch': 3.0}
 60%|██████    | 9/15 [00:00<00:00, 14.27it/s] 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.6044, 'grad_norm': 0.41387563943862915, 'learning_rate': 0.004, 'epoch': 3.33}
 67%|██████▋   | 10/15 [00:00<00:00, 15.16it/s]                                               {'loss': 0.6284, 'grad_norm': 0.2503780424594879, 'learning_rate': 0.003333333333333333, 'epoch': 3.67}
 73%|███████▎  | 11/15 [00:00<00:00, 15.16it/s] 80%|████████  | 12/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.6039, 'grad_norm': 0.3932095766067505, 'learning_rate': 0.0026666666666666666, 'epoch': 4.0}
 80%|████████  | 12/15 [00:00<00:00, 15.63it/s]                                               {'loss': 0.5987, 'grad_norm': 0.3844151794910431, 'learning_rate': 0.002, 'epoch': 4.33}
 87%|████████▋ | 13/15 [00:00<00:00, 15.63it/s] 93%|█████████▎| 14/15 [00:00<00:00, 14.30it/s]                                               {'loss': 0.6364, 'grad_norm': 0.19402293860912323, 'learning_rate': 0.0013333333333333333, 'epoch': 4.67}
 93%|█████████▎| 14/15 [00:00<00:00, 14.30it/s]                                               {'loss': 0.6117, 'grad_norm': 0.3712274432182312, 'learning_rate': 0.0006666666666666666, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.30it/s]                                               {'train_runtime': 1.0805, 'train_samples_per_second': 393.337, 'train_steps_per_second': 13.882, 'train_loss': 0.6144083062807719, 'epoch': 5.0}
100%|██████████| 15/15 [00:01<00:00, 14.30it/s]100%|██████████| 15/15 [00:01<00:00, 13.89it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:348: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/33 [00:00<?, ?it/s] 12%|█▏        | 4/33 [00:00<00:00, 35.42it/s] 24%|██▍       | 8/33 [00:00<00:00, 28.00it/s] 33%|███▎      | 11/33 [00:00<00:00, 26.66it/s] 42%|████▏     | 14/33 [00:00<00:00, 25.80it/s] 52%|█████▏    | 17/33 [00:00<00:00, 25.73it/s] 61%|██████    | 20/33 [00:00<00:00, 25.41it/s] 70%|██████▉   | 23/33 [00:00<00:00, 25.26it/s] 79%|███████▉  | 26/33 [00:00<00:00, 25.30it/s] 88%|████████▊ | 29/33 [00:01<00:00, 25.01it/s] 97%|█████████▋| 32/33 [00:01<00:00, 25.32it/s]100%|██████████| 33/33 [00:01<00:00, 26.12it/s]
{'eval_loss': 0.6325609683990479, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.6912751677852349, 'eval_runtime': 1.3047, 'eval_samples_per_second': 799.415, 'eval_steps_per_second': 25.293}

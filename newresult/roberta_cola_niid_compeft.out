nohup: ignoring input
/home/suxiaoxin/.conda/envs/sxx/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at ./data/glue/glue.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./model/models--roberta-base/ and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 589,824 || all params: 125,236,994 || trainable%: 0.4710
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): RobertaForSequenceClassification(
      (roberta): RobertaModel(
        (embeddings): RobertaEmbeddings(
          (word_embeddings): Embedding(50265, 768, padding_idx=1)
          (position_embeddings): Embedding(514, 768, padding_idx=1)
          (token_type_embeddings): Embedding(1, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): RobertaEncoder(
          (layer): ModuleList(
            (0-11): 12 x RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSdpaSelfAttention(
                  (query): lora.Linear(
                    (base_layer): Linear(in_features=768, out_features=768, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.05, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=768, out_features=16, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=16, out_features=768, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): lora.Linear(
                    (base_layer): Linear(in_features=768, out_features=768, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.05, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=768, out_features=16, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=16, out_features=768, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (classifier): RobertaClassificationHead(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (out_proj): Linear(in_features=768, out_features=2, bias=True)
      )
    )
  )
)
ROUND:0
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:01<00:47,  1.65s/it]                                              {'loss': 0.6388, 'grad_norm': 1.4005168676376343, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:01<00:47,  1.65s/it]  7%|▋         | 2/30 [00:01<00:23,  1.19it/s]                                              {'loss': 1.0447, 'grad_norm': 6.367481231689453, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:23,  1.19it/s] 10%|█         | 3/30 [00:02<00:15,  1.71it/s]                                              {'loss': 1.4145, 'grad_norm': 5.909910202026367, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:02<00:15,  1.71it/s] 13%|█▎        | 4/30 [00:02<00:11,  2.20it/s]                                              {'loss': 1.6263, 'grad_norm': 5.317267894744873, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:02<00:11,  2.20it/s] 17%|█▋        | 5/30 [00:02<00:09,  2.66it/s]                                              {'loss': 0.9472, 'grad_norm': 3.1480088233947754, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:02<00:09,  2.66it/s] 20%|██        | 6/30 [00:02<00:06,  3.48it/s]                                              {'loss': 1.0693, 'grad_norm': 5.5416388511657715, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:06,  3.48it/s] 23%|██▎       | 7/30 [00:03<00:06,  3.52it/s]                                              {'loss': 0.7133, 'grad_norm': 1.850606083869934, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:03<00:06,  3.52it/s] 27%|██▋       | 8/30 [00:03<00:06,  3.49it/s]                                              {'loss': 0.8409, 'grad_norm': 5.16675329208374, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:03<00:06,  3.49it/s] 30%|███       | 9/30 [00:03<00:05,  3.50it/s]                                              {'loss': 0.7365, 'grad_norm': 2.135141372680664, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:03<00:05,  3.50it/s] 33%|███▎      | 10/30 [00:03<00:05,  3.48it/s]                                               {'loss': 0.7032, 'grad_norm': 0.6126570701599121, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:05,  3.48it/s] 37%|███▋      | 11/30 [00:04<00:05,  3.54it/s]                                               {'loss': 0.716, 'grad_norm': 0.8783379793167114, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:04<00:05,  3.54it/s]                                               {'loss': 0.5712, 'grad_norm': 2.3020858764648438, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:04<00:05,  3.54it/s] 43%|████▎     | 13/30 [00:04<00:03,  4.32it/s]                                               {'loss': 0.8319, 'grad_norm': 1.6218489408493042, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:04<00:03,  4.32it/s] 47%|████▋     | 14/30 [00:04<00:03,  4.17it/s]                                               {'loss': 0.656, 'grad_norm': 1.144134759902954, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:03,  4.17it/s] 50%|█████     | 15/30 [00:05<00:03,  4.04it/s]                                               {'loss': 0.7155, 'grad_norm': 5.533949851989746, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:05<00:03,  4.04it/s] 53%|█████▎    | 16/30 [00:05<00:03,  3.94it/s]                                               {'loss': 0.7041, 'grad_norm': 1.0108829736709595, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:05<00:03,  3.94it/s] 57%|█████▋    | 17/30 [00:05<00:03,  3.86it/s]                                               {'loss': 0.6973, 'grad_norm': 1.9846556186676025, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:05<00:03,  3.86it/s]                                               {'loss': 0.7075, 'grad_norm': 1.420720100402832, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:03,  3.86it/s] 63%|██████▎   | 19/30 [00:05<00:02,  4.56it/s]                                               {'loss': 0.6819, 'grad_norm': 1.0436923503875732, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  4.56it/s] 67%|██████▋   | 20/30 [00:06<00:02,  4.33it/s]                                               {'loss': 0.6773, 'grad_norm': 0.6120210886001587, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:06<00:02,  4.33it/s] 70%|███████   | 21/30 [00:06<00:02,  4.17it/s]                                               {'loss': 0.7204, 'grad_norm': 0.9211769104003906, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:06<00:02,  4.17it/s] 73%|███████▎  | 22/30 [00:06<00:01,  4.01it/s]                                               {'loss': 0.7136, 'grad_norm': 1.3112200498580933, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:06<00:01,  4.01it/s] 77%|███████▋  | 23/30 [00:07<00:01,  3.93it/s]                                               {'loss': 0.69, 'grad_norm': 1.8725273609161377, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:07<00:01,  3.93it/s]                                               {'loss': 0.6584, 'grad_norm': 1.2874832153320312, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:07<00:01,  3.93it/s] 83%|████████▎ | 25/30 [00:07<00:01,  4.50it/s]                                               {'loss': 0.6622, 'grad_norm': 1.0944267511367798, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:07<00:01,  4.50it/s] 87%|████████▋ | 26/30 [00:07<00:00,  4.29it/s]                                               {'loss': 0.6514, 'grad_norm': 1.177304983139038, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:07<00:00,  4.29it/s] 90%|█████████ | 27/30 [00:07<00:00,  4.13it/s]                                               {'loss': 0.6678, 'grad_norm': 0.9974465370178223, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:07<00:00,  4.13it/s] 93%|█████████▎| 28/30 [00:08<00:00,  4.27it/s]                                               {'loss': 0.6799, 'grad_norm': 0.6460588574409485, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:08<00:00,  4.27it/s] 97%|█████████▋| 29/30 [00:08<00:00,  4.69it/s]                                               {'loss': 0.6138, 'grad_norm': 1.343395709991455, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:08<00:00,  4.69it/s]                                               {'loss': 0.6637, 'grad_norm': 1.2779037952423096, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.69it/s]                                               {'train_runtime': 8.4898, 'train_samples_per_second': 50.06, 'train_steps_per_second': 3.534, 'train_loss': 0.7804885586102803, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.69it/s]100%|██████████| 30/30 [00:08<00:00,  3.54it/s]
CLIENT:64
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.56it/s]                                              {'loss': 0.9727, 'grad_norm': 1.871934175491333, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.56it/s]  7%|▋         | 2/30 [00:00<00:06,  4.51it/s]                                              {'loss': 0.5338, 'grad_norm': 1.2208521366119385, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.51it/s] 10%|█         | 3/30 [00:00<00:05,  4.69it/s]                                              {'loss': 0.3484, 'grad_norm': 1.727776050567627, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.69it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.77it/s]                                              {'loss': 0.1922, 'grad_norm': 1.5756406784057617, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.77it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.34it/s]                                              {'loss': 0.1555, 'grad_norm': 0.5358352661132812, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:04,  5.34it/s] 20%|██        | 6/30 [00:01<00:08,  2.71it/s]                                              {'loss': 0.0228, 'grad_norm': 0.4462833106517792, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:08,  2.71it/s] 23%|██▎       | 7/30 [00:01<00:07,  3.16it/s]                                              {'loss': 0.0153, 'grad_norm': 0.3370838761329651, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:07,  3.16it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.55it/s]                                              {'loss': 0.0035, 'grad_norm': 0.09431260079145432, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.55it/s] 30%|███       | 9/30 [00:02<00:05,  3.87it/s]                                              {'loss': 0.0093, 'grad_norm': 0.4377182722091675, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  3.87it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.45it/s]                                               {'loss': 0.011, 'grad_norm': 0.4709473252296448, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.45it/s] 37%|███▋      | 11/30 [00:02<00:03,  4.96it/s]                                               {'loss': 0.4549, 'grad_norm': 1.5269911289215088, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  4.96it/s]                                               {'loss': 0.0007, 'grad_norm': 0.027298256754875183, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.96it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.39it/s]                                               {'loss': 0.0006, 'grad_norm': 0.011512666940689087, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.39it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.49it/s]                                               {'loss': 0.4722, 'grad_norm': 2.5536413192749023, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:02,  6.49it/s] 50%|█████     | 15/30 [00:03<00:02,  5.85it/s]                                               {'loss': 0.0009, 'grad_norm': 0.01757085509598255, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.85it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.44it/s]                                               {'loss': 0.0016, 'grad_norm': 0.04468359798192978, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.44it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.62it/s]                                               {'loss': 0.0023, 'grad_norm': 0.054326381534338, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.62it/s]                                               {'loss': 0.0019, 'grad_norm': 0.06037186458706856, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.62it/s] 63%|██████▎   | 19/30 [00:03<00:01,  7.10it/s]                                               {'loss': 0.4224, 'grad_norm': 1.783085823059082, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  7.10it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.01it/s]                                               {'loss': 0.0024, 'grad_norm': 0.07571293413639069, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.01it/s] 70%|███████   | 21/30 [00:04<00:01,  6.99it/s]                                               {'loss': 0.0043, 'grad_norm': 0.13900451362133026, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  6.99it/s] 73%|███████▎  | 22/30 [00:04<00:01,  6.88it/s]                                               {'loss': 0.007, 'grad_norm': 0.21014520525932312, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  6.88it/s] 77%|███████▋  | 23/30 [00:04<00:01,  6.75it/s]                                               {'loss': 0.0065, 'grad_norm': 0.193483367562294, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  6.75it/s]                                               {'loss': 0.0023, 'grad_norm': 0.06673794984817505, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  6.75it/s] 83%|████████▎ | 25/30 [00:04<00:00,  8.16it/s]                                               {'loss': 0.4436, 'grad_norm': 5.388617038726807, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  8.16it/s] 87%|████████▋ | 26/30 [00:04<00:00,  7.99it/s]                                               {'loss': 0.0054, 'grad_norm': 0.143761545419693, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  7.99it/s] 90%|█████████ | 27/30 [00:04<00:00,  7.68it/s]                                               {'loss': 0.0055, 'grad_norm': 0.12900234758853912, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  7.68it/s] 93%|█████████▎| 28/30 [00:04<00:00,  7.46it/s]                                               {'loss': 0.0044, 'grad_norm': 0.13673634827136993, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  7.46it/s] 97%|█████████▋| 29/30 [00:05<00:00,  7.23it/s]                                               {'loss': 0.0036, 'grad_norm': 0.09153585880994797, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  7.23it/s]                                               {'loss': 0.0033, 'grad_norm': 0.09267550706863403, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  7.23it/s]                                               {'train_runtime': 5.2734, 'train_samples_per_second': 80.593, 'train_steps_per_second': 5.689, 'train_loss': 0.1370069779048208, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  7.23it/s]100%|██████████| 30/30 [00:05<00:00,  5.69it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.36it/s]                                              {'loss': 0.59, 'grad_norm': 0.25399214029312134, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.36it/s]  7%|▋         | 2/30 [00:00<00:06,  4.55it/s]                                              {'loss': 0.6675, 'grad_norm': 0.6621816754341125, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.55it/s] 10%|█         | 3/30 [00:00<00:05,  4.62it/s]                                              {'loss': 0.7623, 'grad_norm': 0.5680248141288757, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.62it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.60it/s]                                              {'loss': 0.651, 'grad_norm': 1.1885638236999512, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.60it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.89it/s]                                              {'loss': 0.6632, 'grad_norm': 0.4213665723800659, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.89it/s]                                              {'loss': 0.7468, 'grad_norm': 1.7116076946258545, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.89it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.52it/s]                                              {'loss': 0.6151, 'grad_norm': 0.5424310564994812, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.52it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.21it/s]                                              {'loss': 0.7354, 'grad_norm': 0.9516140818595886, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.21it/s] 30%|███       | 9/30 [00:01<00:04,  4.96it/s]                                              {'loss': 0.6729, 'grad_norm': 0.4524596035480499, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.96it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.87it/s]                                               {'loss': 0.5883, 'grad_norm': 0.9332521557807922, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.87it/s] 37%|███▋      | 11/30 [00:02<00:03,  4.85it/s]                                               {'loss': 0.6355, 'grad_norm': 0.3359020948410034, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  4.85it/s]                                               {'loss': 0.4847, 'grad_norm': 0.9527598023414612, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.85it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.26it/s]                                               {'loss': 0.5497, 'grad_norm': 1.1038286685943604, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.26it/s] 47%|████▋     | 14/30 [00:02<00:03,  4.74it/s]                                               {'loss': 0.5589, 'grad_norm': 0.56981360912323, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  4.74it/s] 50%|█████     | 15/30 [00:03<00:03,  4.38it/s]                                               {'loss': 0.8235, 'grad_norm': 2.836682081222534, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.38it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.17it/s]                                               {'loss': 0.6076, 'grad_norm': 0.8298165202140808, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.17it/s] 57%|█████▋    | 17/30 [00:03<00:03,  4.27it/s]                                               {'loss': 0.4804, 'grad_norm': 1.5088376998901367, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  4.27it/s]                                               {'loss': 0.7358, 'grad_norm': 1.1862049102783203, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.27it/s] 63%|██████▎   | 19/30 [00:03<00:02,  5.04it/s]                                               {'loss': 0.5274, 'grad_norm': 0.9665462374687195, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  5.04it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.99it/s]                                               {'loss': 0.6681, 'grad_norm': 0.8185516595840454, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.99it/s] 70%|███████   | 21/30 [00:04<00:01,  4.94it/s]                                               {'loss': 0.5942, 'grad_norm': 0.5948647260665894, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  4.94it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.81it/s]                                               {'loss': 0.545, 'grad_norm': 0.8182544708251953, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.81it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.93it/s]                                               {'loss': 0.8632, 'grad_norm': 2.707021474838257, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.93it/s]                                               {'loss': 0.4697, 'grad_norm': 1.7301082611083984, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  4.93it/s] 83%|████████▎ | 25/30 [00:05<00:00,  5.53it/s]                                               {'loss': 0.5562, 'grad_norm': 0.5269181132316589, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  5.53it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.43it/s]                                               {'loss': 0.6396, 'grad_norm': 0.901508629322052, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.43it/s] 90%|█████████ | 27/30 [00:05<00:00,  5.77it/s]                                               {'loss': 0.5227, 'grad_norm': 0.933239221572876, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  5.77it/s] 93%|█████████▎| 28/30 [00:05<00:00,  5.66it/s]                                               {'loss': 0.66, 'grad_norm': 1.0919649600982666, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  5.66it/s] 97%|█████████▋| 29/30 [00:05<00:00,  5.55it/s]                                               {'loss': 0.5645, 'grad_norm': 1.2086607217788696, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  5.55it/s]                                               {'loss': 0.4147, 'grad_norm': 1.5233778953552246, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.55it/s]                                               {'train_runtime': 6.0049, 'train_samples_per_second': 70.775, 'train_steps_per_second': 4.996, 'train_loss': 0.6198077897230784, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.55it/s]100%|██████████| 30/30 [00:06<00:00,  5.00it/s]
CLIENT:80
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.37it/s]                                              {'loss': 0.6277, 'grad_norm': 0.7007676362991333, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.37it/s]  7%|▋         | 2/30 [00:00<00:04,  6.54it/s]                                              {'loss': 0.5536, 'grad_norm': 1.015824556350708, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.54it/s] 10%|█         | 3/30 [00:00<00:04,  6.30it/s]                                              {'loss': 0.3641, 'grad_norm': 1.416274070739746, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.30it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.50it/s]                                              {'loss': 0.2786, 'grad_norm': 0.8449075222015381, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.50it/s]                                              {'loss': 0.2612, 'grad_norm': 1.018212080001831, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.50it/s] 20%|██        | 6/30 [00:00<00:02,  8.70it/s]                                              {'loss': 0.8928, 'grad_norm': 3.7048964500427246, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02,  8.70it/s] 23%|██▎       | 7/30 [00:00<00:02,  7.95it/s]                                              {'loss': 0.2274, 'grad_norm': 2.389568328857422, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  7.95it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.83it/s]                                              {'loss': 0.8015, 'grad_norm': 4.00150203704834, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.83it/s] 30%|███       | 9/30 [00:01<00:02,  7.26it/s]                                              {'loss': 0.2178, 'grad_norm': 4.267513275146484, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.26it/s] 33%|███▎      | 10/30 [00:01<00:02,  6.77it/s]                                               {'loss': 0.4352, 'grad_norm': 6.086329936981201, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  6.77it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.30it/s]                                               {'loss': 0.0828, 'grad_norm': 3.9781007766723633, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.30it/s]                                               {'loss': 0.6602, 'grad_norm': 14.941079139709473, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.30it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.32it/s]                                               {'loss': 0.3862, 'grad_norm': 1.2051881551742554, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.32it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.01it/s]                                               {'loss': 0.2161, 'grad_norm': 4.052003860473633, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.01it/s] 50%|█████     | 15/30 [00:02<00:02,  6.19it/s]                                               {'loss': 0.9072, 'grad_norm': 7.204794883728027, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.19it/s] 53%|█████▎    | 16/30 [00:02<00:02,  5.99it/s]                                               {'loss': 0.455, 'grad_norm': 6.618232727050781, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  5.99it/s] 57%|█████▋    | 17/30 [00:02<00:02,  5.76it/s]                                               {'loss': 0.1594, 'grad_norm': 2.712836742401123, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  5.76it/s]                                               {'loss': 0.6931, 'grad_norm': 2.087141513824463, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:02,  5.76it/s] 63%|██████▎   | 19/30 [00:02<00:01,  6.91it/s]                                               {'loss': 0.1078, 'grad_norm': 2.0118799209594727, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  6.91it/s] 67%|██████▋   | 20/30 [00:02<00:01,  6.99it/s]                                               {'loss': 0.525, 'grad_norm': 1.7062792778015137, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  6.99it/s] 70%|███████   | 21/30 [00:03<00:01,  6.01it/s]                                               {'loss': 0.2763, 'grad_norm': 1.7016469240188599, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  6.01it/s] 73%|███████▎  | 22/30 [00:03<00:01,  5.88it/s]                                               {'loss': 0.5817, 'grad_norm': 1.7789480686187744, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  5.88it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.88it/s]                                               {'loss': 0.2396, 'grad_norm': 1.0779695510864258, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.88it/s] 80%|████████  | 24/30 [00:03<00:00,  6.03it/s]                                               {'loss': 0.4799, 'grad_norm': 1.272999882698059, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.03it/s] 83%|████████▎ | 25/30 [00:03<00:00,  5.76it/s]                                               {'loss': 0.2148, 'grad_norm': 1.6460269689559937, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  5.76it/s] 87%|████████▋ | 26/30 [00:03<00:00,  5.86it/s]                                               {'loss': 0.3584, 'grad_norm': 0.7227937579154968, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  5.86it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.54it/s]                                               {'loss': 0.3704, 'grad_norm': 1.1229984760284424, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.54it/s] 93%|█████████▎| 28/30 [00:04<00:00,  5.14it/s]                                               {'loss': 0.411, 'grad_norm': 2.4662933349609375, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  5.14it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.18it/s]                                               {'loss': 0.2289, 'grad_norm': 0.7166114449501038, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.18it/s]                                               {'loss': 0.5063, 'grad_norm': 4.354124546051025, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.18it/s]                                               {'train_runtime': 4.861, 'train_samples_per_second': 87.43, 'train_steps_per_second': 6.172, 'train_loss': 0.41733457098404564, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.18it/s]100%|██████████| 30/30 [00:04<00:00,  6.17it/s]
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.52it/s]                                              {'loss': 0.6888, 'grad_norm': 0.8841153979301453, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.52it/s]                                              {'loss': 0.7561, 'grad_norm': 0.8362323045730591, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.52it/s] 10%|█         | 3/30 [00:00<00:02, 10.86it/s]                                              {'loss': 0.7057, 'grad_norm': 0.4058668613433838, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.86it/s]                                              {'loss': 0.6521, 'grad_norm': 0.9173121452331543, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.86it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.48it/s]                                              {'loss': 0.7248, 'grad_norm': 0.6597058773040771, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.48it/s]                                              {'loss': 0.7029, 'grad_norm': 0.5055615901947021, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.48it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.47it/s]                                              {'loss': 0.6853, 'grad_norm': 0.3144643306732178, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  9.47it/s] 27%|██▋       | 8/30 [00:00<00:02,  7.73it/s]                                              {'loss': 0.7102, 'grad_norm': 0.41733574867248535, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  7.73it/s] 30%|███       | 9/30 [00:01<00:03,  6.89it/s]                                              {'loss': 0.7267, 'grad_norm': 0.3539634644985199, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.89it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.54it/s]                                               {'loss': 0.6781, 'grad_norm': 0.195914626121521, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.54it/s] 37%|███▋      | 11/30 [00:01<00:03,  6.18it/s]                                               {'loss': 0.6694, 'grad_norm': 0.5187716484069824, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  6.18it/s] 40%|████      | 12/30 [00:01<00:02,  6.18it/s]                                               {'loss': 0.5155, 'grad_norm': 0.9829460978507996, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.18it/s] 43%|████▎     | 13/30 [00:01<00:03,  5.52it/s]                                               {'loss': 0.6774, 'grad_norm': 0.5203406810760498, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:03,  5.52it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.39it/s]                                               {'loss': 0.6286, 'grad_norm': 0.3958926796913147, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.39it/s] 50%|█████     | 15/30 [00:02<00:03,  4.99it/s]                                               {'loss': 0.7465, 'grad_norm': 0.9490768313407898, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:03,  4.99it/s] 53%|█████▎    | 16/30 [00:02<00:03,  4.48it/s]                                               {'loss': 0.668, 'grad_norm': 0.4988964796066284, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:03,  4.48it/s] 57%|█████▋    | 17/30 [00:02<00:02,  4.44it/s]                                               {'loss': 0.6746, 'grad_norm': 0.46701744198799133, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  4.44it/s] 60%|██████    | 18/30 [00:03<00:02,  4.33it/s]                                               {'loss': 0.6553, 'grad_norm': 0.49170705676078796, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.33it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.36it/s]                                               {'loss': 0.7057, 'grad_norm': 0.6182738542556763, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.36it/s] 67%|██████▋   | 20/30 [00:03<00:02,  4.41it/s]                                               {'loss': 0.6934, 'grad_norm': 0.5554694533348083, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:02,  4.41it/s] 70%|███████   | 21/30 [00:03<00:02,  4.49it/s]                                               {'loss': 0.676, 'grad_norm': 0.39144715666770935, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:02,  4.49it/s] 73%|███████▎  | 22/30 [00:03<00:01,  4.45it/s]                                               {'loss': 0.6942, 'grad_norm': 0.5146713256835938, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  4.45it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.37it/s]                                               {'loss': 0.6587, 'grad_norm': 0.4391045868396759, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.37it/s] 80%|████████  | 24/30 [00:04<00:01,  4.53it/s]                                               {'loss': 0.6495, 'grad_norm': 0.46995019912719727, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  4.53it/s] 83%|████████▎ | 25/30 [00:04<00:01,  4.71it/s]                                               {'loss': 0.7057, 'grad_norm': 0.38282984495162964, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:01,  4.71it/s] 87%|████████▋ | 26/30 [00:04<00:00,  4.80it/s]                                               {'loss': 0.6687, 'grad_norm': 0.36368563771247864, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  4.80it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.55it/s]                                               {'loss': 0.6472, 'grad_norm': 0.478702574968338, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.55it/s] 93%|█████████▎| 28/30 [00:05<00:00,  5.74it/s]                                               {'loss': 0.6454, 'grad_norm': 0.20705491304397583, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  5.74it/s] 97%|█████████▋| 29/30 [00:05<00:00,  6.41it/s]                                               {'loss': 0.7093, 'grad_norm': 0.44281482696533203, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  6.41it/s]100%|██████████| 30/30 [00:05<00:00,  6.26it/s]                                               {'loss': 0.6511, 'grad_norm': 0.4042463004589081, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.26it/s]                                               {'train_runtime': 5.8406, 'train_samples_per_second': 72.766, 'train_steps_per_second': 5.136, 'train_loss': 0.679032051563263, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.26it/s]100%|██████████| 30/30 [00:05<00:00,  5.14it/s]
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.35it/s]                                              {'loss': 0.5429, 'grad_norm': 0.3323134481906891, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.35it/s]  7%|▋         | 2/30 [00:00<00:06,  4.66it/s]                                              {'loss': 0.7396, 'grad_norm': 0.6940767168998718, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.66it/s] 10%|█         | 3/30 [00:00<00:05,  4.81it/s]                                              {'loss': 0.7259, 'grad_norm': 0.5771657824516296, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.81it/s] 13%|█▎        | 4/30 [00:00<00:05,  5.08it/s]                                              {'loss': 0.6381, 'grad_norm': 0.5914069414138794, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  5.08it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.64it/s]                                              {'loss': 0.6791, 'grad_norm': 0.46004533767700195, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.64it/s]                                              {'loss': 0.7871, 'grad_norm': 1.5754685401916504, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.64it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.92it/s]                                              {'loss': 0.6424, 'grad_norm': 0.5827187895774841, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.92it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.88it/s]                                              {'loss': 0.6931, 'grad_norm': 0.4361380338668823, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.88it/s] 30%|███       | 9/30 [00:01<00:02,  7.01it/s]                                              {'loss': 0.6313, 'grad_norm': 0.38450488448143005, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.01it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.16it/s]                                               {'loss': 0.6692, 'grad_norm': 0.22819282114505768, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.16it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.17it/s]                                               {'loss': 0.6681, 'grad_norm': 0.20492105185985565, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.17it/s]                                               {'loss': 0.6355, 'grad_norm': 1.2692853212356567, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.17it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.78it/s]                                               {'loss': 0.6644, 'grad_norm': 0.8052757382392883, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.78it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.39it/s]                                               {'loss': 0.6718, 'grad_norm': 0.42716261744499207, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.39it/s] 50%|█████     | 15/30 [00:02<00:01,  7.82it/s]                                               {'loss': 0.6654, 'grad_norm': 0.8588471412658691, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.82it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.91it/s]                                               {'loss': 0.6002, 'grad_norm': 0.5135666131973267, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.91it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.84it/s]                                               {'loss': 0.6503, 'grad_norm': 0.38684868812561035, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.84it/s]                                               {'loss': 0.637, 'grad_norm': 0.5888269543647766, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.84it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.79it/s]                                               {'loss': 0.6334, 'grad_norm': 0.8219261765480042, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.79it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.45it/s]                                               {'loss': 0.6345, 'grad_norm': 0.3489810824394226, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.45it/s] 70%|███████   | 21/30 [00:02<00:01,  8.15it/s]                                               {'loss': 0.6299, 'grad_norm': 0.4063757061958313, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.15it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.99it/s]                                               {'loss': 0.6179, 'grad_norm': 0.5095772743225098, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.99it/s] 77%|███████▋  | 23/30 [00:03<00:00,  7.98it/s]                                               {'loss': 0.581, 'grad_norm': 0.480997771024704, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.98it/s]                                               {'loss': 0.5314, 'grad_norm': 0.7003899812698364, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.98it/s] 83%|████████▎ | 25/30 [00:03<00:00,  8.83it/s]                                               {'loss': 0.7148, 'grad_norm': 0.7760977149009705, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  8.83it/s] 87%|████████▋ | 26/30 [00:03<00:00,  8.54it/s]                                               {'loss': 0.5833, 'grad_norm': 0.6849678158760071, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  8.54it/s] 90%|█████████ | 27/30 [00:03<00:00,  8.12it/s]                                               {'loss': 0.6109, 'grad_norm': 0.5320348739624023, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  8.12it/s] 93%|█████████▎| 28/30 [00:03<00:00,  7.93it/s]                                               {'loss': 0.5921, 'grad_norm': 0.45830807089805603, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  7.93it/s] 97%|█████████▋| 29/30 [00:03<00:00,  7.68it/s]                                               {'loss': 0.5245, 'grad_norm': 0.8357928991317749, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  7.68it/s]                                               {'loss': 0.5786, 'grad_norm': 0.7819607257843018, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  7.68it/s]                                               {'train_runtime': 4.0848, 'train_samples_per_second': 104.045, 'train_steps_per_second': 7.344, 'train_loss': 0.6391143759091695, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.68it/s]100%|██████████| 30/30 [00:04<00:00,  7.35it/s]
CLIENT:43
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.32it/s]                                              {'loss': 0.5379, 'grad_norm': 0.4597685933113098, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.32it/s]  7%|▋         | 2/30 [00:00<00:03,  7.74it/s]                                              {'loss': 0.7718, 'grad_norm': 1.219869613647461, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.74it/s] 10%|█         | 3/30 [00:00<00:03,  7.13it/s]                                              {'loss': 0.7299, 'grad_norm': 0.5379438400268555, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.13it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.41it/s]                                              {'loss': 0.7469, 'grad_norm': 1.0939141511917114, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.41it/s] 17%|█▋        | 5/30 [00:00<00:03,  8.07it/s]                                              {'loss': 0.6736, 'grad_norm': 0.3037467300891876, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  8.07it/s]                                              {'loss': 0.7669, 'grad_norm': 1.3577905893325806, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02,  8.07it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.82it/s]                                              {'loss': 0.6884, 'grad_norm': 0.4107373356819153, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.82it/s] 27%|██▋       | 8/30 [00:00<00:02,  8.37it/s]                                              {'loss': 0.6863, 'grad_norm': 0.236302450299263, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  8.37it/s] 30%|███       | 9/30 [00:01<00:02,  8.35it/s]                                              {'loss': 0.662, 'grad_norm': 0.27035653591156006, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  8.35it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.59it/s]                                               {'loss': 0.6636, 'grad_norm': 0.28594592213630676, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.59it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.84it/s]                                               {'loss': 0.6748, 'grad_norm': 0.18145623803138733, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.84it/s]                                               {'loss': 0.6376, 'grad_norm': 1.3635226488113403, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.84it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.43it/s]                                               {'loss': 0.6503, 'grad_norm': 0.3772719204425812, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.43it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.53it/s]                                               {'loss': 0.6161, 'grad_norm': 0.40015411376953125, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.53it/s] 50%|█████     | 15/30 [00:01<00:01,  8.52it/s]                                               {'loss': 0.692, 'grad_norm': 0.7727009057998657, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  8.52it/s] 53%|█████▎    | 16/30 [00:01<00:01,  7.88it/s]                                               {'loss': 0.6458, 'grad_norm': 0.33605024218559265, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  7.88it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.75it/s]                                               {'loss': 0.6614, 'grad_norm': 0.29993003606796265, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.75it/s] 60%|██████    | 18/30 [00:02<00:01,  7.81it/s]                                               {'loss': 0.7145, 'grad_norm': 0.6329452991485596, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.81it/s] 63%|██████▎   | 19/30 [00:02<00:01,  6.20it/s]                                               {'loss': 0.6859, 'grad_norm': 0.8080011606216431, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  6.20it/s] 67%|██████▋   | 20/30 [00:02<00:01,  5.42it/s]                                               {'loss': 0.6565, 'grad_norm': 0.1768110692501068, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  5.42it/s] 70%|███████   | 21/30 [00:02<00:01,  5.10it/s]                                               {'loss': 0.685, 'grad_norm': 0.26819634437561035, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  5.10it/s] 73%|███████▎  | 22/30 [00:03<00:01,  5.29it/s]                                               {'loss': 0.6575, 'grad_norm': 0.4043060839176178, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  5.29it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.71it/s]                                               {'loss': 0.6535, 'grad_norm': 0.3793719708919525, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.71it/s]                                               {'loss': 0.6583, 'grad_norm': 0.3663519024848938, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:01,  5.71it/s] 83%|████████▎ | 25/30 [00:03<00:00,  6.64it/s]                                               {'loss': 0.7424, 'grad_norm': 0.7347246408462524, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  6.64it/s] 87%|████████▋ | 26/30 [00:03<00:00,  6.46it/s]                                               {'loss': 0.6313, 'grad_norm': 0.3131698668003082, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  6.46it/s] 90%|█████████ | 27/30 [00:03<00:00,  6.44it/s]                                               {'loss': 0.6505, 'grad_norm': 0.43941211700439453, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  6.44it/s] 93%|█████████▎| 28/30 [00:03<00:00,  6.90it/s]                                               {'loss': 0.6594, 'grad_norm': 0.33283117413520813, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  6.90it/s] 97%|█████████▋| 29/30 [00:04<00:00,  7.14it/s]                                               {'loss': 0.619, 'grad_norm': 0.5138232707977295, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  7.14it/s]                                               {'loss': 0.5985, 'grad_norm': 0.40348848700523376, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.14it/s]                                               {'train_runtime': 4.2426, 'train_samples_per_second': 100.174, 'train_steps_per_second': 7.071, 'train_loss': 0.6705928206443786, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.14it/s]100%|██████████| 30/30 [00:04<00:00,  7.08it/s]
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:28,  1.00it/s]                                              {'loss': 0.682, 'grad_norm': 0.6487892270088196, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:28,  1.00it/s]                                              {'loss': 0.7365, 'grad_norm': 0.8148269057273865, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:27,  1.00it/s] 10%|█         | 3/30 [00:01<00:08,  3.14it/s]                                              {'loss': 0.7168, 'grad_norm': 0.4599253237247467, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:08,  3.14it/s]                                              {'loss': 0.6823, 'grad_norm': 0.5245240926742554, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  3.14it/s] 17%|█▋        | 5/30 [00:01<00:04,  5.19it/s]                                              {'loss': 0.701, 'grad_norm': 0.39455899596214294, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:04,  5.19it/s]                                              {'loss': 0.7474, 'grad_norm': 1.4640787839889526, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.19it/s] 23%|██▎       | 7/30 [00:01<00:03,  7.32it/s]                                              {'loss': 0.6218, 'grad_norm': 0.36953848600387573, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  7.32it/s]                                              {'loss': 0.6929, 'grad_norm': 0.3168694078922272, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  7.32it/s] 30%|███       | 9/30 [00:01<00:02,  7.86it/s]                                              {'loss': 0.7516, 'grad_norm': 0.4121895134449005, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.86it/s]                                              {'loss': 0.6586, 'grad_norm': 0.17572355270385742, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.86it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.24it/s]                                               {'loss': 0.6775, 'grad_norm': 0.21055461466312408, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.24it/s]                                               {'loss': 0.4848, 'grad_norm': 1.1582809686660767, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  7.24it/s] 43%|████▎     | 13/30 [00:02<00:02,  8.17it/s]                                               {'loss': 0.6576, 'grad_norm': 0.9787609577178955, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  8.17it/s]                                               {'loss': 0.61, 'grad_norm': 1.306656837463379, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  8.17it/s] 50%|█████     | 15/30 [00:02<00:01,  7.83it/s]                                               {'loss': 0.7153, 'grad_norm': 1.8279019594192505, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.83it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.60it/s]                                               {'loss': 0.6314, 'grad_norm': 0.4232509136199951, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.60it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.36it/s]                                               {'loss': 0.6398, 'grad_norm': 0.488411545753479, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.36it/s]                                               {'loss': 0.6263, 'grad_norm': 0.5973438024520874, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.36it/s] 63%|██████▎   | 19/30 [00:02<00:01,  7.76it/s]                                               {'loss': 0.5714, 'grad_norm': 1.715735673904419, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.76it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.19it/s]                                               {'loss': 0.7247, 'grad_norm': 1.135233998298645, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.19it/s] 70%|███████   | 21/30 [00:03<00:01,  7.15it/s]                                               {'loss': 0.7104, 'grad_norm': 0.603685736656189, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.15it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.91it/s]                                               {'loss': 0.6184, 'grad_norm': 0.7819553017616272, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.91it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.87it/s]                                               {'loss': 0.7521, 'grad_norm': 1.5122252702713013, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.87it/s]                                               {'loss': 0.5679, 'grad_norm': 2.509436845779419, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.87it/s] 83%|████████▎ | 25/30 [00:03<00:00,  7.68it/s]                                               {'loss': 0.6922, 'grad_norm': 0.5544222593307495, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  7.68it/s] 87%|████████▋ | 26/30 [00:04<00:00,  6.77it/s]                                               {'loss': 0.595, 'grad_norm': 0.6705347895622253, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  6.77it/s] 90%|█████████ | 27/30 [00:04<00:00,  6.24it/s]                                               {'loss': 0.6718, 'grad_norm': 0.5806758403778076, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  6.24it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.09it/s]                                               {'loss': 0.7137, 'grad_norm': 0.9033098816871643, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.09it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.72it/s]                                               {'loss': 0.6112, 'grad_norm': 0.6382206678390503, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.72it/s]                                               {'loss': 0.607, 'grad_norm': 0.6388480067253113, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.72it/s]                                               {'train_runtime': 4.7704, 'train_samples_per_second': 89.092, 'train_steps_per_second': 6.289, 'train_loss': 0.6623150169849396, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.72it/s]100%|██████████| 30/30 [00:04<00:00,  6.29it/s]
CLIENT:7
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.6482, 'grad_norm': 0.48440036177635193, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 10.41it/s]  7%|▋         | 2/30 [00:00<00:02, 11.61it/s]                                              {'loss': 0.67, 'grad_norm': 0.734569251537323, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 11.61it/s]                                              {'loss': 0.6706, 'grad_norm': 0.8394435048103333, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.61it/s] 13%|█▎        | 4/30 [00:00<00:02, 10.29it/s]                                              {'loss': 0.6089, 'grad_norm': 0.8721417188644409, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.29it/s]                                              {'loss': 0.705, 'grad_norm': 1.4170769453048706, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.29it/s] 20%|██        | 6/30 [00:00<00:01, 12.12it/s]                                              {'loss': 0.961, 'grad_norm': 1.9929624795913696, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 12.12it/s]                                              {'loss': 0.5588, 'grad_norm': 0.6798190474510193, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 12.12it/s] 27%|██▋       | 8/30 [00:00<00:01, 12.44it/s]                                              {'loss': 0.6662, 'grad_norm': 5.735291957855225, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 12.44it/s]                                              {'loss': 0.7647, 'grad_norm': 1.6427652835845947, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 12.44it/s] 33%|███▎      | 10/30 [00:00<00:01, 11.98it/s]                                               {'loss': 0.5653, 'grad_norm': 0.701953113079071, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.98it/s]                                               {'loss': 0.4908, 'grad_norm': 1.2453519105911255, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 11.98it/s] 40%|████      | 12/30 [00:00<00:01, 12.44it/s]                                               {'loss': 0.5102, 'grad_norm': 1.149135708808899, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 12.44it/s]                                               {'loss': 0.7596, 'grad_norm': 0.9438140392303467, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 12.44it/s] 47%|████▋     | 14/30 [00:01<00:01, 12.41it/s]                                               {'loss': 0.5697, 'grad_norm': 0.7509909272193909, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 12.41it/s]                                               {'loss': 0.6497, 'grad_norm': 1.0618948936462402, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 12.41it/s] 53%|█████▎    | 16/30 [00:01<00:01, 12.72it/s]                                               {'loss': 0.5963, 'grad_norm': 0.7650233507156372, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 12.72it/s]                                               {'loss': 0.6025, 'grad_norm': 1.32370126247406, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.72it/s] 60%|██████    | 18/30 [00:01<00:00, 14.25it/s]                                               {'loss': 0.6839, 'grad_norm': 1.0355395078659058, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 14.25it/s]                                               {'loss': 0.4932, 'grad_norm': 1.595774531364441, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.25it/s] 67%|██████▋   | 20/30 [00:01<00:00, 13.84it/s]                                               {'loss': 0.6627, 'grad_norm': 0.5598867535591125, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.84it/s]                                               {'loss': 0.6292, 'grad_norm': 1.1532946825027466, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.84it/s] 73%|███████▎  | 22/30 [00:01<00:00, 13.62it/s]                                               {'loss': 0.6275, 'grad_norm': 0.712239682674408, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.62it/s]                                               {'loss': 0.5654, 'grad_norm': 0.8759034276008606, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.62it/s] 80%|████████  | 24/30 [00:01<00:00, 15.01it/s]                                               {'loss': 0.702, 'grad_norm': 0.9447341561317444, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 15.01it/s]                                               {'loss': 0.6196, 'grad_norm': 0.5370454788208008, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 15.01it/s] 87%|████████▋ | 26/30 [00:01<00:00, 13.92it/s]                                               {'loss': 0.5739, 'grad_norm': 0.6895399689674377, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 13.92it/s]                                               {'loss': 0.5488, 'grad_norm': 1.306820273399353, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.92it/s] 93%|█████████▎| 28/30 [00:02<00:00, 13.74it/s]                                               {'loss': 0.5786, 'grad_norm': 0.805790364742279, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.74it/s]                                               {'loss': 0.5987, 'grad_norm': 0.88437819480896, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.74it/s]100%|██████████| 30/30 [00:02<00:00, 15.07it/s]                                               {'loss': 0.6319, 'grad_norm': 1.2930991649627686, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 15.07it/s]                                               {'train_runtime': 2.3233, 'train_samples_per_second': 182.927, 'train_steps_per_second': 12.913, 'train_loss': 0.6304266969362895, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 15.07it/s]100%|██████████| 30/30 [00:02<00:00, 12.92it/s]
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:02,  9.71it/s]                                              {'loss': 0.9842, 'grad_norm': 2.0161242485046387, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02,  9.71it/s]                                              {'loss': 0.5864, 'grad_norm': 1.085045337677002, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02,  9.71it/s] 10%|█         | 3/30 [00:00<00:02, 11.04it/s]                                              {'loss': 0.42, 'grad_norm': 1.5830849409103394, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.04it/s]                                              {'loss': 0.3752, 'grad_norm': 1.2272568941116333, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.04it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.94it/s]                                              {'loss': 0.1316, 'grad_norm': 1.176522970199585, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.94it/s]                                              {'loss': 0.0376, 'grad_norm': 0.6250765323638916, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.94it/s] 23%|██▎       | 7/30 [00:00<00:01, 12.98it/s]                                              {'loss': 0.0509, 'grad_norm': 0.7619750499725342, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 12.98it/s]                                              {'loss': 0.0136, 'grad_norm': 0.35880786180496216, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 12.98it/s] 30%|███       | 9/30 [00:00<00:01, 12.98it/s]                                              {'loss': 0.0118, 'grad_norm': 0.6542733311653137, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 12.98it/s]                                              {'loss': 0.237, 'grad_norm': 1.6744003295898438, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 12.98it/s] 37%|███▋      | 11/30 [00:00<00:01, 13.13it/s]                                               {'loss': 0.0016, 'grad_norm': 0.02368435636162758, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.13it/s]                                               {'loss': 0.0023, 'grad_norm': 0.05467021092772484, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 13.13it/s] 43%|████▎     | 13/30 [00:00<00:01, 13.91it/s]                                               {'loss': 0.0013, 'grad_norm': 0.017307355999946594, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 13.91it/s]                                               {'loss': 0.0017, 'grad_norm': 0.03558754920959473, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.91it/s] 50%|█████     | 15/30 [00:01<00:01, 13.62it/s]                                               {'loss': 0.4562, 'grad_norm': 1.4138628244400024, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.62it/s]                                               {'loss': 0.0016, 'grad_norm': 0.02592976577579975, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.62it/s] 57%|█████▋    | 17/30 [00:01<00:00, 13.43it/s]                                               {'loss': 0.0031, 'grad_norm': 0.0501333512365818, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.43it/s]                                               {'loss': 0.0016, 'grad_norm': 0.030667103826999664, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.43it/s] 63%|██████▎   | 19/30 [00:01<00:00, 14.56it/s]                                               {'loss': 0.0019, 'grad_norm': 0.02694862335920334, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.56it/s]                                               {'loss': 0.0018, 'grad_norm': 0.027213377878069878, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.56it/s] 70%|███████   | 21/30 [00:01<00:00, 14.02it/s]                                               {'loss': 0.0023, 'grad_norm': 0.032192692160606384, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.02it/s]                                               {'loss': 0.0027, 'grad_norm': 0.04815196245908737, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 14.02it/s] 77%|███████▋  | 23/30 [00:01<00:00, 13.32it/s]                                               {'loss': 0.4442, 'grad_norm': 2.3112761974334717, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.32it/s]                                               {'loss': 0.0034, 'grad_norm': 0.05784108117222786, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 13.32it/s] 83%|████████▎ | 25/30 [00:01<00:00, 14.07it/s]                                               {'loss': 0.0039, 'grad_norm': 0.052641939371824265, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.07it/s]                                               {'loss': 0.0038, 'grad_norm': 0.05567127838730812, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 14.07it/s] 90%|█████████ | 27/30 [00:02<00:00, 13.68it/s]                                               {'loss': 0.3172, 'grad_norm': 1.3147056102752686, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.68it/s]                                               {'loss': 0.0033, 'grad_norm': 0.04964581876993179, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.68it/s] 97%|█████████▋| 29/30 [00:02<00:00, 13.40it/s]                                               {'loss': 0.0029, 'grad_norm': 0.05199353024363518, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.40it/s]                                               {'loss': 0.0043, 'grad_norm': 0.07717021554708481, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.40it/s]                                               {'train_runtime': 2.3069, 'train_samples_per_second': 184.234, 'train_steps_per_second': 13.005, 'train_loss': 0.13698408410418778, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.40it/s]100%|██████████| 30/30 [00:02<00:00, 13.01it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  6%|▌         | 4/66 [00:00<00:01, 32.26it/s] 12%|█▏        | 8/66 [00:00<00:01, 30.55it/s] 18%|█▊        | 12/66 [00:00<00:01, 28.57it/s] 23%|██▎       | 15/66 [00:00<00:02, 24.21it/s] 27%|██▋       | 18/66 [00:00<00:02, 21.30it/s] 32%|███▏      | 21/66 [00:00<00:02, 18.60it/s] 35%|███▍      | 23/66 [00:01<00:02, 16.90it/s] 38%|███▊      | 25/66 [00:01<00:02, 16.36it/s] 41%|████      | 27/66 [00:01<00:02, 15.52it/s] 44%|████▍     | 29/66 [00:01<00:02, 15.03it/s] 47%|████▋     | 31/66 [00:01<00:02, 15.24it/s] 50%|█████     | 33/66 [00:01<00:02, 14.77it/s] 53%|█████▎    | 35/66 [00:01<00:02, 15.08it/s] 56%|█████▌    | 37/66 [00:02<00:01, 14.97it/s] 59%|█████▉    | 39/66 [00:02<00:01, 15.43it/s] 62%|██████▏   | 41/66 [00:02<00:01, 15.36it/s] 65%|██████▌   | 43/66 [00:02<00:01, 15.08it/s] 68%|██████▊   | 45/66 [00:02<00:01, 14.03it/s] 71%|███████   | 47/66 [00:02<00:01, 13.24it/s] 74%|███████▍  | 49/66 [00:02<00:01, 13.09it/s] 77%|███████▋  | 51/66 [00:03<00:01, 12.93it/s] 80%|████████  | 53/66 [00:03<00:01, 11.71it/s] 83%|████████▎ | 55/66 [00:03<00:00, 12.60it/s] 86%|████████▋ | 57/66 [00:03<00:00, 12.94it/s] 89%|████████▉ | 59/66 [00:03<00:00, 14.01it/s] 92%|█████████▏| 61/66 [00:03<00:00, 13.66it/s] 95%|█████████▌| 63/66 [00:04<00:00, 13.63it/s] 98%|█████████▊| 65/66 [00:04<00:00, 13.83it/s]100%|██████████| 66/66 [00:04<00:00, 15.74it/s]
{'eval_loss': 0.6658759117126465, 'eval_model_preparation_time': 0.006, 'eval_acc': 0.6663470757430489, 'eval_runtime': 4.2633, 'eval_samples_per_second': 244.647, 'eval_steps_per_second': 15.481}
ROUND:1
CLIENT:26
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:09,  3.19it/s]                                              {'loss': 0.8481, 'grad_norm': 0.9939854741096497, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:09,  3.19it/s]  7%|▋         | 2/30 [00:00<00:14,  1.98it/s]                                              {'loss': 0.5913, 'grad_norm': 0.9928523898124695, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:14,  1.98it/s] 10%|█         | 3/30 [00:01<00:09,  2.92it/s]                                              {'loss': 0.6816, 'grad_norm': 0.6970906853675842, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:09,  2.92it/s] 13%|█▎        | 4/30 [00:01<00:06,  3.81it/s]                                              {'loss': 0.9717, 'grad_norm': 2.213484048843384, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:06,  3.81it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.58it/s]                                              {'loss': 0.7828, 'grad_norm': 1.3857306241989136, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.58it/s] 20%|██        | 6/30 [00:01<00:04,  5.18it/s]                                              {'loss': 1.4513, 'grad_norm': 4.373118877410889, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.18it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.55it/s]                                              {'loss': 0.5706, 'grad_norm': 0.5120501518249512, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.55it/s]                                              {'loss': 0.8036, 'grad_norm': 1.3628700971603394, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.55it/s] 30%|███       | 9/30 [00:01<00:02,  7.58it/s]                                              {'loss': 0.6927, 'grad_norm': 0.9294852614402771, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.58it/s]                                              {'loss': 0.6187, 'grad_norm': 0.6464499831199646, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.58it/s] 37%|███▋      | 11/30 [00:02<00:02,  8.90it/s]                                               {'loss': 0.5776, 'grad_norm': 1.0841732025146484, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:02,  8.90it/s]                                               {'loss': 0.51, 'grad_norm': 0.9796513319015503, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  8.90it/s] 43%|████▎     | 13/30 [00:02<00:01,  9.86it/s]                                               {'loss': 0.6657, 'grad_norm': 0.8593305945396423, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:01,  9.86it/s]                                               {'loss': 0.5902, 'grad_norm': 0.9734364748001099, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  9.86it/s] 50%|█████     | 15/30 [00:02<00:01,  8.97it/s]                                               {'loss': 0.7839, 'grad_norm': 1.4562803506851196, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.97it/s] 53%|█████▎    | 16/30 [00:02<00:01,  8.48it/s]                                               {'loss': 0.57, 'grad_norm': 1.0432212352752686, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.48it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.96it/s]                                               {'loss': 0.6397, 'grad_norm': 0.6289942860603333, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.96it/s]                                               {'loss': 0.7268, 'grad_norm': 1.1072614192962646, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.96it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.09it/s]                                               {'loss': 0.6552, 'grad_norm': 0.7572056651115417, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.09it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.95it/s]                                               {'loss': 0.6131, 'grad_norm': 0.5301802754402161, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.95it/s] 70%|███████   | 21/30 [00:03<00:01,  7.73it/s]                                               {'loss': 0.5765, 'grad_norm': 1.0876046419143677, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.73it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.48it/s]                                               {'loss': 0.6123, 'grad_norm': 0.6175093650817871, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.48it/s] 77%|███████▋  | 23/30 [00:03<00:00,  7.44it/s]                                               {'loss': 0.5746, 'grad_norm': 0.7031671404838562, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.44it/s]                                               {'loss': 0.5925, 'grad_norm': 1.3546233177185059, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.44it/s] 83%|████████▎ | 25/30 [00:03<00:00,  8.01it/s]                                               {'loss': 0.5788, 'grad_norm': 0.7941539287567139, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  8.01it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.93it/s]                                               {'loss': 0.5633, 'grad_norm': 0.6009547114372253, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.93it/s] 90%|█████████ | 27/30 [00:03<00:00,  8.11it/s]                                               {'loss': 0.5976, 'grad_norm': 0.7490911483764648, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  8.11it/s] 93%|█████████▎| 28/30 [00:04<00:00,  8.15it/s]                                               {'loss': 0.5893, 'grad_norm': 0.7535085082054138, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  8.15it/s] 97%|█████████▋| 29/30 [00:04<00:00,  8.25it/s]                                               {'loss': 0.7261, 'grad_norm': 1.881557822227478, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  8.25it/s]                                               {'loss': 0.4421, 'grad_norm': 1.8010694980621338, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  8.25it/s]                                               {'train_runtime': 4.4318, 'train_samples_per_second': 95.898, 'train_steps_per_second': 6.769, 'train_loss': 0.6732438017924627, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  8.25it/s]100%|██████████| 30/30 [00:04<00:00,  6.80it/s]
CLIENT:80
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.75it/s]                                              {'loss': 0.6176, 'grad_norm': 0.6980419158935547, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.75it/s]  7%|▋         | 2/30 [00:00<00:04,  5.85it/s]                                              {'loss': 0.5229, 'grad_norm': 1.1136494874954224, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  5.85it/s] 10%|█         | 3/30 [00:00<00:05,  5.16it/s]                                              {'loss': 0.3193, 'grad_norm': 1.1630958318710327, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  5.16it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.87it/s]                                              {'loss': 0.2532, 'grad_norm': 0.761917769908905, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.87it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.11it/s]                                              {'loss': 0.2957, 'grad_norm': 1.5136737823486328, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.11it/s] 20%|██        | 6/30 [00:01<00:05,  4.14it/s]                                              {'loss': 0.9354, 'grad_norm': 3.8062055110931396, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.14it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.43it/s]                                              {'loss': 0.1783, 'grad_norm': 1.156166434288025, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.43it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.84it/s]                                              {'loss': 0.8729, 'grad_norm': 4.63914155960083, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.84it/s] 30%|███       | 9/30 [00:01<00:04,  5.15it/s]                                              {'loss': 0.2267, 'grad_norm': 7.384729385375977, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  5.15it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.16it/s]                                               {'loss': 0.4769, 'grad_norm': 3.9581143856048584, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.16it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.48it/s]                                               {'loss': 0.0682, 'grad_norm': 1.8610445261001587, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.48it/s]                                               {'loss': 0.7531, 'grad_norm': 5.828464031219482, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.48it/s] 43%|████▎     | 13/30 [00:02<00:02,  5.92it/s]                                               {'loss': 0.3144, 'grad_norm': 4.121632099151611, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  5.92it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.84it/s]                                               {'loss': 0.1861, 'grad_norm': 2.2029528617858887, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.84it/s] 50%|█████     | 15/30 [00:02<00:02,  5.66it/s]                                               {'loss': 0.7056, 'grad_norm': 6.821364879608154, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.66it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.70it/s]                                               {'loss': 0.4168, 'grad_norm': 9.3563814163208, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.70it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.81it/s]                                               {'loss': 0.1199, 'grad_norm': 2.199890613555908, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.81it/s] 60%|██████    | 18/30 [00:03<00:01,  6.60it/s]                                               {'loss': 0.7611, 'grad_norm': 3.066063165664673, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  6.60it/s] 63%|██████▎   | 19/30 [00:03<00:01,  6.78it/s]                                               {'loss': 0.1066, 'grad_norm': 1.4124525785446167, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  6.78it/s] 67%|██████▋   | 20/30 [00:03<00:01,  6.90it/s]                                               {'loss': 0.558, 'grad_norm': 4.660811901092529, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  6.90it/s] 70%|███████   | 21/30 [00:03<00:01,  6.84it/s]                                               {'loss': 0.2781, 'grad_norm': 1.6065809726715088, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  6.84it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.82it/s]                                               {'loss': 0.4944, 'grad_norm': 2.4395811557769775, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.82it/s] 77%|███████▋  | 23/30 [00:03<00:00,  7.21it/s]                                               {'loss': 0.2112, 'grad_norm': 0.9428426027297974, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.21it/s] 80%|████████  | 24/30 [00:04<00:00,  7.71it/s]                                               {'loss': 0.4118, 'grad_norm': 4.416589736938477, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  7.71it/s] 83%|████████▎ | 25/30 [00:04<00:00,  7.31it/s]                                               {'loss': 0.2069, 'grad_norm': 1.4288920164108276, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  7.31it/s] 87%|████████▋ | 26/30 [00:04<00:00,  7.82it/s]                                               {'loss': 0.326, 'grad_norm': 1.1561217308044434, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  7.82it/s]                                               {'loss': 0.3343, 'grad_norm': 1.1429475545883179, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  7.82it/s] 93%|█████████▎| 28/30 [00:04<00:00,  9.57it/s]                                               {'loss': 0.3947, 'grad_norm': 7.794069766998291, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  9.57it/s]                                               {'loss': 0.1802, 'grad_norm': 1.6832777261734009, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  9.57it/s]100%|██████████| 30/30 [00:04<00:00, 11.96it/s]                                               {'loss': 0.437, 'grad_norm': 4.7611541748046875, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00, 11.96it/s]                                               {'train_runtime': 4.7006, 'train_samples_per_second': 90.415, 'train_steps_per_second': 6.382, 'train_loss': 0.398775785168012, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00, 11.96it/s]100%|██████████| 30/30 [00:04<00:00,  6.39it/s]
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  7.03it/s]                                              {'loss': 0.8495, 'grad_norm': 1.10186767578125, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  7.03it/s]                                              {'loss': 0.6397, 'grad_norm': 0.5209438800811768, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.03it/s] 10%|█         | 3/30 [00:00<00:02, 10.09it/s]                                              {'loss': 0.5446, 'grad_norm': 0.4761994183063507, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.09it/s]                                              {'loss': 0.5479, 'grad_norm': 0.7576579451560974, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.09it/s] 17%|█▋        | 5/30 [00:00<00:02, 10.38it/s]                                              {'loss': 0.8525, 'grad_norm': 2.0076918601989746, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.38it/s]                                              {'loss': 0.9765, 'grad_norm': 2.036756992340088, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 10.38it/s] 23%|██▎       | 7/30 [00:00<00:02, 10.70it/s]                                              {'loss': 0.4957, 'grad_norm': 0.5811898112297058, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02, 10.70it/s]                                              {'loss': 0.705, 'grad_norm': 1.6262770891189575, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02, 10.70it/s] 30%|███       | 9/30 [00:00<00:01, 11.48it/s]                                              {'loss': 0.5859, 'grad_norm': 1.3349597454071045, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 11.48it/s]                                              {'loss': 0.5784, 'grad_norm': 1.2733079195022583, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.48it/s] 37%|███▋      | 11/30 [00:00<00:01, 12.03it/s]                                               {'loss': 0.757, 'grad_norm': 2.307206392288208, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.03it/s]                                               {'loss': 0.5715, 'grad_norm': 2.7405688762664795, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 12.03it/s] 43%|████▎     | 13/30 [00:01<00:01, 13.45it/s]                                               {'loss': 0.5736, 'grad_norm': 1.778708577156067, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 13.45it/s]                                               {'loss': 0.4978, 'grad_norm': 1.0013526678085327, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.45it/s] 50%|█████     | 15/30 [00:01<00:01, 13.12it/s]                                               {'loss': 0.8737, 'grad_norm': 3.004389762878418, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.12it/s]                                               {'loss': 0.8061, 'grad_norm': 3.7812376022338867, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.12it/s] 57%|█████▋    | 17/30 [00:01<00:00, 13.21it/s]                                               {'loss': 0.5777, 'grad_norm': 2.4400479793548584, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.21it/s]                                               {'loss': 0.8833, 'grad_norm': 4.484184741973877, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.21it/s] 63%|██████▎   | 19/30 [00:01<00:00, 14.06it/s]                                               {'loss': 0.7396, 'grad_norm': 3.063817024230957, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.06it/s]                                               {'loss': 0.4284, 'grad_norm': 2.0291152000427246, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.06it/s] 70%|███████   | 21/30 [00:01<00:00, 13.23it/s]                                               {'loss': 0.7846, 'grad_norm': 1.5302807092666626, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.23it/s]                                               {'loss': 0.7532, 'grad_norm': 1.7729851007461548, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.23it/s] 77%|███████▋  | 23/30 [00:01<00:00, 12.65it/s]                                               {'loss': 0.6308, 'grad_norm': 1.3050438165664673, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 12.65it/s]                                               {'loss': 0.4837, 'grad_norm': 2.1387574672698975, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 12.65it/s] 83%|████████▎ | 25/30 [00:02<00:00, 13.27it/s]                                               {'loss': 0.6612, 'grad_norm': 2.0627193450927734, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 13.27it/s]                                               {'loss': 0.692, 'grad_norm': 1.0532982349395752, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.27it/s] 90%|█████████ | 27/30 [00:02<00:00, 12.30it/s]                                               {'loss': 0.6034, 'grad_norm': 2.3813416957855225, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 12.30it/s]                                               {'loss': 0.4764, 'grad_norm': 2.2661144733428955, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.30it/s] 97%|█████████▋| 29/30 [00:02<00:00, 12.28it/s]                                               {'loss': 0.7603, 'grad_norm': 2.256270408630371, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.28it/s]                                               {'loss': 0.783, 'grad_norm': 3.0154011249542236, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.28it/s]                                               {'train_runtime': 2.5329, 'train_samples_per_second': 167.789, 'train_steps_per_second': 11.844, 'train_loss': 0.6704300532738368, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.28it/s]100%|██████████| 30/30 [00:02<00:00, 11.87it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  7.05it/s]                                              {'loss': 0.5316, 'grad_norm': 0.2725612223148346, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  7.05it/s]                                              {'loss': 0.6674, 'grad_norm': 1.2100982666015625, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.05it/s] 10%|█         | 3/30 [00:00<00:02, 10.42it/s]                                              {'loss': 0.6816, 'grad_norm': 0.6912403702735901, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.42it/s]                                              {'loss': 0.7906, 'grad_norm': 1.4757755994796753, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.42it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.47it/s]                                              {'loss': 0.7194, 'grad_norm': 9.015792846679688, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.47it/s]                                              {'loss': 0.8917, 'grad_norm': 2.026684522628784, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:02, 11.47it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.58it/s]                                              {'loss': 0.6147, 'grad_norm': 0.8385410308837891, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.58it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.13it/s]                                              {'loss': 0.6784, 'grad_norm': 0.6350992321968079, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.13it/s] 30%|███       | 9/30 [00:01<00:03,  5.71it/s]                                              {'loss': 0.7128, 'grad_norm': 0.8007116913795471, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.71it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.23it/s]                                               {'loss': 0.6301, 'grad_norm': 0.7173876166343689, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.23it/s]                                               {'loss': 0.6602, 'grad_norm': 0.42931172251701355, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  6.23it/s] 40%|████      | 12/30 [00:01<00:02,  7.79it/s]                                               {'loss': 0.6698, 'grad_norm': 0.8478082418441772, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.79it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.72it/s]                                               {'loss': 0.6029, 'grad_norm': 0.42314302921295166, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.72it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.81it/s]                                               {'loss': 0.6703, 'grad_norm': 0.8196530342102051, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.81it/s]                                               {'loss': 0.649, 'grad_norm': 1.2942959070205688, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.81it/s] 53%|█████▎    | 16/30 [00:02<00:01,  8.51it/s]                                               {'loss': 0.6181, 'grad_norm': 0.6269630193710327, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.51it/s] 57%|█████▋    | 17/30 [00:02<00:01,  8.81it/s]                                               {'loss': 0.6248, 'grad_norm': 0.6574848890304565, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  8.81it/s]                                               {'loss': 0.6488, 'grad_norm': 0.6596475839614868, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  8.81it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.44it/s]                                               {'loss': 0.6382, 'grad_norm': 1.1437119245529175, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  9.44it/s]                                               {'loss': 0.6424, 'grad_norm': 0.4169081747531891, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  9.44it/s] 70%|███████   | 21/30 [00:02<00:00,  9.94it/s]                                               {'loss': 0.6473, 'grad_norm': 0.6336113214492798, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00,  9.94it/s]                                               {'loss': 0.6051, 'grad_norm': 0.8566697835922241, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  9.94it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.84it/s]                                               {'loss': 0.6369, 'grad_norm': 0.7394747734069824, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  9.84it/s]                                               {'loss': 0.5821, 'grad_norm': 1.0398921966552734, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  9.84it/s] 83%|████████▎ | 25/30 [00:03<00:00, 10.58it/s]                                               {'loss': 0.6238, 'grad_norm': 1.0140728950500488, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 10.58it/s]                                               {'loss': 0.5828, 'grad_norm': 1.0713696479797363, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 10.58it/s] 90%|█████████ | 27/30 [00:03<00:00, 10.72it/s]                                               {'loss': 0.6003, 'grad_norm': 0.8412403464317322, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 10.72it/s]                                               {'loss': 0.6605, 'grad_norm': 0.6450709104537964, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 10.72it/s] 97%|█████████▋| 29/30 [00:03<00:00, 11.22it/s]                                               {'loss': 0.5364, 'grad_norm': 0.7132145762443542, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 11.22it/s]                                               {'loss': 0.4957, 'grad_norm': 1.7834926843643188, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.22it/s]                                               {'train_runtime': 3.5661, 'train_samples_per_second': 119.179, 'train_steps_per_second': 8.413, 'train_loss': 0.6437918702761333, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.22it/s]100%|██████████| 30/30 [00:03<00:00,  8.41it/s]
CLIENT:77
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:27,  1.04it/s]                                              {'loss': 0.8897, 'grad_norm': 1.4806134700775146, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:27,  1.04it/s]                                              {'loss': 0.5747, 'grad_norm': 1.1617608070373535, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:26,  1.04it/s] 10%|█         | 3/30 [00:01<00:08,  3.14it/s]                                              {'loss': 0.5501, 'grad_norm': 0.6749267578125, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:08,  3.14it/s]                                              {'loss': 0.3481, 'grad_norm': 0.6668570637702942, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  3.14it/s] 17%|█▋        | 5/30 [00:01<00:04,  5.04it/s]                                              {'loss': 0.3459, 'grad_norm': 1.2475022077560425, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:04,  5.04it/s] 20%|██        | 6/30 [00:01<00:04,  5.56it/s]                                              {'loss': 0.4165, 'grad_norm': 1.6031239032745361, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.56it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.12it/s]                                              {'loss': 0.5962, 'grad_norm': 2.245358467102051, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.12it/s]                                              {'loss': 0.4176, 'grad_norm': 1.1895031929016113, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.12it/s] 30%|███       | 9/30 [00:01<00:02,  7.45it/s]                                              {'loss': 0.3507, 'grad_norm': 0.8857530355453491, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.45it/s]                                              {'loss': 0.3121, 'grad_norm': 1.4695323705673218, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.45it/s] 37%|███▋      | 11/30 [00:01<00:02,  8.61it/s]                                               {'loss': 0.4708, 'grad_norm': 0.8257505893707275, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.61it/s]                                               {'loss': 0.0941, 'grad_norm': 1.1994843482971191, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.61it/s] 43%|████▎     | 13/30 [00:02<00:01, 10.64it/s]                                               {'loss': 0.5298, 'grad_norm': 1.5463736057281494, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:01, 10.64it/s]                                               {'loss': 0.2941, 'grad_norm': 0.734919011592865, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01, 10.64it/s] 50%|█████     | 15/30 [00:02<00:01, 10.82it/s]                                               {'loss': 0.7602, 'grad_norm': 3.325894832611084, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01, 10.82it/s]                                               {'loss': 0.4562, 'grad_norm': 1.2754571437835693, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01, 10.82it/s] 57%|█████▋    | 17/30 [00:02<00:01, 11.15it/s]                                               {'loss': 0.2602, 'grad_norm': 1.0977534055709839, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01, 11.15it/s]                                               {'loss': 0.126, 'grad_norm': 1.3872525691986084, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 11.15it/s] 63%|██████▎   | 19/30 [00:02<00:00, 12.54it/s]                                               {'loss': 0.2939, 'grad_norm': 0.6515219807624817, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 12.54it/s]                                               {'loss': 0.3552, 'grad_norm': 1.1835477352142334, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 12.54it/s] 70%|███████   | 21/30 [00:02<00:00, 12.07it/s]                                               {'loss': 0.4773, 'grad_norm': 2.1544394493103027, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 12.07it/s]                                               {'loss': 0.0851, 'grad_norm': 1.171742558479309, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 12.07it/s] 77%|███████▋  | 23/30 [00:02<00:00, 11.37it/s]                                               {'loss': 0.4237, 'grad_norm': 1.3316442966461182, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 11.37it/s]                                               {'loss': 0.1191, 'grad_norm': 1.4636764526367188, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 11.37it/s] 83%|████████▎ | 25/30 [00:03<00:00, 12.45it/s]                                               {'loss': 0.6337, 'grad_norm': 2.9192583560943604, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 12.45it/s]                                               {'loss': 0.3773, 'grad_norm': 1.4810388088226318, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 12.45it/s] 90%|█████████ | 27/30 [00:03<00:00, 12.63it/s]                                               {'loss': 0.3221, 'grad_norm': 1.4649854898452759, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 12.63it/s]                                               {'loss': 0.3512, 'grad_norm': 1.6470568180084229, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 12.63it/s] 97%|█████████▋| 29/30 [00:03<00:00, 12.46it/s]                                               {'loss': 0.2351, 'grad_norm': 0.9918503761291504, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 12.46it/s]                                               {'loss': 0.0959, 'grad_norm': 1.452006459236145, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.46it/s]                                               {'train_runtime': 3.4826, 'train_samples_per_second': 122.036, 'train_steps_per_second': 8.614, 'train_loss': 0.38542405466238655, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.46it/s]100%|██████████| 30/30 [00:03<00:00,  8.62it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.65it/s]                                              {'loss': 0.923, 'grad_norm': 1.5346399545669556, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.65it/s]  7%|▋         | 2/30 [00:00<00:05,  5.50it/s]                                              {'loss': 0.5535, 'grad_norm': 1.0685778856277466, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.50it/s] 10%|█         | 3/30 [00:00<00:03,  6.79it/s]                                              {'loss': 0.7406, 'grad_norm': 0.8647505640983582, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  6.79it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.08it/s]                                              {'loss': 0.7329, 'grad_norm': 1.368914246559143, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.08it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.10it/s]                                              {'loss': 0.6664, 'grad_norm': 1.3443305492401123, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.10it/s] 20%|██        | 6/30 [00:00<00:03,  7.60it/s]                                              {'loss': 1.1863, 'grad_norm': 2.8853657245635986, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.60it/s]                                              {'loss': 0.345, 'grad_norm': 1.3956003189086914, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:03,  7.60it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.91it/s]                                              {'loss': 0.5079, 'grad_norm': 0.5395026803016663, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.91it/s] 30%|███       | 9/30 [00:01<00:02,  8.00it/s]                                              {'loss': 0.6761, 'grad_norm': 1.1647944450378418, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  8.00it/s] 33%|███▎      | 10/30 [00:01<00:02,  8.08it/s]                                               {'loss': 0.593, 'grad_norm': 14.365755081176758, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.08it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.92it/s]                                               {'loss': 0.6945, 'grad_norm': 0.9382747411727905, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.92it/s] 40%|████      | 12/30 [00:01<00:02,  8.26it/s]                                               {'loss': 0.4123, 'grad_norm': 1.1163318157196045, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.26it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.45it/s]                                               {'loss': 0.4665, 'grad_norm': 1.0648545026779175, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.45it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.79it/s]                                               {'loss': 0.5139, 'grad_norm': 1.0118345022201538, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.79it/s] 50%|█████     | 15/30 [00:02<00:02,  7.48it/s]                                               {'loss': 0.6161, 'grad_norm': 1.3033695220947266, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.48it/s]                                               {'loss': 0.4909, 'grad_norm': 1.1768172979354858, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.48it/s] 57%|█████▋    | 17/30 [00:02<00:01,  9.17it/s]                                               {'loss': 0.5921, 'grad_norm': 1.473031997680664, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  9.17it/s]                                               {'loss': 0.7002, 'grad_norm': 1.841416835784912, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  9.17it/s] 63%|██████▎   | 19/30 [00:02<00:01, 10.49it/s]                                               {'loss': 0.5617, 'grad_norm': 0.9966627359390259, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01, 10.49it/s]                                               {'loss': 0.4532, 'grad_norm': 1.7869048118591309, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 10.49it/s] 70%|███████   | 21/30 [00:02<00:00, 11.07it/s]                                               {'loss': 0.5661, 'grad_norm': 1.2250189781188965, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.07it/s]                                               {'loss': 0.5624, 'grad_norm': 0.8131792545318604, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.07it/s] 77%|███████▋  | 23/30 [00:02<00:00, 10.75it/s]                                               {'loss': 0.6896, 'grad_norm': 1.5874041318893433, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 10.75it/s]                                               {'loss': 0.3618, 'grad_norm': 5.954042911529541, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 10.75it/s] 83%|████████▎ | 25/30 [00:02<00:00, 11.86it/s]                                               {'loss': 0.3783, 'grad_norm': 2.3448259830474854, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 11.86it/s]                                               {'loss': 0.5021, 'grad_norm': 1.1426771879196167, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 11.86it/s] 90%|█████████ | 27/30 [00:02<00:00, 11.91it/s]                                               {'loss': 0.6554, 'grad_norm': 1.8131376504898071, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 11.91it/s]                                               {'loss': 0.5543, 'grad_norm': 1.2902899980545044, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 11.91it/s] 97%|█████████▋| 29/30 [00:03<00:00, 11.29it/s]                                               {'loss': 0.4931, 'grad_norm': 0.9534119367599487, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 11.29it/s]                                               {'loss': 0.361, 'grad_norm': 2.9659833908081055, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.29it/s]                                               {'train_runtime': 3.3816, 'train_samples_per_second': 125.679, 'train_steps_per_second': 8.871, 'train_loss': 0.5850138753652573, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.29it/s]100%|██████████| 30/30 [00:03<00:00,  8.89it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.66it/s]                                              {'loss': 0.7018, 'grad_norm': 0.603247880935669, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.66it/s]  7%|▋         | 2/30 [00:00<00:04,  6.71it/s]                                              {'loss': 0.7095, 'grad_norm': 0.5375897288322449, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.71it/s]                                              {'loss': 0.6772, 'grad_norm': 0.745998740196228, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.71it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.79it/s]                                              {'loss': 0.7122, 'grad_norm': 1.050765037536621, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.79it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.40it/s]                                              {'loss': 0.755, 'grad_norm': 0.8438299298286438, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.40it/s] 20%|██        | 6/30 [00:00<00:03,  7.75it/s]                                              {'loss': 0.7334, 'grad_norm': 1.096942663192749, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.75it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.56it/s]                                              {'loss': 0.6443, 'grad_norm': 0.5043968558311462, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.56it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.77it/s]                                              {'loss': 0.6935, 'grad_norm': 0.3246692419052124, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.77it/s] 30%|███       | 9/30 [00:01<00:03,  5.83it/s]                                              {'loss': 0.6997, 'grad_norm': 0.48943668603897095, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.83it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.49it/s]                                               {'loss': 0.6875, 'grad_norm': 0.3825919032096863, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.49it/s] 37%|███▋      | 11/30 [00:01<00:03,  5.33it/s]                                               {'loss': 0.6415, 'grad_norm': 0.47909560799598694, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  5.33it/s]                                               {'loss': 0.5162, 'grad_norm': 1.9604687690734863, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:03,  5.33it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.14it/s]                                               {'loss': 0.6477, 'grad_norm': 0.34017443656921387, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.14it/s] 47%|████▋     | 14/30 [00:02<00:03,  5.30it/s]                                               {'loss': 0.6366, 'grad_norm': 0.39025139808654785, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  5.30it/s] 50%|█████     | 15/30 [00:02<00:03,  4.76it/s]                                               {'loss': 0.7187, 'grad_norm': 1.2861270904541016, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:03,  4.76it/s] 53%|█████▎    | 16/30 [00:02<00:03,  4.48it/s]                                               {'loss': 0.6228, 'grad_norm': 1.5540919303894043, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:03,  4.48it/s] 57%|█████▋    | 17/30 [00:03<00:03,  4.18it/s]                                               {'loss': 0.5502, 'grad_norm': 1.3185298442840576, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  4.18it/s] 60%|██████    | 18/30 [00:03<00:02,  4.93it/s]                                               {'loss': 0.53, 'grad_norm': 1.4525924921035767, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.93it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.64it/s]                                               {'loss': 0.6029, 'grad_norm': 0.9078948497772217, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.64it/s] 67%|██████▋   | 20/30 [00:03<00:02,  4.28it/s]                                               {'loss': 0.7259, 'grad_norm': 1.3364675045013428, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:02,  4.28it/s] 70%|███████   | 21/30 [00:04<00:02,  4.02it/s]                                               {'loss': 0.7107, 'grad_norm': 1.468813180923462, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:02,  4.02it/s] 73%|███████▎  | 22/30 [00:04<00:02,  3.89it/s]                                               {'loss': 0.5556, 'grad_norm': 1.1424919366836548, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:02,  3.89it/s] 77%|███████▋  | 23/30 [00:04<00:01,  3.90it/s]                                               {'loss': 0.6976, 'grad_norm': 1.5279054641723633, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  3.90it/s]                                               {'loss': 0.7984, 'grad_norm': 3.2344343662261963, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  3.90it/s] 83%|████████▎ | 25/30 [00:04<00:00,  5.01it/s]                                               {'loss': 0.6759, 'grad_norm': 2.3367676734924316, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  5.01it/s] 87%|████████▋ | 26/30 [00:04<00:00,  5.53it/s]                                               {'loss': 0.6921, 'grad_norm': 2.2416131496429443, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  5.53it/s] 90%|█████████ | 27/30 [00:05<00:00,  5.32it/s]                                               {'loss': 0.5617, 'grad_norm': 1.220842957496643, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  5.32it/s] 93%|█████████▎| 28/30 [00:05<00:00,  5.27it/s]                                               {'loss': 0.6095, 'grad_norm': 1.3103970289230347, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  5.27it/s] 97%|█████████▋| 29/30 [00:05<00:00,  5.26it/s]                                               {'loss': 0.5784, 'grad_norm': 1.3609817028045654, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  5.26it/s]100%|██████████| 30/30 [00:05<00:00,  5.91it/s]                                               {'loss': 0.6054, 'grad_norm': 1.9388586282730103, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.91it/s]                                               {'train_runtime': 5.8171, 'train_samples_per_second': 73.06, 'train_steps_per_second': 5.157, 'train_loss': 0.656384261449178, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.91it/s]100%|██████████| 30/30 [00:05<00:00,  5.16it/s]
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  7.26it/s]                                              {'loss': 0.5917, 'grad_norm': 0.4855591058731079, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  7.26it/s]  7%|▋         | 2/30 [00:00<00:03,  8.27it/s]                                              {'loss': 0.747, 'grad_norm': 0.731853187084198, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.27it/s]                                              {'loss': 0.7103, 'grad_norm': 0.46706491708755493, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.27it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.29it/s]                                              {'loss': 0.7083, 'grad_norm': 1.0564905405044556, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  9.29it/s]                                              {'loss': 0.6715, 'grad_norm': 0.4708024263381958, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  9.29it/s] 20%|██        | 6/30 [00:00<00:02, 11.31it/s]                                              {'loss': 0.8267, 'grad_norm': 1.4518277645111084, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.31it/s]                                              {'loss': 0.7052, 'grad_norm': 2.5162084102630615, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02, 11.31it/s] 27%|██▋       | 8/30 [00:00<00:01, 11.32it/s]                                              {'loss': 0.6911, 'grad_norm': 0.48327744007110596, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 11.32it/s]                                              {'loss': 0.6933, 'grad_norm': 0.33770930767059326, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 11.32it/s] 33%|███▎      | 10/30 [00:00<00:01, 11.02it/s]                                               {'loss': 0.6787, 'grad_norm': 0.2691548466682434, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.02it/s]                                               {'loss': 0.6729, 'grad_norm': 0.3048129677772522, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 11.02it/s] 40%|████      | 12/30 [00:01<00:01, 12.20it/s]                                               {'loss': 0.6196, 'grad_norm': 0.7023082375526428, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 12.20it/s]                                               {'loss': 0.6521, 'grad_norm': 0.3390549421310425, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 12.20it/s] 47%|████▋     | 14/30 [00:01<00:01, 11.65it/s]                                               {'loss': 0.6934, 'grad_norm': 0.31927165389060974, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.65it/s]                                               {'loss': 0.6032, 'grad_norm': 0.7181061506271362, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 11.65it/s] 53%|█████▎    | 16/30 [00:01<00:01, 11.68it/s]                                               {'loss': 0.6693, 'grad_norm': 0.6590461730957031, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.68it/s]                                               {'loss': 0.6949, 'grad_norm': 0.4236900210380554, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.68it/s] 60%|██████    | 18/30 [00:01<00:00, 13.42it/s]                                               {'loss': 0.6457, 'grad_norm': 0.4125911295413971, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.42it/s]                                               {'loss': 0.7595, 'grad_norm': 1.2924983501434326, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.42it/s] 67%|██████▋   | 20/30 [00:01<00:00, 13.52it/s]                                               {'loss': 0.6732, 'grad_norm': 0.39367419481277466, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.52it/s]                                               {'loss': 0.666, 'grad_norm': 0.2541393041610718, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.52it/s] 73%|███████▎  | 22/30 [00:01<00:00, 13.39it/s]                                               {'loss': 0.6783, 'grad_norm': 0.30734527111053467, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.39it/s]                                               {'loss': 0.6915, 'grad_norm': 0.8913184404373169, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.39it/s] 80%|████████  | 24/30 [00:01<00:00, 13.81it/s]                                               {'loss': 0.6381, 'grad_norm': 0.5374159812927246, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 13.81it/s]                                               {'loss': 0.7119, 'grad_norm': 0.531690776348114, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 13.81it/s] 87%|████████▋ | 26/30 [00:02<00:00, 12.65it/s]                                               {'loss': 0.6411, 'grad_norm': 0.3473764955997467, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 12.65it/s]                                               {'loss': 0.6336, 'grad_norm': 0.43615707755088806, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 12.65it/s] 93%|█████████▎| 28/30 [00:02<00:00, 11.93it/s]                                               {'loss': 0.6815, 'grad_norm': 0.3452516496181488, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 11.93it/s]                                               {'loss': 0.616, 'grad_norm': 0.4066797196865082, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 11.93it/s]100%|██████████| 30/30 [00:02<00:00, 12.51it/s]                                               {'loss': 0.6368, 'grad_norm': 0.4406980872154236, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.51it/s]                                               {'train_runtime': 2.6564, 'train_samples_per_second': 159.99, 'train_steps_per_second': 11.293, 'train_loss': 0.6767463048299154, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.51it/s]100%|██████████| 30/30 [00:02<00:00, 11.30it/s]
CLIENT:20
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.43it/s]                                              {'loss': 0.5478, 'grad_norm': 0.5812103748321533, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.43it/s]  7%|▋         | 2/30 [00:00<00:03,  7.65it/s]                                              {'loss': 0.7618, 'grad_norm': 1.2859669923782349, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.65it/s] 10%|█         | 3/30 [00:00<00:03,  7.12it/s]                                              {'loss': 0.7483, 'grad_norm': 0.5537125468254089, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.12it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.84it/s]                                              {'loss': 0.68, 'grad_norm': 0.46253031492233276, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.84it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.84it/s]                                              {'loss': 0.6383, 'grad_norm': 0.3676093816757202, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.84it/s] 20%|██        | 6/30 [00:01<00:07,  3.08it/s]                                              {'loss': 0.7695, 'grad_norm': 1.5140886306762695, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:07,  3.08it/s]                                              {'loss': 0.6991, 'grad_norm': 0.357557475566864, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:07,  3.08it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.50it/s]                                              {'loss': 0.6691, 'grad_norm': 1.7822977304458618, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.50it/s] 30%|███       | 9/30 [00:01<00:04,  4.97it/s]                                              {'loss': 0.6534, 'grad_norm': 0.36514076590538025, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.97it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.43it/s]                                               {'loss': 0.6589, 'grad_norm': 0.20566391944885254, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.43it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.75it/s]                                               {'loss': 0.6881, 'grad_norm': 0.15543150901794434, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.75it/s]                                               {'loss': 0.6433, 'grad_norm': 1.9003046751022339, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.75it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.24it/s]                                               {'loss': 0.6809, 'grad_norm': 0.4184291958808899, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.24it/s]                                               {'loss': 0.6978, 'grad_norm': 0.5728164315223694, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.24it/s] 50%|█████     | 15/30 [00:02<00:01,  8.43it/s]                                               {'loss': 0.5969, 'grad_norm': 0.9816309213638306, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.43it/s]                                               {'loss': 0.6546, 'grad_norm': 0.4252481460571289, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.43it/s] 57%|█████▋    | 17/30 [00:02<00:01,  9.73it/s]                                               {'loss': 0.7119, 'grad_norm': 0.5307085514068604, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  9.73it/s]                                               {'loss': 0.6273, 'grad_norm': 0.399455189704895, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  9.73it/s] 63%|██████▎   | 19/30 [00:02<00:00, 11.38it/s]                                               {'loss': 0.7808, 'grad_norm': 1.1831740140914917, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 11.38it/s]                                               {'loss': 0.6704, 'grad_norm': 0.24279743432998657, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 11.38it/s] 70%|███████   | 21/30 [00:02<00:00, 11.91it/s]                                               {'loss': 0.669, 'grad_norm': 0.23089389503002167, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.91it/s]                                               {'loss': 0.6769, 'grad_norm': 0.36785852909088135, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.91it/s] 77%|███████▋  | 23/30 [00:02<00:00, 12.13it/s]                                               {'loss': 0.6557, 'grad_norm': 0.6125146746635437, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 12.13it/s]                                               {'loss': 0.6261, 'grad_norm': 0.3661799430847168, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00, 12.13it/s] 83%|████████▎ | 25/30 [00:03<00:00, 13.51it/s]                                               {'loss': 0.7026, 'grad_norm': 0.360532283782959, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 13.51it/s]                                               {'loss': 0.6525, 'grad_norm': 0.26831331849098206, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 13.51it/s] 90%|█████████ | 27/30 [00:03<00:00, 13.39it/s]                                               {'loss': 0.6917, 'grad_norm': 0.3525238037109375, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 13.39it/s]                                               {'loss': 0.6585, 'grad_norm': 0.20711752772331238, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 13.39it/s] 97%|█████████▋| 29/30 [00:03<00:00, 13.20it/s]                                               {'loss': 0.6195, 'grad_norm': 0.4790233075618744, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 13.20it/s]                                               {'loss': 0.6431, 'grad_norm': 0.33678558468818665, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.20it/s]                                               {'train_runtime': 3.5228, 'train_samples_per_second': 120.644, 'train_steps_per_second': 8.516, 'train_loss': 0.6724597712357839, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.20it/s]100%|██████████| 30/30 [00:03<00:00,  8.52it/s]
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.42it/s]                                              {'loss': 0.6876, 'grad_norm': 0.6635821461677551, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.42it/s]  7%|▋         | 2/30 [00:00<00:03,  8.64it/s]                                              {'loss': 0.7202, 'grad_norm': 0.7692286968231201, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.64it/s] 10%|█         | 3/30 [00:00<00:03,  7.97it/s]                                              {'loss': 0.7091, 'grad_norm': 0.4950854778289795, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.97it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.10it/s]                                              {'loss': 0.679, 'grad_norm': 0.4977737069129944, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.10it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.95it/s]                                              {'loss': 0.6955, 'grad_norm': 0.3779393136501312, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.95it/s] 20%|██        | 6/30 [00:01<00:09,  2.64it/s]                                              {'loss': 0.7565, 'grad_norm': 1.5553569793701172, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:09,  2.64it/s] 23%|██▎       | 7/30 [00:01<00:06,  3.34it/s]                                              {'loss': 0.6136, 'grad_norm': 0.45085573196411133, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:06,  3.34it/s] 27%|██▋       | 8/30 [00:01<00:06,  3.65it/s]                                              {'loss': 0.685, 'grad_norm': 0.2798253893852234, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:06,  3.65it/s] 30%|███       | 9/30 [00:02<00:05,  3.70it/s]                                              {'loss': 0.7477, 'grad_norm': 0.4096759557723999, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  3.70it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.14it/s]                                               {'loss': 0.6541, 'grad_norm': 1.8067153692245483, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.14it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.52it/s]                                               {'loss': 0.677, 'grad_norm': 0.21933375298976898, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.52it/s]                                               {'loss': 0.4796, 'grad_norm': 1.2788639068603516, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.52it/s] 43%|████▎     | 13/30 [00:02<00:02,  5.75it/s]                                               {'loss': 0.6431, 'grad_norm': 0.47867363691329956, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  5.75it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.83it/s]                                               {'loss': 0.599, 'grad_norm': 0.5351079106330872, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.83it/s] 50%|█████     | 15/30 [00:03<00:02,  5.77it/s]                                               {'loss': 0.7339, 'grad_norm': 1.7998911142349243, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.77it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.93it/s]                                               {'loss': 0.6329, 'grad_norm': 0.44208574295043945, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.93it/s] 57%|█████▋    | 17/30 [00:03<00:02,  6.00it/s]                                               {'loss': 0.6336, 'grad_norm': 0.5034390687942505, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  6.00it/s]                                               {'loss': 0.6234, 'grad_norm': 0.6170581579208374, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  6.00it/s] 63%|██████▎   | 19/30 [00:03<00:01,  6.05it/s]                                               {'loss': 0.5441, 'grad_norm': 1.701071858406067, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  6.05it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.77it/s]                                               {'loss': 0.7325, 'grad_norm': 1.0478298664093018, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.77it/s] 70%|███████   | 21/30 [00:04<00:01,  5.25it/s]                                               {'loss': 0.7046, 'grad_norm': 0.6690165996551514, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.25it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.89it/s]                                               {'loss': 0.6078, 'grad_norm': 0.7318764925003052, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.89it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.78it/s]                                               {'loss': 0.7542, 'grad_norm': 1.5695101022720337, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.78it/s]                                               {'loss': 0.5541, 'grad_norm': 2.5921313762664795, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  4.78it/s] 83%|████████▎ | 25/30 [00:04<00:00,  5.60it/s]                                               {'loss': 0.6865, 'grad_norm': 0.9130193591117859, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  5.60it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.19it/s]                                               {'loss': 0.5836, 'grad_norm': 0.7121955156326294, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.19it/s] 90%|█████████ | 27/30 [00:05<00:00,  5.03it/s]                                               {'loss': 0.6719, 'grad_norm': 0.6622928977012634, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  5.03it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.85it/s]                                               {'loss': 0.7055, 'grad_norm': 0.9545961022377014, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.85it/s] 97%|█████████▋| 29/30 [00:05<00:00,  4.82it/s]                                               {'loss': 0.6155, 'grad_norm': 0.7505636215209961, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  4.82it/s]                                               {'loss': 0.5822, 'grad_norm': 0.6350828409194946, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.82it/s]                                               {'train_runtime': 5.9758, 'train_samples_per_second': 71.12, 'train_steps_per_second': 5.02, 'train_loss': 0.6571078489224116, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.82it/s]100%|██████████| 30/30 [00:05<00:00,  5.02it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  6%|▌         | 4/66 [00:00<00:01, 38.45it/s] 12%|█▏        | 8/66 [00:00<00:01, 32.33it/s] 18%|█▊        | 12/66 [00:00<00:01, 31.43it/s] 24%|██▍       | 16/66 [00:00<00:01, 29.96it/s] 30%|███       | 20/66 [00:00<00:01, 29.84it/s] 36%|███▋      | 24/66 [00:00<00:01, 29.88it/s] 42%|████▏     | 28/66 [00:00<00:01, 29.74it/s] 47%|████▋     | 31/66 [00:01<00:01, 29.61it/s] 52%|█████▏    | 34/66 [00:01<00:01, 28.19it/s] 56%|█████▌    | 37/66 [00:01<00:01, 27.90it/s] 61%|██████    | 40/66 [00:01<00:00, 27.76it/s] 65%|██████▌   | 43/66 [00:01<00:00, 27.21it/s] 70%|██████▉   | 46/66 [00:01<00:00, 27.59it/s] 74%|███████▍  | 49/66 [00:01<00:00, 27.58it/s] 79%|███████▉  | 52/66 [00:01<00:00, 26.18it/s] 83%|████████▎ | 55/66 [00:01<00:00, 22.65it/s] 88%|████████▊ | 58/66 [00:02<00:00, 19.74it/s] 92%|█████████▏| 61/66 [00:02<00:00, 17.32it/s] 95%|█████████▌| 63/66 [00:02<00:00, 16.24it/s]100%|██████████| 66/66 [00:02<00:00, 24.79it/s]
{'eval_loss': 0.6608160138130188, 'eval_model_preparation_time': 0.0058, 'eval_acc': 0.6663470757430489, 'eval_runtime': 2.7061, 'eval_samples_per_second': 385.42, 'eval_steps_per_second': 24.389}
ROUND:2
CLIENT:75
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  7.47it/s]                                              {'loss': 0.8022, 'grad_norm': 1.1897709369659424, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  7.47it/s]  7%|▋         | 2/30 [00:00<00:03,  7.23it/s]                                              {'loss': 0.5459, 'grad_norm': 0.8712155222892761, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.23it/s] 10%|█         | 3/30 [00:00<00:03,  7.13it/s]                                              {'loss': 0.5296, 'grad_norm': 0.7163386344909668, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.13it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.94it/s]                                              {'loss': 0.8152, 'grad_norm': 2.007730007171631, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.94it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.78it/s]                                              {'loss': 1.0328, 'grad_norm': 4.078757286071777, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.78it/s] 20%|██        | 6/30 [00:01<00:08,  2.75it/s]                                              {'loss': 0.9262, 'grad_norm': 6.168826580047607, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:08,  2.75it/s] 23%|██▎       | 7/30 [00:01<00:06,  3.39it/s]                                              {'loss': 0.3926, 'grad_norm': 4.346799373626709, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:06,  3.39it/s] 27%|██▋       | 8/30 [00:01<00:05,  3.95it/s]                                              {'loss': 0.6793, 'grad_norm': 2.895021677017212, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  3.95it/s] 30%|███       | 9/30 [00:01<00:04,  4.36it/s]                                              {'loss': 0.7015, 'grad_norm': 0.9034966230392456, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.36it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.90it/s]                                               {'loss': 0.6445, 'grad_norm': 0.24139495193958282, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.90it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.78it/s]                                               {'loss': 0.6537, 'grad_norm': 0.13928738236427307, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.78it/s]                                               {'loss': 0.5613, 'grad_norm': 0.531034529209137, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.78it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.66it/s]                                               {'loss': 0.5859, 'grad_norm': 0.7350521683692932, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.66it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.97it/s]                                               {'loss': 0.6723, 'grad_norm': 0.3807171881198883, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.97it/s] 50%|█████     | 15/30 [00:02<00:01,  8.34it/s]                                               {'loss': 0.8946, 'grad_norm': 2.591822862625122, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.34it/s]                                               {'loss': 0.5336, 'grad_norm': 1.1586467027664185, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.34it/s] 57%|█████▋    | 17/30 [00:02<00:01,  9.39it/s]                                               {'loss': 0.6218, 'grad_norm': 0.5431158542633057, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  9.39it/s]                                               {'loss': 0.7399, 'grad_norm': 0.5949434041976929, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  9.39it/s] 63%|██████▎   | 19/30 [00:02<00:00, 11.01it/s]                                               {'loss': 0.6641, 'grad_norm': 0.4349629878997803, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 11.01it/s]                                               {'loss': 0.645, 'grad_norm': 0.1569972187280655, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:00, 11.01it/s] 70%|███████   | 21/30 [00:03<00:00, 10.92it/s]                                               {'loss': 0.7316, 'grad_norm': 0.5448081493377686, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:00, 10.92it/s]                                               {'loss': 0.6485, 'grad_norm': 0.30710095167160034, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00, 10.92it/s] 77%|███████▋  | 23/30 [00:03<00:00, 10.53it/s]                                               {'loss': 0.6667, 'grad_norm': 0.6824285984039307, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00, 10.53it/s]                                               {'loss': 0.6793, 'grad_norm': 0.42719340324401855, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00, 10.53it/s] 83%|████████▎ | 25/30 [00:03<00:00, 10.91it/s]                                               {'loss': 0.6804, 'grad_norm': 0.2552851140499115, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 10.91it/s]                                               {'loss': 0.5421, 'grad_norm': 1.0121725797653198, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 10.91it/s] 90%|█████████ | 27/30 [00:03<00:00, 11.16it/s]                                               {'loss': 0.7368, 'grad_norm': 0.7046526670455933, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 11.16it/s]                                               {'loss': 0.6849, 'grad_norm': 0.13572117686271667, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 11.16it/s] 97%|█████████▋| 29/30 [00:03<00:00, 11.63it/s]                                               {'loss': 0.7198, 'grad_norm': 0.789855420589447, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 11.63it/s]                                               {'loss': 0.6191, 'grad_norm': 0.5881563425064087, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.63it/s]                                               {'train_runtime': 3.9515, 'train_samples_per_second': 107.554, 'train_steps_per_second': 7.592, 'train_loss': 0.6783775468667348, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.63it/s]100%|██████████| 30/30 [00:03<00:00,  7.59it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:10,  2.77it/s]                                              {'loss': 0.546, 'grad_norm': 0.44551926851272583, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:10,  2.77it/s]  7%|▋         | 2/30 [00:00<00:09,  2.91it/s]                                              {'loss': 0.7401, 'grad_norm': 0.6836040616035461, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:09,  2.91it/s] 10%|█         | 3/30 [00:01<00:09,  2.97it/s]                                              {'loss': 0.7392, 'grad_norm': 0.6428766846656799, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:09,  2.97it/s] 13%|█▎        | 4/30 [00:01<00:08,  2.93it/s]                                              {'loss': 0.7474, 'grad_norm': 5.397031307220459, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  2.93it/s] 17%|█▋        | 5/30 [00:01<00:08,  2.89it/s]                                              {'loss': 0.7167, 'grad_norm': 0.4131118059158325, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:08,  2.89it/s] 20%|██        | 6/30 [00:01<00:06,  3.61it/s]                                              {'loss': 0.7934, 'grad_norm': 1.2353861331939697, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.61it/s] 23%|██▎       | 7/30 [00:02<00:06,  3.46it/s]                                              {'loss': 0.6923, 'grad_norm': 0.33464735746383667, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:06,  3.46it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.35it/s]                                              {'loss': 0.7151, 'grad_norm': 0.8609501719474792, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.35it/s] 30%|███       | 9/30 [00:02<00:06,  3.14it/s]                                              {'loss': 0.6512, 'grad_norm': 0.7135461568832397, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:06,  3.14it/s] 33%|███▎      | 10/30 [00:03<00:06,  3.01it/s]                                               {'loss': 0.6749, 'grad_norm': 0.24182868003845215, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:06,  3.01it/s] 37%|███▋      | 11/30 [00:03<00:06,  2.97it/s]                                               {'loss': 0.6767, 'grad_norm': 0.31919631361961365, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:06,  2.97it/s] 40%|████      | 12/30 [00:03<00:05,  3.53it/s]                                               {'loss': 0.528, 'grad_norm': 2.029790163040161, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:05,  3.53it/s] 43%|████▎     | 13/30 [00:04<00:05,  3.25it/s]                                               {'loss': 0.6831, 'grad_norm': 1.1897413730621338, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:04<00:05,  3.25it/s] 47%|████▋     | 14/30 [00:04<00:05,  3.11it/s]                                               {'loss': 0.6188, 'grad_norm': 2.764700174331665, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:05,  3.11it/s] 50%|█████     | 15/30 [00:04<00:04,  3.49it/s]                                               {'loss': 0.8411, 'grad_norm': 4.290319442749023, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:04,  3.49it/s] 53%|█████▎    | 16/30 [00:04<00:03,  3.78it/s]                                               {'loss': 0.7188, 'grad_norm': 1.5767831802368164, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:03,  3.78it/s] 57%|█████▋    | 17/30 [00:05<00:03,  4.11it/s]                                               {'loss': 0.6156, 'grad_norm': 0.9293308258056641, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:05<00:03,  4.11it/s]                                               {'loss': 0.7341, 'grad_norm': 1.734009861946106, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:02,  4.11it/s] 63%|██████▎   | 19/30 [00:05<00:02,  5.10it/s]                                               {'loss': 0.6385, 'grad_norm': 3.0148072242736816, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  5.10it/s] 67%|██████▋   | 20/30 [00:05<00:02,  4.96it/s]                                               {'loss': 0.7667, 'grad_norm': 2.674985885620117, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  4.96it/s] 70%|███████   | 21/30 [00:05<00:01,  4.86it/s]                                               {'loss': 0.7278, 'grad_norm': 3.1749470233917236, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:01,  4.86it/s] 73%|███████▎  | 22/30 [00:05<00:01,  5.24it/s]                                               {'loss': 0.6888, 'grad_norm': 2.8514251708984375, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:01,  5.24it/s] 77%|███████▋  | 23/30 [00:06<00:01,  5.33it/s]                                               {'loss': 0.6617, 'grad_norm': 0.5448943972587585, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  5.33it/s]                                               {'loss': 0.6705, 'grad_norm': 1.2423380613327026, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  5.33it/s] 83%|████████▎ | 25/30 [00:06<00:00,  5.86it/s]                                               {'loss': 0.7093, 'grad_norm': 1.1174097061157227, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:00,  5.86it/s] 87%|████████▋ | 26/30 [00:06<00:00,  5.50it/s]                                               {'loss': 0.6817, 'grad_norm': 0.847555935382843, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:00,  5.50it/s] 90%|█████████ | 27/30 [00:06<00:00,  5.24it/s]                                               {'loss': 0.6668, 'grad_norm': 0.8519827723503113, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  5.24it/s] 93%|█████████▎| 28/30 [00:07<00:00,  5.05it/s]                                               {'loss': 0.663, 'grad_norm': 1.120409369468689, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  5.05it/s] 97%|█████████▋| 29/30 [00:07<00:00,  5.18it/s]                                               {'loss': 0.6985, 'grad_norm': 3.3630659580230713, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  5.18it/s]                                               {'loss': 0.7043, 'grad_norm': 0.9617338180541992, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  5.18it/s]                                               {'train_runtime': 7.3821, 'train_samples_per_second': 57.571, 'train_steps_per_second': 4.064, 'train_loss': 0.6903391301631927, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  5.18it/s]100%|██████████| 30/30 [00:07<00:00,  4.07it/s]
CLIENT:46
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  7.23it/s]                                              {'loss': 0.5653, 'grad_norm': 4.3464860916137695, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  7.23it/s]  7%|▋         | 2/30 [00:00<00:03,  7.17it/s]                                              {'loss': 0.73, 'grad_norm': 0.6138741374015808, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.17it/s] 10%|█         | 3/30 [00:00<00:04,  6.52it/s]                                              {'loss': 0.663, 'grad_norm': 0.5224518179893494, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.52it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.55it/s]                                              {'loss': 0.78, 'grad_norm': 0.8339412212371826, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.55it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.61it/s]                                              {'loss': 0.7601, 'grad_norm': 0.777074933052063, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.61it/s] 20%|██        | 6/30 [00:00<00:03,  6.57it/s]                                              {'loss': 0.7536, 'grad_norm': 1.3888450860977173, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.57it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.60it/s]                                              {'loss': 0.6672, 'grad_norm': 0.23337115347385406, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.60it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.37it/s]                                              {'loss': 0.7015, 'grad_norm': 0.39309829473495483, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.37it/s] 30%|███       | 9/30 [00:01<00:04,  5.10it/s]                                              {'loss': 0.6773, 'grad_norm': 0.4047900438308716, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  5.10it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.33it/s]                                               {'loss': 0.6675, 'grad_norm': 0.29943692684173584, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.33it/s] 37%|███▋      | 11/30 [00:01<00:03,  5.24it/s]                                               {'loss': 0.6616, 'grad_norm': 0.21914754807949066, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  5.24it/s]                                               {'loss': 0.7974, 'grad_norm': 8.839868545532227, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.24it/s] 43%|████▎     | 13/30 [00:02<00:02,  5.85it/s]                                               {'loss': 0.6304, 'grad_norm': 0.7277926802635193, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  5.85it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.48it/s]                                               {'loss': 0.6799, 'grad_norm': 0.4977650046348572, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.48it/s] 50%|█████     | 15/30 [00:02<00:02,  5.10it/s]                                               {'loss': 0.5922, 'grad_norm': 1.0402929782867432, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.10it/s] 53%|█████▎    | 16/30 [00:02<00:02,  4.91it/s]                                               {'loss': 0.6792, 'grad_norm': 0.4020766019821167, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  4.91it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.54it/s]                                               {'loss': 0.7172, 'grad_norm': 0.40106597542762756, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.54it/s] 60%|██████    | 18/30 [00:03<00:02,  5.09it/s]                                               {'loss': 0.6561, 'grad_norm': 0.43274956941604614, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.09it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.38it/s]                                               {'loss': 0.7757, 'grad_norm': 0.8304303884506226, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.38it/s] 67%|██████▋   | 20/30 [00:03<00:02,  4.18it/s]                                               {'loss': 0.6534, 'grad_norm': 0.22669734060764313, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:02,  4.18it/s] 70%|███████   | 21/30 [00:04<00:02,  3.90it/s]                                               {'loss': 0.6675, 'grad_norm': 2.160400390625, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:02,  3.90it/s] 73%|███████▎  | 22/30 [00:04<00:02,  3.67it/s]                                               {'loss': 0.666, 'grad_norm': 0.23913905024528503, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:02,  3.67it/s] 77%|███████▋  | 23/30 [00:04<00:01,  3.75it/s]                                               {'loss': 0.6033, 'grad_norm': 0.42549392580986023, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  3.75it/s]                                               {'loss': 0.6622, 'grad_norm': 0.4285849332809448, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  3.75it/s] 83%|████████▎ | 25/30 [00:05<00:01,  4.38it/s]                                               {'loss': 0.6836, 'grad_norm': 0.23250292241573334, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  4.38it/s] 87%|████████▋ | 26/30 [00:05<00:00,  4.26it/s]                                               {'loss': 0.6577, 'grad_norm': 0.26859134435653687, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  4.26it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.08it/s]                                               {'loss': 0.6819, 'grad_norm': 0.29760634899139404, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.08it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.07it/s]                                               {'loss': 0.6567, 'grad_norm': 0.22816132009029388, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.07it/s] 97%|█████████▋| 29/30 [00:06<00:00,  4.08it/s]                                               {'loss': 0.6177, 'grad_norm': 0.36820992827415466, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  4.08it/s]100%|██████████| 30/30 [00:06<00:00,  4.70it/s]                                               {'loss': 0.6247, 'grad_norm': 0.4105777144432068, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.70it/s]                                               {'train_runtime': 6.372, 'train_samples_per_second': 66.698, 'train_steps_per_second': 4.708, 'train_loss': 0.6776626249154408, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.70it/s]100%|██████████| 30/30 [00:06<00:00,  4.71it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.72it/s]                                              {'loss': 0.5308, 'grad_norm': 0.2816990613937378, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.72it/s]  7%|▋         | 2/30 [00:00<00:05,  4.80it/s]                                              {'loss': 0.661, 'grad_norm': 1.125185251235962, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  4.80it/s] 10%|█         | 3/30 [00:00<00:05,  5.08it/s]                                              {'loss': 0.6755, 'grad_norm': 0.679750919342041, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  5.08it/s] 13%|█▎        | 4/30 [00:00<00:05,  5.03it/s]                                              {'loss': 0.7831, 'grad_norm': 1.516330361366272, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  5.03it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.99it/s]                                              {'loss': 0.6983, 'grad_norm': 1.1149929761886597, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.99it/s]                                              {'loss': 0.87, 'grad_norm': 1.9032012224197388, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.99it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.22it/s]                                              {'loss': 0.6618, 'grad_norm': 1.4619591236114502, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.22it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.41it/s]                                              {'loss': 0.6666, 'grad_norm': 0.5159071683883667, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.41it/s] 30%|███       | 9/30 [00:01<00:03,  6.64it/s]                                              {'loss': 0.6901, 'grad_norm': 0.6112637519836426, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.64it/s] 33%|███▎      | 10/30 [00:01<00:02,  6.85it/s]                                               {'loss': 0.6432, 'grad_norm': 0.6056081056594849, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  6.85it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.87it/s]                                               {'loss': 0.6597, 'grad_norm': 0.45141837000846863, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.87it/s]                                               {'loss': 0.6826, 'grad_norm': 0.9358169436454773, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.87it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.25it/s]                                               {'loss': 0.612, 'grad_norm': 0.6168850064277649, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.25it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.94it/s]                                               {'loss': 0.6636, 'grad_norm': 0.6705077886581421, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.94it/s] 50%|█████     | 15/30 [00:02<00:01,  7.69it/s]                                               {'loss': 0.6454, 'grad_norm': 1.1509138345718384, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.69it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.93it/s]                                               {'loss': 0.6274, 'grad_norm': 0.8741655945777893, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.93it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.87it/s]                                               {'loss': 0.6208, 'grad_norm': 0.62534099817276, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.87it/s]                                               {'loss': 0.6466, 'grad_norm': 0.7626217603683472, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.87it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.94it/s]                                               {'loss': 0.63, 'grad_norm': 1.1647984981536865, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.94it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.34it/s]                                               {'loss': 0.6409, 'grad_norm': 0.43635472655296326, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.34it/s] 70%|███████   | 21/30 [00:03<00:01,  7.74it/s]                                               {'loss': 0.6514, 'grad_norm': 0.5686256289482117, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.74it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.20it/s]                                               {'loss': 0.6127, 'grad_norm': 0.799319326877594, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.20it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.83it/s]                                               {'loss': 0.6386, 'grad_norm': 0.7414502501487732, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.83it/s]                                               {'loss': 0.5719, 'grad_norm': 1.0164375305175781, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.83it/s] 83%|████████▎ | 25/30 [00:03<00:00,  7.99it/s]                                               {'loss': 0.6107, 'grad_norm': 1.0806292295455933, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  7.99it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.84it/s]                                               {'loss': 0.578, 'grad_norm': 1.0769083499908447, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.84it/s] 90%|█████████ | 27/30 [00:03<00:00,  6.79it/s]                                               {'loss': 0.6003, 'grad_norm': 1.113869547843933, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  6.79it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.04it/s]                                               {'loss': 0.6515, 'grad_norm': 0.6908279657363892, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.04it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.57it/s]                                               {'loss': 0.512, 'grad_norm': 0.7588598132133484, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.57it/s]                                               {'loss': 0.4965, 'grad_norm': 1.726629376411438, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.57it/s]                                               {'train_runtime': 4.4891, 'train_samples_per_second': 94.673, 'train_steps_per_second': 6.683, 'train_loss': 0.64110107421875, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.57it/s]100%|██████████| 30/30 [00:04<00:00,  6.68it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.25it/s]                                              {'loss': 0.7025, 'grad_norm': 0.547176718711853, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.25it/s]  7%|▋         | 2/30 [00:00<00:08,  3.26it/s]                                              {'loss': 0.7052, 'grad_norm': 0.5354588031768799, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:08,  3.26it/s] 10%|█         | 3/30 [00:01<00:09,  2.76it/s]                                              {'loss': 0.6713, 'grad_norm': 0.7660427689552307, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:09,  2.76it/s] 13%|█▎        | 4/30 [00:01<00:08,  2.91it/s]                                              {'loss': 0.7107, 'grad_norm': 1.0804466009140015, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  2.91it/s] 17%|█▋        | 5/30 [00:01<00:07,  3.30it/s]                                              {'loss': 0.7447, 'grad_norm': 0.7433990836143494, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:07,  3.30it/s] 20%|██        | 6/30 [00:01<00:06,  3.63it/s]                                              {'loss': 0.7389, 'grad_norm': 1.3561755418777466, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.63it/s] 23%|██▎       | 7/30 [00:02<00:06,  3.44it/s]                                              {'loss': 0.6435, 'grad_norm': 0.515716016292572, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:06,  3.44it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.41it/s]                                              {'loss': 0.6898, 'grad_norm': 0.3637346029281616, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.41it/s] 30%|███       | 9/30 [00:02<00:06,  3.37it/s]                                              {'loss': 0.6988, 'grad_norm': 0.49810245633125305, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:06,  3.37it/s] 33%|███▎      | 10/30 [00:03<00:06,  3.28it/s]                                               {'loss': 0.6846, 'grad_norm': 0.3884618878364563, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:06,  3.28it/s] 37%|███▋      | 11/30 [00:03<00:05,  3.33it/s]                                               {'loss': 0.6371, 'grad_norm': 0.6485539674758911, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:05,  3.33it/s] 40%|████      | 12/30 [00:03<00:04,  3.80it/s]                                               {'loss': 0.5179, 'grad_norm': 1.9023332595825195, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:04,  3.80it/s] 43%|████▎     | 13/30 [00:03<00:04,  3.64it/s]                                               {'loss': 0.6406, 'grad_norm': 0.40692266821861267, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:04,  3.64it/s] 47%|████▋     | 14/30 [00:04<00:04,  3.47it/s]                                               {'loss': 0.6347, 'grad_norm': 0.792878270149231, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:04,  3.47it/s] 50%|█████     | 15/30 [00:04<00:04,  3.31it/s]                                               {'loss': 0.7003, 'grad_norm': 1.1976468563079834, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:04,  3.31it/s] 53%|█████▎    | 16/30 [00:04<00:04,  3.24it/s]                                               {'loss': 0.6079, 'grad_norm': 2.1635875701904297, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:04,  3.24it/s] 57%|█████▋    | 17/30 [00:05<00:03,  3.51it/s]                                               {'loss': 0.5463, 'grad_norm': 1.5182229280471802, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:05<00:03,  3.51it/s]                                               {'loss': 0.5008, 'grad_norm': 2.541295051574707, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:03,  3.51it/s] 63%|██████▎   | 19/30 [00:05<00:02,  4.57it/s]                                               {'loss': 0.5857, 'grad_norm': 2.092270851135254, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  4.57it/s] 67%|██████▋   | 20/30 [00:05<00:02,  4.76it/s]                                               {'loss': 0.7391, 'grad_norm': 2.395209789276123, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  4.76it/s] 70%|███████   | 21/30 [00:05<00:01,  4.58it/s]                                               {'loss': 0.7066, 'grad_norm': 1.9157593250274658, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:01,  4.58it/s] 73%|███████▎  | 22/30 [00:05<00:01,  4.57it/s]                                               {'loss': 0.5806, 'grad_norm': 1.636637568473816, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:01,  4.57it/s] 77%|███████▋  | 23/30 [00:06<00:01,  4.78it/s]                                               {'loss': 0.6904, 'grad_norm': 1.8752593994140625, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  4.78it/s] 80%|████████  | 24/30 [00:06<00:01,  5.55it/s]                                               {'loss': 0.7231, 'grad_norm': 4.094388484954834, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  5.55it/s] 83%|████████▎ | 25/30 [00:06<00:00,  5.22it/s]                                               {'loss': 0.7084, 'grad_norm': 2.703883171081543, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:00,  5.22it/s] 87%|████████▋ | 26/30 [00:06<00:00,  4.75it/s]                                               {'loss': 0.6992, 'grad_norm': 1.737148404121399, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:00,  4.75it/s] 90%|█████████ | 27/30 [00:06<00:00,  4.64it/s]                                               {'loss': 0.5673, 'grad_norm': 1.8812264204025269, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  4.64it/s] 93%|█████████▎| 28/30 [00:07<00:00,  5.20it/s]                                               {'loss': 0.6306, 'grad_norm': 5.515214920043945, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  5.20it/s]                                               {'loss': 0.5549, 'grad_norm': 1.619120478630066, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  5.20it/s]100%|██████████| 30/30 [00:07<00:00,  7.29it/s]                                               {'loss': 0.5883, 'grad_norm': 2.9481217861175537, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  7.29it/s]                                               {'train_runtime': 7.335, 'train_samples_per_second': 57.941, 'train_steps_per_second': 4.09, 'train_loss': 0.6516578833262126, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  7.29it/s]100%|██████████| 30/30 [00:07<00:00,  4.09it/s]
CLIENT:39
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.29it/s]                                              {'loss': 0.6494, 'grad_norm': 0.6251629590988159, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.29it/s]  7%|▋         | 2/30 [00:00<00:06,  4.53it/s]                                              {'loss': 0.6232, 'grad_norm': 0.8715434074401855, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.53it/s] 10%|█         | 3/30 [00:00<00:06,  4.47it/s]                                              {'loss': 0.6098, 'grad_norm': 0.7637079358100891, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.47it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.42it/s]                                              {'loss': 0.4118, 'grad_norm': 1.0291436910629272, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.42it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.87it/s]                                              {'loss': 0.6958, 'grad_norm': 1.4835339784622192, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.87it/s]                                              {'loss': 0.5289, 'grad_norm': 1.5400294065475464, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.87it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.13it/s]                                              {'loss': 0.531, 'grad_norm': 1.6657276153564453, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.13it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.89it/s]                                              {'loss': 0.5866, 'grad_norm': 1.0167723894119263, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.89it/s] 30%|███       | 9/30 [00:01<00:03,  5.53it/s]                                              {'loss': 0.6308, 'grad_norm': 1.3696694374084473, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.53it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.18it/s]                                               {'loss': 0.4143, 'grad_norm': 0.9690646529197693, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.18it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.39it/s]                                               {'loss': 0.436, 'grad_norm': 1.0488353967666626, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.39it/s] 40%|████      | 12/30 [00:02<00:02,  6.12it/s]                                               {'loss': 0.3825, 'grad_norm': 0.9297726154327393, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.12it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.53it/s]                                               {'loss': 0.5516, 'grad_norm': 0.7789397835731506, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.53it/s] 47%|████▋     | 14/30 [00:02<00:03,  5.28it/s]                                               {'loss': 0.4255, 'grad_norm': 0.6403478980064392, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  5.28it/s] 50%|█████     | 15/30 [00:02<00:02,  5.51it/s]                                               {'loss': 0.8263, 'grad_norm': 3.3365724086761475, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.51it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.27it/s]                                               {'loss': 0.4501, 'grad_norm': 0.8650978207588196, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.27it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.22it/s]                                               {'loss': 0.4637, 'grad_norm': 0.9454188346862793, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.22it/s]                                               {'loss': 0.251, 'grad_norm': 2.508793354034424, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.22it/s] 63%|██████▎   | 19/30 [00:03<00:01,  5.74it/s]                                               {'loss': 0.4195, 'grad_norm': 0.8805861473083496, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  5.74it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.42it/s]                                               {'loss': 0.5874, 'grad_norm': 2.1953625679016113, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.42it/s] 70%|███████   | 21/30 [00:03<00:01,  5.22it/s]                                               {'loss': 0.5136, 'grad_norm': 1.1035425662994385, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.22it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.06it/s]                                               {'loss': 0.4958, 'grad_norm': 0.8724085688591003, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.06it/s] 77%|███████▋  | 23/30 [00:04<00:01,  5.27it/s]                                               {'loss': 0.5808, 'grad_norm': 1.4504934549331665, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  5.27it/s] 80%|████████  | 24/30 [00:04<00:01,  5.72it/s]                                               {'loss': 0.2449, 'grad_norm': 2.484893798828125, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  5.72it/s] 83%|████████▎ | 25/30 [00:04<00:01,  4.93it/s]                                               {'loss': 0.2509, 'grad_norm': 1.5876877307891846, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:01,  4.93it/s] 87%|████████▋ | 26/30 [00:05<00:00,  4.09it/s]                                               {'loss': 0.5166, 'grad_norm': 1.1742404699325562, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  4.09it/s] 90%|█████████ | 27/30 [00:05<00:00,  3.77it/s]                                               {'loss': 0.4036, 'grad_norm': 0.9471550583839417, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  3.77it/s] 93%|█████████▎| 28/30 [00:05<00:00,  3.65it/s]                                               {'loss': 0.5754, 'grad_norm': 2.3934552669525146, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  3.65it/s] 97%|█████████▋| 29/30 [00:06<00:00,  3.51it/s]                                               {'loss': 0.6041, 'grad_norm': 2.3244192600250244, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  3.51it/s]                                               {'loss': 0.6991, 'grad_norm': 4.422671318054199, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  3.51it/s]                                               {'train_runtime': 6.186, 'train_samples_per_second': 68.703, 'train_steps_per_second': 4.85, 'train_loss': 0.5119987229506174, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  3.51it/s]100%|██████████| 30/30 [00:06<00:00,  4.86it/s]
CLIENT:23
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.9106, 'grad_norm': 1.5039013624191284, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 10.50it/s]  7%|▋         | 2/30 [00:00<00:02, 11.19it/s]                                              {'loss': 0.4985, 'grad_norm': 1.3615875244140625, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 11.19it/s]                                              {'loss': 0.3773, 'grad_norm': 1.302402377128601, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.19it/s] 13%|█▎        | 4/30 [00:00<00:02, 11.32it/s]                                              {'loss': 0.163, 'grad_norm': 1.0863301753997803, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.32it/s]                                              {'loss': 0.3714, 'grad_norm': 0.7267386317253113, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.32it/s] 20%|██        | 6/30 [00:00<00:01, 12.58it/s]                                              {'loss': 0.0673, 'grad_norm': 0.9609974026679993, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 12.58it/s]                                              {'loss': 0.0486, 'grad_norm': 1.0289270877838135, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 12.58it/s] 27%|██▋       | 8/30 [00:00<00:01, 11.71it/s]                                              {'loss': 0.3742, 'grad_norm': 2.2796173095703125, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 11.71it/s]                                              {'loss': 0.4206, 'grad_norm': 1.0288162231445312, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 11.71it/s] 33%|███▎      | 10/30 [00:00<00:01, 11.30it/s]                                               {'loss': 0.3396, 'grad_norm': 4.6063361167907715, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.30it/s]                                               {'loss': 0.688, 'grad_norm': 2.6697776317596436, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 11.30it/s] 40%|████      | 12/30 [00:01<00:01, 11.54it/s]                                               {'loss': 0.947, 'grad_norm': 4.474366188049316, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 11.54it/s]                                               {'loss': 0.4907, 'grad_norm': 2.7581825256347656, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 11.54it/s] 47%|████▋     | 14/30 [00:01<00:01, 10.80it/s]                                               {'loss': 0.241, 'grad_norm': 1.5848618745803833, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 10.80it/s]                                               {'loss': 0.0282, 'grad_norm': 0.33451616764068604, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 10.80it/s] 53%|█████▎    | 16/30 [00:01<00:01, 10.68it/s]                                               {'loss': 0.3057, 'grad_norm': 2.1266884803771973, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 10.68it/s]                                               {'loss': 0.503, 'grad_norm': 4.441473007202148, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 10.68it/s] 60%|██████    | 18/30 [00:01<00:01, 11.63it/s]                                               {'loss': 0.0499, 'grad_norm': 0.7088817954063416, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 11.63it/s]                                               {'loss': 0.2275, 'grad_norm': 1.2289690971374512, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 11.63it/s] 67%|██████▋   | 20/30 [00:01<00:00, 10.42it/s]                                               {'loss': 0.4268, 'grad_norm': 2.764970541000366, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 10.42it/s]                                               {'loss': 0.2904, 'grad_norm': 3.1670942306518555, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 10.42it/s] 73%|███████▎  | 22/30 [00:01<00:00, 10.52it/s]                                               {'loss': 0.3127, 'grad_norm': 1.3693760633468628, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 10.52it/s]                                               {'loss': 0.0667, 'grad_norm': 0.8490602970123291, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 10.52it/s] 80%|████████  | 24/30 [00:02<00:00, 12.18it/s]                                               {'loss': 0.102, 'grad_norm': 2.3577609062194824, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 12.18it/s]                                               {'loss': 0.0865, 'grad_norm': 1.2469874620437622, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.18it/s] 87%|████████▋ | 26/30 [00:02<00:00, 11.81it/s]                                               {'loss': 0.3482, 'grad_norm': 1.7577584981918335, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 11.81it/s]                                               {'loss': 0.4794, 'grad_norm': 5.318414688110352, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 11.81it/s] 93%|█████████▎| 28/30 [00:02<00:00, 11.63it/s]                                               {'loss': 0.2172, 'grad_norm': 1.1453979015350342, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 11.63it/s]                                               {'loss': 0.1942, 'grad_norm': 1.947695255279541, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 11.63it/s]100%|██████████| 30/30 [00:02<00:00, 12.70it/s]                                               {'loss': 0.0691, 'grad_norm': 1.1760727167129517, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.70it/s]                                               {'train_runtime': 2.6719, 'train_samples_per_second': 159.065, 'train_steps_per_second': 11.228, 'train_loss': 0.3215090349937479, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.70it/s]100%|██████████| 30/30 [00:02<00:00, 11.23it/s]
CLIENT:20
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.71it/s]                                              {'loss': 0.5541, 'grad_norm': 0.6128731369972229, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.71it/s]  7%|▋         | 2/30 [00:00<00:04,  6.85it/s]                                              {'loss': 0.7483, 'grad_norm': 0.9028060436248779, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.85it/s] 10%|█         | 3/30 [00:00<00:04,  6.03it/s]                                              {'loss': 0.7382, 'grad_norm': 0.5550400614738464, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.03it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.30it/s]                                              {'loss': 0.6697, 'grad_norm': 2.5780036449432373, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.30it/s] 17%|█▋        | 5/30 [00:00<00:05,  4.96it/s]                                              {'loss': 0.6409, 'grad_norm': 0.37770968675613403, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:05,  4.96it/s] 20%|██        | 6/30 [00:01<00:09,  2.46it/s]                                              {'loss': 0.7816, 'grad_norm': 1.4381369352340698, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:09,  2.46it/s] 23%|██▎       | 7/30 [00:01<00:08,  2.87it/s]                                              {'loss': 0.6863, 'grad_norm': 0.317091166973114, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:08,  2.87it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.57it/s]                                              {'loss': 0.6783, 'grad_norm': 0.4822423458099365, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.57it/s] 30%|███       | 9/30 [00:02<00:05,  3.77it/s]                                              {'loss': 0.6573, 'grad_norm': 0.39400792121887207, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  3.77it/s] 33%|███▎      | 10/30 [00:02<00:05,  4.00it/s]                                               {'loss': 0.6571, 'grad_norm': 0.21208833158016205, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  4.00it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.41it/s]                                               {'loss': 0.6865, 'grad_norm': 0.1794607937335968, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.41it/s]                                               {'loss': 0.7396, 'grad_norm': 2.8179917335510254, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:04,  4.41it/s] 43%|████▎     | 13/30 [00:03<00:03,  5.41it/s]                                               {'loss': 0.6804, 'grad_norm': 0.38786694407463074, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  5.41it/s] 47%|████▋     | 14/30 [00:03<00:02,  5.38it/s]                                               {'loss': 0.7269, 'grad_norm': 0.7184644341468811, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:02,  5.38it/s] 50%|█████     | 15/30 [00:03<00:02,  5.35it/s]                                               {'loss': 0.5728, 'grad_norm': 0.9078698754310608, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.35it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.30it/s]                                               {'loss': 0.6621, 'grad_norm': 0.610228419303894, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.30it/s]                                               {'loss': 0.7141, 'grad_norm': 0.5113376379013062, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.30it/s] 60%|██████    | 18/30 [00:03<00:01,  7.00it/s]                                               {'loss': 0.6226, 'grad_norm': 0.41526660323143005, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  7.00it/s] 63%|██████▎   | 19/30 [00:03<00:01,  7.11it/s]                                               {'loss': 0.8079, 'grad_norm': 1.1564128398895264, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  7.11it/s] 67%|██████▋   | 20/30 [00:04<00:01,  6.91it/s]                                               {'loss': 0.6718, 'grad_norm': 0.23070955276489258, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  6.91it/s] 70%|███████   | 21/30 [00:04<00:01,  7.28it/s]                                               {'loss': 0.667, 'grad_norm': 2.9204232692718506, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  7.28it/s] 73%|███████▎  | 22/30 [00:04<00:01,  7.12it/s]                                               {'loss': 0.6852, 'grad_norm': 0.41079941391944885, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  7.12it/s] 77%|███████▋  | 23/30 [00:04<00:00,  7.51it/s]                                               {'loss': 0.601, 'grad_norm': 0.36248907446861267, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:00,  7.51it/s]                                               {'loss': 0.6273, 'grad_norm': 0.3424602150917053, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  7.51it/s] 83%|████████▎ | 25/30 [00:04<00:00,  8.28it/s]                                               {'loss': 0.7051, 'grad_norm': 0.33602210879325867, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  8.28it/s] 87%|████████▋ | 26/30 [00:04<00:00,  7.88it/s]                                               {'loss': 0.6471, 'grad_norm': 0.2858883738517761, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  7.88it/s] 90%|█████████ | 27/30 [00:04<00:00,  7.45it/s]                                               {'loss': 0.7034, 'grad_norm': 0.34960564970970154, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  7.45it/s] 93%|█████████▎| 28/30 [00:05<00:00,  7.19it/s]                                               {'loss': 0.6575, 'grad_norm': 0.23568852245807648, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  7.19it/s] 97%|█████████▋| 29/30 [00:05<00:00,  7.70it/s]                                               {'loss': 0.6266, 'grad_norm': 0.45320773124694824, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  7.70it/s]                                               {'loss': 0.6439, 'grad_norm': 0.3541478216648102, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  7.70it/s]                                               {'train_runtime': 5.3958, 'train_samples_per_second': 78.765, 'train_steps_per_second': 5.56, 'train_loss': 0.6753562827905019, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  7.70it/s]100%|██████████| 30/30 [00:05<00:00,  5.56it/s]
CLIENT:70
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  4.86it/s]                                              {'loss': 0.9356, 'grad_norm': 1.9235072135925293, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  4.86it/s]  7%|▋         | 2/30 [00:00<00:06,  4.19it/s]                                              {'loss': 0.4834, 'grad_norm': 1.398881196975708, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.19it/s] 10%|█         | 3/30 [00:00<00:05,  4.59it/s]                                              {'loss': 0.25, 'grad_norm': 1.7461704015731812, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.59it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.22it/s]                                              {'loss': 0.4593, 'grad_norm': 1.5007578134536743, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.22it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.67it/s]                                              {'loss': 0.1006, 'grad_norm': 1.007688045501709, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.67it/s] 20%|██        | 6/30 [00:01<00:07,  3.03it/s]                                              {'loss': 2.3585, 'grad_norm': 11.117534637451172, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:07,  3.03it/s]                                              {'loss': 0.3109, 'grad_norm': 1.319140076637268, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:07,  3.03it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.41it/s]                                              {'loss': 0.0252, 'grad_norm': 0.4709240794181824, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.41it/s] 30%|███       | 9/30 [00:01<00:04,  4.96it/s]                                              {'loss': 0.4151, 'grad_norm': 3.930800676345825, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.96it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.64it/s]                                               {'loss': 0.3739, 'grad_norm': 1.327962875366211, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.64it/s]                                               {'loss': 0.1777, 'grad_norm': 0.9700564742088318, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.64it/s] 40%|████      | 12/30 [00:02<00:02,  7.95it/s]                                               {'loss': 0.0664, 'grad_norm': 2.974607229232788, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  7.95it/s]                                               {'loss': 0.0566, 'grad_norm': 0.8333489894866943, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.95it/s] 47%|████▋     | 14/30 [00:02<00:01,  8.05it/s]                                               {'loss': 0.0369, 'grad_norm': 0.5962764024734497, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  8.05it/s] 50%|█████     | 15/30 [00:02<00:01,  7.82it/s]                                               {'loss': 0.502, 'grad_norm': 2.3717992305755615, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.82it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.40it/s]                                               {'loss': 0.2154, 'grad_norm': 0.9519816637039185, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.40it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.30it/s]                                               {'loss': 0.4127, 'grad_norm': 1.59635329246521, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.30it/s] 60%|██████    | 18/30 [00:03<00:02,  5.33it/s]                                               {'loss': 0.8296, 'grad_norm': 8.055574417114258, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.33it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.39it/s]                                               {'loss': 0.2383, 'grad_norm': 0.8947134017944336, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.39it/s] 67%|██████▋   | 20/30 [00:03<00:02,  4.72it/s]                                               {'loss': 0.0817, 'grad_norm': 1.2754902839660645, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:02,  4.72it/s] 70%|███████   | 21/30 [00:03<00:01,  5.05it/s]                                               {'loss': 0.0765, 'grad_norm': 1.1797722578048706, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.05it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.40it/s]                                               {'loss': 0.4081, 'grad_norm': 2.3374221324920654, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.40it/s] 77%|███████▋  | 23/30 [00:04<00:01,  5.92it/s]                                               {'loss': 0.5768, 'grad_norm': 3.8764588832855225, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  5.92it/s] 80%|████████  | 24/30 [00:04<00:00,  6.46it/s]                                               {'loss': 0.0825, 'grad_norm': 1.3660283088684082, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  6.46it/s] 83%|████████▎ | 25/30 [00:04<00:00,  5.58it/s]                                               {'loss': 0.3604, 'grad_norm': 1.926830530166626, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  5.58it/s] 87%|████████▋ | 26/30 [00:04<00:00,  4.47it/s]                                               {'loss': 0.2894, 'grad_norm': 1.8648028373718262, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  4.47it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.33it/s]                                               {'loss': 0.5117, 'grad_norm': 2.843548059463501, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.33it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.40it/s]                                               {'loss': 0.0612, 'grad_norm': 1.037431240081787, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.40it/s] 97%|█████████▋| 29/30 [00:05<00:00,  4.39it/s]                                               {'loss': 0.469, 'grad_norm': 1.8563153743743896, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  4.39it/s]                                               {'loss': 0.0763, 'grad_norm': 1.4967721700668335, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.39it/s]                                               {'train_runtime': 5.8647, 'train_samples_per_second': 72.468, 'train_steps_per_second': 5.115, 'train_loss': 0.37472251647462446, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.39it/s]100%|██████████| 30/30 [00:05<00:00,  5.12it/s]
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:01<00:34,  1.18s/it]                                              {'loss': 0.5736, 'grad_norm': 0.3919512629508972, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:01<00:34,  1.18s/it]  7%|▋         | 2/30 [00:01<00:16,  1.65it/s]                                              {'loss': 0.7242, 'grad_norm': 1.2363704442977905, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:16,  1.65it/s] 10%|█         | 3/30 [00:01<00:11,  2.26it/s]                                              {'loss': 0.721, 'grad_norm': 0.7155824899673462, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:11,  2.26it/s] 13%|█▎        | 4/30 [00:01<00:09,  2.76it/s]                                              {'loss': 0.7175, 'grad_norm': 0.7214653491973877, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:09,  2.76it/s] 17%|█▋        | 5/30 [00:02<00:07,  3.22it/s]                                              {'loss': 0.6549, 'grad_norm': 0.37748298048973083, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:02<00:07,  3.22it/s] 20%|██        | 6/30 [00:02<00:05,  4.01it/s]                                              {'loss': 0.9228, 'grad_norm': 1.622778058052063, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:05,  4.01it/s] 23%|██▎       | 7/30 [00:02<00:05,  4.04it/s]                                              {'loss': 0.6402, 'grad_norm': 0.5076701045036316, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:05,  4.04it/s] 27%|██▋       | 8/30 [00:02<00:05,  4.34it/s]                                              {'loss': 0.7473, 'grad_norm': 0.641652524471283, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:05,  4.34it/s] 30%|███       | 9/30 [00:02<00:04,  4.48it/s]                                              {'loss': 0.6564, 'grad_norm': 0.37939998507499695, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.48it/s] 33%|███▎      | 10/30 [00:03<00:04,  4.49it/s]                                               {'loss': 0.6459, 'grad_norm': 0.35885363817214966, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:04,  4.49it/s] 37%|███▋      | 11/30 [00:03<00:04,  4.68it/s]                                               {'loss': 0.6801, 'grad_norm': 0.21047860383987427, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:04,  4.68it/s]                                               {'loss': 0.5924, 'grad_norm': 1.2140039205551147, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:03,  4.68it/s] 43%|████▎     | 13/30 [00:03<00:03,  5.21it/s]                                               {'loss': 0.6555, 'grad_norm': 0.36224988102912903, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  5.21it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.83it/s]                                               {'loss': 0.6479, 'grad_norm': 0.3272251486778259, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.83it/s] 50%|█████     | 15/30 [00:04<00:03,  4.81it/s]                                               {'loss': 0.6443, 'grad_norm': 0.5972617864608765, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:03,  4.81it/s] 53%|█████▎    | 16/30 [00:04<00:03,  3.97it/s]                                               {'loss': 0.6631, 'grad_norm': 0.4458559453487396, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:03,  3.97it/s] 57%|█████▋    | 17/30 [00:04<00:03,  3.66it/s]                                               {'loss': 0.6523, 'grad_norm': 0.31678199768066406, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:03,  3.66it/s] 60%|██████    | 18/30 [00:04<00:02,  4.27it/s]                                               {'loss': 0.6871, 'grad_norm': 0.42599037289619446, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.27it/s] 63%|██████▎   | 19/30 [00:05<00:02,  3.68it/s]                                               {'loss': 0.7123, 'grad_norm': 1.0053420066833496, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  3.68it/s] 67%|██████▋   | 20/30 [00:05<00:02,  3.41it/s]                                               {'loss': 0.6742, 'grad_norm': 0.26475951075553894, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  3.41it/s] 70%|███████   | 21/30 [00:05<00:02,  3.24it/s]                                               {'loss': 0.6594, 'grad_norm': 0.34774863719940186, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:02,  3.24it/s] 73%|███████▎  | 22/30 [00:06<00:02,  3.15it/s]                                               {'loss': 0.6637, 'grad_norm': 0.5728161931037903, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:06<00:02,  3.15it/s] 77%|███████▋  | 23/30 [00:06<00:02,  3.19it/s]                                               {'loss': 0.723, 'grad_norm': 0.9531838893890381, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:02,  3.19it/s] 80%|████████  | 24/30 [00:06<00:01,  3.79it/s]                                               {'loss': 0.6379, 'grad_norm': 0.40700626373291016, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  3.79it/s] 83%|████████▎ | 25/30 [00:07<00:01,  3.67it/s]                                               {'loss': 0.6749, 'grad_norm': 2.3190057277679443, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:07<00:01,  3.67it/s] 87%|████████▋ | 26/30 [00:07<00:01,  3.49it/s]                                               {'loss': 0.6471, 'grad_norm': 0.4211263656616211, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:07<00:01,  3.49it/s] 90%|█████████ | 27/30 [00:07<00:00,  3.33it/s]                                               {'loss': 0.6099, 'grad_norm': 0.47868236899375916, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:07<00:00,  3.33it/s] 93%|█████████▎| 28/30 [00:08<00:00,  3.13it/s]                                               {'loss': 0.6678, 'grad_norm': 0.39664217829704285, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:08<00:00,  3.13it/s] 97%|█████████▋| 29/30 [00:08<00:00,  3.02it/s]                                               {'loss': 0.6656, 'grad_norm': 0.7730512619018555, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:08<00:00,  3.02it/s]100%|██████████| 30/30 [00:08<00:00,  3.65it/s]                                               {'loss': 0.6451, 'grad_norm': 0.7634462118148804, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  3.65it/s]                                               {'train_runtime': 8.6594, 'train_samples_per_second': 49.08, 'train_steps_per_second': 3.464, 'train_loss': 0.673571624358495, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  3.65it/s]100%|██████████| 30/30 [00:08<00:00,  3.47it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  3%|▎         | 2/66 [00:00<00:03, 16.21it/s]  6%|▌         | 4/66 [00:00<00:06,  9.83it/s]  9%|▉         | 6/66 [00:00<00:06,  8.77it/s] 11%|█         | 7/66 [00:00<00:07,  8.39it/s] 12%|█▏        | 8/66 [00:00<00:07,  7.98it/s] 14%|█▎        | 9/66 [00:01<00:07,  8.10it/s] 15%|█▌        | 10/66 [00:01<00:07,  7.94it/s] 17%|█▋        | 11/66 [00:01<00:06,  8.06it/s] 18%|█▊        | 12/66 [00:01<00:06,  8.00it/s] 20%|█▉        | 13/66 [00:01<00:06,  8.26it/s] 23%|██▎       | 15/66 [00:01<00:05,  9.32it/s] 26%|██▌       | 17/66 [00:01<00:04,  9.82it/s] 29%|██▉       | 19/66 [00:02<00:04, 10.16it/s] 32%|███▏      | 21/66 [00:02<00:04, 10.01it/s] 33%|███▎      | 22/66 [00:02<00:04,  9.53it/s] 36%|███▋      | 24/66 [00:02<00:04,  9.53it/s] 38%|███▊      | 25/66 [00:02<00:04,  9.58it/s] 41%|████      | 27/66 [00:02<00:04,  9.67it/s] 44%|████▍     | 29/66 [00:03<00:03,  9.96it/s] 47%|████▋     | 31/66 [00:03<00:03, 10.33it/s] 50%|█████     | 33/66 [00:03<00:03, 10.01it/s] 53%|█████▎    | 35/66 [00:03<00:03,  9.64it/s] 55%|█████▍    | 36/66 [00:03<00:03,  9.57it/s] 58%|█████▊    | 38/66 [00:04<00:02,  9.68it/s] 61%|██████    | 40/66 [00:04<00:02, 10.04it/s] 64%|██████▎   | 42/66 [00:04<00:02,  9.70it/s] 67%|██████▋   | 44/66 [00:04<00:02,  9.89it/s] 70%|██████▉   | 46/66 [00:04<00:02,  9.84it/s] 71%|███████   | 47/66 [00:04<00:01,  9.72it/s] 74%|███████▍  | 49/66 [00:05<00:01,  9.90it/s] 77%|███████▋  | 51/66 [00:05<00:01,  9.61it/s] 80%|████████  | 53/66 [00:05<00:01,  9.75it/s] 83%|████████▎ | 55/66 [00:05<00:01,  9.90it/s] 85%|████████▍ | 56/66 [00:05<00:01,  9.58it/s] 88%|████████▊ | 58/66 [00:06<00:00,  9.35it/s] 91%|█████████ | 60/66 [00:06<00:00,  9.34it/s] 92%|█████████▏| 61/66 [00:06<00:00,  9.34it/s] 94%|█████████▍| 62/66 [00:06<00:00,  9.43it/s] 95%|█████████▌| 63/66 [00:06<00:00,  9.38it/s] 98%|█████████▊| 65/66 [00:06<00:00,  9.72it/s]100%|██████████| 66/66 [00:06<00:00,  9.60it/s]
{'eval_loss': 0.6561814546585083, 'eval_model_preparation_time': 0.006, 'eval_acc': 0.6673058485139022, 'eval_runtime': 7.0118, 'eval_samples_per_second': 148.75, 'eval_steps_per_second': 9.413}
ROUND:3
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.65it/s]                                              {'loss': 0.6958, 'grad_norm': 0.6774725914001465, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.65it/s]  7%|▋         | 2/30 [00:00<00:04,  5.70it/s]                                              {'loss': 0.7061, 'grad_norm': 0.7528119087219238, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  5.70it/s] 10%|█         | 3/30 [00:00<00:05,  5.31it/s]                                              {'loss': 0.6967, 'grad_norm': 0.4902608096599579, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  5.31it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.26it/s]                                              {'loss': 0.6756, 'grad_norm': 0.5006309151649475, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.26it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.28it/s]                                              {'loss': 0.6933, 'grad_norm': 0.39454951882362366, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.28it/s] 20%|██        | 6/30 [00:00<00:03,  7.18it/s]                                              {'loss': 0.7558, 'grad_norm': 1.5218555927276611, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.18it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.68it/s]                                              {'loss': 0.6069, 'grad_norm': 0.3533491790294647, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.68it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.73it/s]                                              {'loss': 0.6806, 'grad_norm': 0.2751196026802063, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.73it/s] 30%|███       | 9/30 [00:01<00:03,  5.65it/s]                                              {'loss': 0.7437, 'grad_norm': 0.41542574763298035, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.65it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.58it/s]                                               {'loss': 0.6555, 'grad_norm': 0.20065391063690186, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.58it/s] 37%|███▋      | 11/30 [00:01<00:03,  5.65it/s]                                               {'loss': 0.677, 'grad_norm': 0.22758929431438446, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  5.65it/s]                                               {'loss': 0.4769, 'grad_norm': 1.1780955791473389, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:03,  5.65it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.45it/s]                                               {'loss': 0.6405, 'grad_norm': 0.464579701423645, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.45it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.01it/s]                                               {'loss': 0.6036, 'grad_norm': 0.5507714152336121, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.01it/s] 50%|█████     | 15/30 [00:02<00:02,  6.92it/s]                                               {'loss': 0.7113, 'grad_norm': 1.6742157936096191, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.92it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.54it/s]                                               {'loss': 0.6314, 'grad_norm': 0.44056591391563416, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.54it/s] 57%|█████▋    | 17/30 [00:02<00:01,  6.65it/s]                                               {'loss': 0.6233, 'grad_norm': 0.5243472456932068, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  6.65it/s]                                               {'loss': 0.6152, 'grad_norm': 0.6280138492584229, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  6.65it/s] 63%|██████▎   | 19/30 [00:02<00:01,  7.84it/s]                                               {'loss': 0.5312, 'grad_norm': 1.6381739377975464, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.84it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.72it/s]                                               {'loss': 0.7241, 'grad_norm': 1.0417414903640747, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.72it/s] 70%|███████   | 21/30 [00:03<00:01,  7.29it/s]                                               {'loss': 0.7102, 'grad_norm': 0.6445425152778625, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.29it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.75it/s]                                               {'loss': 0.5952, 'grad_norm': 0.7210351228713989, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.75it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.76it/s]                                               {'loss': 0.7372, 'grad_norm': 1.4820106029510498, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.76it/s]                                               {'loss': 0.5495, 'grad_norm': 2.616351366043091, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.76it/s] 83%|████████▎ | 25/30 [00:03<00:00,  7.36it/s]                                               {'loss': 0.6744, 'grad_norm': 0.5818386673927307, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  7.36it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.07it/s]                                               {'loss': 0.5712, 'grad_norm': 0.7772889733314514, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.07it/s] 90%|█████████ | 27/30 [00:04<00:00,  6.70it/s]                                               {'loss': 0.6594, 'grad_norm': 0.6055659651756287, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  6.70it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.06it/s]                                               {'loss': 0.6867, 'grad_norm': 0.941510796546936, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.06it/s] 97%|█████████▋| 29/30 [00:04<00:00,  6.49it/s]                                               {'loss': 0.5963, 'grad_norm': 0.7454929947853088, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  6.49it/s]                                               {'loss': 0.5577, 'grad_norm': 0.6955010294914246, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.49it/s]                                               {'train_runtime': 4.5448, 'train_samples_per_second': 93.513, 'train_steps_per_second': 6.601, 'train_loss': 0.6494114309549331, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.49it/s]100%|██████████| 30/30 [00:04<00:00,  6.61it/s]
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7343, 'grad_norm': 1.5690377950668335, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 12.11it/s]  7%|▋         | 2/30 [00:00<00:02,  9.59it/s]                                              {'loss': 0.6797, 'grad_norm': 0.5263282656669617, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02,  9.59it/s] 10%|█         | 3/30 [00:00<00:03,  8.42it/s]                                              {'loss': 0.7128, 'grad_norm': 0.6066663861274719, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.42it/s]                                              {'loss': 0.6693, 'grad_norm': 0.8087117671966553, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  8.42it/s] 17%|█▋        | 5/30 [00:00<00:02,  8.82it/s]                                              {'loss': 0.7105, 'grad_norm': 0.7804540991783142, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  8.82it/s]                                              {'loss': 0.7832, 'grad_norm': 1.6278152465820312, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02,  8.82it/s] 23%|██▎       | 7/30 [00:00<00:02, 10.72it/s]                                              {'loss': 0.6693, 'grad_norm': 0.33825960755348206, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02, 10.72it/s]                                              {'loss': 0.6679, 'grad_norm': 0.3352494239807129, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02, 10.72it/s] 30%|███       | 9/30 [00:00<00:02,  9.90it/s]                                              {'loss': 0.6472, 'grad_norm': 0.36896535754203796, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:02,  9.90it/s]                                              {'loss': 0.6895, 'grad_norm': 0.42769622802734375, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  9.90it/s] 37%|███▋      | 11/30 [00:01<00:02,  8.90it/s]                                               {'loss': 0.6844, 'grad_norm': 0.3166790008544922, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.90it/s]                                               {'loss': 0.7907, 'grad_norm': 1.4810312986373901, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.90it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.41it/s]                                               {'loss': 0.7049, 'grad_norm': 0.5609808564186096, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  9.41it/s]                                               {'loss': 0.6859, 'grad_norm': 0.40077805519104004, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  9.41it/s] 50%|█████     | 15/30 [00:01<00:01, 10.00it/s]                                               {'loss': 0.5284, 'grad_norm': 0.704103410243988, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 10.00it/s]                                               {'loss': 0.6784, 'grad_norm': 0.435303270816803, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 10.00it/s] 57%|█████▋    | 17/30 [00:01<00:01, 10.14it/s]                                               {'loss': 0.7156, 'grad_norm': 0.46881186962127686, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 10.14it/s]                                               {'loss': 0.6315, 'grad_norm': 0.508456289768219, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 10.14it/s] 63%|██████▎   | 19/30 [00:01<00:01,  9.97it/s]                                               {'loss': 0.7026, 'grad_norm': 0.6364973187446594, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:01,  9.97it/s]                                               {'loss': 0.658, 'grad_norm': 0.237552210688591, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  9.97it/s] 70%|███████   | 21/30 [00:02<00:01,  8.72it/s]                                               {'loss': 0.6626, 'grad_norm': 0.3176504075527191, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.72it/s] 73%|███████▎  | 22/30 [00:02<00:00,  8.30it/s]                                               {'loss': 0.6699, 'grad_norm': 0.42288336157798767, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  8.30it/s] 77%|███████▋  | 23/30 [00:02<00:00,  7.87it/s]                                               {'loss': 0.6104, 'grad_norm': 0.39621537923812866, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  7.87it/s]                                               {'loss': 0.6425, 'grad_norm': 0.4785601794719696, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  7.87it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.10it/s]                                               {'loss': 0.6868, 'grad_norm': 0.31110990047454834, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00,  9.10it/s]                                               {'loss': 0.6705, 'grad_norm': 0.42842549085617065, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00,  9.10it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.98it/s]                                               {'loss': 0.6802, 'grad_norm': 0.47829771041870117, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00,  9.98it/s]                                               {'loss': 0.6478, 'grad_norm': 0.31005939841270447, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00,  9.98it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.72it/s]                                               {'loss': 0.5942, 'grad_norm': 0.49806931614875793, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  9.72it/s]                                               {'loss': 0.6818, 'grad_norm': 0.6959269046783447, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  9.72it/s]                                               {'train_runtime': 3.2128, 'train_samples_per_second': 132.285, 'train_steps_per_second': 9.338, 'train_loss': 0.6763660728931427, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  9.72it/s]100%|██████████| 30/30 [00:03<00:00,  9.34it/s]
CLIENT:76
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.80it/s]                                              {'loss': 0.7853, 'grad_norm': 0.7047379016876221, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.80it/s]  7%|▋         | 2/30 [00:00<00:03,  7.15it/s]                                              {'loss': 0.6289, 'grad_norm': 0.8085843324661255, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.15it/s] 10%|█         | 3/30 [00:00<00:03,  7.17it/s]                                              {'loss': 0.6611, 'grad_norm': 0.7982718348503113, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.17it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.62it/s]                                              {'loss': 0.7164, 'grad_norm': 1.0704822540283203, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.62it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.22it/s]                                              {'loss': 0.7177, 'grad_norm': 1.22868812084198, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.22it/s]                                              {'loss': 0.9282, 'grad_norm': 1.4388463497161865, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.22it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.61it/s]                                              {'loss': 0.5919, 'grad_norm': 0.39807015657424927, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.61it/s] 27%|██▋       | 8/30 [00:01<00:02,  8.31it/s]                                              {'loss': 0.7211, 'grad_norm': 0.741873562335968, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  8.31it/s] 30%|███       | 9/30 [00:01<00:02,  8.11it/s]                                              {'loss': 0.716, 'grad_norm': 0.6033135652542114, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  8.11it/s] 33%|███▎      | 10/30 [00:01<00:02,  8.00it/s]                                               {'loss': 0.6553, 'grad_norm': 0.5738455653190613, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.00it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.73it/s]                                               {'loss': 0.6761, 'grad_norm': 0.30266889929771423, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.73it/s]                                               {'loss': 0.5621, 'grad_norm': 1.4231559038162231, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.73it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.06it/s]                                               {'loss': 0.6365, 'grad_norm': 0.5270909070968628, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  9.06it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.55it/s]                                               {'loss': 0.6256, 'grad_norm': 0.3441521227359772, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.55it/s] 50%|█████     | 15/30 [00:01<00:01,  8.18it/s]                                               {'loss': 0.6856, 'grad_norm': 1.0147053003311157, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  8.18it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.82it/s]                                               {'loss': 0.6322, 'grad_norm': 0.8845002055168152, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.82it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.60it/s]                                               {'loss': 0.6582, 'grad_norm': 0.6823427677154541, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.60it/s]                                               {'loss': 0.6178, 'grad_norm': 0.7774730920791626, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.60it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.84it/s]                                               {'loss': 0.6744, 'grad_norm': 1.775629997253418, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.84it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.45it/s]                                               {'loss': 0.6559, 'grad_norm': 0.3796127736568451, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.45it/s] 70%|███████   | 21/30 [00:02<00:01,  8.10it/s]                                               {'loss': 0.6475, 'grad_norm': 2.8238041400909424, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.10it/s] 73%|███████▎  | 22/30 [00:02<00:01,  7.94it/s]                                               {'loss': 0.6603, 'grad_norm': 0.646073043346405, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  7.94it/s]                                               {'loss': 0.6452, 'grad_norm': 0.5416561961174011, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  7.94it/s] 80%|████████  | 24/30 [00:02<00:00, 10.46it/s]                                               {'loss': 0.5847, 'grad_norm': 0.8833341002464294, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 10.46it/s]                                               {'loss': 0.6395, 'grad_norm': 0.5803977251052856, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 10.46it/s] 87%|████████▋ | 26/30 [00:02<00:00, 11.27it/s]                                               {'loss': 0.6213, 'grad_norm': 0.36628100275993347, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 11.27it/s]                                               {'loss': 0.702, 'grad_norm': 0.7615260481834412, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 11.27it/s] 93%|█████████▎| 28/30 [00:03<00:00, 11.76it/s]                                               {'loss': 0.6487, 'grad_norm': 0.9948674440383911, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 11.76it/s]                                               {'loss': 0.6895, 'grad_norm': 0.6980786323547363, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 11.76it/s]100%|██████████| 30/30 [00:03<00:00, 13.61it/s]                                               {'loss': 0.5592, 'grad_norm': 1.061752200126648, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.61it/s]                                               {'train_runtime': 3.346, 'train_samples_per_second': 127.016, 'train_steps_per_second': 8.966, 'train_loss': 0.6647940178712209, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.61it/s]100%|██████████| 30/30 [00:03<00:00,  8.97it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7032, 'grad_norm': 0.4975118041038513, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 12.02it/s]  7%|▋         | 2/30 [00:00<00:04,  5.78it/s]                                              {'loss': 0.7017, 'grad_norm': 0.5311216115951538, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  5.78it/s]                                              {'loss': 0.6692, 'grad_norm': 0.7690638899803162, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  5.78it/s] 13%|█▎        | 4/30 [00:00<00:02,  8.83it/s]                                              {'loss': 0.7052, 'grad_norm': 1.053541660308838, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  8.83it/s]                                              {'loss': 0.7417, 'grad_norm': 0.7338591814041138, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  8.83it/s] 20%|██        | 6/30 [00:00<00:04,  5.83it/s]                                              {'loss': 0.7363, 'grad_norm': 1.2656739950180054, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:04,  5.83it/s]                                              {'loss': 0.6421, 'grad_norm': 0.5257023572921753, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  5.83it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.41it/s]                                              {'loss': 0.689, 'grad_norm': 0.34196096658706665, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.41it/s]                                              {'loss': 0.6957, 'grad_norm': 0.5007399320602417, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.41it/s] 33%|███▎      | 10/30 [00:01<00:02,  8.80it/s]                                               {'loss': 0.6889, 'grad_norm': 0.3831619620323181, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.80it/s]                                               {'loss': 0.627, 'grad_norm': 0.45002248883247375, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.80it/s] 40%|████      | 12/30 [00:01<00:01, 10.80it/s]                                               {'loss': 0.5077, 'grad_norm': 1.9071590900421143, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 10.80it/s]                                               {'loss': 0.6341, 'grad_norm': 0.37645426392555237, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 10.80it/s] 47%|████▋     | 14/30 [00:01<00:01, 11.14it/s]                                               {'loss': 0.637, 'grad_norm': 0.4371190071105957, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.14it/s]                                               {'loss': 0.6894, 'grad_norm': 1.1154704093933105, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 11.14it/s] 53%|█████▎    | 16/30 [00:01<00:01, 11.74it/s]                                               {'loss': 0.5999, 'grad_norm': 0.6199571490287781, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.74it/s]                                               {'loss': 0.5401, 'grad_norm': 1.3960503339767456, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.74it/s] 60%|██████    | 18/30 [00:01<00:00, 13.39it/s]                                               {'loss': 0.5345, 'grad_norm': 1.479175329208374, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.39it/s]                                               {'loss': 0.5595, 'grad_norm': 0.9999437928199768, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.39it/s] 67%|██████▋   | 20/30 [00:01<00:00, 13.24it/s]                                               {'loss': 0.7129, 'grad_norm': 1.400902271270752, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.24it/s]                                               {'loss': 0.6815, 'grad_norm': 1.4494489431381226, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 13.24it/s] 73%|███████▎  | 22/30 [00:02<00:00, 13.11it/s]                                               {'loss': 0.5384, 'grad_norm': 0.924479603767395, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 13.11it/s]                                               {'loss': 0.6526, 'grad_norm': 1.330825924873352, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 13.11it/s] 80%|████████  | 24/30 [00:02<00:00, 14.61it/s]                                               {'loss': 0.6941, 'grad_norm': 2.5564212799072266, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 14.61it/s]                                               {'loss': 0.6358, 'grad_norm': 1.0875027179718018, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 14.61it/s] 87%|████████▋ | 26/30 [00:02<00:00, 14.08it/s]                                               {'loss': 0.6734, 'grad_norm': 1.506381869316101, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 14.08it/s]                                               {'loss': 0.5391, 'grad_norm': 1.2435412406921387, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 14.08it/s] 93%|█████████▎| 28/30 [00:02<00:00, 13.84it/s]                                               {'loss': 0.5808, 'grad_norm': 1.034940242767334, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.84it/s]                                               {'loss': 0.5099, 'grad_norm': 1.1026800870895386, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.84it/s]100%|██████████| 30/30 [00:02<00:00, 15.01it/s]                                               {'loss': 0.556, 'grad_norm': 1.966275691986084, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 15.01it/s]                                               {'train_runtime': 2.712, 'train_samples_per_second': 156.709, 'train_steps_per_second': 11.062, 'train_loss': 0.6358897070089976, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 15.01it/s]100%|██████████| 30/30 [00:02<00:00, 11.07it/s]
CLIENT:56
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7645, 'grad_norm': 1.5543670654296875, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 11.77it/s]  7%|▋         | 2/30 [00:00<00:02, 12.60it/s]                                              {'loss': 0.5429, 'grad_norm': 0.9645317792892456, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 12.60it/s]                                              {'loss': 0.6404, 'grad_norm': 0.8380231857299805, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 12.60it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.73it/s]                                              {'loss': 0.915, 'grad_norm': 2.281392812728882, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.73it/s]                                              {'loss': 0.8952, 'grad_norm': 2.1600513458251953, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:01, 12.73it/s] 20%|██        | 6/30 [00:00<00:01, 14.62it/s]                                              {'loss': 0.931, 'grad_norm': 3.5913467407226562, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 14.62it/s]                                              {'loss': 0.477, 'grad_norm': 2.4809951782226562, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.62it/s] 27%|██▋       | 8/30 [00:00<00:01, 14.00it/s]                                              {'loss': 0.6247, 'grad_norm': 0.8621284365653992, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 14.00it/s]                                              {'loss': 0.7301, 'grad_norm': 2.510385751724243, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 14.00it/s] 33%|███▎      | 10/30 [00:00<00:01, 13.45it/s]                                               {'loss': 0.6036, 'grad_norm': 1.016845941543579, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.45it/s]                                               {'loss': 0.6973, 'grad_norm': 2.7535791397094727, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.45it/s] 40%|████      | 12/30 [00:00<00:01, 14.97it/s]                                               {'loss': 0.5286, 'grad_norm': 1.8782285451889038, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 14.97it/s]                                               {'loss': 0.5499, 'grad_norm': 1.1168041229248047, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 14.97it/s] 47%|████▋     | 14/30 [00:00<00:01, 14.18it/s]                                               {'loss': 0.5131, 'grad_norm': 1.470484972000122, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:00<00:01, 14.18it/s]                                               {'loss': 0.7811, 'grad_norm': 3.0284416675567627, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 14.18it/s] 53%|█████▎    | 16/30 [00:01<00:01, 13.69it/s]                                               {'loss': 0.5906, 'grad_norm': 2.8419315814971924, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.69it/s]                                               {'loss': 0.549, 'grad_norm': 2.88700532913208, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.69it/s] 60%|██████    | 18/30 [00:01<00:00, 15.10it/s]                                               {'loss': 1.052, 'grad_norm': 10.841999053955078, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 15.10it/s]                                               {'loss': 0.667, 'grad_norm': 1.9871196746826172, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 15.10it/s] 67%|██████▋   | 20/30 [00:01<00:00, 14.35it/s]                                               {'loss': 0.7159, 'grad_norm': 2.6065423488616943, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.35it/s]                                               {'loss': 0.4998, 'grad_norm': 2.263383388519287, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.35it/s] 73%|███████▎  | 22/30 [00:01<00:00, 14.04it/s]                                               {'loss': 0.6675, 'grad_norm': 4.094479084014893, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 14.04it/s]                                               {'loss': 0.6599, 'grad_norm': 2.0013134479522705, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 14.04it/s]                                               {'loss': 0.6102, 'grad_norm': 2.5928497314453125, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 14.04it/s] 83%|████████▎ | 25/30 [00:01<00:00, 15.22it/s]                                               {'loss': 0.6037, 'grad_norm': 2.500836133956909, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 15.22it/s]                                               {'loss': 0.6117, 'grad_norm': 1.0831410884857178, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 15.22it/s] 90%|█████████ | 27/30 [00:01<00:00, 14.51it/s]                                               {'loss': 0.5812, 'grad_norm': 0.9721843600273132, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 14.51it/s]                                               {'loss': 0.6271, 'grad_norm': 1.9801725149154663, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:01<00:00, 14.51it/s] 97%|█████████▋| 29/30 [00:02<00:00, 14.18it/s]                                               {'loss': 0.7797, 'grad_norm': 2.195401906967163, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 14.18it/s]                                               {'loss': 0.5415, 'grad_norm': 2.112596035003662, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 14.18it/s]                                               {'train_runtime': 2.1688, 'train_samples_per_second': 195.961, 'train_steps_per_second': 13.833, 'train_loss': 0.6650397429863611, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 14.18it/s]100%|██████████| 30/30 [00:02<00:00, 13.84it/s]
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.10it/s]                                              {'loss': 0.8551, 'grad_norm': 2.337038516998291, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.10it/s]  7%|▋         | 2/30 [00:00<00:04,  6.04it/s]                                              {'loss': 0.5378, 'grad_norm': 1.314470887184143, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.04it/s] 10%|█         | 3/30 [00:00<00:04,  5.60it/s]                                              {'loss': 0.2841, 'grad_norm': 1.7820709943771362, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  5.60it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.57it/s]                                              {'loss': 0.3075, 'grad_norm': 0.8400915861129761, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.57it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.76it/s]                                              {'loss': 0.2511, 'grad_norm': 1.5512056350708008, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.76it/s]                                              {'loss': 0.0562, 'grad_norm': 0.9558250308036804, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:04,  5.76it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.27it/s]                                              {'loss': 0.2906, 'grad_norm': 1.0633968114852905, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.27it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.54it/s]                                              {'loss': 0.0164, 'grad_norm': 0.3040052354335785, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.54it/s] 30%|███       | 9/30 [00:01<00:03,  6.15it/s]                                              {'loss': 0.3385, 'grad_norm': 1.0183604955673218, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.15it/s] 33%|███▎      | 10/30 [00:01<00:02,  6.70it/s]                                               {'loss': 0.0219, 'grad_norm': 0.29674509167671204, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  6.70it/s] 37%|███▋      | 11/30 [00:01<00:03,  6.09it/s]                                               {'loss': 0.0151, 'grad_norm': 0.38570693135261536, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  6.09it/s]                                               {'loss': 0.0038, 'grad_norm': 0.0631554052233696, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.09it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.60it/s]                                               {'loss': 0.0024, 'grad_norm': 0.027437543496489525, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.60it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.20it/s]                                               {'loss': 0.0038, 'grad_norm': 0.049307070672512054, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.20it/s] 50%|█████     | 15/30 [00:02<00:02,  6.27it/s]                                               {'loss': 0.3669, 'grad_norm': 1.1539818048477173, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.27it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.28it/s]                                               {'loss': 0.0028, 'grad_norm': 0.037650011479854584, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.28it/s]                                               {'loss': 0.2438, 'grad_norm': 1.275879979133606, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  6.28it/s] 60%|██████    | 18/30 [00:02<00:01,  7.99it/s]                                               {'loss': 0.0048, 'grad_norm': 0.10334624350070953, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.99it/s] 63%|██████▎   | 19/30 [00:02<00:01,  7.47it/s]                                               {'loss': 0.0035, 'grad_norm': 0.04493758827447891, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.47it/s]                                               {'loss': 0.2323, 'grad_norm': 1.92151939868927, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  7.47it/s] 70%|███████   | 21/30 [00:03<00:01,  8.73it/s]                                               {'loss': 0.0077, 'grad_norm': 0.14285004138946533, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  8.73it/s]                                               {'loss': 0.0068, 'grad_norm': 0.07889065891504288, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00,  8.73it/s] 77%|███████▋  | 23/30 [00:03<00:00,  9.93it/s]                                               {'loss': 0.2413, 'grad_norm': 1.4398548603057861, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  9.93it/s]                                               {'loss': 0.0048, 'grad_norm': 0.07797205448150635, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  9.93it/s] 83%|████████▎ | 25/30 [00:03<00:00, 11.92it/s]                                               {'loss': 0.554, 'grad_norm': 2.3644943237304688, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 11.92it/s]                                               {'loss': 0.009, 'grad_norm': 0.1389169543981552, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 11.92it/s] 90%|█████████ | 27/30 [00:03<00:00, 12.27it/s]                                               {'loss': 0.0121, 'grad_norm': 0.2425287663936615, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 12.27it/s]                                               {'loss': 0.0114, 'grad_norm': 0.2261834740638733, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 12.27it/s] 97%|█████████▋| 29/30 [00:03<00:00, 12.54it/s]                                               {'loss': 0.009, 'grad_norm': 0.16253986954689026, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 12.54it/s]                                               {'loss': 0.0139, 'grad_norm': 0.7046534419059753, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.54it/s]                                               {'train_runtime': 3.7455, 'train_samples_per_second': 113.468, 'train_steps_per_second': 8.01, 'train_loss': 0.15695089766134818, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.54it/s]100%|██████████| 30/30 [00:03<00:00,  8.01it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.36it/s]                                              {'loss': 0.8886, 'grad_norm': 1.5757132768630981, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.36it/s]                                              {'loss': 0.5248, 'grad_norm': 1.1312185525894165, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.36it/s] 10%|█         | 3/30 [00:00<00:02, 11.04it/s]                                              {'loss': 0.7315, 'grad_norm': 1.3579283952713013, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.04it/s]                                              {'loss': 0.7156, 'grad_norm': 1.5170987844467163, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.04it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.80it/s]                                              {'loss': 0.6925, 'grad_norm': 1.8088208436965942, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.80it/s]                                              {'loss': 1.1888, 'grad_norm': 5.355564594268799, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.80it/s] 23%|██▎       | 7/30 [00:00<00:01, 14.04it/s]                                              {'loss': 0.3207, 'grad_norm': 1.4980168342590332, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.04it/s]                                              {'loss': 0.5024, 'grad_norm': 1.524904727935791, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 14.04it/s] 30%|███       | 9/30 [00:00<00:01, 13.63it/s]                                              {'loss': 0.7182, 'grad_norm': 1.9556418657302856, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.63it/s]                                              {'loss': 0.6158, 'grad_norm': 1.3291292190551758, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.63it/s] 37%|███▋      | 11/30 [00:00<00:01, 13.36it/s]                                               {'loss': 0.6811, 'grad_norm': 3.2574057579040527, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.36it/s]                                               {'loss': 0.4116, 'grad_norm': 1.8831219673156738, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 13.36it/s] 43%|████▎     | 13/30 [00:00<00:01, 14.85it/s]                                               {'loss': 0.4693, 'grad_norm': 1.2544363737106323, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 14.85it/s]                                               {'loss': 0.5729, 'grad_norm': 1.4077409505844116, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 14.85it/s] 50%|█████     | 15/30 [00:01<00:01, 14.29it/s]                                               {'loss': 0.6098, 'grad_norm': 1.5415016412734985, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 14.29it/s]                                               {'loss': 0.4871, 'grad_norm': 1.751750111579895, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:00, 14.29it/s] 57%|█████▋    | 17/30 [00:01<00:00, 14.29it/s]                                               {'loss': 0.5625, 'grad_norm': 1.7927998304367065, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 14.29it/s]                                               {'loss': 0.6845, 'grad_norm': 2.2551326751708984, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 14.29it/s] 63%|██████▎   | 19/30 [00:01<00:00, 15.39it/s]                                               {'loss': 0.606, 'grad_norm': 1.781101107597351, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 15.39it/s]                                               {'loss': 0.4401, 'grad_norm': 2.0714118480682373, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 15.39it/s] 70%|███████   | 21/30 [00:01<00:00, 14.65it/s]                                               {'loss': 0.5865, 'grad_norm': 1.882001280784607, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.65it/s]                                               {'loss': 0.5558, 'grad_norm': 0.892518937587738, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 14.65it/s] 77%|███████▋  | 23/30 [00:01<00:00, 14.19it/s]                                               {'loss': 0.7284, 'grad_norm': 1.341086745262146, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 14.19it/s]                                               {'loss': 0.3371, 'grad_norm': 1.9895260334014893, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 14.19it/s] 83%|████████▎ | 25/30 [00:01<00:00, 15.22it/s]                                               {'loss': 0.3978, 'grad_norm': 1.5850435495376587, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 15.22it/s]                                               {'loss': 0.4939, 'grad_norm': 1.411254644393921, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 15.22it/s] 90%|█████████ | 27/30 [00:01<00:00, 14.54it/s]                                               {'loss': 0.5692, 'grad_norm': 1.3331124782562256, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 14.54it/s]                                               {'loss': 0.5131, 'grad_norm': 1.279151201248169, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 14.54it/s] 97%|█████████▋| 29/30 [00:02<00:00, 13.95it/s]                                               {'loss': 0.5165, 'grad_norm': 1.285574197769165, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.95it/s]                                               {'loss': 0.2991, 'grad_norm': 3.4052271842956543, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.95it/s]                                               {'train_runtime': 2.2015, 'train_samples_per_second': 193.051, 'train_steps_per_second': 13.627, 'train_loss': 0.5807079374790192, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.95it/s]100%|██████████| 30/30 [00:02<00:00, 13.63it/s]
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.57it/s]                                              {'loss': 0.9251, 'grad_norm': 1.962915062904358, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.57it/s]  7%|▋         | 2/30 [00:00<00:04,  6.86it/s]                                              {'loss': 0.5189, 'grad_norm': 1.2868107557296753, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.86it/s]                                              {'loss': 0.3107, 'grad_norm': 1.7097506523132324, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  6.86it/s] 13%|█▎        | 4/30 [00:00<00:03,  8.25it/s]                                              {'loss': 0.3215, 'grad_norm': 0.8030442595481873, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  8.25it/s] 17%|█▋        | 5/30 [00:00<00:02,  8.56it/s]                                              {'loss': 0.0717, 'grad_norm': 0.7846633791923523, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  8.56it/s]                                              {'loss': 0.0162, 'grad_norm': 0.22635796666145325, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02,  8.56it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.61it/s]                                              {'loss': 0.0145, 'grad_norm': 0.26648497581481934, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  9.61it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.39it/s]                                              {'loss': 0.0041, 'grad_norm': 0.08071727305650711, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  9.39it/s]                                              {'loss': 0.01, 'grad_norm': 0.25370925664901733, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:02,  9.39it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.09it/s]                                               {'loss': 0.2708, 'grad_norm': 1.7709311246871948, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  9.09it/s] 37%|███▋      | 11/30 [00:01<00:02,  8.38it/s]                                               {'loss': 0.0038, 'grad_norm': 0.09837155044078827, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.38it/s]                                               {'loss': 0.004, 'grad_norm': 0.11312294751405716, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.38it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.51it/s]                                               {'loss': 0.0036, 'grad_norm': 0.1110849604010582, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  9.51it/s]                                               {'loss': 0.0047, 'grad_norm': 0.13830390572547913, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  9.51it/s] 50%|█████     | 15/30 [00:01<00:01,  9.63it/s]                                               {'loss': 0.3928, 'grad_norm': 1.3211278915405273, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  9.63it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.69it/s]                                               {'loss': 0.0058, 'grad_norm': 0.14675840735435486, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  9.69it/s] 57%|█████▋    | 17/30 [00:01<00:01,  8.89it/s]                                               {'loss': 0.0158, 'grad_norm': 0.37958788871765137, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01,  8.89it/s]                                               {'loss': 0.0061, 'grad_norm': 0.1886328011751175, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01,  8.89it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.85it/s]                                               {'loss': 0.0039, 'grad_norm': 0.10116122663021088, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  9.85it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.53it/s]                                               {'loss': 0.0079, 'grad_norm': 0.19710245728492737, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  9.53it/s] 70%|███████   | 21/30 [00:02<00:00,  9.53it/s]                                               {'loss': 0.0054, 'grad_norm': 0.12714241445064545, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00,  9.53it/s]                                               {'loss': 0.0045, 'grad_norm': 0.08213278651237488, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  9.53it/s] 77%|███████▋  | 23/30 [00:02<00:00, 10.59it/s]                                               {'loss': 0.4418, 'grad_norm': 1.9579421281814575, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 10.59it/s]                                               {'loss': 0.003, 'grad_norm': 0.04778296872973442, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 10.59it/s] 83%|████████▎ | 25/30 [00:02<00:00, 11.20it/s]                                               {'loss': 0.01, 'grad_norm': 0.20189037919044495, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 11.20it/s]                                               {'loss': 0.0048, 'grad_norm': 0.09390775114297867, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 11.20it/s] 90%|█████████ | 27/30 [00:02<00:00, 11.29it/s]                                               {'loss': 0.3557, 'grad_norm': 0.6485812067985535, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 11.29it/s]                                               {'loss': 0.0038, 'grad_norm': 0.08227040618658066, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 11.29it/s] 97%|█████████▋| 29/30 [00:02<00:00, 11.06it/s]                                               {'loss': 0.0062, 'grad_norm': 0.11457358300685883, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 11.06it/s]                                               {'loss': 0.0087, 'grad_norm': 0.17667602002620697, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.06it/s]                                               {'train_runtime': 3.1075, 'train_samples_per_second': 136.766, 'train_steps_per_second': 9.654, 'train_loss': 0.12518922835588456, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.06it/s]100%|██████████| 30/30 [00:03<00:00,  9.66it/s]
CLIENT:60
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.9393, 'grad_norm': 1.920961618423462, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 12.04it/s]  7%|▋         | 2/30 [00:00<00:02, 12.50it/s]                                              {'loss': 0.5157, 'grad_norm': 1.1122660636901855, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 12.50it/s]                                              {'loss': 0.3307, 'grad_norm': 1.1902674436569214, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 12.50it/s] 13%|█▎        | 4/30 [00:00<00:01, 13.33it/s]                                              {'loss': 0.6981, 'grad_norm': 2.2703163623809814, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:01, 13.33it/s]                                              {'loss': 0.6208, 'grad_norm': 2.3102564811706543, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:01, 13.33it/s] 20%|██        | 6/30 [00:00<00:01, 15.52it/s]                                              {'loss': 0.1407, 'grad_norm': 1.412431001663208, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 15.52it/s]                                              {'loss': 0.5217, 'grad_norm': 4.050453186035156, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 15.52it/s] 27%|██▋       | 8/30 [00:00<00:01, 13.41it/s]                                              {'loss': 0.4961, 'grad_norm': 2.695896625518799, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.41it/s]                                              {'loss': 0.3364, 'grad_norm': 1.276368498802185, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.41it/s] 33%|███▎      | 10/30 [00:00<00:01, 13.91it/s]                                               {'loss': 0.3737, 'grad_norm': 1.4739980697631836, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.91it/s]                                               {'loss': 0.257, 'grad_norm': 1.4188874959945679, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.91it/s] 40%|████      | 12/30 [00:00<00:01, 15.51it/s]                                               {'loss': 0.0952, 'grad_norm': 1.7693833112716675, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 15.51it/s]                                               {'loss': 0.6851, 'grad_norm': 2.196049451828003, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 15.51it/s] 47%|████▋     | 14/30 [00:00<00:01, 14.72it/s]                                               {'loss': 0.3505, 'grad_norm': 1.7844959497451782, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:00<00:01, 14.72it/s]                                               {'loss': 1.2409, 'grad_norm': 5.2256646156311035, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 14.72it/s] 53%|█████▎    | 16/30 [00:01<00:01, 13.99it/s]                                               {'loss': 0.2141, 'grad_norm': 1.2673274278640747, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.99it/s]                                               {'loss': 0.1934, 'grad_norm': 0.541756272315979, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.99it/s] 60%|██████    | 18/30 [00:01<00:00, 15.20it/s]                                               {'loss': 0.0842, 'grad_norm': 1.4860087633132935, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 15.20it/s]                                               {'loss': 0.4477, 'grad_norm': 1.572184443473816, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 15.20it/s] 67%|██████▋   | 20/30 [00:01<00:00, 14.43it/s]                                               {'loss': 0.3979, 'grad_norm': 4.566061019897461, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.43it/s]                                               {'loss': 0.7796, 'grad_norm': 3.215528964996338, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.43it/s] 73%|███████▎  | 22/30 [00:01<00:00, 13.87it/s]                                               {'loss': 0.2882, 'grad_norm': 1.5652716159820557, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.87it/s]                                               {'loss': 0.4755, 'grad_norm': 2.995338201522827, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.87it/s] 80%|████████  | 24/30 [00:01<00:00, 15.08it/s]                                               {'loss': 0.0556, 'grad_norm': 1.1358217000961304, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 15.08it/s]                                               {'loss': 0.3916, 'grad_norm': 1.4584152698516846, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 15.08it/s] 87%|████████▋ | 26/30 [00:01<00:00, 14.33it/s]                                               {'loss': 0.6154, 'grad_norm': 3.9663496017456055, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 14.33it/s]                                               {'loss': 0.1983, 'grad_norm': 1.2331032752990723, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 14.33it/s] 93%|█████████▎| 28/30 [00:01<00:00, 13.77it/s]                                               {'loss': 0.4094, 'grad_norm': 1.9680804014205933, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:01<00:00, 13.77it/s]                                               {'loss': 0.6845, 'grad_norm': 3.3121979236602783, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.77it/s]100%|██████████| 30/30 [00:02<00:00, 15.15it/s]                                               {'loss': 0.1143, 'grad_norm': 2.3710763454437256, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 15.15it/s]                                               {'train_runtime': 2.1443, 'train_samples_per_second': 198.2, 'train_steps_per_second': 13.991, 'train_loss': 0.4317249018698931, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 15.15it/s]100%|██████████| 30/30 [00:02<00:00, 13.99it/s]
CLIENT:67
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.5601, 'grad_norm': 0.6468165516853333, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 11.39it/s]  7%|▋         | 2/30 [00:00<00:02, 12.02it/s]                                              {'loss': 0.7306, 'grad_norm': 0.5971323251724243, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 12.02it/s]                                              {'loss': 0.7366, 'grad_norm': 0.47394585609436035, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 12.02it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.45it/s]                                              {'loss': 0.7084, 'grad_norm': 0.6689370274543762, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.45it/s]                                              {'loss': 0.6461, 'grad_norm': 0.28728294372558594, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.45it/s] 20%|██        | 6/30 [00:00<00:01, 14.91it/s]                                              {'loss': 0.5751, 'grad_norm': 0.732799768447876, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 14.91it/s]                                              {'loss': 0.7039, 'grad_norm': 0.22491659224033356, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.91it/s] 27%|██▋       | 8/30 [00:00<00:01, 14.52it/s]                                              {'loss': 0.6468, 'grad_norm': 0.6302206516265869, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 14.52it/s]                                              {'loss': 0.674, 'grad_norm': 0.5563658475875854, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 14.52it/s] 33%|███▎      | 10/30 [00:00<00:01, 13.84it/s]                                               {'loss': 0.6807, 'grad_norm': 0.23391488194465637, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.84it/s]                                               {'loss': 0.6656, 'grad_norm': 0.17717432975769043, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.84it/s] 40%|████      | 12/30 [00:00<00:01, 15.36it/s]                                               {'loss': 0.7293, 'grad_norm': 1.799420714378357, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 15.36it/s]                                               {'loss': 0.6584, 'grad_norm': 1.9760295152664185, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 15.36it/s] 47%|████▋     | 14/30 [00:00<00:01, 14.37it/s]                                               {'loss': 0.6744, 'grad_norm': 0.5309015512466431, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:00<00:01, 14.37it/s]                                               {'loss': 0.5685, 'grad_norm': 0.5521024465560913, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 14.37it/s] 53%|█████▎    | 16/30 [00:01<00:01, 13.85it/s]                                               {'loss': 0.6608, 'grad_norm': 0.41973012685775757, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.85it/s]                                               {'loss': 0.6908, 'grad_norm': 0.5087435245513916, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.85it/s] 60%|██████    | 18/30 [00:01<00:00, 15.28it/s]                                               {'loss': 0.6744, 'grad_norm': 0.6382030248641968, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 15.28it/s]                                               {'loss': 0.7048, 'grad_norm': 0.954721212387085, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 15.28it/s] 67%|██████▋   | 20/30 [00:01<00:00, 14.53it/s]                                               {'loss': 0.6376, 'grad_norm': 0.2374635934829712, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.53it/s]                                               {'loss': 0.6738, 'grad_norm': 0.23491501808166504, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.53it/s] 73%|███████▎  | 22/30 [00:01<00:00, 13.92it/s]                                               {'loss': 0.609, 'grad_norm': 0.2925271689891815, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.92it/s]                                               {'loss': 0.6041, 'grad_norm': 0.516370415687561, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.92it/s] 80%|████████  | 24/30 [00:01<00:00, 15.17it/s]                                               {'loss': 0.6375, 'grad_norm': 0.495340496301651, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 15.17it/s]                                               {'loss': 0.7132, 'grad_norm': 0.518951416015625, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 15.17it/s] 87%|████████▋ | 26/30 [00:01<00:00, 14.46it/s]                                               {'loss': 0.6462, 'grad_norm': 0.3833599388599396, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 14.46it/s]                                               {'loss': 0.6522, 'grad_norm': 0.468561589717865, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 14.46it/s] 93%|█████████▎| 28/30 [00:01<00:00, 13.90it/s]                                               {'loss': 0.6455, 'grad_norm': 0.3722720146179199, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:01<00:00, 13.90it/s]                                               {'loss': 0.5324, 'grad_norm': 0.47546347975730896, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.90it/s]100%|██████████| 30/30 [00:02<00:00, 15.13it/s]                                               {'loss': 0.5902, 'grad_norm': 0.569116473197937, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 15.13it/s]                                               {'train_runtime': 2.1574, 'train_samples_per_second': 196.997, 'train_steps_per_second': 13.906, 'train_loss': 0.6543596009413402, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 15.13it/s]100%|██████████| 30/30 [00:02<00:00, 13.91it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  8%|▊         | 5/66 [00:00<00:01, 40.07it/s] 15%|█▌        | 10/66 [00:00<00:01, 33.83it/s] 21%|██        | 14/66 [00:00<00:01, 32.32it/s] 27%|██▋       | 18/66 [00:00<00:01, 31.69it/s] 33%|███▎      | 22/66 [00:00<00:01, 31.12it/s] 39%|███▉      | 26/66 [00:00<00:01, 30.80it/s] 45%|████▌     | 30/66 [00:00<00:01, 30.81it/s] 52%|█████▏    | 34/66 [00:01<00:01, 30.72it/s] 58%|█████▊    | 38/66 [00:01<00:00, 30.38it/s] 64%|██████▎   | 42/66 [00:01<00:00, 29.81it/s] 68%|██████▊   | 45/66 [00:01<00:00, 29.55it/s] 73%|███████▎  | 48/66 [00:01<00:00, 29.59it/s] 79%|███████▉  | 52/66 [00:01<00:00, 29.92it/s] 85%|████████▍ | 56/66 [00:01<00:00, 30.02it/s] 91%|█████████ | 60/66 [00:01<00:00, 30.13it/s] 97%|█████████▋| 64/66 [00:02<00:00, 30.15it/s]100%|██████████| 66/66 [00:02<00:00, 31.07it/s]
{'eval_loss': 0.6510804891586304, 'eval_model_preparation_time': 0.0051, 'eval_acc': 0.6673058485139022, 'eval_runtime': 2.1611, 'eval_samples_per_second': 482.617, 'eval_steps_per_second': 30.54}
ROUND:4
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.30it/s]                                              {'loss': 0.5834, 'grad_norm': 1.2066150903701782, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.30it/s]  7%|▋         | 2/30 [00:00<00:04,  6.60it/s]                                              {'loss': 0.4812, 'grad_norm': 1.253586769104004, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.60it/s] 10%|█         | 3/30 [00:00<00:04,  6.73it/s]                                              {'loss': 0.394, 'grad_norm': 0.941906213760376, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.73it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.89it/s]                                              {'loss': 0.3295, 'grad_norm': 0.7999705672264099, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.89it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.98it/s]                                              {'loss': 0.7722, 'grad_norm': 2.4928128719329834, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.98it/s]                                              {'loss': 0.0334, 'grad_norm': 0.6253448724746704, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.98it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.25it/s]                                              {'loss': 0.141, 'grad_norm': 0.7639615535736084, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.25it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.84it/s]                                              {'loss': 0.6648, 'grad_norm': 4.222907066345215, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.84it/s] 30%|███       | 9/30 [00:01<00:02,  7.54it/s]                                              {'loss': 0.7531, 'grad_norm': 4.202439785003662, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.54it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.32it/s]                                               {'loss': 0.7005, 'grad_norm': 3.799502372741699, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.32it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.25it/s]                                               {'loss': 0.2501, 'grad_norm': 1.1724767684936523, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.25it/s]                                               {'loss': 0.6254, 'grad_norm': 6.200440406799316, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.25it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.29it/s]                                               {'loss': 0.4034, 'grad_norm': 1.9102002382278442, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.29it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.81it/s]                                               {'loss': 0.3741, 'grad_norm': 2.143925189971924, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.81it/s] 50%|█████     | 15/30 [00:02<00:01,  7.53it/s]                                               {'loss': 0.5299, 'grad_norm': 1.9871923923492432, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.53it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.32it/s]                                               {'loss': 0.4763, 'grad_norm': 1.9577968120574951, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.32it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.26it/s]                                               {'loss': 0.206, 'grad_norm': 3.08296537399292, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.26it/s]                                               {'loss': 0.6111, 'grad_norm': 0.5902665257453918, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.26it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.15it/s]                                               {'loss': 0.4085, 'grad_norm': 1.3358979225158691, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.15it/s] 67%|██████▋   | 20/30 [00:02<00:01,  7.75it/s]                                               {'loss': 0.494, 'grad_norm': 0.34078237414360046, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  7.75it/s] 70%|███████   | 21/30 [00:02<00:01,  7.59it/s]                                               {'loss': 0.4935, 'grad_norm': 1.6617717742919922, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  7.59it/s] 73%|███████▎  | 22/30 [00:02<00:01,  7.37it/s]                                               {'loss': 0.4913, 'grad_norm': 0.9244189858436584, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  7.37it/s] 77%|███████▋  | 23/30 [00:03<00:00,  7.28it/s]                                               {'loss': 0.202, 'grad_norm': 3.053347110748291, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.28it/s]                                               {'loss': 0.4768, 'grad_norm': 0.5114918351173401, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.28it/s] 83%|████████▎ | 25/30 [00:03<00:00,  8.13it/s]                                               {'loss': 0.4, 'grad_norm': 1.2744592428207397, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  8.13it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.86it/s]                                               {'loss': 0.4839, 'grad_norm': 0.8216522336006165, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.86it/s] 90%|█████████ | 27/30 [00:03<00:00,  7.50it/s]                                               {'loss': 0.5083, 'grad_norm': 0.8973533511161804, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  7.50it/s]                                               {'loss': 0.2138, 'grad_norm': 3.2825162410736084, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  7.50it/s] 97%|█████████▋| 29/30 [00:03<00:00,  8.98it/s]                                               {'loss': 0.3773, 'grad_norm': 1.141298770904541, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  8.98it/s]                                               {'loss': 0.4128, 'grad_norm': 1.1095603704452515, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.98it/s]                                               {'train_runtime': 3.857, 'train_samples_per_second': 110.188, 'train_steps_per_second': 7.778, 'train_loss': 0.4430463312814633, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.98it/s]100%|██████████| 30/30 [00:03<00:00,  7.78it/s]
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7161, 'grad_norm': 1.2234997749328613, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 12.24it/s]  7%|▋         | 2/30 [00:00<00:02, 12.51it/s]                                              {'loss': 0.5149, 'grad_norm': 1.326242208480835, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 12.51it/s]                                              {'loss': 0.3644, 'grad_norm': 0.9709956645965576, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 12.51it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.80it/s]                                              {'loss': 0.1053, 'grad_norm': 1.0016292333602905, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.80it/s]                                              {'loss': 0.4139, 'grad_norm': 1.4656168222427368, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:01, 12.80it/s] 20%|██        | 6/30 [00:00<00:01, 15.03it/s]                                              {'loss': 0.0123, 'grad_norm': 0.2532929480075836, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 15.03it/s]                                              {'loss': 0.3287, 'grad_norm': 0.9358121752738953, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 15.03it/s] 27%|██▋       | 8/30 [00:00<00:01, 14.24it/s]                                              {'loss': 1.198, 'grad_norm': 4.32725715637207, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 14.24it/s]                                              {'loss': 0.0302, 'grad_norm': 0.5675392746925354, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 14.24it/s] 33%|███▎      | 10/30 [00:00<00:01, 13.75it/s]                                               {'loss': 0.3178, 'grad_norm': 1.7030184268951416, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.75it/s]                                               {'loss': 0.028, 'grad_norm': 0.6014112234115601, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.75it/s] 40%|████      | 12/30 [00:00<00:01, 15.05it/s]                                               {'loss': 0.0113, 'grad_norm': 0.2852349877357483, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 15.05it/s]                                               {'loss': 0.5801, 'grad_norm': 3.195749521255493, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 15.05it/s] 47%|████▋     | 14/30 [00:01<00:01, 13.81it/s]                                               {'loss': 0.0289, 'grad_norm': 0.43852606415748596, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.81it/s]                                               {'loss': 0.6112, 'grad_norm': 1.526890516281128, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.81it/s] 53%|█████▎    | 16/30 [00:01<00:01, 13.28it/s]                                               {'loss': 0.2092, 'grad_norm': 1.2744866609573364, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.28it/s]                                               {'loss': 0.0581, 'grad_norm': 0.8378219604492188, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.28it/s] 60%|██████    | 18/30 [00:01<00:00, 14.55it/s]                                               {'loss': 0.6538, 'grad_norm': 4.098398685455322, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 14.55it/s]                                               {'loss': 0.0361, 'grad_norm': 0.6130167841911316, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.55it/s] 67%|██████▋   | 20/30 [00:01<00:00, 14.02it/s]                                               {'loss': 0.736, 'grad_norm': 3.9912068843841553, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.02it/s]                                               {'loss': 0.263, 'grad_norm': 1.070475459098816, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.02it/s] 73%|███████▎  | 22/30 [00:01<00:00, 13.63it/s]                                               {'loss': 0.1864, 'grad_norm': 1.0213637351989746, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.63it/s]                                               {'loss': 0.2283, 'grad_norm': 1.2646437883377075, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.63it/s] 80%|████████  | 24/30 [00:01<00:00, 14.89it/s]                                               {'loss': 0.0521, 'grad_norm': 0.8713698387145996, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 14.89it/s]                                               {'loss': 0.308, 'grad_norm': 0.945003092288971, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.89it/s] 87%|████████▋ | 26/30 [00:01<00:00, 14.24it/s]                                               {'loss': 0.0478, 'grad_norm': 0.7702080011367798, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 14.24it/s]                                               {'loss': 0.2003, 'grad_norm': 1.4346684217453003, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 14.24it/s] 93%|█████████▎| 28/30 [00:02<00:00, 13.33it/s]                                               {'loss': 0.4093, 'grad_norm': 1.779510259628296, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.33it/s]                                               {'loss': 0.226, 'grad_norm': 3.3089845180511475, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.33it/s]100%|██████████| 30/30 [00:02<00:00, 12.06it/s]                                               {'loss': 0.4915, 'grad_norm': 2.7106475830078125, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.06it/s]                                               {'train_runtime': 2.3081, 'train_samples_per_second': 184.135, 'train_steps_per_second': 12.998, 'train_loss': 0.31223428845405576, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.06it/s]100%|██████████| 30/30 [00:02<00:00, 13.00it/s]
CLIENT:79
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.8816, 'grad_norm': 1.5440223217010498, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 11.64it/s]  7%|▋         | 2/30 [00:00<00:02, 11.86it/s]                                              {'loss': 0.4456, 'grad_norm': 1.482916235923767, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 11.86it/s]                                              {'loss': 0.3626, 'grad_norm': 1.1747264862060547, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.86it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.96it/s]                                              {'loss': 0.1263, 'grad_norm': 1.0961594581604004, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.96it/s]                                              {'loss': 0.311, 'grad_norm': 0.7564390897750854, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:01, 12.96it/s] 20%|██        | 6/30 [00:00<00:01, 14.90it/s]                                              {'loss': 1.0717, 'grad_norm': 4.093180179595947, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 14.90it/s]                                              {'loss': 0.1736, 'grad_norm': 0.5230966210365295, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.90it/s] 27%|██▋       | 8/30 [00:00<00:01, 13.68it/s]                                              {'loss': 0.6164, 'grad_norm': 1.3651996850967407, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.68it/s]                                              {'loss': 0.5039, 'grad_norm': 2.9465792179107666, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.68it/s] 33%|███▎      | 10/30 [00:00<00:01, 13.17it/s]                                               {'loss': 0.1372, 'grad_norm': 8.852104187011719, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.17it/s]                                               {'loss': 0.0368, 'grad_norm': 1.1383217573165894, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.17it/s] 40%|████      | 12/30 [00:00<00:01, 14.65it/s]                                               {'loss': 0.02, 'grad_norm': 0.9329896569252014, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 14.65it/s]                                               {'loss': 0.5331, 'grad_norm': 7.521122932434082, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 14.65it/s] 47%|████▋     | 14/30 [00:01<00:01, 13.87it/s]                                               {'loss': 0.034, 'grad_norm': 1.4830976724624634, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.87it/s]                                               {'loss': 0.7387, 'grad_norm': 2.8119962215423584, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.87it/s] 53%|█████▎    | 16/30 [00:01<00:01, 13.65it/s]                                               {'loss': 0.2594, 'grad_norm': 1.2269001007080078, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.65it/s]                                               {'loss': 0.0537, 'grad_norm': 1.0750266313552856, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.65it/s] 60%|██████    | 18/30 [00:01<00:00, 14.90it/s]                                               {'loss': 0.0317, 'grad_norm': 0.6513564586639404, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 14.90it/s]                                               {'loss': 0.3569, 'grad_norm': 8.648131370544434, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.90it/s] 67%|██████▋   | 20/30 [00:01<00:00, 14.30it/s]                                               {'loss': 0.2764, 'grad_norm': 0.9633097052574158, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.30it/s]                                               {'loss': 0.0376, 'grad_norm': 0.8915454149246216, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.30it/s] 73%|███████▎  | 22/30 [00:01<00:00, 13.66it/s]                                               {'loss': 0.5028, 'grad_norm': 5.473183631896973, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.66it/s]                                               {'loss': 0.3282, 'grad_norm': 4.007955551147461, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.66it/s] 80%|████████  | 24/30 [00:01<00:00, 14.94it/s]                                               {'loss': 0.0371, 'grad_norm': 1.898595929145813, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 14.94it/s]                                               {'loss': 0.0314, 'grad_norm': 0.6312583088874817, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.94it/s] 87%|████████▋ | 26/30 [00:01<00:00, 14.32it/s]                                               {'loss': 0.4475, 'grad_norm': 2.236553907394409, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 14.32it/s]                                               {'loss': 0.0334, 'grad_norm': 0.35235098004341125, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 14.32it/s] 93%|█████████▎| 28/30 [00:02<00:00, 13.67it/s]                                               {'loss': 0.2659, 'grad_norm': 0.923422634601593, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.67it/s]                                               {'loss': 0.6531, 'grad_norm': 3.528364658355713, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.67it/s]100%|██████████| 30/30 [00:02<00:00, 14.83it/s]                                               {'loss': 0.0947, 'grad_norm': 1.8522952795028687, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 14.83it/s]                                               {'train_runtime': 2.1994, 'train_samples_per_second': 193.236, 'train_steps_per_second': 13.64, 'train_loss': 0.3134091135114431, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 14.83it/s]100%|██████████| 30/30 [00:02<00:00, 13.65it/s]
CLIENT:14
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.46it/s]                                              {'loss': 0.5754, 'grad_norm': 0.3991168737411499, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.46it/s]  7%|▋         | 2/30 [00:00<00:07,  3.65it/s]                                              {'loss': 0.693, 'grad_norm': 1.0862957239151, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.65it/s] 10%|█         | 3/30 [00:00<00:07,  3.56it/s]                                              {'loss': 0.6682, 'grad_norm': 0.8230907320976257, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.56it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.56it/s]                                              {'loss': 0.6332, 'grad_norm': 0.6673860549926758, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.56it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.62it/s]                                              {'loss': 0.7673, 'grad_norm': 0.8607910871505737, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.62it/s] 20%|██        | 6/30 [00:01<00:05,  4.55it/s]                                              {'loss': 0.9658, 'grad_norm': 2.891798496246338, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.55it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.56it/s]                                              {'loss': 0.6494, 'grad_norm': 0.6621870994567871, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.56it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.65it/s]                                              {'loss': 0.7313, 'grad_norm': 0.8381350636482239, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.65it/s] 30%|███       | 9/30 [00:02<00:04,  4.63it/s]                                              {'loss': 0.6302, 'grad_norm': 0.5288094282150269, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.63it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.84it/s]                                               {'loss': 0.5868, 'grad_norm': 0.5708320736885071, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.84it/s] 37%|███▋      | 11/30 [00:02<00:03,  4.80it/s]                                               {'loss': 0.6534, 'grad_norm': 0.39649835228919983, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  4.80it/s]                                               {'loss': 0.4922, 'grad_norm': 0.5564385056495667, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.80it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.42it/s]                                               {'loss': 0.5761, 'grad_norm': 0.9117193222045898, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.42it/s] 47%|████▋     | 14/30 [00:03<00:03,  5.16it/s]                                               {'loss': 0.5331, 'grad_norm': 0.6501332521438599, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  5.16it/s] 50%|█████     | 15/30 [00:03<00:02,  5.00it/s]                                               {'loss': 0.6995, 'grad_norm': 1.6959608793258667, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.00it/s] 53%|█████▎    | 16/30 [00:03<00:02,  4.72it/s]                                               {'loss': 0.6376, 'grad_norm': 0.7425787448883057, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  4.72it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.60it/s]                                               {'loss': 0.5768, 'grad_norm': 0.5947365760803223, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.60it/s] 60%|██████    | 18/30 [00:03<00:02,  5.26it/s]                                               {'loss': 0.7458, 'grad_norm': 1.8012685775756836, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.26it/s] 63%|██████▎   | 19/30 [00:04<00:02,  4.66it/s]                                               {'loss': 0.5173, 'grad_norm': 1.2066421508789062, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  4.66it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.30it/s]                                               {'loss': 0.6709, 'grad_norm': 0.9939844012260437, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.30it/s] 70%|███████   | 21/30 [00:04<00:02,  4.19it/s]                                               {'loss': 0.5236, 'grad_norm': 1.0720839500427246, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:02,  4.19it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.11it/s]                                               {'loss': 0.6338, 'grad_norm': 0.7351992130279541, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.11it/s] 77%|███████▋  | 23/30 [00:05<00:01,  4.04it/s]                                               {'loss': 0.5727, 'grad_norm': 0.7330506443977356, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  4.04it/s] 80%|████████  | 24/30 [00:05<00:01,  4.78it/s]                                               {'loss': 0.5651, 'grad_norm': 0.8003690242767334, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  4.78it/s] 83%|████████▎ | 25/30 [00:05<00:00,  5.11it/s]                                               {'loss': 0.6721, 'grad_norm': 1.0734124183654785, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  5.11it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.09it/s]                                               {'loss': 0.5621, 'grad_norm': 3.316519021987915, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.09it/s] 90%|█████████ | 27/30 [00:05<00:00,  5.22it/s]                                               {'loss': 0.6152, 'grad_norm': 0.8376705050468445, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  5.22it/s] 93%|█████████▎| 28/30 [00:06<00:00,  5.17it/s]                                               {'loss': 0.5464, 'grad_norm': 0.7158557176589966, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  5.17it/s] 97%|█████████▋| 29/30 [00:06<00:00,  5.28it/s]                                               {'loss': 0.5623, 'grad_norm': 1.5282789468765259, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  5.28it/s]                                               {'loss': 0.4796, 'grad_norm': 0.7587786912918091, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.28it/s]                                               {'train_runtime': 6.4234, 'train_samples_per_second': 66.164, 'train_steps_per_second': 4.67, 'train_loss': 0.6245416949192683, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.28it/s]100%|██████████| 30/30 [00:06<00:00,  4.67it/s]
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  5.90it/s]                                              {'loss': 0.6073, 'grad_norm': 0.5930036902427673, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  5.90it/s]  7%|▋         | 2/30 [00:00<00:04,  6.41it/s]                                              {'loss': 0.725, 'grad_norm': 0.648391842842102, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.41it/s] 10%|█         | 3/30 [00:00<00:03,  7.33it/s]                                              {'loss': 0.6975, 'grad_norm': 0.4138062298297882, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.33it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.42it/s]                                              {'loss': 0.6967, 'grad_norm': 0.7790049910545349, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.42it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.32it/s]                                              {'loss': 0.6659, 'grad_norm': 0.43876150250434875, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.32it/s]                                              {'loss': 0.7986, 'grad_norm': 1.4720935821533203, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.32it/s] 23%|██▎       | 7/30 [00:00<00:02,  7.70it/s]                                              {'loss': 0.6898, 'grad_norm': 1.2089791297912598, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  7.70it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.35it/s]                                              {'loss': 0.6796, 'grad_norm': 0.482587993144989, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.35it/s] 30%|███       | 9/30 [00:01<00:02,  7.88it/s]                                              {'loss': 0.6886, 'grad_norm': 0.38783302903175354, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.88it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.59it/s]                                               {'loss': 0.6769, 'grad_norm': 0.27245059609413147, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.59it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.22it/s]                                               {'loss': 0.6769, 'grad_norm': 0.3421083092689514, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.22it/s]                                               {'loss': 0.6179, 'grad_norm': 0.8256431221961975, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.22it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.45it/s]                                               {'loss': 0.6418, 'grad_norm': 0.33420830965042114, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.45it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.72it/s]                                               {'loss': 0.6692, 'grad_norm': 0.5890607237815857, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.72it/s] 50%|█████     | 15/30 [00:01<00:01,  7.91it/s]                                               {'loss': 0.5913, 'grad_norm': 0.7115603089332581, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  7.91it/s]                                               {'loss': 0.6743, 'grad_norm': 0.8184503316879272, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.91it/s] 57%|█████▋    | 17/30 [00:02<00:01,  9.39it/s]                                               {'loss': 0.6745, 'grad_norm': 0.46343356370925903, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  9.39it/s]                                               {'loss': 0.6588, 'grad_norm': 0.5799679160118103, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  9.39it/s] 63%|██████▎   | 19/30 [00:02<00:00, 11.37it/s]                                               {'loss': 0.7351, 'grad_norm': 1.347408652305603, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 11.37it/s]                                               {'loss': 0.6632, 'grad_norm': 0.2749175727367401, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 11.37it/s] 70%|███████   | 21/30 [00:02<00:00, 11.47it/s]                                               {'loss': 0.6573, 'grad_norm': 0.32885128259658813, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.47it/s]                                               {'loss': 0.6588, 'grad_norm': 0.35160601139068604, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.47it/s] 77%|███████▋  | 23/30 [00:02<00:00, 12.14it/s]                                               {'loss': 0.6677, 'grad_norm': 0.9208483099937439, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 12.14it/s]                                               {'loss': 0.6216, 'grad_norm': 0.5272150635719299, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 12.14it/s]                                               {'loss': 0.6821, 'grad_norm': 0.5263352394104004, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.14it/s] 87%|████████▋ | 26/30 [00:02<00:00, 13.68it/s]                                               {'loss': 0.6166, 'grad_norm': 0.45687657594680786, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.68it/s]                                               {'loss': 0.6162, 'grad_norm': 0.5102985501289368, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.68it/s] 93%|█████████▎| 28/30 [00:02<00:00, 13.30it/s]                                               {'loss': 0.6572, 'grad_norm': 0.40337708592414856, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.30it/s]                                               {'loss': 0.6225, 'grad_norm': 0.5895019173622131, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 13.30it/s]100%|██████████| 30/30 [00:03<00:00, 13.73it/s]                                               {'loss': 0.6117, 'grad_norm': 0.5152273774147034, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.73it/s]                                               {'train_runtime': 3.1199, 'train_samples_per_second': 136.223, 'train_steps_per_second': 9.616, 'train_loss': 0.6646892309188843, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.73it/s]100%|██████████| 30/30 [00:03<00:00,  9.62it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:01<00:32,  1.12s/it]                                              {'loss': 0.7032, 'grad_norm': 0.509086549282074, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:01<00:32,  1.12s/it]  7%|▋         | 2/30 [00:01<00:15,  1.85it/s]                                              {'loss': 0.6975, 'grad_norm': 0.5307100415229797, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:15,  1.85it/s] 10%|█         | 3/30 [00:01<00:10,  2.49it/s]                                              {'loss': 0.6652, 'grad_norm': 0.781707763671875, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:10,  2.49it/s] 13%|█▎        | 4/30 [00:01<00:08,  2.96it/s]                                              {'loss': 0.6989, 'grad_norm': 1.0561587810516357, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  2.96it/s] 17%|█▋        | 5/30 [00:01<00:07,  3.45it/s]                                              {'loss': 0.7428, 'grad_norm': 1.4653980731964111, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:07,  3.45it/s]                                              {'loss': 0.735, 'grad_norm': 1.26859450340271, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:06,  3.45it/s] 23%|██▎       | 7/30 [00:02<00:04,  4.74it/s]                                              {'loss': 0.6337, 'grad_norm': 0.5257669687271118, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:04,  4.74it/s] 27%|██▋       | 8/30 [00:02<00:04,  4.92it/s]                                              {'loss': 0.6942, 'grad_norm': 0.4131312966346741, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:04,  4.92it/s] 30%|███       | 9/30 [00:02<00:04,  5.06it/s]                                              {'loss': 0.702, 'grad_norm': 1.1488327980041504, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  5.06it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.01it/s]                                               {'loss': 0.6919, 'grad_norm': 0.4631965160369873, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.01it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.66it/s]                                               {'loss': 0.6148, 'grad_norm': 0.7229556441307068, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.66it/s]                                               {'loss': 0.4702, 'grad_norm': 2.028418779373169, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.66it/s] 43%|████▎     | 13/30 [00:03<00:02,  7.24it/s]                                               {'loss': 0.627, 'grad_norm': 0.4450678527355194, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:02,  7.24it/s] 47%|████▋     | 14/30 [00:03<00:02,  7.07it/s]                                               {'loss': 0.6307, 'grad_norm': 0.5883185267448425, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:02,  7.07it/s] 50%|█████     | 15/30 [00:03<00:02,  6.97it/s]                                               {'loss': 0.71, 'grad_norm': 1.6665531396865845, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  6.97it/s] 53%|█████▎    | 16/30 [00:03<00:01,  7.47it/s]                                               {'loss': 0.597, 'grad_norm': 1.8768051862716675, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:01,  7.47it/s] 57%|█████▋    | 17/30 [00:03<00:01,  7.73it/s]                                               {'loss': 0.5319, 'grad_norm': 1.5573310852050781, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:01,  7.73it/s]                                               {'loss': 0.5298, 'grad_norm': 1.5098963975906372, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  7.73it/s] 63%|██████▎   | 19/30 [00:03<00:01,  8.55it/s]                                               {'loss': 0.5598, 'grad_norm': 1.078948736190796, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  8.55it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.93it/s]                                               {'loss': 0.684, 'grad_norm': 1.3947546482086182, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.93it/s] 70%|███████   | 21/30 [00:04<00:01,  7.62it/s]                                               {'loss': 0.6717, 'grad_norm': 1.3362656831741333, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  7.62it/s] 73%|███████▎  | 22/30 [00:04<00:01,  7.23it/s]                                               {'loss': 0.5344, 'grad_norm': 0.8374312520027161, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  7.23it/s] 77%|███████▋  | 23/30 [00:04<00:00,  7.04it/s]                                               {'loss': 0.6408, 'grad_norm': 1.274086594581604, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:00,  7.04it/s]                                               {'loss': 0.686, 'grad_norm': 2.708397626876831, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  7.04it/s] 83%|████████▎ | 25/30 [00:04<00:00,  7.85it/s]                                               {'loss': 0.6383, 'grad_norm': 1.1221399307250977, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  7.85it/s] 87%|████████▋ | 26/30 [00:04<00:00,  7.99it/s]                                               {'loss': 0.6634, 'grad_norm': 1.3204787969589233, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  7.99it/s]                                               {'loss': 0.5531, 'grad_norm': 1.2660088539123535, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  7.99it/s] 93%|█████████▎| 28/30 [00:04<00:00,  9.31it/s]                                               {'loss': 0.569, 'grad_norm': 1.1273661851882935, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  9.31it/s]                                               {'loss': 0.5162, 'grad_norm': 1.1519978046417236, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  9.31it/s]100%|██████████| 30/30 [00:05<00:00, 11.35it/s]                                               {'loss': 0.5432, 'grad_norm': 1.9163554906845093, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00, 11.35it/s]                                               {'train_runtime': 5.107, 'train_samples_per_second': 83.219, 'train_steps_per_second': 5.874, 'train_loss': 0.6311876356601716, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00, 11.35it/s]100%|██████████| 30/30 [00:05<00:00,  5.88it/s]
CLIENT:19
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:01<00:33,  1.14s/it]                                              {'loss': 0.6405, 'grad_norm': 0.5409796237945557, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:01<00:33,  1.14s/it]  7%|▋         | 2/30 [00:01<00:15,  1.77it/s]                                              {'loss': 0.6845, 'grad_norm': 0.8244088292121887, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:15,  1.77it/s]                                              {'loss': 0.7096, 'grad_norm': 0.5632830858230591, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:15,  1.77it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.56it/s]                                              {'loss': 0.6468, 'grad_norm': 0.5928254127502441, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.56it/s] 17%|█▋        | 5/30 [00:01<00:06,  4.11it/s]                                              {'loss': 0.6639, 'grad_norm': 0.5727446675300598, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  4.11it/s]                                              {'loss': 0.8312, 'grad_norm': 1.9447360038757324, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.11it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.68it/s]                                              {'loss': 0.6362, 'grad_norm': 0.5734857320785522, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.68it/s]                                              {'loss': 0.7157, 'grad_norm': 0.5502220392227173, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.68it/s] 30%|███       | 9/30 [00:02<00:02,  7.26it/s]                                              {'loss': 0.6604, 'grad_norm': 0.45713645219802856, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:02,  7.26it/s]                                              {'loss': 0.679, 'grad_norm': 0.5254818201065063, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:02,  7.26it/s] 37%|███▋      | 11/30 [00:02<00:02,  8.46it/s]                                               {'loss': 0.6909, 'grad_norm': 0.9134364724159241, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:02,  8.46it/s]                                               {'loss': 0.6295, 'grad_norm': 1.187137246131897, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  8.46it/s] 43%|████▎     | 13/30 [00:02<00:01,  9.97it/s]                                               {'loss': 0.6125, 'grad_norm': 0.8757323622703552, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:01,  9.97it/s]                                               {'loss': 0.6571, 'grad_norm': 0.474686861038208, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  9.97it/s] 50%|█████     | 15/30 [00:02<00:01, 10.63it/s]                                               {'loss': 0.7201, 'grad_norm': 0.9121584296226501, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01, 10.63it/s]                                               {'loss': 0.6607, 'grad_norm': 0.4506169259548187, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01, 10.63it/s] 57%|█████▋    | 17/30 [00:02<00:01, 11.17it/s]                                               {'loss': 0.6461, 'grad_norm': 0.6167334914207458, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01, 11.17it/s]                                               {'loss': 0.6533, 'grad_norm': 1.0797454118728638, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 11.17it/s] 63%|██████▎   | 19/30 [00:02<00:00, 11.30it/s]                                               {'loss': 0.6349, 'grad_norm': 0.5537985563278198, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 11.30it/s]                                               {'loss': 0.655, 'grad_norm': 0.30545225739479065, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 11.30it/s] 70%|███████   | 21/30 [00:02<00:00, 11.71it/s]                                               {'loss': 0.6139, 'grad_norm': 0.8656860589981079, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.71it/s]                                               {'loss': 0.648, 'grad_norm': 0.5488327741622925, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00, 11.71it/s] 77%|███████▋  | 23/30 [00:03<00:00, 11.79it/s]                                               {'loss': 0.6681, 'grad_norm': 0.8138293623924255, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00, 11.79it/s]                                               {'loss': 0.5865, 'grad_norm': 0.9354305863380432, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00, 11.79it/s] 83%|████████▎ | 25/30 [00:03<00:00, 12.71it/s]                                               {'loss': 0.6351, 'grad_norm': 0.7154836058616638, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 12.71it/s]                                               {'loss': 0.6055, 'grad_norm': 0.4339042901992798, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 12.71it/s] 90%|█████████ | 27/30 [00:03<00:00, 11.72it/s]                                               {'loss': 0.5848, 'grad_norm': 0.5707725882530212, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 11.72it/s]                                               {'loss': 0.655, 'grad_norm': 0.6270489692687988, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 11.72it/s] 97%|█████████▋| 29/30 [00:03<00:00, 11.37it/s]                                               {'loss': 0.6632, 'grad_norm': 1.0295624732971191, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 11.37it/s]                                               {'loss': 0.5121, 'grad_norm': 1.1371233463287354, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.37it/s]                                               {'train_runtime': 3.8325, 'train_samples_per_second': 110.893, 'train_steps_per_second': 7.828, 'train_loss': 0.6533309539159139, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.37it/s]100%|██████████| 30/30 [00:03<00:00,  7.83it/s]
CLIENT:28
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  4.01it/s]                                              {'loss': 0.607, 'grad_norm': 0.8326046466827393, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  4.01it/s]  7%|▋         | 2/30 [00:00<00:05,  5.21it/s]                                              {'loss': 0.7041, 'grad_norm': 0.647408664226532, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.21it/s] 10%|█         | 3/30 [00:00<00:04,  6.60it/s]                                              {'loss': 0.7122, 'grad_norm': 0.4909581243991852, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.60it/s]                                              {'loss': 0.7367, 'grad_norm': 0.6380311846733093, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.60it/s] 17%|█▋        | 5/30 [00:00<00:02,  8.79it/s]                                              {'loss': 0.6792, 'grad_norm': 0.5728166699409485, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  8.79it/s]                                              {'loss': 0.6651, 'grad_norm': 2.7239952087402344, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02,  8.79it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.02it/s]                                              {'loss': 0.656, 'grad_norm': 0.3289836347103119, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  9.02it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.17it/s]                                              {'loss': 0.673, 'grad_norm': 0.537707507610321, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  9.17it/s]                                              {'loss': 0.6553, 'grad_norm': 0.3269660472869873, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  9.17it/s] 33%|███▎      | 10/30 [00:01<00:01, 10.41it/s]                                               {'loss': 0.6657, 'grad_norm': 0.19301322102546692, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:01, 10.41it/s]                                               {'loss': 0.6971, 'grad_norm': 0.20114532113075256, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 10.41it/s] 40%|████      | 12/30 [00:01<00:01, 12.15it/s]                                               {'loss': 0.6685, 'grad_norm': 1.136747121810913, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 12.15it/s]                                               {'loss': 0.6434, 'grad_norm': 0.3482051193714142, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 12.15it/s] 47%|████▋     | 14/30 [00:01<00:01, 11.98it/s]                                               {'loss': 0.6512, 'grad_norm': 0.599011242389679, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.98it/s]                                               {'loss': 0.5517, 'grad_norm': 0.9843051433563232, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 11.98it/s] 53%|█████▎    | 16/30 [00:01<00:01, 11.75it/s]                                               {'loss': 0.626, 'grad_norm': 0.4572576880455017, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.75it/s]                                               {'loss': 0.684, 'grad_norm': 0.4751400351524353, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.75it/s] 60%|██████    | 18/30 [00:01<00:01, 11.90it/s]                                               {'loss': 0.6172, 'grad_norm': 0.4761667251586914, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 11.90it/s]                                               {'loss': 0.7301, 'grad_norm': 0.9615659713745117, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 11.90it/s] 67%|██████▋   | 20/30 [00:01<00:00, 11.66it/s]                                               {'loss': 0.6331, 'grad_norm': 0.23733778297901154, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 11.66it/s]                                               {'loss': 0.6474, 'grad_norm': 0.3742387592792511, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.66it/s] 73%|███████▎  | 22/30 [00:02<00:00, 12.00it/s]                                               {'loss': 0.6174, 'grad_norm': 0.4692017436027527, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 12.00it/s]                                               {'loss': 0.5811, 'grad_norm': 0.6811964511871338, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 12.00it/s] 80%|████████  | 24/30 [00:02<00:00, 13.24it/s]                                               {'loss': 0.5958, 'grad_norm': 0.7109788060188293, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 13.24it/s]                                               {'loss': 0.6886, 'grad_norm': 0.5558748245239258, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 13.24it/s] 87%|████████▋ | 26/30 [00:02<00:00, 13.08it/s]                                               {'loss': 0.6117, 'grad_norm': 0.6589981913566589, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.08it/s]                                               {'loss': 0.6913, 'grad_norm': 0.8535416126251221, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.08it/s] 93%|█████████▎| 28/30 [00:02<00:00, 12.97it/s]                                               {'loss': 0.6082, 'grad_norm': 0.4844897985458374, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.97it/s]                                               {'loss': 0.5273, 'grad_norm': 0.7046945691108704, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.97it/s]100%|██████████| 30/30 [00:02<00:00, 14.00it/s]                                               {'loss': 0.5215, 'grad_norm': 0.7310933470726013, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 14.00it/s]                                               {'train_runtime': 2.7459, 'train_samples_per_second': 154.778, 'train_steps_per_second': 10.926, 'train_loss': 0.6449018796284993, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 14.00it/s]100%|██████████| 30/30 [00:02<00:00, 10.93it/s]
CLIENT:94
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.47it/s]                                              {'loss': 0.6035, 'grad_norm': 0.7012687921524048, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.47it/s]  7%|▋         | 2/30 [00:00<00:06,  4.19it/s]                                              {'loss': 0.714, 'grad_norm': 0.5727666020393372, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.19it/s] 10%|█         | 3/30 [00:00<00:06,  4.10it/s]                                              {'loss': 0.777, 'grad_norm': 0.7181863784790039, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.10it/s] 13%|█▎        | 4/30 [00:00<00:06,  4.23it/s]                                              {'loss': 0.6791, 'grad_norm': 0.8503714203834534, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:06,  4.23it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.42it/s]                                              {'loss': 0.6917, 'grad_norm': 0.3293690085411072, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.42it/s] 20%|██        | 6/30 [00:02<00:11,  2.13it/s]                                              {'loss': 0.7368, 'grad_norm': 1.2955725193023682, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:11,  2.13it/s] 23%|██▎       | 7/30 [00:02<00:09,  2.55it/s]                                              {'loss': 0.6914, 'grad_norm': 0.2737177014350891, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:09,  2.55it/s] 27%|██▋       | 8/30 [00:02<00:07,  2.98it/s]                                              {'loss': 0.6907, 'grad_norm': 0.3484560251235962, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:07,  2.98it/s] 30%|███       | 9/30 [00:02<00:06,  3.33it/s]                                              {'loss': 0.6766, 'grad_norm': 0.36299073696136475, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:06,  3.33it/s] 33%|███▎      | 10/30 [00:02<00:05,  3.81it/s]                                               {'loss': 0.656, 'grad_norm': 0.3682881295681, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  3.81it/s] 37%|███▋      | 11/30 [00:03<00:04,  4.29it/s]                                               {'loss': 0.6645, 'grad_norm': 0.142353817820549, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:04,  4.29it/s]                                               {'loss': 0.6108, 'grad_norm': 0.9415826797485352, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:04,  4.29it/s] 43%|████▎     | 13/30 [00:03<00:02,  5.95it/s]                                               {'loss': 0.6522, 'grad_norm': 0.3584838807582855, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:02,  5.95it/s] 47%|████▋     | 14/30 [00:03<00:02,  5.95it/s]                                               {'loss': 0.6841, 'grad_norm': 0.352798193693161, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:02,  5.95it/s] 50%|█████     | 15/30 [00:03<00:02,  6.02it/s]                                               {'loss': 0.6777, 'grad_norm': 0.6925176382064819, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  6.02it/s] 53%|█████▎    | 16/30 [00:03<00:02,  6.06it/s]                                               {'loss': 0.6493, 'grad_norm': 0.42024582624435425, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  6.06it/s] 57%|█████▋    | 17/30 [00:03<00:01,  6.70it/s]                                               {'loss': 0.6687, 'grad_norm': 0.23709914088249207, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:01,  6.70it/s]                                               {'loss': 0.6842, 'grad_norm': 0.589937150478363, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  6.70it/s] 63%|██████▎   | 19/30 [00:04<00:01,  7.50it/s]                                               {'loss': 0.6825, 'grad_norm': 0.6235268115997314, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:01,  7.50it/s] 67%|██████▋   | 20/30 [00:04<00:01,  6.64it/s]                                               {'loss': 0.6798, 'grad_norm': 0.19889895617961884, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  6.64it/s] 70%|███████   | 21/30 [00:04<00:01,  6.56it/s]                                               {'loss': 0.667, 'grad_norm': 0.2614133059978485, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  6.56it/s] 73%|███████▎  | 22/30 [00:04<00:01,  6.62it/s]                                               {'loss': 0.637, 'grad_norm': 0.24816173315048218, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  6.62it/s]                                               {'loss': 0.6811, 'grad_norm': 0.6599502563476562, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  6.62it/s] 80%|████████  | 24/30 [00:04<00:00,  8.41it/s]                                               {'loss': 0.6356, 'grad_norm': 0.3103283643722534, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  8.41it/s] 83%|████████▎ | 25/30 [00:04<00:00,  7.47it/s]                                               {'loss': 0.7114, 'grad_norm': 0.6304976344108582, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  7.47it/s]                                               {'loss': 0.674, 'grad_norm': 0.410925954580307, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  7.47it/s] 90%|█████████ | 27/30 [00:05<00:00,  8.55it/s]                                               {'loss': 0.6287, 'grad_norm': 0.4370085895061493, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  8.55it/s] 93%|█████████▎| 28/30 [00:05<00:00,  8.44it/s]                                               {'loss': 0.6667, 'grad_norm': 0.2630443572998047, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  8.44it/s] 97%|█████████▋| 29/30 [00:05<00:00,  7.39it/s]                                               {'loss': 0.6476, 'grad_norm': 0.47355955839157104, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  7.39it/s]                                               {'loss': 0.632, 'grad_norm': 0.7479552626609802, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  7.39it/s]                                               {'train_runtime': 5.6698, 'train_samples_per_second': 74.958, 'train_steps_per_second': 5.291, 'train_loss': 0.6717183729012807, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  7.39it/s]100%|██████████| 30/30 [00:05<00:00,  5.30it/s]
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.47it/s]                                              {'loss': 0.7072, 'grad_norm': 1.319387435913086, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.47it/s]                                              {'loss': 0.558, 'grad_norm': 0.9636070728302002, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.47it/s] 10%|█         | 3/30 [00:00<00:02,  9.56it/s]                                              {'loss': 0.5506, 'grad_norm': 0.951701283454895, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02,  9.56it/s]                                              {'loss': 0.8327, 'grad_norm': 1.9575514793395996, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  9.56it/s] 17%|█▋        | 5/30 [00:00<00:02, 10.56it/s]                                              {'loss': 0.9456, 'grad_norm': 2.9390358924865723, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.56it/s]                                              {'loss': 1.0832, 'grad_norm': 4.21027135848999, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 10.56it/s] 23%|██▎       | 7/30 [00:00<00:02, 10.63it/s]                                              {'loss': 0.4649, 'grad_norm': 0.6664107441902161, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02, 10.63it/s]                                              {'loss': 0.78, 'grad_norm': 1.5086454153060913, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02, 10.63it/s] 30%|███       | 9/30 [00:00<00:02, 10.49it/s]                                              {'loss': 0.614, 'grad_norm': 0.7933766841888428, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:02, 10.49it/s]                                              {'loss': 0.612, 'grad_norm': 0.6877017021179199, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 10.49it/s] 37%|███▋      | 11/30 [00:01<00:01, 11.03it/s]                                               {'loss': 0.5804, 'grad_norm': 0.8155260682106018, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 11.03it/s]                                               {'loss': 0.42, 'grad_norm': 2.5224039554595947, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 11.03it/s] 43%|████▎     | 13/30 [00:01<00:01, 11.17it/s]                                               {'loss': 0.641, 'grad_norm': 0.7864898443222046, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 11.17it/s]                                               {'loss': 0.5505, 'grad_norm': 0.945133626461029, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.17it/s] 50%|█████     | 15/30 [00:01<00:01, 11.09it/s]                                               {'loss': 0.861, 'grad_norm': 2.6120190620422363, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 11.09it/s]                                               {'loss': 0.5157, 'grad_norm': 0.735241174697876, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.09it/s] 57%|█████▋    | 17/30 [00:01<00:01, 11.52it/s]                                               {'loss': 0.4202, 'grad_norm': 1.2062532901763916, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.52it/s]                                               {'loss': 0.3317, 'grad_norm': 1.8795429468154907, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 11.52it/s] 63%|██████▎   | 19/30 [00:01<00:00, 13.22it/s]                                               {'loss': 0.5244, 'grad_norm': 0.9607718586921692, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.22it/s]                                               {'loss': 0.6453, 'grad_norm': 0.9019384980201721, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.22it/s] 70%|███████   | 21/30 [00:01<00:00, 13.03it/s]                                               {'loss': 0.5538, 'grad_norm': 1.0658433437347412, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.03it/s]                                               {'loss': 0.4284, 'grad_norm': 0.677413284778595, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.03it/s] 77%|███████▋  | 23/30 [00:01<00:00, 12.74it/s]                                               {'loss': 0.5535, 'grad_norm': 1.0052493810653687, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 12.74it/s]                                               {'loss': 0.5484, 'grad_norm': 1.2280528545379639, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 12.74it/s] 83%|████████▎ | 25/30 [00:02<00:00, 13.39it/s]                                               {'loss': 0.628, 'grad_norm': 1.0539453029632568, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 13.39it/s]                                               {'loss': 0.476, 'grad_norm': 0.7451648712158203, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.39it/s] 90%|█████████ | 27/30 [00:02<00:00, 13.00it/s]                                               {'loss': 0.4659, 'grad_norm': 1.099380373954773, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.00it/s]                                               {'loss': 0.5175, 'grad_norm': 0.6139459609985352, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.00it/s] 97%|█████████▋| 29/30 [00:02<00:00, 12.81it/s]                                               {'loss': 0.6002, 'grad_norm': 1.7713559865951538, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.81it/s]                                               {'loss': 0.3723, 'grad_norm': 1.0726596117019653, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.81it/s]                                               {'train_runtime': 2.6461, 'train_samples_per_second': 160.615, 'train_steps_per_second': 11.338, 'train_loss': 0.5927444010972976, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.81it/s]100%|██████████| 30/30 [00:02<00:00, 11.37it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  3%|▎         | 2/66 [00:00<00:04, 14.55it/s]  6%|▌         | 4/66 [00:00<00:08,  7.34it/s]  8%|▊         | 5/66 [00:00<00:08,  6.80it/s]  9%|▉         | 6/66 [00:00<00:08,  6.76it/s] 11%|█         | 7/66 [00:00<00:09,  6.45it/s] 12%|█▏        | 8/66 [00:01<00:09,  6.43it/s] 14%|█▎        | 9/66 [00:01<00:09,  6.16it/s] 15%|█▌        | 10/66 [00:01<00:08,  6.37it/s] 17%|█▋        | 11/66 [00:01<00:08,  6.65it/s] 18%|█▊        | 12/66 [00:01<00:07,  7.19it/s] 21%|██        | 14/66 [00:01<00:06,  8.57it/s] 24%|██▍       | 16/66 [00:02<00:05,  9.46it/s] 27%|██▋       | 18/66 [00:02<00:04,  9.93it/s] 30%|███       | 20/66 [00:02<00:04,  9.73it/s] 32%|███▏      | 21/66 [00:02<00:04,  9.64it/s] 33%|███▎      | 22/66 [00:02<00:04,  9.33it/s] 35%|███▍      | 23/66 [00:02<00:04,  9.42it/s] 36%|███▋      | 24/66 [00:02<00:04,  9.45it/s] 39%|███▉      | 26/66 [00:03<00:03, 10.31it/s] 42%|████▏     | 28/66 [00:03<00:03, 10.51it/s] 45%|████▌     | 30/66 [00:03<00:03, 11.31it/s] 48%|████▊     | 32/66 [00:03<00:03, 10.98it/s] 52%|█████▏    | 34/66 [00:03<00:02, 10.97it/s] 55%|█████▍    | 36/66 [00:03<00:02, 11.13it/s] 58%|█████▊    | 38/66 [00:04<00:02, 10.98it/s] 61%|██████    | 40/66 [00:04<00:02, 11.17it/s] 64%|██████▎   | 42/66 [00:04<00:02, 10.58it/s] 67%|██████▋   | 44/66 [00:04<00:02, 10.30it/s] 70%|██████▉   | 46/66 [00:04<00:01, 10.01it/s] 73%|███████▎  | 48/66 [00:05<00:01, 10.08it/s] 76%|███████▌  | 50/66 [00:05<00:01, 10.31it/s] 79%|███████▉  | 52/66 [00:05<00:01, 10.72it/s] 82%|████████▏ | 54/66 [00:05<00:01, 10.31it/s] 85%|████████▍ | 56/66 [00:05<00:00, 10.40it/s] 88%|████████▊ | 58/66 [00:06<00:00, 10.33it/s] 91%|█████████ | 60/66 [00:06<00:00, 10.33it/s] 94%|█████████▍| 62/66 [00:06<00:00,  9.92it/s] 95%|█████████▌| 63/66 [00:06<00:00,  9.82it/s] 98%|█████████▊| 65/66 [00:06<00:00, 10.82it/s]100%|██████████| 66/66 [00:06<00:00,  9.71it/s]
{'eval_loss': 0.6467371582984924, 'eval_model_preparation_time': 0.0102, 'eval_acc': 0.6711409395973155, 'eval_runtime': 6.9581, 'eval_samples_per_second': 149.897, 'eval_steps_per_second': 9.485}
ROUND:5
CLIENT:79
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.34it/s]                                              {'loss': 0.8651, 'grad_norm': 1.5685855150222778, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.34it/s]  7%|▋         | 2/30 [00:00<00:06,  4.07it/s]                                              {'loss': 0.4291, 'grad_norm': 1.5206984281539917, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.07it/s] 10%|█         | 3/30 [00:00<00:07,  3.80it/s]                                              {'loss': 0.3547, 'grad_norm': 1.465187668800354, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.80it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.61it/s]                                              {'loss': 0.1207, 'grad_norm': 1.1299870014190674, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.61it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.61it/s]                                              {'loss': 0.3121, 'grad_norm': 0.7912303805351257, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.61it/s] 20%|██        | 6/30 [00:01<00:05,  4.22it/s]                                              {'loss': 1.0297, 'grad_norm': 3.7952306270599365, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.22it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.08it/s]                                              {'loss': 0.1707, 'grad_norm': 0.549028754234314, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.08it/s] 27%|██▋       | 8/30 [00:02<00:05,  4.04it/s]                                              {'loss': 0.5937, 'grad_norm': 1.3676269054412842, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:05,  4.04it/s] 30%|███       | 9/30 [00:02<00:05,  4.14it/s]                                              {'loss': 0.4949, 'grad_norm': 2.5243606567382812, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  4.14it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.14it/s]                                               {'loss': 0.2056, 'grad_norm': 3.7793095111846924, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.14it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.27it/s]                                               {'loss': 0.0313, 'grad_norm': 0.38159406185150146, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.27it/s] 40%|████      | 12/30 [00:02<00:03,  5.08it/s]                                               {'loss': 0.0127, 'grad_norm': 0.11771195381879807, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.08it/s] 43%|████▎     | 13/30 [00:03<00:03,  4.38it/s]                                               {'loss': 0.4638, 'grad_norm': 3.060896396636963, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  4.38it/s] 47%|████▋     | 14/30 [00:03<00:04,  3.97it/s]                                               {'loss': 0.0364, 'grad_norm': 0.6704366207122803, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:04,  3.97it/s] 50%|█████     | 15/30 [00:03<00:04,  3.71it/s]                                               {'loss': 0.866, 'grad_norm': 1.9827184677124023, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:04,  3.71it/s] 53%|█████▎    | 16/30 [00:04<00:03,  3.53it/s]                                               {'loss': 0.2504, 'grad_norm': 1.0400493144989014, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:03,  3.53it/s] 57%|█████▋    | 17/30 [00:04<00:03,  3.48it/s]                                               {'loss': 0.0536, 'grad_norm': 0.9564797282218933, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:03,  3.48it/s] 60%|██████    | 18/30 [00:04<00:02,  4.21it/s]                                               {'loss': 0.0466, 'grad_norm': 1.0622944831848145, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.21it/s] 63%|██████▎   | 19/30 [00:04<00:02,  3.89it/s]                                               {'loss': 0.2828, 'grad_norm': 1.1034601926803589, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  3.89it/s] 67%|██████▋   | 20/30 [00:05<00:02,  3.70it/s]                                               {'loss': 0.2694, 'grad_norm': 2.388345718383789, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  3.70it/s] 70%|███████   | 21/30 [00:05<00:02,  3.46it/s]                                               {'loss': 0.0393, 'grad_norm': 0.5258598923683167, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:02,  3.46it/s] 73%|███████▎  | 22/30 [00:05<00:02,  3.45it/s]                                               {'loss': 0.4832, 'grad_norm': 4.553299903869629, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:02,  3.45it/s] 77%|███████▋  | 23/30 [00:05<00:02,  3.41it/s]                                               {'loss': 0.1542, 'grad_norm': 0.9062330722808838, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:02,  3.41it/s] 80%|████████  | 24/30 [00:06<00:01,  4.17it/s]                                               {'loss': 0.0486, 'grad_norm': 0.8223903179168701, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  4.17it/s] 83%|████████▎ | 25/30 [00:06<00:01,  3.88it/s]                                               {'loss': 0.0757, 'grad_norm': 1.5670214891433716, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:01,  3.88it/s] 87%|████████▋ | 26/30 [00:06<00:01,  3.77it/s]                                               {'loss': 0.3532, 'grad_norm': 5.011074542999268, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:01,  3.77it/s] 90%|█████████ | 27/30 [00:06<00:00,  3.74it/s]                                               {'loss': 0.0499, 'grad_norm': 0.9145233035087585, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  3.74it/s] 93%|█████████▎| 28/30 [00:07<00:00,  3.64it/s]                                               {'loss': 0.2422, 'grad_norm': 13.44582462310791, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  3.64it/s] 97%|█████████▋| 29/30 [00:07<00:00,  3.43it/s]                                               {'loss': 0.5823, 'grad_norm': 6.751392841339111, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  3.43it/s]100%|██████████| 30/30 [00:07<00:00,  3.83it/s]                                               {'loss': 0.0542, 'grad_norm': 1.3211663961410522, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  3.83it/s]                                               {'train_runtime': 7.904, 'train_samples_per_second': 53.77, 'train_steps_per_second': 3.796, 'train_loss': 0.2990647537012895, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  3.83it/s]100%|██████████| 30/30 [00:07<00:00,  3.80it/s]
CLIENT:75
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.79it/s]                                              {'loss': 0.7672, 'grad_norm': 1.2701534032821655, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.79it/s]  7%|▋         | 2/30 [00:00<00:05,  5.36it/s]                                              {'loss': 0.5223, 'grad_norm': 0.9975126385688782, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.36it/s] 10%|█         | 3/30 [00:00<00:05,  5.27it/s]                                              {'loss': 0.5246, 'grad_norm': 0.9041746258735657, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  5.27it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.57it/s]                                              {'loss': 0.7342, 'grad_norm': 1.7156383991241455, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.57it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.89it/s]                                              {'loss': 0.8265, 'grad_norm': 2.533567428588867, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.89it/s] 20%|██        | 6/30 [00:01<00:03,  6.19it/s]                                              {'loss': 0.6295, 'grad_norm': 6.067404747009277, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:03,  6.19it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.74it/s]                                              {'loss': 0.493, 'grad_norm': 2.374000310897827, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.74it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.77it/s]                                              {'loss': 0.6588, 'grad_norm': 1.1018691062927246, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.77it/s] 30%|███       | 9/30 [00:01<00:03,  5.34it/s]                                              {'loss': 0.6719, 'grad_norm': 2.916919708251953, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.34it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.89it/s]                                               {'loss': 0.7069, 'grad_norm': 3.126924514770508, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.89it/s] 37%|███▋      | 11/30 [00:01<00:03,  5.91it/s]                                               {'loss': 0.6228, 'grad_norm': 1.386875033378601, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  5.91it/s]                                               {'loss': 0.5623, 'grad_norm': 4.169905185699463, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.91it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.60it/s]                                               {'loss': 0.559, 'grad_norm': 1.3042958974838257, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.60it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.42it/s]                                               {'loss': 0.6737, 'grad_norm': 8.669968605041504, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.42it/s] 50%|█████     | 15/30 [00:02<00:02,  6.35it/s]                                               {'loss': 0.7329, 'grad_norm': 4.193394184112549, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.35it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.92it/s]                                               {'loss': 0.535, 'grad_norm': 2.5915167331695557, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.92it/s]                                               {'loss': 0.5875, 'grad_norm': 1.9244563579559326, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  6.92it/s] 60%|██████    | 18/30 [00:02<00:01,  8.82it/s]                                               {'loss': 0.6472, 'grad_norm': 3.570225715637207, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  8.82it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.83it/s]                                               {'loss': 0.7164, 'grad_norm': 2.671098232269287, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.83it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.03it/s]                                               {'loss': 0.6242, 'grad_norm': 9.82461166381836, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.03it/s] 70%|███████   | 21/30 [00:03<00:01,  5.91it/s]                                               {'loss': 0.671, 'grad_norm': 1.389566421508789, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.91it/s] 73%|███████▎  | 22/30 [00:03<00:01,  5.49it/s]                                               {'loss': 0.6726, 'grad_norm': 1.2763842344284058, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  5.49it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.30it/s]                                               {'loss': 0.4961, 'grad_norm': 2.693291187286377, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.30it/s] 80%|████████  | 24/30 [00:03<00:01,  5.87it/s]                                               {'loss': 0.7178, 'grad_norm': 2.377701759338379, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:01,  5.87it/s] 83%|████████▎ | 25/30 [00:04<00:00,  5.46it/s]                                               {'loss': 0.5735, 'grad_norm': 0.7123791575431824, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  5.46it/s] 87%|████████▋ | 26/30 [00:04<00:00,  4.99it/s]                                               {'loss': 0.4469, 'grad_norm': 2.269294023513794, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  4.99it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.46it/s]                                               {'loss': 0.6394, 'grad_norm': 1.523914098739624, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.46it/s] 93%|█████████▎| 28/30 [00:04<00:00,  5.79it/s]                                               {'loss': 0.6205, 'grad_norm': 1.3604730367660522, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  5.79it/s] 97%|█████████▋| 29/30 [00:04<00:00,  6.07it/s]                                               {'loss': 0.7096, 'grad_norm': 5.122135162353516, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  6.07it/s]                                               {'loss': 0.6743, 'grad_norm': 1.6717758178710938, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.07it/s]                                               {'train_runtime': 4.9951, 'train_samples_per_second': 85.083, 'train_steps_per_second': 6.006, 'train_loss': 0.633922819296519, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.07it/s]100%|██████████| 30/30 [00:04<00:00,  6.01it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.68it/s]                                              {'loss': 0.5873, 'grad_norm': 0.3240213692188263, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.68it/s]                                              {'loss': 0.6668, 'grad_norm': 0.6461731195449829, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.68it/s] 10%|█         | 3/30 [00:00<00:02, 11.41it/s]                                              {'loss': 0.7347, 'grad_norm': 0.5255005955696106, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.41it/s]                                              {'loss': 0.6473, 'grad_norm': 0.986103355884552, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.41it/s] 17%|█▋        | 5/30 [00:00<00:02, 12.09it/s]                                              {'loss': 0.6736, 'grad_norm': 0.5193711519241333, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.09it/s]                                              {'loss': 0.7805, 'grad_norm': 1.7623757123947144, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 12.09it/s] 23%|██▎       | 7/30 [00:00<00:01, 13.76it/s]                                              {'loss': 0.6145, 'grad_norm': 0.562305748462677, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 13.76it/s]                                              {'loss': 0.7384, 'grad_norm': 0.9895976185798645, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.76it/s] 30%|███       | 9/30 [00:00<00:01, 13.21it/s]                                              {'loss': 0.6613, 'grad_norm': 0.5161697268486023, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.21it/s]                                              {'loss': 0.5952, 'grad_norm': 0.7474743723869324, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.21it/s] 37%|███▋      | 11/30 [00:00<00:01, 12.96it/s]                                               {'loss': 0.6297, 'grad_norm': 0.35112622380256653, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.96it/s]                                               {'loss': 0.4769, 'grad_norm': 1.0204026699066162, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 12.96it/s] 43%|████▎     | 13/30 [00:00<00:01, 14.25it/s]                                               {'loss': 0.5256, 'grad_norm': 1.2100801467895508, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 14.25it/s]                                               {'loss': 0.5356, 'grad_norm': 0.4810348451137543, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 14.25it/s] 50%|█████     | 15/30 [00:01<00:01, 13.93it/s]                                               {'loss': 0.8332, 'grad_norm': 2.768615245819092, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.93it/s]                                               {'loss': 0.5834, 'grad_norm': 1.057045817375183, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.93it/s] 57%|█████▋    | 17/30 [00:01<00:00, 13.45it/s]                                               {'loss': 0.4481, 'grad_norm': 1.3671650886535645, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.45it/s]                                               {'loss': 0.6776, 'grad_norm': 1.4213860034942627, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.45it/s] 63%|██████▎   | 19/30 [00:01<00:00, 14.41it/s]                                               {'loss': 0.4828, 'grad_norm': 0.8636089563369751, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.41it/s]                                               {'loss': 0.6185, 'grad_norm': 0.8575323820114136, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.41it/s] 70%|███████   | 21/30 [00:01<00:00, 13.76it/s]                                               {'loss': 0.5429, 'grad_norm': 0.806257426738739, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.76it/s]                                               {'loss': 0.5101, 'grad_norm': 0.7311801314353943, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.76it/s] 77%|███████▋  | 23/30 [00:01<00:00, 12.74it/s]                                               {'loss': 0.8598, 'grad_norm': 3.3051435947418213, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 12.74it/s]                                               {'loss': 0.4163, 'grad_norm': 1.8762263059616089, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 12.74it/s] 83%|████████▎ | 25/30 [00:01<00:00, 13.11it/s]                                               {'loss': 0.5064, 'grad_norm': 1.108015537261963, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 13.11it/s]                                               {'loss': 0.5954, 'grad_norm': 1.0314849615097046, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 13.11it/s] 90%|█████████ | 27/30 [00:02<00:00, 12.56it/s]                                               {'loss': 0.4592, 'grad_norm': 0.8769471049308777, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 12.56it/s]                                               {'loss': 0.6299, 'grad_norm': 1.2965551614761353, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.56it/s] 97%|█████████▋| 29/30 [00:02<00:00, 12.55it/s]                                               {'loss': 0.5095, 'grad_norm': 1.204973816871643, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.55it/s]                                               {'loss': 0.3918, 'grad_norm': 1.7138245105743408, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.55it/s]                                               {'train_runtime': 2.3426, 'train_samples_per_second': 181.425, 'train_steps_per_second': 12.806, 'train_loss': 0.5977439115444819, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.55it/s]100%|██████████| 30/30 [00:02<00:00, 12.81it/s]
CLIENT:15
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.50it/s]                                              {'loss': 0.8701, 'grad_norm': 1.8856027126312256, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.50it/s]  7%|▋         | 2/30 [00:00<00:07,  3.94it/s]                                              {'loss': 0.4043, 'grad_norm': 1.5824337005615234, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.94it/s] 10%|█         | 3/30 [00:00<00:06,  4.14it/s]                                              {'loss': 0.2605, 'grad_norm': 1.4855194091796875, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.14it/s] 13%|█▎        | 4/30 [00:00<00:06,  4.23it/s]                                              {'loss': 0.1122, 'grad_norm': 1.084079623222351, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:06,  4.23it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.43it/s]                                              {'loss': 0.0612, 'grad_norm': 0.9030901193618774, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.43it/s] 20%|██        | 6/30 [00:01<00:04,  5.16it/s]                                              {'loss': 0.0059, 'grad_norm': 0.08962579816579819, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.16it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.87it/s]                                              {'loss': 0.0051, 'grad_norm': 0.06418255716562271, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.87it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.97it/s]                                              {'loss': 0.0024, 'grad_norm': 0.03172961249947548, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.97it/s] 30%|███       | 9/30 [00:01<00:04,  4.81it/s]                                              {'loss': 0.0026, 'grad_norm': 0.0465799979865551, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.81it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.03it/s]                                               {'loss': 0.0021, 'grad_norm': 0.04025541618466377, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.03it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.68it/s]                                               {'loss': 0.0013, 'grad_norm': 0.021290142089128494, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.68it/s]                                               {'loss': 0.0008, 'grad_norm': 0.024110106751322746, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.68it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.45it/s]                                               {'loss': 0.0008, 'grad_norm': 0.023335356265306473, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.45it/s]                                               {'loss': 0.0006, 'grad_norm': 0.013751653023064137, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.45it/s] 50%|█████     | 15/30 [00:02<00:01,  8.72it/s]                                               {'loss': 0.0005, 'grad_norm': 0.00804002396762371, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.72it/s] 53%|█████▎    | 16/30 [00:02<00:01,  8.91it/s]                                               {'loss': 0.0004, 'grad_norm': 0.005921135190874338, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.91it/s]                                               {'loss': 0.0005, 'grad_norm': 0.007468079682439566, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  8.91it/s] 60%|██████    | 18/30 [00:02<00:01, 10.80it/s]                                               {'loss': 0.0004, 'grad_norm': 0.005538784898817539, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 10.80it/s]                                               {'loss': 0.0003, 'grad_norm': 0.0036616292782127857, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01, 10.80it/s] 67%|██████▋   | 20/30 [00:02<00:00, 11.32it/s]                                               {'loss': 0.0003, 'grad_norm': 0.004277125000953674, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 11.32it/s]                                               {'loss': 0.0003, 'grad_norm': 0.003364041680470109, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:00, 11.32it/s] 73%|███████▎  | 22/30 [00:03<00:00, 11.44it/s]                                               {'loss': 0.0003, 'grad_norm': 0.0026924358680844307, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00, 11.44it/s]                                               {'loss': 0.0002, 'grad_norm': 0.002955771517008543, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00, 11.44it/s] 80%|████████  | 24/30 [00:03<00:00, 10.81it/s]                                               {'loss': 0.0002, 'grad_norm': 0.0034530875273048878, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00, 10.81it/s]                                               {'loss': 0.0003, 'grad_norm': 0.0025831013917922974, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 10.81it/s] 87%|████████▋ | 26/30 [00:03<00:00,  9.61it/s]                                               {'loss': 0.0003, 'grad_norm': 0.0030941548757255077, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  9.61it/s]                                               {'loss': 0.0003, 'grad_norm': 0.002472079358994961, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  9.61it/s] 93%|█████████▎| 28/30 [00:03<00:00,  8.39it/s]                                               {'loss': 0.0002, 'grad_norm': 0.0040271105244755745, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  8.39it/s] 97%|█████████▋| 29/30 [00:04<00:00,  7.51it/s]                                               {'loss': 0.0003, 'grad_norm': 0.001995846861973405, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  7.51it/s]100%|██████████| 30/30 [00:04<00:00,  7.73it/s]                                               {'loss': 0.0003, 'grad_norm': 0.002294279169291258, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.73it/s]                                               {'train_runtime': 4.4119, 'train_samples_per_second': 96.331, 'train_steps_per_second': 6.8, 'train_loss': 0.0578323498183939, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.73it/s]100%|██████████| 30/30 [00:04<00:00,  6.80it/s]
CLIENT:38
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.51it/s]                                              {'loss': 0.6169, 'grad_norm': 0.36263948678970337, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.51it/s]  7%|▋         | 2/30 [00:00<00:07,  3.96it/s]                                              {'loss': 0.6714, 'grad_norm': 1.0182808637619019, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.96it/s] 10%|█         | 3/30 [00:00<00:06,  3.93it/s]                                              {'loss': 0.6668, 'grad_norm': 0.6984830498695374, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  3.93it/s] 13%|█▎        | 4/30 [00:01<00:06,  3.86it/s]                                              {'loss': 0.7401, 'grad_norm': 1.121431827545166, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:06,  3.86it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.95it/s]                                              {'loss': 0.7572, 'grad_norm': 0.8827025294303894, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.95it/s] 20%|██        | 6/30 [00:02<00:13,  1.81it/s]                                              {'loss': 0.8533, 'grad_norm': 1.1897445917129517, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:13,  1.81it/s] 23%|██▎       | 7/30 [00:02<00:10,  2.11it/s]                                              {'loss': 0.6095, 'grad_norm': 0.5411384105682373, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:10,  2.11it/s] 27%|██▋       | 8/30 [00:02<00:09,  2.43it/s]                                              {'loss': 0.6912, 'grad_norm': 0.5654356479644775, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:09,  2.43it/s] 30%|███       | 9/30 [00:03<00:07,  2.67it/s]                                              {'loss': 0.6538, 'grad_norm': 2.330129623413086, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:03<00:07,  2.67it/s] 33%|███▎      | 10/30 [00:03<00:06,  2.91it/s]                                               {'loss': 0.5974, 'grad_norm': 0.9557033181190491, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:06,  2.91it/s] 37%|███▋      | 11/30 [00:03<00:06,  3.05it/s]                                               {'loss': 0.5621, 'grad_norm': 0.9737966060638428, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:06,  3.05it/s] 40%|████      | 12/30 [00:03<00:04,  3.84it/s]                                               {'loss': 0.4835, 'grad_norm': 0.6707842350006104, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:04,  3.84it/s] 43%|████▎     | 13/30 [00:04<00:04,  3.70it/s]                                               {'loss': 0.6107, 'grad_norm': 0.5312992930412292, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:04<00:04,  3.70it/s] 47%|████▋     | 14/30 [00:04<00:04,  3.57it/s]                                               {'loss': 0.5962, 'grad_norm': 0.460795521736145, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:04,  3.57it/s] 50%|█████     | 15/30 [00:04<00:04,  3.58it/s]                                               {'loss': 0.5328, 'grad_norm': 0.6085750460624695, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:04,  3.58it/s] 53%|█████▎    | 16/30 [00:05<00:03,  3.84it/s]                                               {'loss': 0.5988, 'grad_norm': 0.7167631387710571, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:05<00:03,  3.84it/s] 57%|█████▋    | 17/30 [00:05<00:03,  4.01it/s]                                               {'loss': 0.4932, 'grad_norm': 1.4892513751983643, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:05<00:03,  4.01it/s]                                               {'loss': 0.6485, 'grad_norm': 2.3960468769073486, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:02,  4.01it/s] 63%|██████▎   | 19/30 [00:05<00:02,  5.00it/s]                                               {'loss': 0.5134, 'grad_norm': 0.6943254470825195, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  5.00it/s] 67%|██████▋   | 20/30 [00:05<00:01,  5.03it/s]                                               {'loss': 0.5638, 'grad_norm': 1.0453778505325317, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:01,  5.03it/s] 70%|███████   | 21/30 [00:05<00:01,  4.84it/s]                                               {'loss': 0.4077, 'grad_norm': 1.5272326469421387, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:01,  4.84it/s] 73%|███████▎  | 22/30 [00:06<00:01,  5.06it/s]                                               {'loss': 0.5783, 'grad_norm': 1.443400263786316, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:06<00:01,  5.06it/s] 77%|███████▋  | 23/30 [00:06<00:01,  5.45it/s]                                               {'loss': 0.6509, 'grad_norm': 1.3156358003616333, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  5.45it/s]                                               {'loss': 0.4954, 'grad_norm': 1.7767983675003052, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  5.45it/s] 83%|████████▎ | 25/30 [00:06<00:00,  6.68it/s]                                               {'loss': 0.489, 'grad_norm': 1.2624435424804688, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:00,  6.68it/s] 87%|████████▋ | 26/30 [00:06<00:00,  6.67it/s]                                               {'loss': 0.4895, 'grad_norm': 0.878513514995575, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:00,  6.67it/s] 90%|█████████ | 27/30 [00:06<00:00,  6.08it/s]                                               {'loss': 0.5296, 'grad_norm': 1.1027759313583374, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  6.08it/s] 93%|█████████▎| 28/30 [00:07<00:00,  5.58it/s]                                               {'loss': 0.5586, 'grad_norm': 1.051016926765442, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  5.58it/s] 97%|█████████▋| 29/30 [00:07<00:00,  5.58it/s]                                               {'loss': 0.4795, 'grad_norm': 1.18116295337677, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  5.58it/s]100%|██████████| 30/30 [00:07<00:00,  6.33it/s]                                               {'loss': 0.44, 'grad_norm': 2.4447410106658936, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  6.33it/s]                                               {'train_runtime': 7.544, 'train_samples_per_second': 56.336, 'train_steps_per_second': 3.977, 'train_loss': 0.585977632800738, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  6.33it/s]100%|██████████| 30/30 [00:07<00:00,  3.98it/s]
CLIENT:11
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:23,  1.24it/s]                                              {'loss': 0.6135, 'grad_norm': 0.8862455487251282, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:23,  1.24it/s]  7%|▋         | 2/30 [00:01<00:12,  2.24it/s]                                              {'loss': 0.7142, 'grad_norm': 0.6634425520896912, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:12,  2.24it/s] 10%|█         | 3/30 [00:01<00:08,  3.19it/s]                                              {'loss': 0.7423, 'grad_norm': 0.5657837986946106, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:08,  3.19it/s] 13%|█▎        | 4/30 [00:01<00:06,  3.93it/s]                                              {'loss': 0.6981, 'grad_norm': 0.5276263952255249, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:06,  3.93it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.58it/s]                                              {'loss': 0.7128, 'grad_norm': 0.3812945783138275, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.58it/s]                                              {'loss': 0.7496, 'grad_norm': 1.152408242225647, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.58it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.03it/s]                                              {'loss': 0.6758, 'grad_norm': 0.3610178828239441, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.03it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.09it/s]                                              {'loss': 0.6816, 'grad_norm': 0.19281740486621857, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.09it/s] 30%|███       | 9/30 [00:02<00:03,  5.33it/s]                                              {'loss': 0.6454, 'grad_norm': 0.29852405190467834, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:03,  5.33it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.95it/s]                                               {'loss': 0.6574, 'grad_norm': 0.14830297231674194, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.95it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.15it/s]                                               {'loss': 0.6706, 'grad_norm': 0.1638479083776474, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.15it/s] 40%|████      | 12/30 [00:02<00:03,  4.87it/s]                                               {'loss': 0.5816, 'grad_norm': 1.1445425748825073, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.87it/s] 43%|████▎     | 13/30 [00:03<00:03,  4.60it/s]                                               {'loss': 0.6387, 'grad_norm': 0.24796144664287567, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  4.60it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.49it/s]                                               {'loss': 0.6866, 'grad_norm': 0.38899195194244385, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.49it/s] 50%|█████     | 15/30 [00:03<00:03,  4.54it/s]                                               {'loss': 0.7104, 'grad_norm': 0.7017995119094849, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.54it/s] 53%|█████▎    | 16/30 [00:03<00:02,  4.82it/s]                                               {'loss': 0.671, 'grad_norm': 0.5156978368759155, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  4.82it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.89it/s]                                               {'loss': 0.6653, 'grad_norm': 0.2832340896129608, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.89it/s] 60%|██████    | 18/30 [00:04<00:02,  4.99it/s]                                               {'loss': 0.675, 'grad_norm': 0.7291452288627625, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.99it/s] 63%|██████▎   | 19/30 [00:04<00:02,  4.71it/s]                                               {'loss': 0.7048, 'grad_norm': 0.6898061633110046, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  4.71it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.46it/s]                                               {'loss': 0.6617, 'grad_norm': 0.2879161834716797, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.46it/s] 70%|███████   | 21/30 [00:04<00:02,  4.39it/s]                                               {'loss': 0.6564, 'grad_norm': 0.27709975838661194, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:02,  4.39it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.49it/s]                                               {'loss': 0.6467, 'grad_norm': 0.3820492625236511, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.49it/s] 77%|███████▋  | 23/30 [00:05<00:01,  5.18it/s]                                               {'loss': 0.663, 'grad_norm': 0.37650203704833984, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  5.18it/s] 80%|████████  | 24/30 [00:05<00:01,  5.30it/s]                                               {'loss': 0.6452, 'grad_norm': 0.7419331669807434, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.30it/s] 83%|████████▎ | 25/30 [00:05<00:01,  4.55it/s]                                               {'loss': 0.6496, 'grad_norm': 0.6141515970230103, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  4.55it/s] 87%|████████▋ | 26/30 [00:05<00:00,  4.29it/s]                                               {'loss': 0.6076, 'grad_norm': 0.3557058870792389, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  4.29it/s] 90%|█████████ | 27/30 [00:06<00:00,  4.45it/s]                                               {'loss': 0.6558, 'grad_norm': 0.44487807154655457, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  4.45it/s] 93%|█████████▎| 28/30 [00:06<00:00,  4.19it/s]                                               {'loss': 0.6766, 'grad_norm': 0.37747958302497864, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  4.19it/s] 97%|█████████▋| 29/30 [00:06<00:00,  4.18it/s]                                               {'loss': 0.6514, 'grad_norm': 0.5985342264175415, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  4.18it/s]100%|██████████| 30/30 [00:06<00:00,  4.49it/s]                                               {'loss': 0.6391, 'grad_norm': 0.5731711983680725, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.49it/s]                                               {'train_runtime': 7.0936, 'train_samples_per_second': 59.913, 'train_steps_per_second': 4.229, 'train_loss': 0.6682625571886699, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  4.49it/s]100%|██████████| 30/30 [00:07<00:00,  4.23it/s]
CLIENT:40
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.8222, 'grad_norm': 1.6294664144515991, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 10.82it/s]  7%|▋         | 2/30 [00:00<00:02, 10.55it/s]                                              {'loss': 0.4307, 'grad_norm': 1.5754022598266602, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 10.55it/s]                                              {'loss': 0.4267, 'grad_norm': 1.1811684370040894, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.55it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.81it/s]                                              {'loss': 0.1655, 'grad_norm': 0.6057288646697998, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.81it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.36it/s]                                              {'loss': 0.0721, 'grad_norm': 0.819203794002533, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.36it/s] 20%|██        | 6/30 [00:00<00:03,  6.72it/s]                                              {'loss': 0.0188, 'grad_norm': 0.426742821931839, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.72it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.25it/s]                                              {'loss': 0.3619, 'grad_norm': 0.8887510299682617, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.25it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.98it/s]                                              {'loss': 0.0065, 'grad_norm': 0.07683601975440979, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.98it/s] 30%|███       | 9/30 [00:01<00:03,  5.41it/s]                                              {'loss': 0.359, 'grad_norm': 2.4071812629699707, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.41it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.47it/s]                                               {'loss': 0.3133, 'grad_norm': 1.491112232208252, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.47it/s] 37%|███▋      | 11/30 [00:01<00:03,  5.73it/s]                                               {'loss': 0.013, 'grad_norm': 0.15610361099243164, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  5.73it/s]                                               {'loss': 0.0178, 'grad_norm': 0.22302381694316864, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:03,  5.73it/s] 43%|████▎     | 13/30 [00:01<00:02,  6.90it/s]                                               {'loss': 0.3131, 'grad_norm': 1.4472917318344116, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.90it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.85it/s]                                               {'loss': 0.0134, 'grad_norm': 0.14340625703334808, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.85it/s] 50%|█████     | 15/30 [00:02<00:02,  6.59it/s]                                               {'loss': 0.2335, 'grad_norm': 0.7142362594604492, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.59it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.95it/s]                                               {'loss': 0.2596, 'grad_norm': 0.8839259743690491, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.95it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.13it/s]                                               {'loss': 0.0291, 'grad_norm': 0.2812855839729309, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.13it/s]                                               {'loss': 0.0381, 'grad_norm': 0.33127883076667786, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.13it/s] 63%|██████▎   | 19/30 [00:02<00:01,  7.82it/s]                                               {'loss': 0.0374, 'grad_norm': 0.35783055424690247, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.82it/s] 67%|██████▋   | 20/30 [00:03<00:01,  6.67it/s]                                               {'loss': 0.036, 'grad_norm': 0.36482080817222595, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  6.67it/s] 70%|███████   | 21/30 [00:03<00:01,  5.93it/s]                                               {'loss': 0.0397, 'grad_norm': 0.43158969283103943, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.93it/s] 73%|███████▎  | 22/30 [00:03<00:01,  5.60it/s]                                               {'loss': 0.0298, 'grad_norm': 0.4076940715312958, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  5.60it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.35it/s]                                               {'loss': 0.4705, 'grad_norm': 1.5386217832565308, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.35it/s] 80%|████████  | 24/30 [00:03<00:00,  6.15it/s]                                               {'loss': 1.3361, 'grad_norm': 2.1766746044158936, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.15it/s] 83%|████████▎ | 25/30 [00:03<00:00,  5.37it/s]                                               {'loss': 0.0187, 'grad_norm': 0.25871777534484863, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  5.37it/s] 87%|████████▋ | 26/30 [00:04<00:00,  4.32it/s]                                               {'loss': 0.459, 'grad_norm': 1.7338719367980957, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  4.32it/s] 90%|█████████ | 27/30 [00:04<00:00,  3.84it/s]                                               {'loss': 0.0225, 'grad_norm': 0.3457781970500946, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  3.84it/s] 93%|█████████▎| 28/30 [00:05<00:00,  3.54it/s]                                               {'loss': 0.2344, 'grad_norm': 0.5785673260688782, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  3.54it/s] 97%|█████████▋| 29/30 [00:05<00:00,  3.33it/s]                                               {'loss': 0.0185, 'grad_norm': 0.2531488537788391, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  3.33it/s]100%|██████████| 30/30 [00:05<00:00,  3.76it/s]                                               {'loss': 0.0157, 'grad_norm': 0.22448861598968506, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  3.76it/s]                                               {'train_runtime': 5.6342, 'train_samples_per_second': 75.432, 'train_steps_per_second': 5.325, 'train_loss': 0.22042402615770698, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  3.76it/s]100%|██████████| 30/30 [00:05<00:00,  5.33it/s]
CLIENT:45
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.72it/s]                                              {'loss': 0.5971, 'grad_norm': 0.7248117327690125, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.72it/s]  7%|▋         | 2/30 [00:00<00:05,  4.94it/s]                                              {'loss': 0.7043, 'grad_norm': 0.5911054611206055, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  4.94it/s] 10%|█         | 3/30 [00:00<00:04,  5.41it/s]                                              {'loss': 0.6868, 'grad_norm': 0.40442660450935364, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  5.41it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.81it/s]                                              {'loss': 0.6748, 'grad_norm': 0.7344070076942444, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.81it/s] 17%|█▋        | 5/30 [00:00<00:04,  6.13it/s]                                              {'loss': 0.6399, 'grad_norm': 0.3892976939678192, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  6.13it/s] 20%|██        | 6/30 [00:01<00:03,  6.67it/s]                                              {'loss': 0.8041, 'grad_norm': 1.5012251138687134, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:03,  6.67it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.45it/s]                                              {'loss': 0.693, 'grad_norm': 0.4712565243244171, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.45it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.85it/s]                                              {'loss': 0.661, 'grad_norm': 0.5123059153556824, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.85it/s] 30%|███       | 9/30 [00:01<00:03,  5.83it/s]                                              {'loss': 0.6565, 'grad_norm': 0.6688541769981384, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.83it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.31it/s]                                               {'loss': 0.6642, 'grad_norm': 0.24030554294586182, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.31it/s] 37%|███▋      | 11/30 [00:01<00:03,  5.09it/s]                                               {'loss': 0.6773, 'grad_norm': 0.16875958442687988, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  5.09it/s]                                               {'loss': 0.808, 'grad_norm': 1.6479589939117432, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.09it/s] 43%|████▎     | 13/30 [00:02<00:02,  5.77it/s]                                               {'loss': 0.635, 'grad_norm': 0.33378541469573975, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  5.77it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.51it/s]                                               {'loss': 0.698, 'grad_norm': 0.5412618517875671, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.51it/s] 50%|█████     | 15/30 [00:02<00:02,  5.79it/s]                                               {'loss': 0.5678, 'grad_norm': 0.8678101301193237, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.79it/s] 53%|█████▎    | 16/30 [00:02<00:02,  5.75it/s]                                               {'loss': 0.6027, 'grad_norm': 0.3981340527534485, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  5.75it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.38it/s]                                               {'loss': 0.6842, 'grad_norm': 0.3946016728878021, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.38it/s] 60%|██████    | 18/30 [00:03<00:02,  5.96it/s]                                               {'loss': 0.6647, 'grad_norm': 0.6311878561973572, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.96it/s] 63%|██████▎   | 19/30 [00:03<00:01,  5.73it/s]                                               {'loss': 0.7434, 'grad_norm': 0.8922931551933289, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  5.73it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.72it/s]                                               {'loss': 0.6587, 'grad_norm': 0.24581633508205414, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.72it/s] 70%|███████   | 21/30 [00:03<00:01,  5.70it/s]                                               {'loss': 0.6478, 'grad_norm': 0.35749363899230957, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.70it/s] 73%|███████▎  | 22/30 [00:03<00:01,  5.63it/s]                                               {'loss': 0.6448, 'grad_norm': 0.39711496233940125, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  5.63it/s] 77%|███████▋  | 23/30 [00:04<00:01,  5.89it/s]                                               {'loss': 0.5893, 'grad_norm': 0.5752106308937073, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  5.89it/s]                                               {'loss': 0.6153, 'grad_norm': 0.5645065307617188, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  5.89it/s] 83%|████████▎ | 25/30 [00:04<00:00,  6.27it/s]                                               {'loss': 0.6765, 'grad_norm': 0.6547553539276123, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  6.27it/s] 87%|████████▋ | 26/30 [00:04<00:00,  5.72it/s]                                               {'loss': 0.6366, 'grad_norm': 0.479487806558609, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  5.72it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.54it/s]                                               {'loss': 0.6396, 'grad_norm': 0.48898836970329285, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.54it/s] 93%|█████████▎| 28/30 [00:04<00:00,  5.59it/s]                                               {'loss': 0.6308, 'grad_norm': 0.4219071865081787, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  5.59it/s] 97%|█████████▋| 29/30 [00:05<00:00,  5.38it/s]                                               {'loss': 0.5554, 'grad_norm': 0.4633750915527344, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  5.38it/s]                                               {'loss': 0.5748, 'grad_norm': 0.7651907205581665, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.38it/s]                                               {'train_runtime': 5.3051, 'train_samples_per_second': 80.111, 'train_steps_per_second': 5.655, 'train_loss': 0.6577438473701477, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.38it/s]100%|██████████| 30/30 [00:05<00:00,  5.66it/s]
CLIENT:39
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.75it/s]                                              {'loss': 0.6366, 'grad_norm': 0.5829275250434875, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.75it/s]  7%|▋         | 2/30 [00:00<00:04,  6.39it/s]                                              {'loss': 0.587, 'grad_norm': 0.9610573053359985, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.39it/s] 10%|█         | 3/30 [00:00<00:03,  7.06it/s]                                              {'loss': 0.5982, 'grad_norm': 0.8040332198143005, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.06it/s]                                              {'loss': 0.4, 'grad_norm': 1.2018914222717285, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.06it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.04it/s]                                              {'loss': 0.6931, 'grad_norm': 1.9735000133514404, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  9.04it/s] 20%|██        | 6/30 [00:01<00:07,  3.18it/s]                                              {'loss': 0.5117, 'grad_norm': 3.8557658195495605, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:07,  3.18it/s] 23%|██▎       | 7/30 [00:01<00:05,  3.87it/s]                                              {'loss': 0.542, 'grad_norm': 1.496854543685913, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  3.87it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.72it/s]                                              {'loss': 0.5642, 'grad_norm': 0.9797526001930237, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.72it/s] 30%|███       | 9/30 [00:01<00:03,  5.39it/s]                                              {'loss': 0.5851, 'grad_norm': 1.0957379341125488, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.39it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.87it/s]                                               {'loss': 0.4323, 'grad_norm': 1.4167231321334839, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.87it/s] 37%|███▋      | 11/30 [00:02<00:02,  6.56it/s]                                               {'loss': 0.4617, 'grad_norm': 1.110459327697754, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:02,  6.56it/s]                                               {'loss': 0.3592, 'grad_norm': 1.175438404083252, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.56it/s] 43%|████▎     | 13/30 [00:02<00:02,  8.50it/s]                                               {'loss': 0.5367, 'grad_norm': 1.5629068613052368, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  8.50it/s]                                               {'loss': 0.3928, 'grad_norm': 0.75462406873703, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  8.50it/s] 50%|█████     | 15/30 [00:02<00:01,  9.37it/s]                                               {'loss': 0.643, 'grad_norm': 2.490703582763672, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  9.37it/s]                                               {'loss': 0.3985, 'grad_norm': 0.9265520572662354, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  9.37it/s] 57%|█████▋    | 17/30 [00:02<00:01,  9.72it/s]                                               {'loss': 0.5046, 'grad_norm': 1.042170524597168, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  9.72it/s]                                               {'loss': 0.2704, 'grad_norm': 2.864020586013794, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  9.72it/s] 63%|██████▎   | 19/30 [00:02<00:01, 10.56it/s]                                               {'loss': 0.3494, 'grad_norm': 1.4587574005126953, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01, 10.56it/s]                                               {'loss': 0.5121, 'grad_norm': 1.559786319732666, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 10.56it/s] 70%|███████   | 21/30 [00:02<00:00, 10.91it/s]                                               {'loss': 0.4476, 'grad_norm': 1.19707453250885, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 10.91it/s]                                               {'loss': 0.395, 'grad_norm': 1.1368541717529297, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 10.91it/s] 77%|███████▋  | 23/30 [00:03<00:00, 10.08it/s]                                               {'loss': 0.4988, 'grad_norm': 1.3508622646331787, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00, 10.08it/s]                                               {'loss': 0.1235, 'grad_norm': 2.866394519805908, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00, 10.08it/s] 83%|████████▎ | 25/30 [00:03<00:00, 10.28it/s]                                               {'loss': 0.2306, 'grad_norm': 2.396759271621704, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 10.28it/s]                                               {'loss': 0.5232, 'grad_norm': 1.5549077987670898, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 10.28it/s] 90%|█████████ | 27/30 [00:03<00:00,  9.11it/s]                                               {'loss': 0.2996, 'grad_norm': 1.5163559913635254, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  9.11it/s] 93%|█████████▎| 28/30 [00:03<00:00,  8.81it/s]                                               {'loss': 0.5223, 'grad_norm': 3.042968273162842, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  8.81it/s]                                               {'loss': 0.502, 'grad_norm': 2.5445258617401123, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  8.81it/s]100%|██████████| 30/30 [00:03<00:00, 10.81it/s]                                               {'loss': 0.5289, 'grad_norm': 3.960735321044922, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 10.81it/s]                                               {'train_runtime': 3.8705, 'train_samples_per_second': 109.804, 'train_steps_per_second': 7.751, 'train_loss': 0.46834314838051794, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 10.81it/s]100%|██████████| 30/30 [00:03<00:00,  7.75it/s]
CLIENT:62
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.91it/s]                                              {'loss': 0.861, 'grad_norm': 2.2149887084960938, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.91it/s]                                              {'loss': 0.4939, 'grad_norm': 1.0051448345184326, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.91it/s] 10%|█         | 3/30 [00:00<00:03,  8.05it/s]                                              {'loss': 0.3032, 'grad_norm': 1.4412895441055298, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.05it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.54it/s]                                              {'loss': 0.4385, 'grad_norm': 1.1622910499572754, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.54it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.08it/s]                                              {'loss': 0.415, 'grad_norm': 2.1682991981506348, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.08it/s] 20%|██        | 6/30 [00:00<00:04,  5.48it/s]                                              {'loss': 0.5596, 'grad_norm': 2.016611337661743, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:04,  5.48it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.72it/s]                                              {'loss': 0.2703, 'grad_norm': 1.5190966129302979, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.72it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.90it/s]                                              {'loss': 0.1686, 'grad_norm': 0.7877434492111206, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.90it/s] 30%|███       | 9/30 [00:01<00:03,  5.60it/s]                                              {'loss': 0.5873, 'grad_norm': 2.7826993465423584, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.60it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.34it/s]                                               {'loss': 0.5635, 'grad_norm': 4.004830837249756, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.34it/s] 37%|███▋      | 11/30 [00:01<00:03,  5.93it/s]                                               {'loss': 0.2235, 'grad_norm': 1.8497555255889893, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  5.93it/s]                                               {'loss': 0.4521, 'grad_norm': 4.697874546051025, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:03,  5.93it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.46it/s]                                               {'loss': 0.4793, 'grad_norm': 2.3917362689971924, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.46it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.24it/s]                                               {'loss': 0.4805, 'grad_norm': 2.4432835578918457, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.24it/s]                                               {'loss': 0.1306, 'grad_norm': 2.4355294704437256, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.24it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.51it/s]                                               {'loss': 0.5334, 'grad_norm': 3.172024726867676, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.51it/s]                                               {'loss': 0.5111, 'grad_norm': 4.03017520904541, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.51it/s] 60%|██████    | 18/30 [00:02<00:01,  9.16it/s]                                               {'loss': 0.706, 'grad_norm': 5.221728324890137, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  9.16it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.09it/s]                                               {'loss': 0.2897, 'grad_norm': 2.1454267501831055, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  9.09it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.61it/s]                                               {'loss': 0.2916, 'grad_norm': 1.116875410079956, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.61it/s]                                               {'loss': 0.5284, 'grad_norm': 2.8450770378112793, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.61it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.48it/s]                                               {'loss': 0.7892, 'grad_norm': 4.330192565917969, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  9.48it/s]                                               {'loss': 0.3326, 'grad_norm': 5.235708236694336, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  9.48it/s] 80%|████████  | 24/30 [00:03<00:00, 11.40it/s]                                               {'loss': 0.0657, 'grad_norm': 1.249922752380371, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00, 11.40it/s]                                               {'loss': 0.3234, 'grad_norm': 4.647363185882568, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 11.40it/s] 87%|████████▋ | 26/30 [00:03<00:00, 10.29it/s]                                               {'loss': 0.4677, 'grad_norm': 2.4013609886169434, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 10.29it/s]                                               {'loss': 0.3353, 'grad_norm': 1.5472577810287476, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 10.29it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.23it/s]                                               {'loss': 0.3148, 'grad_norm': 1.1799075603485107, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  9.23it/s] 97%|█████████▋| 29/30 [00:03<00:00,  8.69it/s]                                               {'loss': 0.5328, 'grad_norm': 2.481778144836426, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  8.69it/s]                                               {'loss': 0.7983, 'grad_norm': 6.085827350616455, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.69it/s]                                               {'train_runtime': 3.8816, 'train_samples_per_second': 109.491, 'train_steps_per_second': 7.729, 'train_loss': 0.4415548582871755, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.69it/s]100%|██████████| 30/30 [00:03<00:00,  7.73it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  6%|▌         | 4/66 [00:00<00:02, 30.59it/s] 12%|█▏        | 8/66 [00:00<00:03, 19.07it/s] 17%|█▋        | 11/66 [00:00<00:03, 17.69it/s] 20%|█▉        | 13/66 [00:00<00:03, 16.70it/s] 23%|██▎       | 15/66 [00:00<00:03, 15.85it/s] 26%|██▌       | 17/66 [00:00<00:02, 16.84it/s] 29%|██▉       | 19/66 [00:01<00:02, 16.81it/s] 32%|███▏      | 21/66 [00:01<00:02, 16.09it/s] 36%|███▋      | 24/66 [00:01<00:02, 16.50it/s] 39%|███▉      | 26/66 [00:01<00:02, 16.19it/s] 42%|████▏     | 28/66 [00:01<00:03, 12.36it/s] 47%|████▋     | 31/66 [00:01<00:02, 15.59it/s] 52%|█████▏    | 34/66 [00:02<00:01, 18.25it/s] 56%|█████▌    | 37/66 [00:02<00:01, 18.01it/s] 59%|█████▉    | 39/66 [00:02<00:01, 16.80it/s] 62%|██████▏   | 41/66 [00:02<00:01, 16.99it/s] 65%|██████▌   | 43/66 [00:02<00:01, 17.10it/s] 68%|██████▊   | 45/66 [00:02<00:01, 16.55it/s] 71%|███████   | 47/66 [00:02<00:01, 16.29it/s] 74%|███████▍  | 49/66 [00:02<00:01, 16.01it/s] 77%|███████▋  | 51/66 [00:03<00:01, 13.65it/s] 80%|████████  | 53/66 [00:03<00:01, 12.23it/s] 83%|████████▎ | 55/66 [00:03<00:00, 11.44it/s] 86%|████████▋ | 57/66 [00:03<00:00, 11.19it/s] 89%|████████▉ | 59/66 [00:03<00:00, 11.22it/s] 92%|█████████▏| 61/66 [00:04<00:00, 11.46it/s] 95%|█████████▌| 63/66 [00:04<00:00, 11.14it/s] 98%|█████████▊| 65/66 [00:04<00:00, 11.05it/s]100%|██████████| 66/66 [00:04<00:00, 14.69it/s]
{'eval_loss': 0.6399435997009277, 'eval_model_preparation_time': 0.0059, 'eval_acc': 0.6768935762224353, 'eval_runtime': 4.5661, 'eval_samples_per_second': 228.42, 'eval_steps_per_second': 14.454}
ROUND:6
CLIENT:69
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.93it/s]                                              {'loss': 0.7455, 'grad_norm': 1.442606806755066, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.93it/s]  7%|▋         | 2/30 [00:00<00:03,  8.80it/s]                                              {'loss': 0.6472, 'grad_norm': 0.7239293456077576, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.80it/s]                                              {'loss': 0.6283, 'grad_norm': 0.70299232006073, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.80it/s] 13%|█▎        | 4/30 [00:00<00:02, 10.05it/s]                                              {'loss': 0.7015, 'grad_norm': 1.9956049919128418, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.05it/s]                                              {'loss': 0.6386, 'grad_norm': 0.47042784094810486, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.05it/s] 20%|██        | 6/30 [00:00<00:01, 12.30it/s]                                              {'loss': 0.9371, 'grad_norm': 2.6695175170898438, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 12.30it/s]                                              {'loss': 0.6568, 'grad_norm': 0.6025426983833313, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 12.30it/s] 27%|██▋       | 8/30 [00:00<00:01, 12.03it/s]                                              {'loss': 0.6783, 'grad_norm': 0.6425543427467346, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 12.03it/s]                                              {'loss': 0.6339, 'grad_norm': 0.5612683296203613, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 12.03it/s] 33%|███▎      | 10/30 [00:00<00:01, 12.19it/s]                                               {'loss': 0.6033, 'grad_norm': 0.49783068895339966, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 12.19it/s]                                               {'loss': 0.6568, 'grad_norm': 0.5509728789329529, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.19it/s] 40%|████      | 12/30 [00:01<00:01, 12.32it/s]                                               {'loss': 0.6315, 'grad_norm': 1.6232138872146606, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 12.32it/s]                                               {'loss': 0.6023, 'grad_norm': 0.8300945162773132, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 12.32it/s] 47%|████▋     | 14/30 [00:01<00:01, 12.20it/s]                                               {'loss': 0.5904, 'grad_norm': 1.3437286615371704, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 12.20it/s]                                               {'loss': 0.7416, 'grad_norm': 3.1374268531799316, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 12.20it/s] 53%|█████▎    | 16/30 [00:01<00:01, 12.35it/s]                                               {'loss': 0.5946, 'grad_norm': 0.8920700550079346, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 12.35it/s]                                               {'loss': 0.513, 'grad_norm': 1.1642308235168457, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.35it/s] 60%|██████    | 18/30 [00:01<00:00, 13.92it/s]                                               {'loss': 0.6715, 'grad_norm': 2.182495355606079, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.92it/s]                                               {'loss': 0.5123, 'grad_norm': 1.1001352071762085, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.92it/s] 67%|██████▋   | 20/30 [00:01<00:00, 13.50it/s]                                               {'loss': 0.5589, 'grad_norm': 1.8985575437545776, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.50it/s]                                               {'loss': 0.4639, 'grad_norm': 1.5376410484313965, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.50it/s] 73%|███████▎  | 22/30 [00:01<00:00, 13.35it/s]                                               {'loss': 0.5975, 'grad_norm': 1.6322851181030273, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.35it/s]                                               {'loss': 0.6609, 'grad_norm': 2.5321788787841797, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.35it/s] 80%|████████  | 24/30 [00:01<00:00, 14.59it/s]                                               {'loss': 0.4756, 'grad_norm': 2.888448476791382, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 14.59it/s]                                               {'loss': 0.4773, 'grad_norm': 1.3875627517700195, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.59it/s] 87%|████████▋ | 26/30 [00:02<00:00, 13.92it/s]                                               {'loss': 0.5867, 'grad_norm': 1.2227139472961426, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.92it/s]                                               {'loss': 0.5143, 'grad_norm': 8.359298706054688, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.92it/s] 93%|█████████▎| 28/30 [00:02<00:00, 13.49it/s]                                               {'loss': 0.4606, 'grad_norm': 1.2913286685943604, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.49it/s]                                               {'loss': 0.4986, 'grad_norm': 1.9221136569976807, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.49it/s]100%|██████████| 30/30 [00:02<00:00, 13.97it/s]                                               {'loss': 0.392, 'grad_norm': 3.8325865268707275, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.97it/s]                                               {'train_runtime': 2.4021, 'train_samples_per_second': 176.93, 'train_steps_per_second': 12.489, 'train_loss': 0.6023552010456721, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.97it/s]100%|██████████| 30/30 [00:02<00:00, 12.49it/s]
CLIENT:74
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.42it/s]                                              {'loss': 0.6635, 'grad_norm': 1.4043166637420654, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.42it/s]  7%|▋         | 2/30 [00:00<00:07,  3.68it/s]                                              {'loss': 0.4665, 'grad_norm': 1.165482521057129, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.68it/s] 10%|█         | 3/30 [00:00<00:07,  3.67it/s]                                              {'loss': 0.74, 'grad_norm': 2.398948907852173, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.67it/s] 13%|█▎        | 4/30 [00:01<00:06,  3.82it/s]                                              {'loss': 0.6134, 'grad_norm': 1.4685702323913574, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:06,  3.82it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.75it/s]                                              {'loss': 0.4738, 'grad_norm': 0.9801689982414246, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.75it/s] 20%|██        | 6/30 [00:01<00:08,  2.71it/s]                                              {'loss': 0.8466, 'grad_norm': 1.9084464311599731, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:08,  2.71it/s] 23%|██▎       | 7/30 [00:02<00:07,  2.89it/s]                                              {'loss': 0.3229, 'grad_norm': 1.2071943283081055, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:07,  2.89it/s] 27%|██▋       | 8/30 [00:02<00:07,  3.01it/s]                                              {'loss': 0.7022, 'grad_norm': 1.5900933742523193, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:07,  3.01it/s] 30%|███       | 9/30 [00:02<00:06,  3.12it/s]                                              {'loss': 0.4739, 'grad_norm': 0.6588205099105835, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:06,  3.12it/s] 33%|███▎      | 10/30 [00:03<00:06,  3.28it/s]                                               {'loss': 0.4848, 'grad_norm': 0.6999963521957397, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:06,  3.28it/s] 37%|███▋      | 11/30 [00:03<00:05,  3.35it/s]                                               {'loss': 0.4064, 'grad_norm': 1.2358304262161255, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:05,  3.35it/s] 40%|████      | 12/30 [00:03<00:04,  4.19it/s]                                               {'loss': 0.1686, 'grad_norm': 1.5950450897216797, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:04,  4.19it/s] 43%|████▎     | 13/30 [00:03<00:04,  3.92it/s]                                               {'loss': 0.4372, 'grad_norm': 0.929790735244751, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:04,  3.92it/s] 47%|████▋     | 14/30 [00:04<00:04,  3.76it/s]                                               {'loss': 0.4218, 'grad_norm': 0.8367030620574951, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:04,  3.76it/s] 50%|█████     | 15/30 [00:04<00:04,  3.65it/s]                                               {'loss': 0.5688, 'grad_norm': 2.1989715099334717, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:04,  3.65it/s] 53%|█████▎    | 16/30 [00:04<00:03,  3.64it/s]                                               {'loss': 0.4223, 'grad_norm': 1.2989550828933716, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:03,  3.64it/s] 57%|█████▋    | 17/30 [00:04<00:03,  3.78it/s]                                               {'loss': 0.31, 'grad_norm': 1.4178143739700317, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:03,  3.78it/s]                                               {'loss': 0.7237, 'grad_norm': 3.448481798171997, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:03,  3.78it/s] 63%|██████▎   | 19/30 [00:05<00:02,  4.60it/s]                                               {'loss': 0.3172, 'grad_norm': 1.0311782360076904, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  4.60it/s] 67%|██████▋   | 20/30 [00:05<00:02,  4.30it/s]                                               {'loss': 0.4692, 'grad_norm': 1.605160117149353, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  4.30it/s] 70%|███████   | 21/30 [00:05<00:02,  4.08it/s]                                               {'loss': 0.4095, 'grad_norm': 1.1976706981658936, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:02,  4.08it/s] 73%|███████▎  | 22/30 [00:05<00:01,  4.13it/s]                                               {'loss': 0.3451, 'grad_norm': 0.916958749294281, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:01,  4.13it/s] 77%|███████▋  | 23/30 [00:06<00:01,  4.15it/s]                                               {'loss': 0.4744, 'grad_norm': 1.583800196647644, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  4.15it/s]                                               {'loss': 0.183, 'grad_norm': 2.846377372741699, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  4.15it/s] 83%|████████▎ | 25/30 [00:06<00:00,  5.01it/s]                                               {'loss': 0.1665, 'grad_norm': 1.9128451347351074, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:00,  5.01it/s] 87%|████████▋ | 26/30 [00:06<00:00,  4.83it/s]                                               {'loss': 0.5405, 'grad_norm': 1.3309500217437744, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:00,  4.83it/s] 90%|█████████ | 27/30 [00:06<00:00,  4.74it/s]                                               {'loss': 0.5222, 'grad_norm': 2.1449480056762695, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  4.74it/s] 93%|█████████▎| 28/30 [00:07<00:00,  4.63it/s]                                               {'loss': 0.3055, 'grad_norm': 1.0592690706253052, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  4.63it/s] 97%|█████████▋| 29/30 [00:07<00:00,  4.76it/s]                                               {'loss': 0.3464, 'grad_norm': 1.247333288192749, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  4.76it/s]                                               {'loss': 0.5988, 'grad_norm': 5.190335750579834, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  4.76it/s]                                               {'train_runtime': 7.5744, 'train_samples_per_second': 56.11, 'train_steps_per_second': 3.961, 'train_loss': 0.4641494741042455, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  4.76it/s]100%|██████████| 30/30 [00:07<00:00,  3.96it/s]
CLIENT:34
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.36it/s]                                              {'loss': 0.7237, 'grad_norm': 1.8479890823364258, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.36it/s]  7%|▋         | 2/30 [00:00<00:08,  3.48it/s]                                              {'loss': 0.489, 'grad_norm': 1.164400577545166, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:08,  3.48it/s] 10%|█         | 3/30 [00:00<00:07,  3.62it/s]                                              {'loss': 0.3511, 'grad_norm': 1.5771541595458984, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.62it/s] 13%|█▎        | 4/30 [00:01<00:06,  4.06it/s]                                              {'loss': 0.6242, 'grad_norm': 2.9769020080566406, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:06,  4.06it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.33it/s]                                              {'loss': 0.4968, 'grad_norm': 1.9603564739227295, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.33it/s] 20%|██        | 6/30 [00:02<00:11,  2.15it/s]                                              {'loss': 1.3674, 'grad_norm': 7.499242305755615, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:11,  2.15it/s] 23%|██▎       | 7/30 [00:02<00:08,  2.74it/s]                                              {'loss': 0.3266, 'grad_norm': 0.692497193813324, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:08,  2.74it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.19it/s]                                              {'loss': 0.3837, 'grad_norm': 1.3155264854431152, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.19it/s] 30%|███       | 9/30 [00:02<00:05,  3.97it/s]                                              {'loss': 0.748, 'grad_norm': 2.2790865898132324, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  3.97it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.18it/s]                                               {'loss': 0.3658, 'grad_norm': 0.9232958555221558, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.18it/s] 37%|███▋      | 11/30 [00:03<00:04,  4.48it/s]                                               {'loss': 0.6333, 'grad_norm': 1.5867646932601929, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:04,  4.48it/s] 40%|████      | 12/30 [00:03<00:03,  5.33it/s]                                               {'loss': 0.4671, 'grad_norm': 2.223292827606201, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:03,  5.33it/s] 43%|████▎     | 13/30 [00:03<00:03,  4.94it/s]                                               {'loss': 0.2992, 'grad_norm': 1.726659893989563, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  4.94it/s] 47%|████▋     | 14/30 [00:03<00:03,  5.08it/s]                                               {'loss': 0.4127, 'grad_norm': 0.9739187955856323, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  5.08it/s] 50%|█████     | 15/30 [00:03<00:03,  4.84it/s]                                               {'loss': 0.7292, 'grad_norm': 2.2795944213867188, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.84it/s] 53%|█████▎    | 16/30 [00:03<00:02,  4.94it/s]                                               {'loss': 0.4633, 'grad_norm': 1.3362212181091309, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  4.94it/s] 57%|█████▋    | 17/30 [00:04<00:02,  4.92it/s]                                               {'loss': 0.4658, 'grad_norm': 1.0873632431030273, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:02,  4.92it/s]                                               {'loss': 0.7579, 'grad_norm': 3.126328229904175, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.92it/s] 63%|██████▎   | 19/30 [00:04<00:02,  5.45it/s]                                               {'loss': 0.3842, 'grad_norm': 1.1351759433746338, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  5.45it/s] 67%|██████▋   | 20/30 [00:04<00:01,  5.20it/s]                                               {'loss': 0.4586, 'grad_norm': 0.7597121000289917, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  5.20it/s] 70%|███████   | 21/30 [00:04<00:01,  5.18it/s]                                               {'loss': 0.3835, 'grad_norm': 1.344276785850525, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.18it/s] 73%|███████▎  | 22/30 [00:05<00:01,  5.43it/s]                                               {'loss': 0.4921, 'grad_norm': 0.8981653451919556, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:01,  5.43it/s] 77%|███████▋  | 23/30 [00:05<00:01,  5.64it/s]                                               {'loss': 0.3918, 'grad_norm': 1.3429276943206787, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  5.64it/s]                                               {'loss': 0.413, 'grad_norm': 1.424800157546997, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.64it/s] 83%|████████▎ | 25/30 [00:05<00:00,  6.23it/s]                                               {'loss': 0.6435, 'grad_norm': 3.439774990081787, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  6.23it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.65it/s]                                               {'loss': 0.5118, 'grad_norm': 2.018796920776367, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.65it/s] 90%|█████████ | 27/30 [00:05<00:00,  5.24it/s]                                               {'loss': 0.3101, 'grad_norm': 0.9253599047660828, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  5.24it/s] 93%|█████████▎| 28/30 [00:06<00:00,  5.08it/s]                                               {'loss': 0.2589, 'grad_norm': 1.672391414642334, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  5.08it/s] 97%|█████████▋| 29/30 [00:06<00:00,  4.96it/s]                                               {'loss': 0.6319, 'grad_norm': 2.338589906692505, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  4.96it/s]                                               {'loss': 0.2401, 'grad_norm': 2.189671277999878, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.96it/s]                                               {'train_runtime': 6.5476, 'train_samples_per_second': 64.91, 'train_steps_per_second': 4.582, 'train_loss': 0.5074744244416555, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.96it/s]100%|██████████| 30/30 [00:06<00:00,  4.58it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  9.34it/s]                                              {'loss': 0.5318, 'grad_norm': 0.36303451657295227, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  9.34it/s]                                              {'loss': 0.6426, 'grad_norm': 1.0974760055541992, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02,  9.34it/s] 10%|█         | 3/30 [00:00<00:02, 11.84it/s]                                              {'loss': 0.6594, 'grad_norm': 0.6752068996429443, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.84it/s]                                              {'loss': 0.7544, 'grad_norm': 1.5241906642913818, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.84it/s] 17%|█▋        | 5/30 [00:00<00:02, 12.16it/s]                                              {'loss': 0.6943, 'grad_norm': 1.1830459833145142, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.16it/s]                                              {'loss': 0.8395, 'grad_norm': 2.143972635269165, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:01, 12.16it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.52it/s]                                              {'loss': 0.6996, 'grad_norm': 0.9995288848876953, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.52it/s]                                              {'loss': 0.6583, 'grad_norm': 1.0222995281219482, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.52it/s] 30%|███       | 9/30 [00:01<00:03,  5.93it/s]                                              {'loss': 0.6834, 'grad_norm': 0.8654546737670898, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.93it/s]                                              {'loss': 0.6595, 'grad_norm': 0.9610885381698608, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.93it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.28it/s]                                               {'loss': 0.6314, 'grad_norm': 0.7471988201141357, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.28it/s]                                               {'loss': 0.6849, 'grad_norm': 2.1589317321777344, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.28it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.03it/s]                                               {'loss': 0.5728, 'grad_norm': 0.8979249000549316, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  9.03it/s]                                               {'loss': 0.6353, 'grad_norm': 1.3452692031860352, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  9.03it/s] 50%|█████     | 15/30 [00:01<00:01,  9.76it/s]                                               {'loss': 0.7135, 'grad_norm': 2.687319040298462, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  9.76it/s]                                               {'loss': 0.5707, 'grad_norm': 1.146632432937622, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  9.76it/s] 57%|█████▋    | 17/30 [00:02<00:01, 10.57it/s]                                               {'loss': 0.5389, 'grad_norm': 1.242260217666626, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01, 10.57it/s]                                               {'loss': 0.6732, 'grad_norm': 4.47764253616333, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 10.57it/s] 63%|██████▎   | 19/30 [00:02<00:00, 12.16it/s]                                               {'loss': 0.4963, 'grad_norm': 1.395836591720581, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 12.16it/s]                                               {'loss': 0.5658, 'grad_norm': 1.2270753383636475, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 12.16it/s] 70%|███████   | 21/30 [00:02<00:00, 12.16it/s]                                               {'loss': 0.5061, 'grad_norm': 1.1383099555969238, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 12.16it/s]                                               {'loss': 0.5477, 'grad_norm': 1.836403489112854, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 12.16it/s] 77%|███████▋  | 23/30 [00:02<00:00, 12.19it/s]                                               {'loss': 0.6117, 'grad_norm': 1.970974087715149, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 12.19it/s]                                               {'loss': 0.4677, 'grad_norm': 2.277005910873413, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 12.19it/s] 83%|████████▎ | 25/30 [00:02<00:00, 11.56it/s]                                               {'loss': 0.4885, 'grad_norm': 1.3149467706680298, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 11.56it/s]                                               {'loss': 0.4973, 'grad_norm': 1.781130313873291, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 11.56it/s] 90%|█████████ | 27/30 [00:02<00:00, 10.34it/s]                                               {'loss': 0.4876, 'grad_norm': 2.021367073059082, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 10.34it/s]                                               {'loss': 0.5116, 'grad_norm': 1.822049617767334, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 10.34it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.88it/s]                                               {'loss': 0.4623, 'grad_norm': 1.4297873973846436, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  9.88it/s]                                               {'loss': 0.3865, 'grad_norm': 2.8034005165100098, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  9.88it/s]                                               {'train_runtime': 3.2247, 'train_samples_per_second': 131.794, 'train_steps_per_second': 9.303, 'train_loss': 0.5957440962394078, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  9.88it/s]100%|██████████| 30/30 [00:03<00:00,  9.32it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.38it/s]                                              {'loss': 0.5713, 'grad_norm': 0.6912546157836914, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.38it/s]  7%|▋         | 2/30 [00:00<00:03,  7.07it/s]                                              {'loss': 0.7161, 'grad_norm': 0.6151012778282166, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.07it/s] 10%|█         | 3/30 [00:00<00:03,  7.80it/s]                                              {'loss': 0.7205, 'grad_norm': 0.5691688656806946, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.80it/s] 13%|█▎        | 4/30 [00:00<00:03,  8.08it/s]                                              {'loss': 0.7305, 'grad_norm': 0.7622173428535461, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  8.08it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.59it/s]                                              {'loss': 0.7063, 'grad_norm': 0.5590575337409973, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.59it/s]                                              {'loss': 0.7606, 'grad_norm': 1.2751410007476807, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.59it/s] 23%|██▎       | 7/30 [00:00<00:03,  7.23it/s]                                              {'loss': 0.6687, 'grad_norm': 0.6503593921661377, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:03,  7.23it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.48it/s]                                              {'loss': 0.7161, 'grad_norm': 0.7566468119621277, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.48it/s] 30%|███       | 9/30 [00:01<00:03,  5.86it/s]                                              {'loss': 0.623, 'grad_norm': 0.7732035517692566, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.86it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.50it/s]                                               {'loss': 0.674, 'grad_norm': 2.2397007942199707, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.50it/s] 37%|███▋      | 11/30 [00:01<00:03,  5.63it/s]                                               {'loss': 0.668, 'grad_norm': 0.45982807874679565, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  5.63it/s]                                               {'loss': 0.5511, 'grad_norm': 1.1491813659667969, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:03,  5.63it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.01it/s]                                               {'loss': 0.6249, 'grad_norm': 0.7677342891693115, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.01it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.02it/s]                                               {'loss': 0.6054, 'grad_norm': 0.5307272672653198, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.02it/s] 50%|█████     | 15/30 [00:02<00:02,  5.76it/s]                                               {'loss': 0.8273, 'grad_norm': 3.324871063232422, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.76it/s] 53%|█████▎    | 16/30 [00:02<00:02,  5.86it/s]                                               {'loss': 0.7462, 'grad_norm': 4.737482070922852, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  5.86it/s] 57%|█████▋    | 17/30 [00:02<00:02,  5.67it/s]                                               {'loss': 0.5879, 'grad_norm': 1.4513580799102783, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  5.67it/s]                                               {'loss': 0.6626, 'grad_norm': 6.017791271209717, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:02,  5.67it/s] 63%|██████▎   | 19/30 [00:03<00:01,  6.03it/s]                                               {'loss': 0.6095, 'grad_norm': 1.1145628690719604, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  6.03it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.60it/s]                                               {'loss': 0.7505, 'grad_norm': 2.789625883102417, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.60it/s] 70%|███████   | 21/30 [00:03<00:01,  5.32it/s]                                               {'loss': 0.6681, 'grad_norm': 0.8595165014266968, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.32it/s] 73%|███████▎  | 22/30 [00:03<00:01,  4.71it/s]                                               {'loss': 0.6371, 'grad_norm': 1.4009344577789307, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  4.71it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.30it/s]                                               {'loss': 0.6365, 'grad_norm': 1.14735746383667, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.30it/s]                                               {'loss': 0.6395, 'grad_norm': 4.124044895172119, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  4.30it/s] 83%|████████▎ | 25/30 [00:04<00:01,  4.71it/s]                                               {'loss': 0.6335, 'grad_norm': 2.5638811588287354, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:01,  4.71it/s] 87%|████████▋ | 26/30 [00:04<00:00,  4.38it/s]                                               {'loss': 0.6298, 'grad_norm': 2.7127270698547363, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  4.38it/s] 90%|█████████ | 27/30 [00:04<00:00,  4.25it/s]                                               {'loss': 0.6352, 'grad_norm': 0.6159346103668213, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  4.25it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.33it/s]                                               {'loss': 0.5976, 'grad_norm': 0.726650595664978, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.33it/s] 97%|█████████▋| 29/30 [00:05<00:00,  4.44it/s]                                               {'loss': 0.5784, 'grad_norm': 1.3547401428222656, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  4.44it/s]                                               {'loss': 0.6232, 'grad_norm': 4.294258117675781, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.44it/s]                                               {'train_runtime': 5.5505, 'train_samples_per_second': 76.57, 'train_steps_per_second': 5.405, 'train_loss': 0.6599813938140869, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.44it/s]100%|██████████| 30/30 [00:05<00:00,  5.41it/s]
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.70it/s]                                              {'loss': 0.7769, 'grad_norm': 2.4188215732574463, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.70it/s]  7%|▋         | 2/30 [00:00<00:06,  4.58it/s]                                              {'loss': 0.466, 'grad_norm': 1.5379396677017212, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.58it/s] 10%|█         | 3/30 [00:00<00:05,  4.58it/s]                                              {'loss': 0.1892, 'grad_norm': 1.5916047096252441, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.58it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.52it/s]                                              {'loss': 0.321, 'grad_norm': 0.8807126879692078, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.52it/s] 17%|█▋        | 5/30 [00:01<00:04,  5.06it/s]                                              {'loss': 0.2519, 'grad_norm': 1.6319079399108887, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:04,  5.06it/s] 20%|██        | 6/30 [00:01<00:06,  3.70it/s]                                              {'loss': 0.0361, 'grad_norm': 0.7404865026473999, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.70it/s] 23%|██▎       | 7/30 [00:01<00:05,  3.92it/s]                                              {'loss': 0.185, 'grad_norm': 1.49663245677948, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  3.92it/s] 27%|██▋       | 8/30 [00:01<00:05,  4.24it/s]                                              {'loss': 0.0712, 'grad_norm': 2.74345064163208, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  4.24it/s] 30%|███       | 9/30 [00:02<00:04,  4.77it/s]                                              {'loss': 0.3348, 'grad_norm': 1.0492839813232422, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.77it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.06it/s]                                               {'loss': 0.0157, 'grad_norm': 0.5639996528625488, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.06it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.35it/s]                                               {'loss': 0.0099, 'grad_norm': 0.40082961320877075, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.35it/s]                                               {'loss': 0.0084, 'grad_norm': 0.12724991142749786, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.35it/s] 43%|████▎     | 13/30 [00:02<00:02,  5.97it/s]                                               {'loss': 0.006, 'grad_norm': 0.0867365300655365, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  5.97it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.80it/s]                                               {'loss': 0.0049, 'grad_norm': 0.06954412907361984, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.80it/s] 50%|█████     | 15/30 [00:02<00:02,  6.07it/s]                                               {'loss': 0.3624, 'grad_norm': 1.1501775979995728, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.07it/s] 53%|█████▎    | 16/30 [00:03<00:02,  6.34it/s]                                               {'loss': 0.0045, 'grad_norm': 0.054633837193250656, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  6.34it/s]                                               {'loss': 0.3238, 'grad_norm': 0.8692145943641663, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  6.34it/s] 60%|██████    | 18/30 [00:03<00:01,  7.94it/s]                                               {'loss': 0.0063, 'grad_norm': 0.07989563792943954, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  7.94it/s] 63%|██████▎   | 19/30 [00:03<00:01,  7.87it/s]                                               {'loss': 0.0063, 'grad_norm': 0.08517128974199295, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  7.87it/s]                                               {'loss': 0.3183, 'grad_norm': 0.8457179069519043, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.87it/s] 70%|███████   | 21/30 [00:03<00:01,  7.88it/s]                                               {'loss': 0.009, 'grad_norm': 0.12105928361415863, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.88it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.84it/s]                                               {'loss': 0.0102, 'grad_norm': 0.1260601282119751, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.84it/s] 77%|███████▋  | 23/30 [00:03<00:00,  8.15it/s]                                               {'loss': 0.2778, 'grad_norm': 0.8370232582092285, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  8.15it/s]                                               {'loss': 0.0109, 'grad_norm': 0.13105808198451996, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  8.15it/s] 83%|████████▎ | 25/30 [00:04<00:00,  9.03it/s]                                               {'loss': 0.6384, 'grad_norm': 1.9516208171844482, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  9.03it/s] 87%|████████▋ | 26/30 [00:04<00:00,  8.86it/s]                                               {'loss': 0.0175, 'grad_norm': 0.23130472004413605, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  8.86it/s] 90%|█████████ | 27/30 [00:04<00:00,  8.42it/s]                                               {'loss': 0.014, 'grad_norm': 0.16144207119941711, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  8.42it/s] 93%|█████████▎| 28/30 [00:04<00:00,  8.63it/s]                                               {'loss': 0.0157, 'grad_norm': 0.16726019978523254, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  8.63it/s]                                               {'loss': 0.0138, 'grad_norm': 0.1459730565547943, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  8.63it/s]                                               {'loss': 0.0123, 'grad_norm': 0.1419862061738968, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  8.63it/s]                                               {'train_runtime': 4.7272, 'train_samples_per_second': 89.906, 'train_steps_per_second': 6.346, 'train_loss': 0.15727632138878106, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  8.63it/s]100%|██████████| 30/30 [00:04<00:00,  6.34it/s]
CLIENT:32
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.12it/s]                                              {'loss': 0.6641, 'grad_norm': 0.5087848901748657, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.12it/s]  7%|▋         | 2/30 [00:00<00:05,  5.25it/s]                                              {'loss': 0.6708, 'grad_norm': 0.542449414730072, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.25it/s] 10%|█         | 3/30 [00:00<00:05,  4.61it/s]                                              {'loss': 0.6484, 'grad_norm': 0.7958215475082397, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.61it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.45it/s]                                              {'loss': 0.7165, 'grad_norm': 1.4271267652511597, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.45it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.53it/s]                                              {'loss': 0.6769, 'grad_norm': 0.5918664932250977, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.53it/s]                                              {'loss': 0.8568, 'grad_norm': 2.5467607975006104, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.53it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.74it/s]                                              {'loss': 0.6429, 'grad_norm': 0.3426932096481323, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.74it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.08it/s]                                              {'loss': 0.746, 'grad_norm': 0.6442697048187256, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.08it/s] 30%|███       | 9/30 [00:01<00:03,  6.35it/s]                                              {'loss': 0.6615, 'grad_norm': 0.3564111292362213, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.35it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.59it/s]                                               {'loss': 0.6417, 'grad_norm': 0.30433589220046997, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.59it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.56it/s]                                               {'loss': 0.6551, 'grad_norm': 0.30660271644592285, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.56it/s] 40%|████      | 12/30 [00:02<00:02,  6.21it/s]                                               {'loss': 0.5762, 'grad_norm': 1.4642640352249146, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.21it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.65it/s]                                               {'loss': 0.6074, 'grad_norm': 0.5325846076011658, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.65it/s] 47%|████▋     | 14/30 [00:02<00:03,  5.13it/s]                                               {'loss': 0.6216, 'grad_norm': 0.4083128869533539, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  5.13it/s] 50%|█████     | 15/30 [00:02<00:03,  4.76it/s]                                               {'loss': 0.7466, 'grad_norm': 0.8891614675521851, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:03,  4.76it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.52it/s]                                               {'loss': 0.6619, 'grad_norm': 0.6846749186515808, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.52it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.48it/s]                                               {'loss': 0.6229, 'grad_norm': 0.6088432669639587, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.48it/s] 60%|██████    | 18/30 [00:03<00:02,  4.45it/s]                                               {'loss': 0.6695, 'grad_norm': 1.109121322631836, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.45it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.04it/s]                                               {'loss': 0.6076, 'grad_norm': 0.6892648339271545, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.04it/s] 67%|██████▋   | 20/30 [00:04<00:02,  3.87it/s]                                               {'loss': 0.6536, 'grad_norm': 0.3925645351409912, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  3.87it/s] 70%|███████   | 21/30 [00:04<00:02,  3.58it/s]                                               {'loss': 0.6073, 'grad_norm': 0.7221006155014038, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:02,  3.58it/s] 73%|███████▎  | 22/30 [00:04<00:02,  3.52it/s]                                               {'loss': 0.6151, 'grad_norm': 0.6649274230003357, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:02,  3.52it/s] 77%|███████▋  | 23/30 [00:04<00:01,  3.57it/s]                                               {'loss': 0.6059, 'grad_norm': 0.736668050289154, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  3.57it/s] 80%|████████  | 24/30 [00:05<00:01,  3.83it/s]                                               {'loss': 0.6026, 'grad_norm': 0.8939601182937622, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  3.83it/s] 83%|████████▎ | 25/30 [00:05<00:01,  3.76it/s]                                               {'loss': 0.5973, 'grad_norm': 0.8367247581481934, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  3.76it/s] 87%|████████▋ | 26/30 [00:05<00:01,  3.71it/s]                                               {'loss': 0.5946, 'grad_norm': 0.5098490118980408, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:01,  3.71it/s] 90%|█████████ | 27/30 [00:06<00:00,  3.70it/s]                                               {'loss': 0.5849, 'grad_norm': 0.6938952207565308, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  3.70it/s] 93%|█████████▎| 28/30 [00:06<00:00,  3.56it/s]                                               {'loss': 0.5883, 'grad_norm': 0.5989527106285095, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  3.56it/s] 97%|█████████▋| 29/30 [00:06<00:00,  3.52it/s]                                               {'loss': 0.559, 'grad_norm': 0.7661360502243042, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  3.52it/s]100%|██████████| 30/30 [00:06<00:00,  4.06it/s]                                               {'loss': 0.5801, 'grad_norm': 1.1499948501586914, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.06it/s]                                               {'train_runtime': 7.1854, 'train_samples_per_second': 59.147, 'train_steps_per_second': 4.175, 'train_loss': 0.6427716533342998, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  4.06it/s]100%|██████████| 30/30 [00:07<00:00,  4.18it/s]
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:10,  2.89it/s]                                              {'loss': 0.551, 'grad_norm': 1.100968360900879, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:10,  2.89it/s]  7%|▋         | 2/30 [00:00<00:09,  2.84it/s]                                              {'loss': 0.4397, 'grad_norm': 1.361973524093628, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:09,  2.84it/s] 10%|█         | 3/30 [00:01<00:09,  2.75it/s]                                              {'loss': 0.3766, 'grad_norm': 0.7829321622848511, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:09,  2.75it/s] 13%|█▎        | 4/30 [00:01<00:09,  2.73it/s]                                              {'loss': 0.3106, 'grad_norm': 0.788497269153595, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:09,  2.73it/s] 17%|█▋        | 5/30 [00:01<00:09,  2.77it/s]                                              {'loss': 0.7873, 'grad_norm': 2.521243095397949, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:09,  2.77it/s] 20%|██        | 6/30 [00:01<00:06,  3.50it/s]                                              {'loss': 0.0294, 'grad_norm': 0.5515074133872986, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.50it/s] 23%|██▎       | 7/30 [00:02<00:07,  3.20it/s]                                              {'loss': 0.1356, 'grad_norm': 0.8794174194335938, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:07,  3.20it/s] 27%|██▋       | 8/30 [00:02<00:07,  3.05it/s]                                              {'loss': 0.6164, 'grad_norm': 2.6364965438842773, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:07,  3.05it/s] 30%|███       | 9/30 [00:03<00:07,  2.96it/s]                                              {'loss': 0.7215, 'grad_norm': 3.2224490642547607, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:03<00:07,  2.96it/s] 33%|███▎      | 10/30 [00:03<00:06,  2.92it/s]                                               {'loss': 0.8003, 'grad_norm': 2.1618764400482178, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:06,  2.92it/s] 37%|███▋      | 11/30 [00:03<00:06,  2.92it/s]                                               {'loss': 0.2527, 'grad_norm': 1.6437605619430542, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:06,  2.92it/s] 40%|████      | 12/30 [00:03<00:05,  3.54it/s]                                               {'loss': 0.7929, 'grad_norm': 3.5211172103881836, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:05,  3.54it/s] 43%|████▎     | 13/30 [00:04<00:05,  3.28it/s]                                               {'loss': 0.38, 'grad_norm': 5.169908046722412, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:04<00:05,  3.28it/s] 47%|████▋     | 14/30 [00:04<00:05,  3.15it/s]                                               {'loss': 0.3849, 'grad_norm': 2.261002779006958, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:05,  3.15it/s] 50%|█████     | 15/30 [00:04<00:04,  3.05it/s]                                               {'loss': 0.4989, 'grad_norm': 2.3132667541503906, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:04,  3.05it/s] 53%|█████▎    | 16/30 [00:05<00:04,  2.97it/s]                                               {'loss': 0.454, 'grad_norm': 1.4956140518188477, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:05<00:04,  2.97it/s] 57%|█████▋    | 17/30 [00:05<00:04,  3.07it/s]                                               {'loss': 0.2785, 'grad_norm': 3.3022267818450928, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:05<00:04,  3.07it/s] 60%|██████    | 18/30 [00:05<00:03,  3.80it/s]                                               {'loss': 0.4925, 'grad_norm': 2.4108083248138428, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:03,  3.80it/s] 63%|██████▎   | 19/30 [00:05<00:03,  3.65it/s]                                               {'loss': 0.412, 'grad_norm': 4.359567165374756, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:06<00:03,  3.65it/s] 67%|██████▋   | 20/30 [00:06<00:02,  3.55it/s]                                               {'loss': 0.521, 'grad_norm': 1.909972906112671, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:06<00:02,  3.55it/s] 70%|███████   | 21/30 [00:06<00:02,  3.51it/s]                                               {'loss': 0.485, 'grad_norm': 2.102682590484619, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:06<00:02,  3.51it/s] 73%|███████▎  | 22/30 [00:06<00:02,  3.47it/s]                                               {'loss': 0.4829, 'grad_norm': 1.813437819480896, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:06<00:02,  3.47it/s] 77%|███████▋  | 23/30 [00:07<00:02,  3.45it/s]                                               {'loss': 0.2386, 'grad_norm': 3.38425612449646, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:07<00:02,  3.45it/s] 80%|████████  | 24/30 [00:07<00:01,  4.27it/s]                                               {'loss': 0.4448, 'grad_norm': 1.6161048412322998, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:07<00:01,  4.27it/s] 83%|████████▎ | 25/30 [00:07<00:01,  3.98it/s]                                               {'loss': 0.4035, 'grad_norm': 1.7174588441848755, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:07<00:01,  3.98it/s] 87%|████████▋ | 26/30 [00:07<00:01,  3.78it/s]                                               {'loss': 0.4858, 'grad_norm': 1.1633093357086182, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:07<00:01,  3.78it/s] 90%|█████████ | 27/30 [00:08<00:00,  3.58it/s]                                               {'loss': 0.4892, 'grad_norm': 1.929956078529358, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:08<00:00,  3.58it/s] 93%|█████████▎| 28/30 [00:08<00:00,  3.57it/s]                                               {'loss': 0.1931, 'grad_norm': 3.0547192096710205, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:08<00:00,  3.57it/s] 97%|█████████▋| 29/30 [00:08<00:00,  3.81it/s]                                               {'loss': 0.3654, 'grad_norm': 1.2447855472564697, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:08<00:00,  3.81it/s]                                               {'loss': 0.4166, 'grad_norm': 1.5235402584075928, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  3.81it/s]                                               {'train_runtime': 8.8204, 'train_samples_per_second': 48.184, 'train_steps_per_second': 3.401, 'train_loss': 0.4413536574691534, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  3.81it/s]100%|██████████| 30/30 [00:08<00:00,  3.40it/s]
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  5.81it/s]                                              {'loss': 0.8505, 'grad_norm': 2.087567090988159, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  5.81it/s]  7%|▋         | 2/30 [00:00<00:04,  6.84it/s]                                              {'loss': 0.4477, 'grad_norm': 1.5506694316864014, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.84it/s] 10%|█         | 3/30 [00:00<00:03,  7.23it/s]                                              {'loss': 0.2292, 'grad_norm': 1.9352947473526, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.23it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.49it/s]                                              {'loss': 0.2901, 'grad_norm': 0.7159271240234375, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.49it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.95it/s]                                              {'loss': 0.0405, 'grad_norm': 0.5399853587150574, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.95it/s] 20%|██        | 6/30 [00:01<00:06,  4.00it/s]                                              {'loss': 0.012, 'grad_norm': 0.14889661967754364, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  4.00it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.92it/s]                                              {'loss': 0.0095, 'grad_norm': 0.2409103512763977, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.92it/s]                                              {'loss': 0.0047, 'grad_norm': 0.2454848289489746, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.92it/s] 30%|███       | 9/30 [00:01<00:02,  7.00it/s]                                              {'loss': 0.0076, 'grad_norm': 0.3134128451347351, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.00it/s]                                              {'loss': 0.263, 'grad_norm': 4.706463813781738, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.00it/s] 37%|███▋      | 11/30 [00:01<00:02,  8.33it/s]                                               {'loss': 0.0034, 'grad_norm': 0.06592004001140594, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.33it/s]                                               {'loss': 0.0041, 'grad_norm': 0.07757004350423813, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.33it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.11it/s]                                               {'loss': 0.0028, 'grad_norm': 0.04364519193768501, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  9.11it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.07it/s]                                               {'loss': 0.0034, 'grad_norm': 0.07228217273950577, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  9.07it/s] 50%|█████     | 15/30 [00:02<00:01,  8.91it/s]                                               {'loss': 0.3801, 'grad_norm': 0.7637854814529419, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.91it/s] 53%|█████▎    | 16/30 [00:02<00:01,  8.07it/s]                                               {'loss': 0.0031, 'grad_norm': 0.05616727098822594, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.07it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.70it/s]                                               {'loss': 0.0057, 'grad_norm': 0.10046353936195374, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.70it/s]                                               {'loss': 0.004, 'grad_norm': 0.04687507450580597, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.70it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.04it/s]                                               {'loss': 0.0041, 'grad_norm': 0.061160337179899216, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  9.04it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.29it/s]                                               {'loss': 0.0037, 'grad_norm': 0.05389764904975891, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.29it/s]                                               {'loss': 0.0048, 'grad_norm': 0.07312595844268799, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.29it/s] 73%|███████▎  | 22/30 [00:02<00:00,  8.44it/s]                                               {'loss': 0.0057, 'grad_norm': 0.08953554183244705, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  8.44it/s]                                               {'loss': 0.3967, 'grad_norm': 1.4527709484100342, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  8.44it/s] 80%|████████  | 24/30 [00:03<00:00,  9.78it/s]                                               {'loss': 0.0047, 'grad_norm': 0.09507257491350174, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  9.78it/s] 83%|████████▎ | 25/30 [00:03<00:00,  9.67it/s]                                               {'loss': 0.0048, 'grad_norm': 0.09383179247379303, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  9.67it/s]                                               {'loss': 0.0063, 'grad_norm': 0.13861611485481262, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  9.67it/s] 90%|█████████ | 27/30 [00:03<00:00,  9.22it/s]                                               {'loss': 0.2831, 'grad_norm': 0.9066261053085327, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  9.22it/s] 93%|█████████▎| 28/30 [00:03<00:00,  8.92it/s]                                               {'loss': 0.0054, 'grad_norm': 0.11255152523517609, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  8.92it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.07it/s]                                               {'loss': 0.0044, 'grad_norm': 0.08231699466705322, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  9.07it/s]                                               {'loss': 0.0049, 'grad_norm': 0.09857181459665298, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  9.07it/s]                                               {'train_runtime': 3.7899, 'train_samples_per_second': 112.141, 'train_steps_per_second': 7.916, 'train_loss': 0.10966991990959893, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  9.07it/s]100%|██████████| 30/30 [00:03<00:00,  7.92it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.76it/s]                                              {'loss': 0.7078, 'grad_norm': 0.5667701959609985, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.76it/s]  7%|▋         | 2/30 [00:00<00:05,  5.41it/s]                                              {'loss': 0.6895, 'grad_norm': 0.5466285943984985, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.41it/s] 10%|█         | 3/30 [00:00<00:04,  5.59it/s]                                              {'loss': 0.6598, 'grad_norm': 0.8204058408737183, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  5.59it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.51it/s]                                              {'loss': 0.6944, 'grad_norm': 1.081884503364563, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.51it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.44it/s]                                              {'loss': 0.729, 'grad_norm': 0.7427249550819397, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.44it/s]                                              {'loss': 0.7314, 'grad_norm': 1.3539522886276245, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.44it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.94it/s]                                              {'loss': 0.6343, 'grad_norm': 0.5555568933486938, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.94it/s] 27%|██▋       | 8/30 [00:01<00:03,  7.07it/s]                                              {'loss': 0.6801, 'grad_norm': 0.39216259121894836, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  7.07it/s] 30%|███       | 9/30 [00:01<00:03,  6.97it/s]                                              {'loss': 0.6919, 'grad_norm': 0.5818355679512024, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.97it/s] 33%|███▎      | 10/30 [00:01<00:02,  6.78it/s]                                               {'loss': 0.688, 'grad_norm': 0.46983468532562256, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  6.78it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.35it/s]                                               {'loss': 0.5961, 'grad_norm': 0.6194751262664795, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.35it/s]                                               {'loss': 0.4528, 'grad_norm': 2.348389148712158, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.35it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.93it/s]                                               {'loss': 0.6131, 'grad_norm': 0.5971975922584534, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.93it/s] 47%|████▋     | 14/30 [00:02<00:01,  8.12it/s]                                               {'loss': 0.6248, 'grad_norm': 0.947828471660614, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  8.12it/s] 50%|█████     | 15/30 [00:02<00:01,  8.13it/s]                                               {'loss': 0.6997, 'grad_norm': 2.190943479537964, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.13it/s] 53%|█████▎    | 16/30 [00:02<00:01,  8.10it/s]                                               {'loss': 0.5879, 'grad_norm': 1.380439281463623, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.10it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.56it/s]                                               {'loss': 0.5032, 'grad_norm': 1.717835545539856, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.56it/s]                                               {'loss': 0.5325, 'grad_norm': 1.736814022064209, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.56it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.34it/s]                                               {'loss': 0.5109, 'grad_norm': 1.2577574253082275, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  9.34it/s]                                               {'loss': 0.6686, 'grad_norm': 1.4246788024902344, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  9.34it/s] 70%|███████   | 21/30 [00:02<00:00,  9.76it/s]                                               {'loss': 0.5853, 'grad_norm': 1.1152029037475586, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00,  9.76it/s]                                               {'loss': 0.4951, 'grad_norm': 0.853436291217804, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  9.76it/s] 77%|███████▋  | 23/30 [00:02<00:00, 10.44it/s]                                               {'loss': 0.5621, 'grad_norm': 1.1446683406829834, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 10.44it/s]                                               {'loss': 0.5918, 'grad_norm': 2.57942795753479, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 10.44it/s] 83%|████████▎ | 25/30 [00:03<00:00, 11.89it/s]                                               {'loss': 0.5955, 'grad_norm': 1.1198292970657349, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 11.89it/s]                                               {'loss': 0.6069, 'grad_norm': 1.1777935028076172, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 11.89it/s] 90%|█████████ | 27/30 [00:03<00:00, 12.03it/s]                                               {'loss': 0.4745, 'grad_norm': 1.4566431045532227, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 12.03it/s]                                               {'loss': 0.494, 'grad_norm': 1.2206329107284546, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 12.03it/s] 97%|█████████▋| 29/30 [00:03<00:00, 12.04it/s]                                               {'loss': 0.4656, 'grad_norm': 1.2320237159729004, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 12.04it/s]                                               {'loss': 0.4575, 'grad_norm': 1.9955908060073853, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.04it/s]                                               {'train_runtime': 3.5732, 'train_samples_per_second': 118.942, 'train_steps_per_second': 8.396, 'train_loss': 0.6008015592892965, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.04it/s]100%|██████████| 30/30 [00:03<00:00,  8.42it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  5%|▍         | 3/66 [00:00<00:02, 23.87it/s]  9%|▉         | 6/66 [00:00<00:02, 22.32it/s] 14%|█▎        | 9/66 [00:00<00:03, 18.03it/s] 18%|█▊        | 12/66 [00:00<00:02, 18.23it/s] 21%|██        | 14/66 [00:00<00:02, 17.80it/s] 24%|██▍       | 16/66 [00:00<00:02, 17.06it/s] 27%|██▋       | 18/66 [00:00<00:02, 17.30it/s] 32%|███▏      | 21/66 [00:01<00:02, 18.98it/s] 35%|███▍      | 23/66 [00:01<00:02, 17.69it/s] 38%|███▊      | 25/66 [00:01<00:02, 17.79it/s] 41%|████      | 27/66 [00:01<00:02, 18.20it/s] 45%|████▌     | 30/66 [00:01<00:01, 18.12it/s] 50%|█████     | 33/66 [00:01<00:01, 18.91it/s] 53%|█████▎    | 35/66 [00:01<00:01, 18.97it/s] 58%|█████▊    | 38/66 [00:02<00:01, 20.65it/s] 62%|██████▏   | 41/66 [00:02<00:01, 22.36it/s] 67%|██████▋   | 44/66 [00:02<00:01, 21.15it/s] 71%|███████   | 47/66 [00:02<00:00, 19.22it/s] 74%|███████▍  | 49/66 [00:02<00:00, 18.84it/s] 79%|███████▉  | 52/66 [00:02<00:00, 18.51it/s] 83%|████████▎ | 55/66 [00:02<00:00, 20.56it/s] 88%|████████▊ | 58/66 [00:03<00:00, 19.39it/s] 92%|█████████▏| 61/66 [00:03<00:00, 18.47it/s] 95%|█████████▌| 63/66 [00:03<00:00, 16.94it/s] 98%|█████████▊| 65/66 [00:03<00:00, 16.52it/s]100%|██████████| 66/66 [00:03<00:00, 18.74it/s]
{'eval_loss': 0.6337003111839294, 'eval_model_preparation_time': 0.0058, 'eval_acc': 0.6931927133269415, 'eval_runtime': 3.5692, 'eval_samples_per_second': 292.218, 'eval_steps_per_second': 18.491}
ROUND:7
CLIENT:71
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7421, 'grad_norm': 0.9237565994262695, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 10.25it/s]  7%|▋         | 2/30 [00:00<00:02, 11.13it/s]                                              {'loss': 0.5284, 'grad_norm': 1.0431548357009888, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 11.13it/s]                                              {'loss': 0.4611, 'grad_norm': 0.6428897380828857, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.13it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.09it/s]                                              {'loss': 0.8863, 'grad_norm': 3.334881544113159, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.09it/s]                                              {'loss': 0.726, 'grad_norm': 2.2380056381225586, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.09it/s] 20%|██        | 6/30 [00:00<00:01, 13.86it/s]                                              {'loss': 1.1978, 'grad_norm': 4.621476650238037, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 13.86it/s]                                              {'loss': 0.5403, 'grad_norm': 1.2466034889221191, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 13.86it/s] 27%|██▋       | 8/30 [00:00<00:01, 12.64it/s]                                              {'loss': 0.6584, 'grad_norm': 1.1667810678482056, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 12.64it/s]                                              {'loss': 0.5999, 'grad_norm': 1.5197871923446655, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 12.64it/s] 33%|███▎      | 10/30 [00:00<00:01, 11.88it/s]                                               {'loss': 0.6244, 'grad_norm': 1.2155663967132568, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.88it/s]                                               {'loss': 0.6045, 'grad_norm': 0.8673000931739807, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 11.88it/s] 40%|████      | 12/30 [00:00<00:01, 13.52it/s]                                               {'loss': 0.5676, 'grad_norm': 1.3263682126998901, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 13.52it/s]                                               {'loss': 0.5703, 'grad_norm': 1.0470596551895142, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 13.52it/s] 47%|████▋     | 14/30 [00:01<00:01, 13.18it/s]                                               {'loss': 0.5101, 'grad_norm': 3.3492236137390137, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.18it/s]                                               {'loss': 0.7258, 'grad_norm': 1.95169997215271, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.18it/s] 53%|█████▎    | 16/30 [00:01<00:01, 12.54it/s]                                               {'loss': 0.4995, 'grad_norm': 1.0426198244094849, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 12.54it/s]                                               {'loss': 0.522, 'grad_norm': 1.1135799884796143, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.54it/s] 60%|██████    | 18/30 [00:01<00:00, 13.66it/s]                                               {'loss': 0.6406, 'grad_norm': 3.1356899738311768, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.66it/s]                                               {'loss': 0.491, 'grad_norm': 1.4664562940597534, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.66it/s] 67%|██████▋   | 20/30 [00:01<00:00, 12.42it/s]                                               {'loss': 0.4854, 'grad_norm': 0.8928796052932739, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 12.42it/s]                                               {'loss': 0.48, 'grad_norm': 1.2154090404510498, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 12.42it/s] 73%|███████▎  | 22/30 [00:01<00:00, 11.87it/s]                                               {'loss': 0.4772, 'grad_norm': 0.9483134150505066, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 11.87it/s]                                               {'loss': 0.4928, 'grad_norm': 1.4293447732925415, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 11.87it/s] 80%|████████  | 24/30 [00:01<00:00, 12.58it/s]                                               {'loss': 0.3199, 'grad_norm': 1.4947547912597656, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 12.58it/s]                                               {'loss': 0.5093, 'grad_norm': 1.6202176809310913, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 12.58it/s] 87%|████████▋ | 26/30 [00:02<00:00, 12.20it/s]                                               {'loss': 0.2878, 'grad_norm': 1.2940934896469116, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 12.20it/s]                                               {'loss': 0.5443, 'grad_norm': 2.7534096240997314, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 12.20it/s] 93%|█████████▎| 28/30 [00:02<00:00, 12.19it/s]                                               {'loss': 0.3818, 'grad_norm': 1.0189546346664429, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.19it/s]                                               {'loss': 0.5601, 'grad_norm': 2.265190362930298, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.19it/s]100%|██████████| 30/30 [00:02<00:00, 12.92it/s]                                               {'loss': 0.1646, 'grad_norm': 1.7948462963104248, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.92it/s]                                               {'train_runtime': 2.4637, 'train_samples_per_second': 172.505, 'train_steps_per_second': 12.177, 'train_loss': 0.5599829331040382, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.92it/s]100%|██████████| 30/30 [00:02<00:00, 12.18it/s]
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.75it/s]                                              {'loss': 0.728, 'grad_norm': 1.5687708854675293, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.75it/s]  7%|▋         | 2/30 [00:00<00:06,  4.63it/s]                                              {'loss': 0.6508, 'grad_norm': 0.5991576313972473, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.63it/s] 10%|█         | 3/30 [00:00<00:05,  4.96it/s]                                              {'loss': 0.7102, 'grad_norm': 0.5392770767211914, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.96it/s] 13%|█▎        | 4/30 [00:00<00:05,  5.03it/s]                                              {'loss': 0.7139, 'grad_norm': 1.295094609260559, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  5.03it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.34it/s]                                              {'loss': 0.6669, 'grad_norm': 1.0927749872207642, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.34it/s] 20%|██        | 6/30 [00:02<00:11,  2.06it/s]                                              {'loss': 0.6597, 'grad_norm': 1.718043327331543, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:11,  2.06it/s] 23%|██▎       | 7/30 [00:02<00:09,  2.38it/s]                                              {'loss': 0.6801, 'grad_norm': 0.6004325747489929, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:09,  2.38it/s] 27%|██▋       | 8/30 [00:02<00:08,  2.66it/s]                                              {'loss': 0.6429, 'grad_norm': 0.33513006567955017, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:08,  2.66it/s] 30%|███       | 9/30 [00:02<00:07,  2.83it/s]                                              {'loss': 0.6357, 'grad_norm': 0.5589000582695007, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:07,  2.83it/s] 33%|███▎      | 10/30 [00:03<00:06,  3.04it/s]                                               {'loss': 0.7243, 'grad_norm': 0.5984925627708435, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:06,  3.04it/s] 37%|███▋      | 11/30 [00:03<00:05,  3.40it/s]                                               {'loss': 0.7025, 'grad_norm': 0.43319275975227356, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:05,  3.40it/s] 40%|████      | 12/30 [00:03<00:04,  4.14it/s]                                               {'loss': 0.8683, 'grad_norm': 1.6389000415802002, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:04,  4.14it/s] 43%|████▎     | 13/30 [00:03<00:03,  4.35it/s]                                               {'loss': 0.7477, 'grad_norm': 0.740246593952179, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  4.35it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.40it/s]                                               {'loss': 0.7203, 'grad_norm': 0.5919838547706604, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.40it/s] 50%|█████     | 15/30 [00:04<00:03,  4.57it/s]                                               {'loss': 0.5072, 'grad_norm': 0.7886082530021667, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:03,  4.57it/s] 53%|█████▎    | 16/30 [00:04<00:03,  4.45it/s]                                               {'loss': 0.6855, 'grad_norm': 0.5819040536880493, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:03,  4.45it/s] 57%|█████▋    | 17/30 [00:04<00:02,  4.59it/s]                                               {'loss': 0.7306, 'grad_norm': 0.6287625432014465, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:02,  4.59it/s] 60%|██████    | 18/30 [00:04<00:02,  5.17it/s]                                               {'loss': 0.6202, 'grad_norm': 0.5574087500572205, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  5.17it/s] 63%|██████▎   | 19/30 [00:04<00:02,  5.24it/s]                                               {'loss': 0.6915, 'grad_norm': 0.8160592317581177, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  5.24it/s] 67%|██████▋   | 20/30 [00:05<00:01,  5.17it/s]                                               {'loss': 0.6642, 'grad_norm': 0.27616965770721436, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:01,  5.17it/s] 70%|███████   | 21/30 [00:05<00:01,  4.98it/s]                                               {'loss': 0.6713, 'grad_norm': 0.3347358703613281, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:01,  4.98it/s] 73%|███████▎  | 22/30 [00:05<00:01,  5.30it/s]                                               {'loss': 0.6885, 'grad_norm': 0.42802026867866516, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:01,  5.30it/s] 77%|███████▋  | 23/30 [00:05<00:01,  5.48it/s]                                               {'loss': 0.6349, 'grad_norm': 0.528410017490387, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  5.48it/s]                                               {'loss': 0.6338, 'grad_norm': 0.5749024748802185, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.48it/s] 83%|████████▎ | 25/30 [00:05<00:00,  6.75it/s]                                               {'loss': 0.6858, 'grad_norm': 0.3755495548248291, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  6.75it/s] 87%|████████▋ | 26/30 [00:05<00:00,  7.18it/s]                                               {'loss': 0.6484, 'grad_norm': 0.3433077037334442, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  7.18it/s] 90%|█████████ | 27/30 [00:06<00:00,  6.31it/s]                                               {'loss': 0.6755, 'grad_norm': 0.38777151703834534, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  6.31it/s] 93%|█████████▎| 28/30 [00:06<00:00,  5.75it/s]                                               {'loss': 0.6523, 'grad_norm': 0.3481057584285736, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  5.75it/s] 97%|█████████▋| 29/30 [00:06<00:00,  5.48it/s]                                               {'loss': 0.6281, 'grad_norm': 0.4764474034309387, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  5.48it/s]                                               {'loss': 0.6772, 'grad_norm': 0.8967584371566772, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.48it/s]                                               {'train_runtime': 6.7953, 'train_samples_per_second': 62.543, 'train_steps_per_second': 4.415, 'train_loss': 0.6782120784123739, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.48it/s]100%|██████████| 30/30 [00:06<00:00,  4.42it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.58it/s]                                              {'loss': 0.801, 'grad_norm': 1.7200002670288086, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.58it/s]  7%|▋         | 2/30 [00:00<00:05,  5.33it/s]                                              {'loss': 0.445, 'grad_norm': 1.3365564346313477, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.33it/s] 10%|█         | 3/30 [00:00<00:04,  5.74it/s]                                              {'loss': 0.7425, 'grad_norm': 1.7900270223617554, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  5.74it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.23it/s]                                              {'loss': 0.7142, 'grad_norm': 1.6390033960342407, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.23it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.00it/s]                                              {'loss': 0.6978, 'grad_norm': 2.306180953979492, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.00it/s]                                              {'loss': 1.1579, 'grad_norm': 3.280148506164551, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.00it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.24it/s]                                              {'loss': 0.331, 'grad_norm': 1.5483721494674683, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.24it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.92it/s]                                              {'loss': 0.5289, 'grad_norm': 3.311842441558838, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.92it/s] 30%|███       | 9/30 [00:01<00:02,  7.41it/s]                                              {'loss': 0.674, 'grad_norm': 2.1366658210754395, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.41it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.16it/s]                                               {'loss': 0.5907, 'grad_norm': 1.797911286354065, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.16it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.25it/s]                                               {'loss': 0.6836, 'grad_norm': 1.342625617980957, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.25it/s]                                               {'loss': 0.3698, 'grad_norm': 1.479489803314209, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.25it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.54it/s]                                               {'loss': 0.4403, 'grad_norm': 1.1270948648452759, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.54it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.31it/s]                                               {'loss': 0.5403, 'grad_norm': 0.9885008931159973, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.31it/s] 50%|█████     | 15/30 [00:02<00:02,  7.03it/s]                                               {'loss': 0.5448, 'grad_norm': 1.3092472553253174, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.03it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.07it/s]                                               {'loss': 0.4711, 'grad_norm': 1.013239860534668, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.07it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.49it/s]                                               {'loss': 0.5011, 'grad_norm': 1.6854859590530396, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.49it/s]                                               {'loss': 0.6419, 'grad_norm': 2.2902631759643555, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.49it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.80it/s]                                               {'loss': 0.5528, 'grad_norm': 1.4634311199188232, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.80it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.93it/s]                                               {'loss': 0.3662, 'grad_norm': 1.4631733894348145, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.93it/s] 70%|███████   | 21/30 [00:02<00:01,  8.29it/s]                                               {'loss': 0.5285, 'grad_norm': 1.3583037853240967, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.29it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.97it/s]                                               {'loss': 0.4865, 'grad_norm': 1.159371018409729, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.97it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.63it/s]                                               {'loss': 0.6765, 'grad_norm': 2.0202476978302, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.63it/s]                                               {'loss': 0.2193, 'grad_norm': 1.7488080263137817, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.63it/s] 83%|████████▎ | 25/30 [00:03<00:00,  6.55it/s]                                               {'loss': 0.3377, 'grad_norm': 1.6948871612548828, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  6.55it/s] 87%|████████▋ | 26/30 [00:03<00:00,  5.99it/s]                                               {'loss': 0.3929, 'grad_norm': 0.9918201565742493, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  5.99it/s] 90%|█████████ | 27/30 [00:03<00:00,  5.67it/s]                                               {'loss': 0.5443, 'grad_norm': 2.4332902431488037, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  5.67it/s] 93%|█████████▎| 28/30 [00:04<00:00,  5.30it/s]                                               {'loss': 0.4243, 'grad_norm': 2.0508573055267334, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  5.30it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.44it/s]                                               {'loss': 0.4583, 'grad_norm': 1.4455407857894897, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.44it/s]                                               {'loss': 0.1937, 'grad_norm': 2.190216541290283, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.44it/s]                                               {'train_runtime': 4.443, 'train_samples_per_second': 95.656, 'train_steps_per_second': 6.752, 'train_loss': 0.535233757396539, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.44it/s]100%|██████████| 30/30 [00:04<00:00,  6.77it/s]
CLIENT:10
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.24it/s]                                              {'loss': 0.6247, 'grad_norm': 0.7617903351783752, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.24it/s]  7%|▋         | 2/30 [00:00<00:09,  3.04it/s]                                              {'loss': 0.7093, 'grad_norm': 0.8737429976463318, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:09,  3.04it/s] 10%|█         | 3/30 [00:00<00:08,  3.06it/s]                                              {'loss': 0.6972, 'grad_norm': 0.4261673092842102, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:08,  3.06it/s] 13%|█▎        | 4/30 [00:01<00:08,  3.00it/s]                                              {'loss': 0.6825, 'grad_norm': 0.7828695178031921, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  3.00it/s] 17%|█▋        | 5/30 [00:01<00:08,  2.98it/s]                                              {'loss': 0.7017, 'grad_norm': 0.418923944234848, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:08,  2.98it/s] 20%|██        | 6/30 [00:02<00:08,  2.91it/s]                                              {'loss': 0.5663, 'grad_norm': 0.9746158123016357, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:08,  2.91it/s] 23%|██▎       | 7/30 [00:02<00:07,  3.04it/s]                                              {'loss': 0.6477, 'grad_norm': 0.3065531551837921, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:07,  3.04it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.15it/s]                                              {'loss': 0.6868, 'grad_norm': 0.3402504622936249, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.15it/s] 30%|███       | 9/30 [00:02<00:06,  3.27it/s]                                              {'loss': 0.6131, 'grad_norm': 0.390535831451416, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:06,  3.27it/s] 33%|███▎      | 10/30 [00:03<00:06,  3.30it/s]                                               {'loss': 0.5845, 'grad_norm': 0.7528954744338989, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:06,  3.30it/s] 37%|███▋      | 11/30 [00:03<00:05,  3.36it/s]                                               {'loss': 0.6605, 'grad_norm': 0.392360657453537, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:05,  3.36it/s]                                               {'loss': 0.5179, 'grad_norm': 0.4586663246154785, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:05,  3.36it/s] 43%|████▎     | 13/30 [00:03<00:04,  4.03it/s]                                               {'loss': 0.5761, 'grad_norm': 0.5962614417076111, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:04,  4.03it/s] 47%|████▋     | 14/30 [00:04<00:04,  3.65it/s]                                               {'loss': 0.4968, 'grad_norm': 0.5388765335083008, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:04,  3.65it/s] 50%|█████     | 15/30 [00:04<00:04,  3.50it/s]                                               {'loss': 0.9031, 'grad_norm': 2.4908695220947266, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:04,  3.50it/s] 53%|█████▎    | 16/30 [00:04<00:03,  3.64it/s]                                               {'loss': 0.6757, 'grad_norm': 1.3164271116256714, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:03,  3.64it/s] 57%|█████▋    | 17/30 [00:05<00:03,  3.79it/s]                                               {'loss': 0.4746, 'grad_norm': 1.1336143016815186, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:05<00:03,  3.79it/s] 60%|██████    | 18/30 [00:05<00:02,  4.42it/s]                                               {'loss': 0.6704, 'grad_norm': 0.995040237903595, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:02,  4.42it/s] 63%|██████▎   | 19/30 [00:05<00:02,  4.06it/s]                                               {'loss': 0.6174, 'grad_norm': 0.6871634721755981, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  4.06it/s] 67%|██████▋   | 20/30 [00:05<00:02,  3.85it/s]                                               {'loss': 0.5894, 'grad_norm': 0.4210423231124878, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  3.85it/s] 70%|███████   | 21/30 [00:05<00:02,  3.83it/s]                                               {'loss': 0.5416, 'grad_norm': 0.7175443172454834, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:02,  3.83it/s] 73%|███████▎  | 22/30 [00:06<00:02,  3.76it/s]                                               {'loss': 0.5247, 'grad_norm': 0.9878679513931274, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:06<00:02,  3.76it/s] 77%|███████▋  | 23/30 [00:06<00:01,  3.73it/s]                                               {'loss': 0.6231, 'grad_norm': 0.8396021127700806, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  3.73it/s] 80%|████████  | 24/30 [00:06<00:01,  4.58it/s]                                               {'loss': 0.5512, 'grad_norm': 1.7455594539642334, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  4.58it/s] 83%|████████▎ | 25/30 [00:06<00:01,  4.04it/s]                                               {'loss': 0.6915, 'grad_norm': 0.8789846301078796, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:01,  4.04it/s] 87%|████████▋ | 26/30 [00:07<00:01,  3.91it/s]                                               {'loss': 0.538, 'grad_norm': 1.1179851293563843, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:07<00:01,  3.91it/s] 90%|█████████ | 27/30 [00:07<00:00,  3.96it/s]                                               {'loss': 0.5691, 'grad_norm': 0.899479866027832, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:07<00:00,  3.96it/s] 93%|█████████▎| 28/30 [00:07<00:00,  3.89it/s]                                               {'loss': 0.5758, 'grad_norm': 0.6533449292182922, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  3.89it/s] 97%|█████████▋| 29/30 [00:08<00:00,  3.86it/s]                                               {'loss': 0.5928, 'grad_norm': 1.1660332679748535, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:08<00:00,  3.86it/s]100%|██████████| 30/30 [00:08<00:00,  4.55it/s]                                               {'loss': 0.6001, 'grad_norm': 1.3306183815002441, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.55it/s]                                               {'train_runtime': 8.2289, 'train_samples_per_second': 51.647, 'train_steps_per_second': 3.646, 'train_loss': 0.6167884310086568, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.55it/s]100%|██████████| 30/30 [00:08<00:00,  3.65it/s]
CLIENT:44
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  7.81it/s]                                              {'loss': 0.8101, 'grad_norm': 2.0797410011291504, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  7.81it/s]                                              {'loss': 0.4341, 'grad_norm': 1.5537707805633545, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.81it/s] 10%|█         | 3/30 [00:00<00:02, 10.18it/s]                                              {'loss': 0.2777, 'grad_norm': 1.2389761209487915, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.18it/s]                                              {'loss': 0.0799, 'grad_norm': 0.8994500637054443, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.18it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.39it/s]                                              {'loss': 0.0343, 'grad_norm': 0.6888459324836731, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.39it/s]                                              {'loss': 0.0054, 'grad_norm': 0.1881033182144165, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.39it/s] 23%|██▎       | 7/30 [00:00<00:01, 13.51it/s]                                              {'loss': 0.2371, 'grad_norm': 1.7703288793563843, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 13.51it/s]                                              {'loss': 0.0038, 'grad_norm': 0.051673416048288345, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.51it/s] 30%|███       | 9/30 [00:00<00:01, 13.21it/s]                                              {'loss': 0.3507, 'grad_norm': 1.067869782447815, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.21it/s]                                              {'loss': 0.6441, 'grad_norm': 3.7432408332824707, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.21it/s] 37%|███▋      | 11/30 [00:00<00:01, 12.82it/s]                                               {'loss': 0.3487, 'grad_norm': 1.541934609413147, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.82it/s]                                               {'loss': 0.0067, 'grad_norm': 0.12850748002529144, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 12.82it/s] 43%|████▎     | 13/30 [00:01<00:01, 12.29it/s]                                               {'loss': 0.3053, 'grad_norm': 1.035978078842163, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 12.29it/s]                                               {'loss': 0.0122, 'grad_norm': 0.14329996705055237, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 12.29it/s] 50%|█████     | 15/30 [00:01<00:01, 12.09it/s]                                               {'loss': 0.6728, 'grad_norm': 1.828737735748291, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 12.09it/s]                                               {'loss': 0.0257, 'grad_norm': 0.3044010102748871, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 12.09it/s] 57%|█████▋    | 17/30 [00:01<00:01, 11.58it/s]                                               {'loss': 0.4777, 'grad_norm': 1.2157648801803589, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.58it/s]                                               {'loss': 0.0422, 'grad_norm': 0.43059679865837097, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 11.58it/s] 63%|██████▎   | 19/30 [00:01<00:00, 12.65it/s]                                               {'loss': 0.4123, 'grad_norm': 1.3932888507843018, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 12.65it/s]                                               {'loss': 0.0544, 'grad_norm': 0.4550749361515045, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 12.65it/s] 70%|███████   | 21/30 [00:01<00:00, 12.09it/s]                                               {'loss': 0.0639, 'grad_norm': 0.5444026589393616, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 12.09it/s]                                               {'loss': 0.5422, 'grad_norm': 1.0649148225784302, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 12.09it/s] 77%|███████▋  | 23/30 [00:01<00:00, 11.22it/s]                                               {'loss': 0.0677, 'grad_norm': 0.45390233397483826, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 11.22it/s]                                               {'loss': 0.064, 'grad_norm': 0.4814993441104889, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 11.22it/s] 83%|████████▎ | 25/30 [00:02<00:00, 12.28it/s]                                               {'loss': 0.0505, 'grad_norm': 0.4801408052444458, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.28it/s]                                               {'loss': 0.2981, 'grad_norm': 0.8511643409729004, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 12.28it/s] 90%|█████████ | 27/30 [00:02<00:00, 12.32it/s]                                               {'loss': 0.225, 'grad_norm': 0.7169384956359863, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 12.32it/s]                                               {'loss': 0.208, 'grad_norm': 0.7835479974746704, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.32it/s] 97%|█████████▋| 29/30 [00:02<00:00, 11.99it/s]                                               {'loss': 0.2516, 'grad_norm': 2.3735809326171875, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 11.99it/s]                                               {'loss': 1.0048, 'grad_norm': 8.623015403747559, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 11.99it/s]                                               {'train_runtime': 2.6264, 'train_samples_per_second': 161.82, 'train_steps_per_second': 11.423, 'train_loss': 0.2670333251357079, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 11.99it/s]100%|██████████| 30/30 [00:02<00:00, 11.44it/s]
CLIENT:34
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.6976, 'grad_norm': 1.8872817754745483, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 10.90it/s]  7%|▋         | 2/30 [00:00<00:02, 11.35it/s]                                              {'loss': 0.4704, 'grad_norm': 1.1891664266586304, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 11.35it/s]                                              {'loss': 0.3364, 'grad_norm': 1.081729769706726, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.35it/s] 13%|█▎        | 4/30 [00:00<00:02, 11.48it/s]                                              {'loss': 0.6691, 'grad_norm': 3.2323203086853027, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.48it/s]                                              {'loss': 0.5212, 'grad_norm': 2.444643497467041, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.48it/s] 20%|██        | 6/30 [00:01<00:06,  3.94it/s]                                              {'loss': 1.4001, 'grad_norm': 5.24872350692749, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.94it/s]                                              {'loss': 0.3404, 'grad_norm': 2.475262403488159, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  3.94it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.35it/s]                                              {'loss': 0.3428, 'grad_norm': 2.6032660007476807, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.35it/s]                                              {'loss': 0.7567, 'grad_norm': 3.4664289951324463, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.35it/s] 33%|███▎      | 10/30 [00:01<00:02,  6.84it/s]                                               {'loss': 0.3765, 'grad_norm': 1.1518162488937378, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  6.84it/s]                                               {'loss': 0.5947, 'grad_norm': 1.5971317291259766, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.84it/s] 40%|████      | 12/30 [00:01<00:02,  8.65it/s]                                               {'loss': 0.4778, 'grad_norm': 2.414663314819336, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.65it/s]                                               {'loss': 0.288, 'grad_norm': 1.8459569215774536, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  8.65it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.51it/s]                                               {'loss': 0.4265, 'grad_norm': 1.0742692947387695, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  9.51it/s]                                               {'loss': 0.7621, 'grad_norm': 2.463447093963623, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  9.51it/s] 53%|█████▎    | 16/30 [00:02<00:01, 10.30it/s]                                               {'loss': 0.4641, 'grad_norm': 1.6636126041412354, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01, 10.30it/s]                                               {'loss': 0.4714, 'grad_norm': 0.9968653321266174, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01, 10.30it/s] 60%|██████    | 18/30 [00:02<00:00, 12.02it/s]                                               {'loss': 0.7952, 'grad_norm': 4.5995306968688965, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:00, 12.02it/s]                                               {'loss': 0.3451, 'grad_norm': 1.1033580303192139, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 12.02it/s] 67%|██████▋   | 20/30 [00:02<00:00, 11.55it/s]                                               {'loss': 0.4541, 'grad_norm': 0.9955355525016785, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 11.55it/s]                                               {'loss': 0.376, 'grad_norm': 1.2353464365005493, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.55it/s] 73%|███████▎  | 22/30 [00:02<00:00, 11.17it/s]                                               {'loss': 0.5357, 'grad_norm': 1.6340326070785522, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.17it/s]                                               {'loss': 0.3557, 'grad_norm': 5.09557580947876, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 11.17it/s] 80%|████████  | 24/30 [00:02<00:00, 11.32it/s]                                               {'loss': 0.3776, 'grad_norm': 1.5856326818466187, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 11.32it/s]                                               {'loss': 0.6758, 'grad_norm': 2.9394352436065674, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 11.32it/s] 87%|████████▋ | 26/30 [00:02<00:00, 11.24it/s]                                               {'loss': 0.4825, 'grad_norm': 2.9503555297851562, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 11.24it/s]                                               {'loss': 0.2987, 'grad_norm': 0.973383367061615, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 11.24it/s] 93%|█████████▎| 28/30 [00:03<00:00, 11.35it/s]                                               {'loss': 0.262, 'grad_norm': 1.8160971403121948, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 11.35it/s]                                               {'loss': 0.6508, 'grad_norm': 4.417283535003662, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 11.35it/s]100%|██████████| 30/30 [00:03<00:00, 12.95it/s]                                               {'loss': 0.1979, 'grad_norm': 5.730652332305908, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.95it/s]                                               {'train_runtime': 3.2069, 'train_samples_per_second': 132.528, 'train_steps_per_second': 9.355, 'train_loss': 0.5067689806222916, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.95it/s]100%|██████████| 30/30 [00:03<00:00,  9.36it/s]
CLIENT:24
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  7.33it/s]                                              {'loss': 0.8112, 'grad_norm': 1.9536423683166504, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  7.33it/s]  7%|▋         | 2/30 [00:00<00:03,  8.10it/s]                                              {'loss': 0.4449, 'grad_norm': 1.3418587446212769, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.10it/s]                                              {'loss': 0.4442, 'grad_norm': 0.6565262675285339, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.10it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.56it/s]                                              {'loss': 0.1804, 'grad_norm': 0.6060760617256165, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  9.56it/s]                                              {'loss': 0.68, 'grad_norm': 2.9865403175354004, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  9.56it/s] 20%|██        | 6/30 [00:00<00:02, 10.16it/s]                                              {'loss': 2.1656, 'grad_norm': 9.700479507446289, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 10.16it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.48it/s]                                              {'loss': 0.08, 'grad_norm': 0.9552454352378845, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  9.48it/s]                                              {'loss': 0.8118, 'grad_norm': 5.071131706237793, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  9.48it/s] 30%|███       | 9/30 [00:00<00:02, 10.19it/s]                                              {'loss': 0.3112, 'grad_norm': 1.2919496297836304, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:02, 10.19it/s]                                              {'loss': 0.2306, 'grad_norm': 2.0927929878234863, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:01, 10.19it/s] 37%|███▋      | 11/30 [00:01<00:01,  9.77it/s]                                               {'loss': 0.7505, 'grad_norm': 4.658070087432861, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01,  9.77it/s]                                               {'loss': 0.083, 'grad_norm': 1.4800944328308105, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01,  9.77it/s] 43%|████▎     | 13/30 [00:01<00:01, 10.35it/s]                                               {'loss': 0.3719, 'grad_norm': 1.4679503440856934, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 10.35it/s]                                               {'loss': 0.1871, 'grad_norm': 1.3503986597061157, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 10.35it/s] 50%|█████     | 15/30 [00:01<00:01,  9.10it/s]                                               {'loss': 0.6114, 'grad_norm': 2.8137729167938232, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  9.10it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.00it/s]                                               {'loss': 0.5178, 'grad_norm': 2.611682891845703, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  9.00it/s] 57%|█████▋    | 17/30 [00:01<00:01,  8.92it/s]                                               {'loss': 0.3947, 'grad_norm': 1.2189836502075195, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01,  8.92it/s]                                               {'loss': 0.6672, 'grad_norm': 4.654131889343262, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01,  8.92it/s] 63%|██████▎   | 19/30 [00:01<00:01, 10.59it/s]                                               {'loss': 0.2552, 'grad_norm': 1.0667415857315063, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:01, 10.59it/s]                                               {'loss': 0.2972, 'grad_norm': 1.542838454246521, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 10.59it/s] 70%|███████   | 21/30 [00:02<00:00, 10.07it/s]                                               {'loss': 0.5213, 'grad_norm': 2.1272997856140137, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 10.07it/s]                                               {'loss': 0.3548, 'grad_norm': 1.4932799339294434, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 10.07it/s] 77%|███████▋  | 23/30 [00:02<00:00,  8.37it/s]                                               {'loss': 0.4377, 'grad_norm': 1.6095726490020752, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  8.37it/s]                                               {'loss': 0.5371, 'grad_norm': 2.474130868911743, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  8.37it/s] 83%|████████▎ | 25/30 [00:02<00:00,  8.75it/s]                                               {'loss': 0.2881, 'grad_norm': 1.2202578783035278, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00,  8.75it/s] 87%|████████▋ | 26/30 [00:02<00:00,  8.69it/s]                                               {'loss': 0.4245, 'grad_norm': 2.324629783630371, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00,  8.69it/s]                                               {'loss': 0.2386, 'grad_norm': 1.2154440879821777, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00,  8.69it/s] 93%|█████████▎| 28/30 [00:03<00:00,  8.42it/s]                                               {'loss': 0.4116, 'grad_norm': 1.8375170230865479, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  8.42it/s] 97%|█████████▋| 29/30 [00:03<00:00,  8.23it/s]                                               {'loss': 0.4871, 'grad_norm': 2.7283573150634766, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  8.23it/s]                                               {'loss': 0.1277, 'grad_norm': 1.7439011335372925, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.23it/s]                                               {'train_runtime': 3.3561, 'train_samples_per_second': 126.636, 'train_steps_per_second': 8.939, 'train_loss': 0.47080948303143183, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.23it/s]100%|██████████| 30/30 [00:03<00:00,  8.95it/s]
CLIENT:98
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:10,  2.72it/s]                                              {'loss': 0.6974, 'grad_norm': 1.8091542720794678, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:10,  2.72it/s]  7%|▋         | 2/30 [00:00<00:10,  2.79it/s]                                              {'loss': 0.5425, 'grad_norm': 4.475841999053955, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:10,  2.79it/s] 10%|█         | 3/30 [00:01<00:09,  2.80it/s]                                              {'loss': 0.413, 'grad_norm': 0.6803619861602783, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:09,  2.80it/s] 13%|█▎        | 4/30 [00:01<00:09,  2.84it/s]                                              {'loss': 0.2786, 'grad_norm': 0.7892727255821228, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:09,  2.84it/s] 17%|█▋        | 5/30 [00:01<00:08,  2.88it/s]                                              {'loss': 0.3603, 'grad_norm': 0.9119452238082886, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:08,  2.88it/s] 20%|██        | 6/30 [00:01<00:06,  3.66it/s]                                              {'loss': 1.4787, 'grad_norm': 4.973414421081543, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.66it/s] 23%|██▎       | 7/30 [00:02<00:06,  3.39it/s]                                              {'loss': 0.2317, 'grad_norm': 0.7402127981185913, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:06,  3.39it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.25it/s]                                              {'loss': 0.6166, 'grad_norm': 2.5771992206573486, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.25it/s] 30%|███       | 9/30 [00:02<00:06,  3.10it/s]                                              {'loss': 0.3619, 'grad_norm': 0.6782765984535217, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:06,  3.10it/s] 33%|███▎      | 10/30 [00:03<00:06,  3.09it/s]                                               {'loss': 0.3158, 'grad_norm': 0.6486645936965942, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:06,  3.09it/s] 37%|███▋      | 11/30 [00:03<00:06,  3.00it/s]                                               {'loss': 0.3906, 'grad_norm': 0.9080301523208618, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:06,  3.00it/s] 40%|████      | 12/30 [00:03<00:04,  3.67it/s]                                               {'loss': 0.3979, 'grad_norm': 8.330339431762695, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:04,  3.67it/s] 43%|████▎     | 13/30 [00:04<00:04,  3.46it/s]                                               {'loss': 0.1454, 'grad_norm': 1.5619525909423828, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:04<00:04,  3.46it/s] 47%|████▋     | 14/30 [00:04<00:04,  3.28it/s]                                               {'loss': 0.4782, 'grad_norm': 1.3960418701171875, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:04,  3.28it/s] 50%|█████     | 15/30 [00:04<00:04,  3.16it/s]                                               {'loss': 0.4477, 'grad_norm': 1.1527249813079834, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:04,  3.16it/s] 53%|█████▎    | 16/30 [00:05<00:04,  3.11it/s]                                               {'loss': 0.3882, 'grad_norm': 1.0785900354385376, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:05<00:04,  3.11it/s] 57%|█████▋    | 17/30 [00:05<00:03,  3.36it/s]                                               {'loss': 0.2792, 'grad_norm': 1.2382289171218872, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:05<00:03,  3.36it/s]                                               {'loss': 0.7085, 'grad_norm': 2.933345079421997, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:03,  3.36it/s] 63%|██████▎   | 19/30 [00:05<00:02,  4.47it/s]                                               {'loss': 0.3591, 'grad_norm': 1.5137616395950317, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  4.47it/s] 67%|██████▋   | 20/30 [00:05<00:02,  4.52it/s]                                               {'loss': 0.3185, 'grad_norm': 1.0830283164978027, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  4.52it/s] 70%|███████   | 21/30 [00:06<00:01,  4.55it/s]                                               {'loss': 0.3174, 'grad_norm': 1.1261898279190063, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:06<00:01,  4.55it/s] 73%|███████▎  | 22/30 [00:06<00:01,  4.53it/s]                                               {'loss': 0.3748, 'grad_norm': 0.8491796851158142, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:06<00:01,  4.53it/s] 77%|███████▋  | 23/30 [00:06<00:01,  4.84it/s]                                               {'loss': 0.3146, 'grad_norm': 1.023584246635437, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  4.84it/s]                                               {'loss': 0.4552, 'grad_norm': 2.317422389984131, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  4.84it/s] 83%|████████▎ | 25/30 [00:06<00:00,  6.31it/s]                                               {'loss': 0.1908, 'grad_norm': 0.8872714638710022, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:00,  6.31it/s] 87%|████████▋ | 26/30 [00:06<00:00,  6.26it/s]                                               {'loss': 0.4323, 'grad_norm': 1.5762683153152466, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:00,  6.26it/s] 90%|█████████ | 27/30 [00:06<00:00,  5.87it/s]                                               {'loss': 0.4582, 'grad_norm': 2.052410125732422, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  5.87it/s] 93%|█████████▎| 28/30 [00:07<00:00,  5.93it/s]                                               {'loss': 0.1604, 'grad_norm': 1.3021138906478882, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  5.93it/s] 97%|█████████▋| 29/30 [00:07<00:00,  6.10it/s]                                               {'loss': 0.3004, 'grad_norm': 1.5323429107666016, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  6.10it/s]                                               {'loss': 0.5445, 'grad_norm': 3.999316930770874, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  6.10it/s]                                               {'train_runtime': 7.4837, 'train_samples_per_second': 56.79, 'train_steps_per_second': 4.009, 'train_loss': 0.4252867981791496, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  6.10it/s]100%|██████████| 30/30 [00:07<00:00,  4.01it/s]
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.17it/s]                                              {'loss': 0.5955, 'grad_norm': 1.3480437994003296, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.17it/s]  7%|▋         | 2/30 [00:00<00:05,  4.79it/s]                                              {'loss': 0.677, 'grad_norm': 0.7165555953979492, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  4.79it/s] 10%|█         | 3/30 [00:00<00:05,  4.96it/s]                                              {'loss': 0.6703, 'grad_norm': 0.5737361907958984, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.96it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.63it/s]                                              {'loss': 0.7062, 'grad_norm': 0.7277684807777405, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.63it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.32it/s]                                              {'loss': 0.6908, 'grad_norm': 0.847714900970459, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.32it/s] 20%|██        | 6/30 [00:01<00:07,  3.21it/s]                                              {'loss': 0.9902, 'grad_norm': 2.0665183067321777, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:07,  3.21it/s] 23%|██▎       | 7/30 [00:01<00:06,  3.54it/s]                                              {'loss': 0.6128, 'grad_norm': 0.61915522813797, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:06,  3.54it/s] 27%|██▋       | 8/30 [00:01<00:05,  4.13it/s]                                              {'loss': 0.7438, 'grad_norm': 0.8848623633384705, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  4.13it/s] 30%|███       | 9/30 [00:02<00:04,  4.21it/s]                                              {'loss': 0.6266, 'grad_norm': 0.6686041951179504, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.21it/s] 33%|███▎      | 10/30 [00:02<00:05,  3.99it/s]                                               {'loss': 0.6388, 'grad_norm': 0.5498677492141724, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  3.99it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.00it/s]                                               {'loss': 0.6827, 'grad_norm': 0.4098033905029297, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.00it/s]                                               {'loss': 0.6291, 'grad_norm': 1.4947925806045532, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:04,  4.00it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.42it/s]                                               {'loss': 0.6494, 'grad_norm': 0.6334790587425232, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.42it/s] 47%|████▋     | 14/30 [00:03<00:03,  5.19it/s]                                               {'loss': 0.6296, 'grad_norm': 0.7434658408164978, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  5.19it/s] 50%|█████     | 15/30 [00:03<00:02,  5.07it/s]                                               {'loss': 0.605, 'grad_norm': 0.8896011710166931, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.07it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.49it/s]                                               {'loss': 0.6443, 'grad_norm': 0.47129249572753906, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.49it/s] 57%|█████▋    | 17/30 [00:03<00:03,  4.32it/s]                                               {'loss': 0.62, 'grad_norm': 0.615181028842926, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  4.32it/s] 60%|██████    | 18/30 [00:03<00:02,  4.92it/s]                                               {'loss': 0.655, 'grad_norm': 0.9316023588180542, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.92it/s] 63%|██████▎   | 19/30 [00:04<00:02,  4.45it/s]                                               {'loss': 0.6127, 'grad_norm': 0.7751302719116211, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  4.45it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.44it/s]                                               {'loss': 0.6517, 'grad_norm': 0.5080711245536804, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.44it/s] 70%|███████   | 21/30 [00:04<00:02,  4.48it/s]                                               {'loss': 0.5939, 'grad_norm': 0.7941027879714966, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:02,  4.48it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.84it/s]                                               {'loss': 0.6366, 'grad_norm': 0.8664266467094421, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.84it/s] 77%|███████▋  | 23/30 [00:05<00:01,  5.12it/s]                                               {'loss': 0.5922, 'grad_norm': 1.0387382507324219, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  5.12it/s] 80%|████████  | 24/30 [00:05<00:01,  5.40it/s]                                               {'loss': 0.603, 'grad_norm': 0.9363211393356323, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.40it/s] 83%|████████▎ | 25/30 [00:05<00:01,  4.58it/s]                                               {'loss': 0.5081, 'grad_norm': 0.7627118825912476, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  4.58it/s] 87%|████████▋ | 26/30 [00:05<00:00,  4.39it/s]                                               {'loss': 0.5657, 'grad_norm': 0.8114558458328247, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  4.39it/s] 90%|█████████ | 27/30 [00:06<00:00,  4.14it/s]                                               {'loss': 0.5441, 'grad_norm': 1.1888344287872314, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  4.14it/s] 93%|█████████▎| 28/30 [00:06<00:00,  4.40it/s]                                               {'loss': 0.5779, 'grad_norm': 0.7412731051445007, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  4.40it/s] 97%|█████████▋| 29/30 [00:06<00:00,  4.54it/s]                                               {'loss': 0.5351, 'grad_norm': 1.059348702430725, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  4.54it/s]100%|██████████| 30/30 [00:06<00:00,  5.11it/s]                                               {'loss': 0.5505, 'grad_norm': 2.1149370670318604, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.11it/s]                                               {'train_runtime': 6.7658, 'train_samples_per_second': 62.815, 'train_steps_per_second': 4.434, 'train_loss': 0.6346123854319254, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.11it/s]100%|██████████| 30/30 [00:06<00:00,  4.43it/s]
CLIENT:95
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.28it/s]                                              {'loss': 0.6079, 'grad_norm': 0.9577114582061768, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.28it/s]  7%|▋         | 2/30 [00:00<00:05,  5.22it/s]                                              {'loss': 0.7172, 'grad_norm': 0.7817460298538208, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.22it/s] 10%|█         | 3/30 [00:00<00:04,  6.17it/s]                                              {'loss': 0.7306, 'grad_norm': 0.6465742588043213, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.17it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.60it/s]                                              {'loss': 0.665, 'grad_norm': 0.868378221988678, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.60it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.72it/s]                                              {'loss': 0.6667, 'grad_norm': 0.32862919569015503, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.72it/s]                                              {'loss': 0.6901, 'grad_norm': 0.7439845204353333, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.72it/s] 23%|██▎       | 7/30 [00:01<00:02,  7.84it/s]                                              {'loss': 0.6683, 'grad_norm': 0.4233814775943756, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:02,  7.84it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.47it/s]                                              {'loss': 0.6926, 'grad_norm': 0.3284696936607361, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.47it/s] 30%|███       | 9/30 [00:01<00:03,  6.57it/s]                                              {'loss': 0.6214, 'grad_norm': 0.352502703666687, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.57it/s] 33%|███▎      | 10/30 [00:01<00:02,  6.79it/s]                                               {'loss': 0.6582, 'grad_norm': 0.657261312007904, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  6.79it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.02it/s]                                               {'loss': 0.6825, 'grad_norm': 0.3132295310497284, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.02it/s]                                               {'loss': 0.4396, 'grad_norm': 0.49116209149360657, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.02it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.09it/s]                                               {'loss': 0.6848, 'grad_norm': 0.6835975050926208, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.09it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.81it/s]                                               {'loss': 0.6352, 'grad_norm': 0.42421576380729675, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.81it/s] 50%|█████     | 15/30 [00:02<00:02,  7.35it/s]                                               {'loss': 0.6692, 'grad_norm': 1.3745825290679932, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.35it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.01it/s]                                               {'loss': 0.6069, 'grad_norm': 0.6604905724525452, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.01it/s] 57%|█████▋    | 17/30 [00:02<00:01,  6.94it/s]                                               {'loss': 0.6252, 'grad_norm': 0.4362677037715912, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  6.94it/s]                                               {'loss': 0.6261, 'grad_norm': 0.5110424160957336, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  6.94it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.36it/s]                                               {'loss': 0.6159, 'grad_norm': 0.7752087116241455, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.36it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.16it/s]                                               {'loss': 0.6365, 'grad_norm': 0.2829420864582062, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.16it/s] 70%|███████   | 21/30 [00:02<00:01,  6.82it/s]                                               {'loss': 0.6187, 'grad_norm': 0.5316264629364014, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  6.82it/s] 73%|███████▎  | 22/30 [00:03<00:01,  5.96it/s]                                               {'loss': 0.6211, 'grad_norm': 0.6721460223197937, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  5.96it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.64it/s]                                               {'loss': 0.6411, 'grad_norm': 0.5628403425216675, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.64it/s]                                               {'loss': 0.5809, 'grad_norm': 1.7085516452789307, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:01,  5.64it/s] 83%|████████▎ | 25/30 [00:03<00:00,  6.07it/s]                                               {'loss': 0.7439, 'grad_norm': 0.9859544634819031, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  6.07it/s] 87%|████████▋ | 26/30 [00:03<00:00,  6.33it/s]                                               {'loss': 0.57, 'grad_norm': 0.8744112253189087, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  6.33it/s] 90%|█████████ | 27/30 [00:04<00:00,  6.34it/s]                                               {'loss': 0.6026, 'grad_norm': 0.7245181798934937, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  6.34it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.83it/s]                                               {'loss': 0.6119, 'grad_norm': 0.7574481964111328, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.83it/s] 97%|█████████▋| 29/30 [00:04<00:00,  6.95it/s]                                               {'loss': 0.5534, 'grad_norm': 0.9665496349334717, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  6.95it/s]100%|██████████| 30/30 [00:04<00:00,  7.27it/s]                                               {'loss': 0.5024, 'grad_norm': 1.7191781997680664, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.27it/s]                                               {'train_runtime': 4.649, 'train_samples_per_second': 91.418, 'train_steps_per_second': 6.453, 'train_loss': 0.6328674872716268, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.27it/s]100%|██████████| 30/30 [00:04<00:00,  6.45it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  5%|▍         | 3/66 [00:00<00:02, 24.21it/s]  9%|▉         | 6/66 [00:00<00:03, 18.53it/s] 12%|█▏        | 8/66 [00:00<00:03, 16.63it/s] 15%|█▌        | 10/66 [00:00<00:03, 17.46it/s] 18%|█▊        | 12/66 [00:00<00:03, 16.89it/s] 21%|██        | 14/66 [00:00<00:03, 15.94it/s] 24%|██▍       | 16/66 [00:00<00:02, 16.91it/s] 29%|██▉       | 19/66 [00:01<00:02, 17.26it/s] 32%|███▏      | 21/66 [00:01<00:02, 15.81it/s] 35%|███▍      | 23/66 [00:01<00:02, 14.38it/s] 38%|███▊      | 25/66 [00:01<00:02, 14.62it/s] 41%|████      | 27/66 [00:01<00:02, 15.11it/s] 44%|████▍     | 29/66 [00:01<00:02, 14.97it/s] 47%|████▋     | 31/66 [00:01<00:02, 14.09it/s] 50%|█████     | 33/66 [00:02<00:02, 14.60it/s] 53%|█████▎    | 35/66 [00:02<00:02, 15.22it/s] 56%|█████▌    | 37/66 [00:02<00:01, 15.63it/s] 59%|█████▉    | 39/66 [00:02<00:01, 15.76it/s] 62%|██████▏   | 41/66 [00:02<00:01, 16.25it/s] 65%|██████▌   | 43/66 [00:02<00:01, 16.51it/s] 68%|██████▊   | 45/66 [00:02<00:01, 17.21it/s] 71%|███████   | 47/66 [00:02<00:01, 17.02it/s] 74%|███████▍  | 49/66 [00:03<00:01, 16.42it/s] 77%|███████▋  | 51/66 [00:03<00:00, 15.97it/s] 80%|████████  | 53/66 [00:03<00:00, 15.05it/s] 83%|████████▎ | 55/66 [00:03<00:00, 15.46it/s] 86%|████████▋ | 57/66 [00:03<00:00, 16.46it/s] 91%|█████████ | 60/66 [00:03<00:00, 19.89it/s] 95%|█████████▌| 63/66 [00:03<00:00, 19.98it/s]100%|██████████| 66/66 [00:03<00:00, 21.68it/s]100%|██████████| 66/66 [00:03<00:00, 16.79it/s]
{'eval_loss': 0.6285836696624756, 'eval_model_preparation_time': 0.0062, 'eval_acc': 0.6960690316395014, 'eval_runtime': 4.0103, 'eval_samples_per_second': 260.08, 'eval_steps_per_second': 16.458}
ROUND:8
CLIENT:2
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.58it/s]                                              {'loss': 0.652, 'grad_norm': 0.8361956477165222, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.58it/s]  7%|▋         | 2/30 [00:00<00:07,  3.74it/s]                                              {'loss': 0.706, 'grad_norm': 0.9651610851287842, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.74it/s] 10%|█         | 3/30 [00:00<00:07,  3.75it/s]                                              {'loss': 0.6871, 'grad_norm': 0.48294326663017273, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.75it/s] 13%|█▎        | 4/30 [00:01<00:06,  3.77it/s]                                              {'loss': 0.6549, 'grad_norm': 0.612312376499176, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:06,  3.77it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.74it/s]                                              {'loss': 0.653, 'grad_norm': 0.3071262538433075, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.74it/s]                                              {'loss': 0.7981, 'grad_norm': 1.6547129154205322, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.74it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.47it/s]                                              {'loss': 0.7211, 'grad_norm': 0.5590001940727234, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.47it/s] 27%|██▋       | 8/30 [00:01<00:05,  4.23it/s]                                              {'loss': 0.6922, 'grad_norm': 0.4281253218650818, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  4.23it/s] 30%|███       | 9/30 [00:02<00:05,  4.06it/s]                                              {'loss': 0.7498, 'grad_norm': 0.5581915974617004, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  4.06it/s] 33%|███▎      | 10/30 [00:02<00:05,  3.99it/s]                                               {'loss': 0.6729, 'grad_norm': 0.22943365573883057, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  3.99it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.00it/s]                                               {'loss': 0.6695, 'grad_norm': 0.3516150414943695, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.00it/s] 40%|████      | 12/30 [00:02<00:03,  4.80it/s]                                               {'loss': 0.6633, 'grad_norm': 1.2326200008392334, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.80it/s] 43%|████▎     | 13/30 [00:03<00:03,  4.26it/s]                                               {'loss': 0.6782, 'grad_norm': 0.46515384316444397, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  4.26it/s] 47%|████▋     | 14/30 [00:03<00:04,  3.91it/s]                                               {'loss': 0.6144, 'grad_norm': 0.7212835550308228, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:04,  3.91it/s] 50%|█████     | 15/30 [00:03<00:04,  3.74it/s]                                               {'loss': 0.6715, 'grad_norm': 0.910959005355835, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:04,  3.74it/s] 53%|█████▎    | 16/30 [00:04<00:03,  3.69it/s]                                               {'loss': 0.6374, 'grad_norm': 0.5981864333152771, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:03,  3.69it/s] 57%|█████▋    | 17/30 [00:04<00:03,  3.80it/s]                                               {'loss': 0.6215, 'grad_norm': 0.6809017062187195, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:03,  3.80it/s] 60%|██████    | 18/30 [00:04<00:03,  3.93it/s]                                               {'loss': 0.709, 'grad_norm': 1.2022699117660522, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:03,  3.93it/s] 63%|██████▎   | 19/30 [00:04<00:02,  4.14it/s]                                               {'loss': 0.5423, 'grad_norm': 1.3842464685440063, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  4.14it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.25it/s]                                               {'loss': 0.6241, 'grad_norm': 0.3672395646572113, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.25it/s] 70%|███████   | 21/30 [00:05<00:01,  4.51it/s]                                               {'loss': 0.642, 'grad_norm': 0.48041200637817383, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:01,  4.51it/s] 73%|███████▎  | 22/30 [00:05<00:01,  4.75it/s]                                               {'loss': 0.6634, 'grad_norm': 0.553677499294281, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:01,  4.75it/s] 77%|███████▋  | 23/30 [00:05<00:01,  4.44it/s]                                               {'loss': 0.6578, 'grad_norm': 1.2900816202163696, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  4.44it/s] 80%|████████  | 24/30 [00:05<00:01,  5.13it/s]                                               {'loss': 0.7104, 'grad_norm': 1.3086127042770386, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.13it/s] 83%|████████▎ | 25/30 [00:06<00:01,  4.46it/s]                                               {'loss': 0.5972, 'grad_norm': 0.7871634364128113, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:01,  4.46it/s] 87%|████████▋ | 26/30 [00:06<00:00,  4.13it/s]                                               {'loss': 0.6026, 'grad_norm': 0.5995211601257324, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:00,  4.13it/s] 90%|█████████ | 27/30 [00:06<00:00,  3.90it/s]                                               {'loss': 0.6305, 'grad_norm': 0.5889628529548645, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  3.90it/s] 93%|█████████▎| 28/30 [00:06<00:00,  3.98it/s]                                               {'loss': 0.5942, 'grad_norm': 0.7439399361610413, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  3.98it/s] 97%|█████████▋| 29/30 [00:06<00:00,  4.54it/s]                                               {'loss': 0.4974, 'grad_norm': 0.6387143135070801, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  4.54it/s]                                               {'loss': 0.5322, 'grad_norm': 1.429403305053711, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  4.54it/s]                                               {'train_runtime': 7.1154, 'train_samples_per_second': 59.73, 'train_steps_per_second': 4.216, 'train_loss': 0.6515447994073232, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  4.54it/s]100%|██████████| 30/30 [00:07<00:00,  4.22it/s]
CLIENT:49
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  4.85it/s]                                              {'loss': 0.6622, 'grad_norm': 1.2917263507843018, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  4.85it/s]  7%|▋         | 2/30 [00:00<00:05,  5.01it/s]                                              {'loss': 0.3861, 'grad_norm': 1.6146160364151, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.01it/s] 10%|█         | 3/30 [00:00<00:05,  5.18it/s]                                              {'loss': 0.2536, 'grad_norm': 0.6985284686088562, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  5.18it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.73it/s]                                              {'loss': 0.448, 'grad_norm': 1.9755240678787231, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.73it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.69it/s]                                              {'loss': 0.3164, 'grad_norm': 0.7229803800582886, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.69it/s] 20%|██        | 6/30 [00:01<00:09,  2.54it/s]                                              {'loss': 0.0425, 'grad_norm': 0.7532053589820862, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:09,  2.54it/s] 23%|██▎       | 7/30 [00:01<00:07,  3.19it/s]                                              {'loss': 0.5014, 'grad_norm': 2.1970088481903076, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:07,  3.19it/s] 27%|██▋       | 8/30 [00:02<00:05,  4.00it/s]                                              {'loss': 0.2623, 'grad_norm': 0.8746868371963501, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:05,  4.00it/s] 30%|███       | 9/30 [00:02<00:04,  4.62it/s]                                              {'loss': 0.5107, 'grad_norm': 2.050901174545288, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.62it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.29it/s]                                               {'loss': 0.4733, 'grad_norm': 3.291317939758301, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.29it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.66it/s]                                               {'loss': 0.0989, 'grad_norm': 1.4391947984695435, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.66it/s] 40%|████      | 12/30 [00:02<00:02,  6.32it/s]                                               {'loss': 0.0568, 'grad_norm': 1.246409296989441, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.32it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.39it/s]                                               {'loss': 0.2519, 'grad_norm': 1.1654412746429443, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.39it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.90it/s]                                               {'loss': 0.1627, 'grad_norm': 0.7317063212394714, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.90it/s] 50%|█████     | 15/30 [00:03<00:02,  5.60it/s]                                               {'loss': 0.5912, 'grad_norm': 3.6487066745758057, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.60it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.59it/s]                                               {'loss': 0.2492, 'grad_norm': 0.8910301327705383, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.59it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.49it/s]                                               {'loss': 0.1652, 'grad_norm': 0.7062790393829346, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.49it/s] 60%|██████    | 18/30 [00:03<00:02,  5.92it/s]                                               {'loss': 0.0615, 'grad_norm': 1.0247340202331543, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.92it/s] 63%|██████▎   | 19/30 [00:03<00:01,  5.95it/s]                                               {'loss': 0.0462, 'grad_norm': 0.717182993888855, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  5.95it/s]                                               {'loss': 0.6776, 'grad_norm': 8.049592971801758, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.95it/s] 70%|███████   | 21/30 [00:03<00:01,  7.87it/s]                                               {'loss': 0.3134, 'grad_norm': 1.680596947669983, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.87it/s]                                               {'loss': 0.0319, 'grad_norm': 0.42767834663391113, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  7.87it/s] 77%|███████▋  | 23/30 [00:04<00:00,  9.52it/s]                                               {'loss': 0.3369, 'grad_norm': 3.7965126037597656, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:00,  9.52it/s]                                               {'loss': 0.0545, 'grad_norm': 1.017983078956604, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  9.52it/s] 83%|████████▎ | 25/30 [00:04<00:00, 11.56it/s]                                               {'loss': 0.4211, 'grad_norm': 1.624173641204834, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00, 11.56it/s]                                               {'loss': 0.0313, 'grad_norm': 0.4636956751346588, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00, 11.56it/s] 90%|█████████ | 27/30 [00:04<00:00, 10.31it/s]                                               {'loss': 0.1933, 'grad_norm': 1.207448959350586, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00, 10.31it/s]                                               {'loss': 0.152, 'grad_norm': 1.1959203481674194, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00, 10.31it/s] 97%|█████████▋| 29/30 [00:04<00:00, 10.01it/s]                                               {'loss': 0.2937, 'grad_norm': 2.5925557613372803, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00, 10.01it/s]                                               {'loss': 0.3812, 'grad_norm': 4.025935173034668, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00, 10.01it/s]                                               {'train_runtime': 4.8795, 'train_samples_per_second': 87.099, 'train_steps_per_second': 6.148, 'train_loss': 0.28089050985872743, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00, 10.01it/s]100%|██████████| 30/30 [00:04<00:00,  6.16it/s]
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.81it/s]                                              {'loss': 0.7728, 'grad_norm': 1.2295883893966675, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.81it/s]  7%|▋         | 2/30 [00:00<00:07,  3.63it/s]                                              {'loss': 0.5891, 'grad_norm': 0.6779314875602722, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.63it/s] 10%|█         | 3/30 [00:00<00:07,  3.56it/s]                                              {'loss': 0.5252, 'grad_norm': 0.5581083297729492, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.56it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.52it/s]                                              {'loss': 0.553, 'grad_norm': 1.1423616409301758, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.52it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.94it/s]                                              {'loss': 0.8259, 'grad_norm': 2.0793933868408203, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.94it/s] 20%|██        | 6/30 [00:01<00:04,  4.88it/s]                                              {'loss': 0.9268, 'grad_norm': 1.9571267366409302, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.88it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.62it/s]                                              {'loss': 0.5254, 'grad_norm': 0.6786511540412903, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.62it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.73it/s]                                              {'loss': 0.6649, 'grad_norm': 0.7384124398231506, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.73it/s] 30%|███       | 9/30 [00:02<00:04,  4.59it/s]                                              {'loss': 0.5352, 'grad_norm': 1.9073213338851929, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.59it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.47it/s]                                               {'loss': 0.5798, 'grad_norm': 1.317748785018921, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.47it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.42it/s]                                               {'loss': 0.6837, 'grad_norm': 1.0288939476013184, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.42it/s] 40%|████      | 12/30 [00:02<00:03,  5.19it/s]                                               {'loss': 0.4958, 'grad_norm': 1.7225425243377686, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.19it/s] 43%|████▎     | 13/30 [00:02<00:03,  4.75it/s]                                               {'loss': 0.5941, 'grad_norm': 1.5129197835922241, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  4.75it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.63it/s]                                               {'loss': 0.5348, 'grad_norm': 1.2090579271316528, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.63it/s] 50%|█████     | 15/30 [00:03<00:03,  4.68it/s]                                               {'loss': 0.6298, 'grad_norm': 1.851562261581421, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.68it/s] 53%|█████▎    | 16/30 [00:03<00:02,  4.73it/s]                                               {'loss': 0.6734, 'grad_norm': 1.6768723726272583, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  4.73it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.75it/s]                                               {'loss': 0.5807, 'grad_norm': 1.7518072128295898, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.75it/s]                                               {'loss': 0.6693, 'grad_norm': 2.3692429065704346, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.75it/s] 63%|██████▎   | 19/30 [00:04<00:01,  5.55it/s]                                               {'loss': 0.5657, 'grad_norm': 1.9959946870803833, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:01,  5.55it/s] 67%|██████▋   | 20/30 [00:04<00:01,  5.41it/s]                                               {'loss': 0.4542, 'grad_norm': 2.3365442752838135, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  5.41it/s] 70%|███████   | 21/30 [00:04<00:01,  5.12it/s]                                               {'loss': 0.6003, 'grad_norm': 1.551730751991272, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.12it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.01it/s]                                               {'loss': 0.5921, 'grad_norm': 1.3974908590316772, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.01it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.79it/s]                                               {'loss': 0.5376, 'grad_norm': 1.825136423110962, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.79it/s]                                               {'loss': 0.4832, 'grad_norm': 3.5127904415130615, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  4.79it/s] 83%|████████▎ | 25/30 [00:05<00:00,  5.62it/s]                                               {'loss': 0.5086, 'grad_norm': 1.394234299659729, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  5.62it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.34it/s]                                               {'loss': 0.6008, 'grad_norm': 1.6077537536621094, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.34it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.94it/s]                                               {'loss': 0.4527, 'grad_norm': 1.1119216680526733, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.94it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.76it/s]                                               {'loss': 0.3869, 'grad_norm': 2.469923496246338, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.76it/s] 97%|█████████▋| 29/30 [00:06<00:00,  5.05it/s]                                               {'loss': 0.6508, 'grad_norm': 2.5498764514923096, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  5.05it/s]                                               {'loss': 0.4018, 'grad_norm': 2.8196325302124023, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.05it/s]                                               {'train_runtime': 6.3326, 'train_samples_per_second': 67.113, 'train_steps_per_second': 4.737, 'train_loss': 0.5864804406960805, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.05it/s]100%|██████████| 30/30 [00:06<00:00,  4.74it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:09,  2.91it/s]                                              {'loss': 0.7126, 'grad_norm': 1.4574925899505615, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:09,  2.91it/s]  7%|▋         | 2/30 [00:00<00:09,  2.84it/s]                                              {'loss': 0.5034, 'grad_norm': 1.1149154901504517, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:09,  2.84it/s] 10%|█         | 3/30 [00:01<00:09,  2.80it/s]                                              {'loss': 0.5018, 'grad_norm': 0.6423599720001221, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:09,  2.80it/s] 13%|█▎        | 4/30 [00:01<00:09,  2.78it/s]                                              {'loss': 0.4707, 'grad_norm': 1.8604761362075806, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:09,  2.78it/s] 17%|█▋        | 5/30 [00:01<00:08,  2.93it/s]                                              {'loss': 0.6595, 'grad_norm': 3.300421714782715, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:08,  2.93it/s] 20%|██        | 6/30 [00:02<00:12,  1.96it/s]                                              {'loss': 1.143, 'grad_norm': 4.625619411468506, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:12,  1.96it/s] 23%|██▎       | 7/30 [00:02<00:10,  2.15it/s]                                              {'loss': 0.4625, 'grad_norm': 1.1609432697296143, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:10,  2.15it/s] 27%|██▋       | 8/30 [00:03<00:09,  2.30it/s]                                              {'loss': 0.5352, 'grad_norm': 0.75533527135849, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:03<00:09,  2.30it/s] 30%|███       | 9/30 [00:03<00:08,  2.43it/s]                                              {'loss': 0.6234, 'grad_norm': 0.8555293083190918, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:03<00:08,  2.43it/s] 33%|███▎      | 10/30 [00:04<00:07,  2.53it/s]                                               {'loss': 0.558, 'grad_norm': 0.8506292700767517, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:04<00:07,  2.53it/s] 37%|███▋      | 11/30 [00:04<00:07,  2.59it/s]                                               {'loss': 0.7633, 'grad_norm': 1.7324557304382324, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:04<00:07,  2.59it/s] 40%|████      | 12/30 [00:04<00:05,  3.27it/s]                                               {'loss': 0.4327, 'grad_norm': 0.5262113809585571, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:04<00:05,  3.27it/s] 43%|████▎     | 13/30 [00:04<00:05,  3.10it/s]                                               {'loss': 0.6257, 'grad_norm': 0.6421756744384766, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:04<00:05,  3.10it/s] 47%|████▋     | 14/30 [00:05<00:05,  2.94it/s]                                               {'loss': 0.4423, 'grad_norm': 1.2517534494400024, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:05<00:05,  2.94it/s] 50%|█████     | 15/30 [00:05<00:05,  2.87it/s]                                               {'loss': 0.5836, 'grad_norm': 0.4912335276603699, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:05<00:05,  2.87it/s] 53%|█████▎    | 16/30 [00:05<00:04,  2.84it/s]                                               {'loss': 0.6275, 'grad_norm': 0.5163756608963013, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:06<00:04,  2.84it/s] 57%|█████▋    | 17/30 [00:06<00:04,  2.82it/s]                                               {'loss': 0.4919, 'grad_norm': 1.0061637163162231, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:06<00:04,  2.82it/s] 60%|██████    | 18/30 [00:06<00:03,  3.50it/s]                                               {'loss': 0.9236, 'grad_norm': 2.886341094970703, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:06<00:03,  3.50it/s] 63%|██████▎   | 19/30 [00:06<00:03,  3.21it/s]                                               {'loss': 0.5305, 'grad_norm': 0.33095285296440125, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:06<00:03,  3.21it/s] 67%|██████▋   | 20/30 [00:07<00:03,  3.08it/s]                                               {'loss': 0.5445, 'grad_norm': 0.49291881918907166, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:07<00:03,  3.08it/s] 70%|███████   | 21/30 [00:07<00:03,  2.99it/s]                                               {'loss': 0.5702, 'grad_norm': 0.3510943353176117, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:07<00:03,  2.99it/s] 73%|███████▎  | 22/30 [00:07<00:02,  2.90it/s]                                               {'loss': 0.785, 'grad_norm': 1.705317497253418, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:07<00:02,  2.90it/s] 77%|███████▋  | 23/30 [00:08<00:02,  2.88it/s]                                               {'loss': 0.4318, 'grad_norm': 1.2047014236450195, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:08<00:02,  2.88it/s]                                               {'loss': 0.7092, 'grad_norm': 1.000175952911377, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:08<00:02,  2.88it/s] 83%|████████▎ | 25/30 [00:08<00:01,  3.47it/s]                                               {'loss': 0.5361, 'grad_norm': 0.49924859404563904, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:08<00:01,  3.47it/s] 87%|████████▋ | 26/30 [00:09<00:01,  3.27it/s]                                               {'loss': 0.7164, 'grad_norm': 0.93717360496521, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:09<00:01,  3.27it/s] 90%|█████████ | 27/30 [00:09<00:00,  3.15it/s]                                               {'loss': 0.5298, 'grad_norm': 0.5621116757392883, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:09<00:00,  3.15it/s] 93%|█████████▎| 28/30 [00:09<00:00,  3.06it/s]                                               {'loss': 0.5562, 'grad_norm': 0.4442422688007355, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:09<00:00,  3.06it/s] 97%|█████████▋| 29/30 [00:10<00:00,  3.02it/s]                                               {'loss': 0.7046, 'grad_norm': 0.9817256331443787, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:10<00:00,  3.02it/s]100%|██████████| 30/30 [00:10<00:00,  3.64it/s]                                               {'loss': 0.3749, 'grad_norm': 2.317377805709839, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:10<00:00,  3.64it/s]                                               {'train_runtime': 10.342, 'train_samples_per_second': 41.094, 'train_steps_per_second': 2.901, 'train_loss': 0.6016684710979462, 'epoch': 5.0}
100%|██████████| 30/30 [00:10<00:00,  3.64it/s]100%|██████████| 30/30 [00:10<00:00,  2.90it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.59it/s]                                              {'loss': 0.7755, 'grad_norm': 1.7525302171707153, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.59it/s]  7%|▋         | 2/30 [00:00<00:05,  5.01it/s]                                              {'loss': 0.4271, 'grad_norm': 1.3626073598861694, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.01it/s] 10%|█         | 3/30 [00:00<00:05,  4.86it/s]                                              {'loss': 0.7456, 'grad_norm': 2.0107650756835938, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.86it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.33it/s]                                              {'loss': 0.713, 'grad_norm': 1.6504969596862793, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.33it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.94it/s]                                              {'loss': 0.6973, 'grad_norm': 2.3200714588165283, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.94it/s] 20%|██        | 6/30 [00:01<00:06,  3.54it/s]                                              {'loss': 1.1427, 'grad_norm': 3.3564841747283936, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.54it/s]                                              {'loss': 0.3487, 'grad_norm': 2.0671772956848145, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:06,  3.54it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.62it/s]                                              {'loss': 0.5289, 'grad_norm': 2.8056235313415527, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.62it/s] 30%|███       | 9/30 [00:01<00:04,  4.68it/s]                                              {'loss': 0.6403, 'grad_norm': 1.788884162902832, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.68it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.80it/s]                                               {'loss': 0.5496, 'grad_norm': 1.5165653228759766, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.80it/s] 37%|███▋      | 11/30 [00:02<00:03,  4.80it/s]                                               {'loss': 0.6914, 'grad_norm': 1.3098384141921997, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  4.80it/s]                                               {'loss': 0.3922, 'grad_norm': 2.985448122024536, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.80it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.49it/s]                                               {'loss': 0.4347, 'grad_norm': 1.678585171699524, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.49it/s] 47%|████▋     | 14/30 [00:02<00:03,  5.29it/s]                                               {'loss': 0.4893, 'grad_norm': 3.5615737438201904, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  5.29it/s] 50%|█████     | 15/30 [00:03<00:02,  5.07it/s]                                               {'loss': 0.5914, 'grad_norm': 2.2302629947662354, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.07it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.11it/s]                                               {'loss': 0.423, 'grad_norm': 1.6068816184997559, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.11it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.94it/s]                                               {'loss': 0.4785, 'grad_norm': 1.673460602760315, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.94it/s]                                               {'loss': 0.7016, 'grad_norm': 3.369659423828125, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.94it/s] 63%|██████▎   | 19/30 [00:03<00:02,  5.47it/s]                                               {'loss': 0.4881, 'grad_norm': 2.060936212539673, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  5.47it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.18it/s]                                               {'loss': 0.3101, 'grad_norm': 1.8060836791992188, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  5.18it/s] 70%|███████   | 21/30 [00:04<00:01,  4.91it/s]                                               {'loss': 0.633, 'grad_norm': 6.204360485076904, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  4.91it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.51it/s]                                               {'loss': 0.5842, 'grad_norm': 2.767554759979248, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.51it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.35it/s]                                               {'loss': 0.7484, 'grad_norm': 2.8389179706573486, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.35it/s]                                               {'loss': 0.248, 'grad_norm': 2.3090524673461914, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  4.35it/s] 83%|████████▎ | 25/30 [00:05<00:01,  4.83it/s]                                               {'loss': 0.3522, 'grad_norm': 3.176666021347046, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  4.83it/s] 87%|████████▋ | 26/30 [00:05<00:00,  4.52it/s]                                               {'loss': 0.4377, 'grad_norm': 1.1044756174087524, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  4.52it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.18it/s]                                               {'loss': 0.6673, 'grad_norm': 3.325756072998047, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.18it/s] 93%|█████████▎| 28/30 [00:05<00:00,  3.93it/s]                                               {'loss': 0.4449, 'grad_norm': 2.4259092807769775, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  3.93it/s] 97%|█████████▋| 29/30 [00:06<00:00,  3.88it/s]                                               {'loss': 0.4451, 'grad_norm': 1.3217225074768066, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  3.88it/s]                                               {'loss': 0.2805, 'grad_norm': 2.982603073120117, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  3.88it/s]                                               {'train_runtime': 6.4259, 'train_samples_per_second': 66.139, 'train_steps_per_second': 4.669, 'train_loss': 0.5470090667406718, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  3.88it/s]100%|██████████| 30/30 [00:06<00:00,  4.67it/s]
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  9.02it/s]                                              {'loss': 0.5763, 'grad_norm': 0.6909191012382507, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  9.02it/s]  7%|▋         | 2/30 [00:00<00:02,  9.53it/s]                                              {'loss': 0.6833, 'grad_norm': 0.561187207698822, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02,  9.53it/s] 10%|█         | 3/30 [00:00<00:03,  8.87it/s]                                              {'loss': 0.6911, 'grad_norm': 0.541923463344574, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.87it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.67it/s]                                              {'loss': 0.6185, 'grad_norm': 0.5540317296981812, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.67it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.41it/s]                                              {'loss': 0.662, 'grad_norm': 0.4530896544456482, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.41it/s]                                              {'loss': 0.7185, 'grad_norm': 1.5724127292633057, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.41it/s] 23%|██▎       | 7/30 [00:00<00:02,  7.73it/s]                                              {'loss': 0.6525, 'grad_norm': 0.7850194573402405, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  7.73it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.90it/s]                                              {'loss': 0.65, 'grad_norm': 0.5129611492156982, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.90it/s] 30%|███       | 9/30 [00:01<00:02,  7.54it/s]                                              {'loss': 0.6065, 'grad_norm': 0.4597117006778717, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.54it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.48it/s]                                               {'loss': 0.6362, 'grad_norm': 0.32670357823371887, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.48it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.68it/s]                                               {'loss': 0.6507, 'grad_norm': 0.3883714973926544, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.68it/s]                                               {'loss': 0.5435, 'grad_norm': 1.3370331525802612, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.68it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.79it/s]                                               {'loss': 0.6192, 'grad_norm': 1.2469109296798706, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.79it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.70it/s]                                               {'loss': 0.5873, 'grad_norm': 0.7740429043769836, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.70it/s] 50%|█████     | 15/30 [00:01<00:01,  7.58it/s]                                               {'loss': 0.6646, 'grad_norm': 1.5777143239974976, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  7.58it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.35it/s]                                               {'loss': 0.5372, 'grad_norm': 1.0654255151748657, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.35it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.45it/s]                                               {'loss': 0.5658, 'grad_norm': 0.8408780694007874, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.45it/s]                                               {'loss': 0.6454, 'grad_norm': 1.570077657699585, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.45it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.23it/s]                                               {'loss': 0.466, 'grad_norm': 1.315345287322998, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.23it/s]                                               {'loss': 0.5643, 'grad_norm': 0.9814431071281433, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.23it/s] 70%|███████   | 21/30 [00:02<00:01,  8.17it/s]                                               {'loss': 0.4934, 'grad_norm': 1.1318954229354858, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.17it/s] 73%|███████▎  | 22/30 [00:02<00:01,  7.67it/s]                                               {'loss': 0.5144, 'grad_norm': 1.1859216690063477, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  7.67it/s] 77%|███████▋  | 23/30 [00:02<00:00,  7.34it/s]                                               {'loss': 0.4756, 'grad_norm': 1.0301272869110107, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  7.34it/s]                                               {'loss': 0.3753, 'grad_norm': 1.6732149124145508, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.34it/s] 83%|████████▎ | 25/30 [00:03<00:00,  7.19it/s]                                               {'loss': 0.5591, 'grad_norm': 1.8804758787155151, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  7.19it/s] 87%|████████▋ | 26/30 [00:03<00:00,  6.87it/s]                                               {'loss': 0.3886, 'grad_norm': 1.2063227891921997, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  6.87it/s] 90%|█████████ | 27/30 [00:03<00:00,  6.85it/s]                                               {'loss': 0.4827, 'grad_norm': 1.6747784614562988, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  6.85it/s] 93%|█████████▎| 28/30 [00:03<00:00,  6.89it/s]                                               {'loss': 0.3722, 'grad_norm': 1.3339048624038696, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  6.89it/s] 97%|█████████▋| 29/30 [00:03<00:00,  7.27it/s]                                               {'loss': 0.3149, 'grad_norm': 1.42653489112854, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  7.27it/s]                                               {'loss': 0.3551, 'grad_norm': 2.7484378814697266, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  7.27it/s]                                               {'train_runtime': 4.0509, 'train_samples_per_second': 104.914, 'train_steps_per_second': 7.406, 'train_loss': 0.5556742330392201, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.27it/s]100%|██████████| 30/30 [00:04<00:00,  7.41it/s]
CLIENT:87
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.26it/s]                                              {'loss': 0.5831, 'grad_norm': 0.8041399717330933, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.26it/s]  7%|▋         | 2/30 [00:00<00:08,  3.42it/s]                                              {'loss': 0.6761, 'grad_norm': 0.8414618968963623, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:08,  3.42it/s] 10%|█         | 3/30 [00:00<00:07,  3.46it/s]                                              {'loss': 0.7366, 'grad_norm': 0.5366746783256531, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.46it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.40it/s]                                              {'loss': 0.6461, 'grad_norm': 0.5628326535224915, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.40it/s] 17%|█▋        | 5/30 [00:01<00:07,  3.43it/s]                                              {'loss': 0.6205, 'grad_norm': 0.3319259583950043, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:07,  3.43it/s]                                              {'loss': 0.759, 'grad_norm': 1.3453665971755981, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:07,  3.43it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.46it/s]                                              {'loss': 0.6484, 'grad_norm': 0.35255464911460876, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.46it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.50it/s]                                              {'loss': 0.6575, 'grad_norm': 0.36946457624435425, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.50it/s] 30%|███       | 9/30 [00:02<00:04,  4.53it/s]                                              {'loss': 0.6624, 'grad_norm': 0.5256511569023132, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.53it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.52it/s]                                               {'loss': 0.6502, 'grad_norm': 0.40858978033065796, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.52it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.57it/s]                                               {'loss': 0.6218, 'grad_norm': 0.3669987916946411, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.57it/s]                                               {'loss': 0.6052, 'grad_norm': 1.303057312965393, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.57it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.33it/s]                                               {'loss': 0.5972, 'grad_norm': 0.7522680759429932, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.33it/s] 47%|████▋     | 14/30 [00:03<00:03,  5.13it/s]                                               {'loss': 0.5965, 'grad_norm': 0.48727425932884216, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  5.13it/s] 50%|█████     | 15/30 [00:03<00:03,  4.95it/s]                                               {'loss': 0.7447, 'grad_norm': 1.2301157712936401, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.95it/s] 53%|█████▎    | 16/30 [00:03<00:02,  4.75it/s]                                               {'loss': 0.5668, 'grad_norm': 0.6313623189926147, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  4.75it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.61it/s]                                               {'loss': 0.567, 'grad_norm': 0.9272510409355164, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.61it/s]                                               {'loss': 0.8736, 'grad_norm': 2.266047954559326, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.61it/s] 63%|██████▎   | 19/30 [00:04<00:02,  5.23it/s]                                               {'loss': 0.6638, 'grad_norm': 1.1927523612976074, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  5.23it/s] 67%|██████▋   | 20/30 [00:04<00:01,  5.18it/s]                                               {'loss': 0.6519, 'grad_norm': 0.7415684461593628, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  5.18it/s] 70%|███████   | 21/30 [00:04<00:01,  5.13it/s]                                               {'loss': 0.5599, 'grad_norm': 0.9586535096168518, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.13it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.62it/s]                                               {'loss': 0.5764, 'grad_norm': 1.377323865890503, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.62it/s] 77%|███████▋  | 23/30 [00:05<00:01,  4.48it/s]                                               {'loss': 0.6017, 'grad_norm': 1.6037088632583618, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  4.48it/s] 80%|████████  | 24/30 [00:05<00:01,  5.31it/s]                                               {'loss': 0.5071, 'grad_norm': 2.316823959350586, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.31it/s] 83%|████████▎ | 25/30 [00:05<00:00,  5.70it/s]                                               {'loss': 0.5971, 'grad_norm': 1.3350224494934082, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  5.70it/s] 87%|████████▋ | 26/30 [00:05<00:00,  6.22it/s]                                               {'loss': 0.5706, 'grad_norm': 0.9307333827018738, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  6.22it/s] 90%|█████████ | 27/30 [00:05<00:00,  6.95it/s]                                               {'loss': 0.5577, 'grad_norm': 0.9953988790512085, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  6.95it/s]                                               {'loss': 0.5184, 'grad_norm': 0.826919674873352, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  6.95it/s] 97%|█████████▋| 29/30 [00:05<00:00,  8.76it/s]                                               {'loss': 0.4917, 'grad_norm': 1.1337989568710327, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  8.76it/s]                                               {'loss': 0.5666, 'grad_norm': 2.415339469909668, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  8.76it/s]                                               {'train_runtime': 5.8338, 'train_samples_per_second': 72.851, 'train_steps_per_second': 5.142, 'train_loss': 0.6225163161754608, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  8.76it/s]100%|██████████| 30/30 [00:05<00:00,  5.15it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.49it/s]                                              {'loss': 0.5869, 'grad_norm': 0.8470693826675415, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.49it/s]  7%|▋         | 2/30 [00:00<00:05,  5.39it/s]                                              {'loss': 0.7019, 'grad_norm': 0.6142283082008362, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.39it/s] 10%|█         | 3/30 [00:00<00:04,  5.81it/s]                                              {'loss': 0.7207, 'grad_norm': 0.7876706719398499, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  5.81it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.21it/s]                                              {'loss': 0.7373, 'grad_norm': 1.190626859664917, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.21it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.44it/s]                                              {'loss': 0.6998, 'grad_norm': 0.4473331868648529, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.44it/s] 20%|██        | 6/30 [00:01<00:04,  5.09it/s]                                              {'loss': 0.7403, 'grad_norm': 1.4848403930664062, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.09it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.90it/s]                                              {'loss': 0.6622, 'grad_norm': 0.3720266819000244, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.90it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.50it/s]                                              {'loss': 0.7417, 'grad_norm': 0.7629361152648926, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.50it/s] 30%|███       | 9/30 [00:01<00:04,  5.13it/s]                                              {'loss': 0.5906, 'grad_norm': 0.8037195205688477, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  5.13it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.33it/s]                                               {'loss': 0.6704, 'grad_norm': 0.31226035952568054, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.33it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.28it/s]                                               {'loss': 0.6356, 'grad_norm': 0.4292277991771698, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.28it/s]                                               {'loss': 0.5613, 'grad_norm': 1.2064170837402344, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.28it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.73it/s]                                               {'loss': 0.5973, 'grad_norm': 0.4838552176952362, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.73it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.93it/s]                                               {'loss': 0.6086, 'grad_norm': 1.0167096853256226, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.93it/s] 50%|█████     | 15/30 [00:02<00:02,  5.01it/s]                                               {'loss': 0.7367, 'grad_norm': 1.9092947244644165, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.01it/s] 53%|█████▎    | 16/30 [00:03<00:02,  4.71it/s]                                               {'loss': 0.6897, 'grad_norm': 2.0091614723205566, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  4.71it/s] 57%|█████▋    | 17/30 [00:03<00:03,  4.32it/s]                                               {'loss': 0.5866, 'grad_norm': 0.7758829593658447, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  4.32it/s] 60%|██████    | 18/30 [00:03<00:02,  5.13it/s]                                               {'loss': 0.5897, 'grad_norm': 1.6317557096481323, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.13it/s] 63%|██████▎   | 19/30 [00:03<00:02,  5.04it/s]                                               {'loss': 0.5859, 'grad_norm': 1.1886826753616333, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  5.04it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.35it/s]                                               {'loss': 0.6888, 'grad_norm': 1.4320094585418701, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.35it/s] 70%|███████   | 21/30 [00:03<00:01,  5.76it/s]                                               {'loss': 0.6215, 'grad_norm': 0.869621217250824, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.76it/s] 73%|███████▎  | 22/30 [00:04<00:01,  6.23it/s]                                               {'loss': 0.6439, 'grad_norm': 4.0486931800842285, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  6.23it/s] 77%|███████▋  | 23/30 [00:04<00:01,  5.96it/s]                                               {'loss': 0.6235, 'grad_norm': 0.9375691413879395, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  5.96it/s]                                               {'loss': 0.5314, 'grad_norm': 3.9396109580993652, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  5.96it/s] 83%|████████▎ | 25/30 [00:04<00:00,  7.52it/s]                                               {'loss': 0.603, 'grad_norm': 1.0114314556121826, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  7.52it/s] 87%|████████▋ | 26/30 [00:04<00:00,  7.42it/s]                                               {'loss': 0.6119, 'grad_norm': 1.154483675956726, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  7.42it/s] 90%|█████████ | 27/30 [00:04<00:00,  7.28it/s]                                               {'loss': 0.6105, 'grad_norm': 1.0829975605010986, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  7.28it/s] 93%|█████████▎| 28/30 [00:04<00:00,  7.36it/s]                                               {'loss': 0.5543, 'grad_norm': 2.6407310962677, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  7.36it/s] 97%|█████████▋| 29/30 [00:04<00:00,  7.53it/s]                                               {'loss': 0.5697, 'grad_norm': 1.2532817125320435, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  7.53it/s]                                               {'loss': 0.5521, 'grad_norm': 1.9741963148117065, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  7.53it/s]                                               {'train_runtime': 5.1129, 'train_samples_per_second': 83.122, 'train_steps_per_second': 5.867, 'train_loss': 0.6351270139217376, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  7.53it/s]100%|██████████| 30/30 [00:05<00:00,  5.87it/s]
CLIENT:99
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:08,  4.96it/s]                                              {'loss': 0.6249, 'grad_norm': 0.8399869799613953, 'learning_rate': 0.001, 'epoch': 0.11}
  2%|▏         | 1/45 [00:00<00:08,  4.96it/s]                                              {'loss': 0.5445, 'grad_norm': 1.1898105144500732, 'learning_rate': 0.0009777777777777777, 'epoch': 0.22}
  4%|▍         | 2/45 [00:00<00:08,  4.96it/s]  7%|▋         | 3/45 [00:00<00:05,  7.65it/s]                                              {'loss': 0.3612, 'grad_norm': 1.2160139083862305, 'learning_rate': 0.0009555555555555556, 'epoch': 0.33}
  7%|▋         | 3/45 [00:00<00:05,  7.65it/s]  9%|▉         | 4/45 [00:00<00:05,  7.35it/s]                                              {'loss': 0.6494, 'grad_norm': 1.7102018594741821, 'learning_rate': 0.0009333333333333333, 'epoch': 0.44}
  9%|▉         | 4/45 [00:00<00:05,  7.35it/s] 11%|█         | 5/45 [00:00<00:04,  8.01it/s]                                              {'loss': 0.3161, 'grad_norm': 1.0750343799591064, 'learning_rate': 0.0009111111111111111, 'epoch': 0.56}
 11%|█         | 5/45 [00:00<00:04,  8.01it/s] 13%|█▎        | 6/45 [00:00<00:05,  7.67it/s]                                              {'loss': 0.6867, 'grad_norm': 2.809567928314209, 'learning_rate': 0.0008888888888888888, 'epoch': 0.67}
 13%|█▎        | 6/45 [00:00<00:05,  7.67it/s] 16%|█▌        | 7/45 [00:00<00:05,  7.56it/s]                                              {'loss': 0.4201, 'grad_norm': 1.6554378271102905, 'learning_rate': 0.0008666666666666667, 'epoch': 0.78}
 16%|█▌        | 7/45 [00:00<00:05,  7.56it/s] 18%|█▊        | 8/45 [00:01<00:05,  6.96it/s]                                              {'loss': 0.7692, 'grad_norm': 3.3625659942626953, 'learning_rate': 0.0008444444444444444, 'epoch': 0.89}
 18%|█▊        | 8/45 [00:01<00:05,  6.96it/s]                                              {'loss': 0.4623, 'grad_norm': 2.0493879318237305, 'learning_rate': 0.0008222222222222222, 'epoch': 1.0}
 20%|██        | 9/45 [00:01<00:05,  6.96it/s] 22%|██▏       | 10/45 [00:01<00:04,  8.13it/s]                                               {'loss': 0.4024, 'grad_norm': 1.8488506078720093, 'learning_rate': 0.0008, 'epoch': 1.11}
 22%|██▏       | 10/45 [00:01<00:04,  8.13it/s] 24%|██▍       | 11/45 [00:01<00:04,  8.35it/s]                                               {'loss': 0.4338, 'grad_norm': 2.0530152320861816, 'learning_rate': 0.0007777777777777778, 'epoch': 1.22}
 24%|██▍       | 11/45 [00:01<00:04,  8.35it/s] 27%|██▋       | 12/45 [00:01<00:04,  7.72it/s]                                               {'loss': 0.4198, 'grad_norm': 0.7843273282051086, 'learning_rate': 0.0007555555555555555, 'epoch': 1.33}
 27%|██▋       | 12/45 [00:01<00:04,  7.72it/s] 29%|██▉       | 13/45 [00:01<00:04,  7.33it/s]                                               {'loss': 0.3507, 'grad_norm': 1.6212158203125, 'learning_rate': 0.0007333333333333333, 'epoch': 1.44}
 29%|██▉       | 13/45 [00:01<00:04,  7.33it/s] 31%|███       | 14/45 [00:01<00:04,  7.06it/s]                                               {'loss': 0.5679, 'grad_norm': 1.010333776473999, 'learning_rate': 0.0007111111111111111, 'epoch': 1.56}
 31%|███       | 14/45 [00:01<00:04,  7.06it/s] 33%|███▎      | 15/45 [00:02<00:04,  6.73it/s]                                               {'loss': 0.5907, 'grad_norm': 1.5959407091140747, 'learning_rate': 0.000688888888888889, 'epoch': 1.67}
 33%|███▎      | 15/45 [00:02<00:04,  6.73it/s] 36%|███▌      | 16/45 [00:02<00:04,  6.71it/s]                                               {'loss': 0.5078, 'grad_norm': 1.8420894145965576, 'learning_rate': 0.0006666666666666666, 'epoch': 1.78}
 36%|███▌      | 16/45 [00:02<00:04,  6.71it/s] 38%|███▊      | 17/45 [00:02<00:04,  6.86it/s]                                               {'loss': 0.4975, 'grad_norm': 1.4228888750076294, 'learning_rate': 0.0006444444444444444, 'epoch': 1.89}
 38%|███▊      | 17/45 [00:02<00:04,  6.86it/s]                                               {'loss': 0.6188, 'grad_norm': 2.462968349456787, 'learning_rate': 0.0006222222222222223, 'epoch': 2.0}
 40%|████      | 18/45 [00:02<00:03,  6.86it/s] 42%|████▏     | 19/45 [00:02<00:03,  7.02it/s]                                               {'loss': 0.5623, 'grad_norm': 0.6484196782112122, 'learning_rate': 0.0006, 'epoch': 2.11}
 42%|████▏     | 19/45 [00:02<00:03,  7.02it/s] 44%|████▍     | 20/45 [00:02<00:03,  6.81it/s]                                               {'loss': 0.4353, 'grad_norm': 0.7739070653915405, 'learning_rate': 0.0005777777777777778, 'epoch': 2.22}
 44%|████▍     | 20/45 [00:02<00:03,  6.81it/s] 47%|████▋     | 21/45 [00:02<00:03,  6.74it/s]                                               {'loss': 0.5219, 'grad_norm': 0.7425718307495117, 'learning_rate': 0.0005555555555555556, 'epoch': 2.33}
 47%|████▋     | 21/45 [00:02<00:03,  6.74it/s] 49%|████▉     | 22/45 [00:03<00:03,  6.43it/s]                                               {'loss': 0.3497, 'grad_norm': 1.6769452095031738, 'learning_rate': 0.0005333333333333334, 'epoch': 2.44}
 49%|████▉     | 22/45 [00:03<00:03,  6.43it/s] 51%|█████     | 23/45 [00:03<00:03,  6.13it/s]                                               {'loss': 0.3916, 'grad_norm': 1.12889564037323, 'learning_rate': 0.0005111111111111111, 'epoch': 2.56}
 51%|█████     | 23/45 [00:03<00:03,  6.13it/s] 53%|█████▎    | 24/45 [00:03<00:03,  6.15it/s]                                               {'loss': 0.4225, 'grad_norm': 0.6638926863670349, 'learning_rate': 0.0004888888888888889, 'epoch': 2.67}
 53%|█████▎    | 24/45 [00:03<00:03,  6.15it/s] 56%|█████▌    | 25/45 [00:03<00:03,  6.09it/s]                                               {'loss': 0.2888, 'grad_norm': 1.355700135231018, 'learning_rate': 0.00046666666666666666, 'epoch': 2.78}
 56%|█████▌    | 25/45 [00:03<00:03,  6.09it/s] 58%|█████▊    | 26/45 [00:03<00:02,  6.66it/s]                                               {'loss': 0.495, 'grad_norm': 1.1800740957260132, 'learning_rate': 0.0004444444444444444, 'epoch': 2.89}
 58%|█████▊    | 26/45 [00:03<00:02,  6.66it/s]                                               {'loss': 0.4276, 'grad_norm': 1.5537222623825073, 'learning_rate': 0.0004222222222222222, 'epoch': 3.0}
 60%|██████    | 27/45 [00:03<00:02,  6.66it/s] 62%|██████▏   | 28/45 [00:04<00:02,  6.40it/s]                                               {'loss': 0.1301, 'grad_norm': 1.4637457132339478, 'learning_rate': 0.0004, 'epoch': 3.11}
 62%|██████▏   | 28/45 [00:04<00:02,  6.40it/s] 64%|██████▍   | 29/45 [00:04<00:02,  5.75it/s]                                               {'loss': 0.4888, 'grad_norm': 1.377051830291748, 'learning_rate': 0.00037777777777777777, 'epoch': 3.22}
 64%|██████▍   | 29/45 [00:04<00:02,  5.75it/s] 67%|██████▋   | 30/45 [00:04<00:02,  5.43it/s]                                               {'loss': 0.6648, 'grad_norm': 2.7269182205200195, 'learning_rate': 0.00035555555555555557, 'epoch': 3.33}
 67%|██████▋   | 30/45 [00:04<00:02,  5.43it/s] 69%|██████▉   | 31/45 [00:04<00:02,  5.07it/s]                                               {'loss': 0.2209, 'grad_norm': 0.8500816822052002, 'learning_rate': 0.0003333333333333333, 'epoch': 3.44}
 69%|██████▉   | 31/45 [00:04<00:02,  5.07it/s]                                               {'loss': 0.4178, 'grad_norm': 0.9336895942687988, 'learning_rate': 0.0003111111111111111, 'epoch': 3.56}
 71%|███████   | 32/45 [00:04<00:02,  5.07it/s] 73%|███████▎  | 33/45 [00:04<00:01,  6.43it/s]                                               {'loss': 0.3836, 'grad_norm': 2.036216974258423, 'learning_rate': 0.0002888888888888889, 'epoch': 3.67}
 73%|███████▎  | 33/45 [00:04<00:01,  6.43it/s] 76%|███████▌  | 34/45 [00:05<00:01,  6.16it/s]                                               {'loss': 0.6783, 'grad_norm': 2.2234621047973633, 'learning_rate': 0.0002666666666666667, 'epoch': 3.78}
 76%|███████▌  | 34/45 [00:05<00:01,  6.16it/s] 78%|███████▊  | 35/45 [00:05<00:01,  5.57it/s]                                               {'loss': 0.5408, 'grad_norm': 1.8756190538406372, 'learning_rate': 0.00024444444444444443, 'epoch': 3.89}
 78%|███████▊  | 35/45 [00:05<00:01,  5.57it/s] 80%|████████  | 36/45 [00:05<00:01,  5.96it/s]                                               {'loss': 0.1077, 'grad_norm': 1.3283673524856567, 'learning_rate': 0.0002222222222222222, 'epoch': 4.0}
 80%|████████  | 36/45 [00:05<00:01,  5.96it/s] 82%|████████▏ | 37/45 [00:05<00:01,  5.24it/s]                                               {'loss': 0.3636, 'grad_norm': 0.8012057542800903, 'learning_rate': 0.0002, 'epoch': 4.11}
 82%|████████▏ | 37/45 [00:05<00:01,  5.24it/s] 84%|████████▍ | 38/45 [00:05<00:01,  4.96it/s]                                               {'loss': 0.2447, 'grad_norm': 0.9427812695503235, 'learning_rate': 0.00017777777777777779, 'epoch': 4.22}
 84%|████████▍ | 38/45 [00:05<00:01,  4.96it/s] 87%|████████▋ | 39/45 [00:06<00:01,  4.80it/s]                                               {'loss': 0.5513, 'grad_norm': 2.0514934062957764, 'learning_rate': 0.00015555555555555556, 'epoch': 4.33}
 87%|████████▋ | 39/45 [00:06<00:01,  4.80it/s] 89%|████████▉ | 40/45 [00:06<00:01,  4.40it/s]                                               {'loss': 0.2106, 'grad_norm': 1.217154860496521, 'learning_rate': 0.00013333333333333334, 'epoch': 4.44}
 89%|████████▉ | 40/45 [00:06<00:01,  4.40it/s] 91%|█████████ | 41/45 [00:06<00:00,  4.08it/s]                                               {'loss': 0.2602, 'grad_norm': 0.922982394695282, 'learning_rate': 0.0001111111111111111, 'epoch': 4.56}
 91%|█████████ | 41/45 [00:06<00:00,  4.08it/s] 93%|█████████▎| 42/45 [00:07<00:00,  3.82it/s]                                               {'loss': 0.4555, 'grad_norm': 1.631834864616394, 'learning_rate': 8.888888888888889e-05, 'epoch': 4.67}
 93%|█████████▎| 42/45 [00:07<00:00,  3.82it/s] 96%|█████████▌| 43/45 [00:07<00:00,  3.67it/s]                                               {'loss': 0.4298, 'grad_norm': 1.6741507053375244, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.78}
 96%|█████████▌| 43/45 [00:07<00:00,  3.67it/s] 98%|█████████▊| 44/45 [00:07<00:00,  3.57it/s]                                               {'loss': 0.4416, 'grad_norm': 1.5580620765686035, 'learning_rate': 4.4444444444444447e-05, 'epoch': 4.89}
 98%|█████████▊| 44/45 [00:07<00:00,  3.57it/s]100%|██████████| 45/45 [00:07<00:00,  4.15it/s]                                               {'loss': 0.413, 'grad_norm': 1.8754023313522339, 'learning_rate': 2.2222222222222223e-05, 'epoch': 5.0}
100%|██████████| 45/45 [00:07<00:00,  4.15it/s]                                               {'train_runtime': 7.894, 'train_samples_per_second': 86.141, 'train_steps_per_second': 5.701, 'train_loss': 0.4471464405457179, 'epoch': 5.0}
100%|██████████| 45/45 [00:07<00:00,  4.15it/s]100%|██████████| 45/45 [00:07<00:00,  5.70it/s]
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:02,  9.85it/s]                                              {'loss': 0.5366, 'grad_norm': 1.655131459236145, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02,  9.85it/s]                                              {'loss': 0.6708, 'grad_norm': 0.6093798875808716, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02,  9.85it/s] 10%|█         | 3/30 [00:00<00:02, 11.30it/s]                                              {'loss': 0.7076, 'grad_norm': 9.179622650146484, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.30it/s]                                              {'loss': 0.6447, 'grad_norm': 0.6553472280502319, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.30it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.09it/s]                                              {'loss': 0.6284, 'grad_norm': 0.5341688990592957, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.09it/s]                                              {'loss': 0.6211, 'grad_norm': 3.8544609546661377, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.09it/s] 23%|██▎       | 7/30 [00:00<00:02, 10.35it/s]                                              {'loss': 0.634, 'grad_norm': 0.41650745272636414, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02, 10.35it/s]                                              {'loss': 0.6669, 'grad_norm': 0.5535147190093994, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02, 10.35it/s] 30%|███       | 9/30 [00:00<00:02, 10.37it/s]                                              {'loss': 0.608, 'grad_norm': 0.44269493222236633, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:02, 10.37it/s]                                              {'loss': 0.6284, 'grad_norm': 0.3764672577381134, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 10.37it/s] 37%|███▋      | 11/30 [00:01<00:01, 10.67it/s]                                               {'loss': 0.6387, 'grad_norm': 0.41105249524116516, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 10.67it/s]                                               {'loss': 0.5214, 'grad_norm': 0.9574406147003174, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 10.67it/s] 43%|████▎     | 13/30 [00:01<00:01, 10.74it/s]                                               {'loss': 0.5779, 'grad_norm': 0.6693068146705627, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 10.74it/s]                                               {'loss': 0.5575, 'grad_norm': 0.5872660875320435, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 10.74it/s] 50%|█████     | 15/30 [00:01<00:01, 11.13it/s]                                               {'loss': 0.6635, 'grad_norm': 1.7458643913269043, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 11.13it/s]                                               {'loss': 0.5164, 'grad_norm': 0.7626345157623291, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.13it/s] 57%|█████▋    | 17/30 [00:01<00:01, 11.26it/s]                                               {'loss': 0.609, 'grad_norm': 0.7579397559165955, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.26it/s]                                               {'loss': 0.5806, 'grad_norm': 1.264298915863037, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 11.26it/s] 63%|██████▎   | 19/30 [00:01<00:00, 12.70it/s]                                               {'loss': 0.5396, 'grad_norm': 0.9186835289001465, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 12.70it/s]                                               {'loss': 0.593, 'grad_norm': 0.8660960793495178, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 12.70it/s] 70%|███████   | 21/30 [00:01<00:00, 12.08it/s]                                               {'loss': 0.5586, 'grad_norm': 0.8845042586326599, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 12.08it/s]                                               {'loss': 0.5102, 'grad_norm': 1.3500440120697021, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 12.08it/s] 77%|███████▋  | 23/30 [00:02<00:00, 12.08it/s]                                               {'loss': 0.4978, 'grad_norm': 1.1353507041931152, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 12.08it/s]                                               {'loss': 0.4524, 'grad_norm': 2.5504238605499268, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 12.08it/s] 83%|████████▎ | 25/30 [00:02<00:00, 13.00it/s]                                               {'loss': 0.4325, 'grad_norm': 1.3523354530334473, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 13.00it/s]                                               {'loss': 0.43, 'grad_norm': 1.6003901958465576, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.00it/s] 90%|█████████ | 27/30 [00:02<00:00, 12.76it/s]                                               {'loss': 0.4709, 'grad_norm': 1.4250102043151855, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 12.76it/s]                                               {'loss': 0.4794, 'grad_norm': 1.394262433052063, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.76it/s] 97%|█████████▋| 29/30 [00:02<00:00, 12.63it/s]                                               {'loss': 0.3691, 'grad_norm': 1.3393038511276245, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.63it/s]                                               {'loss': 0.4542, 'grad_norm': 1.809830665588379, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.63it/s]                                               {'train_runtime': 2.6697, 'train_samples_per_second': 159.193, 'train_steps_per_second': 11.237, 'train_loss': 0.559970102707545, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.63it/s]100%|██████████| 30/30 [00:02<00:00, 11.24it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  5%|▍         | 3/66 [00:00<00:02, 21.96it/s]  9%|▉         | 6/66 [00:00<00:04, 13.87it/s] 12%|█▏        | 8/66 [00:00<00:04, 14.07it/s] 15%|█▌        | 10/66 [00:00<00:03, 15.29it/s] 18%|█▊        | 12/66 [00:00<00:03, 16.37it/s] 21%|██        | 14/66 [00:00<00:03, 15.79it/s] 24%|██▍       | 16/66 [00:01<00:03, 15.96it/s] 27%|██▋       | 18/66 [00:01<00:02, 16.38it/s] 30%|███       | 20/66 [00:01<00:02, 16.82it/s] 33%|███▎      | 22/66 [00:01<00:02, 16.09it/s] 38%|███▊      | 25/66 [00:01<00:02, 18.31it/s] 42%|████▏     | 28/66 [00:01<00:02, 18.14it/s] 45%|████▌     | 30/66 [00:01<00:02, 16.99it/s] 48%|████▊     | 32/66 [00:01<00:02, 16.75it/s] 52%|█████▏    | 34/66 [00:02<00:01, 16.05it/s] 55%|█████▍    | 36/66 [00:02<00:01, 15.56it/s] 58%|█████▊    | 38/66 [00:02<00:01, 16.39it/s] 61%|██████    | 40/66 [00:02<00:01, 15.67it/s] 64%|██████▎   | 42/66 [00:02<00:01, 15.16it/s] 68%|██████▊   | 45/66 [00:02<00:01, 16.57it/s] 71%|███████   | 47/66 [00:02<00:01, 14.28it/s] 74%|███████▍  | 49/66 [00:03<00:01, 12.54it/s] 77%|███████▋  | 51/66 [00:03<00:01, 13.73it/s] 80%|████████  | 53/66 [00:03<00:00, 13.04it/s] 83%|████████▎ | 55/66 [00:03<00:00, 11.88it/s] 86%|████████▋ | 57/66 [00:03<00:00, 11.61it/s] 89%|████████▉ | 59/66 [00:03<00:00, 12.93it/s] 92%|█████████▏| 61/66 [00:04<00:00, 12.11it/s] 95%|█████████▌| 63/66 [00:04<00:00, 11.91it/s] 98%|█████████▊| 65/66 [00:04<00:00, 12.04it/s]100%|██████████| 66/66 [00:04<00:00, 14.63it/s]
{'eval_loss': 0.6240277886390686, 'eval_model_preparation_time': 0.0169, 'eval_acc': 0.697986577181208, 'eval_runtime': 4.5805, 'eval_samples_per_second': 227.705, 'eval_steps_per_second': 14.409}
ROUND:9
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:11,  2.49it/s]                                              {'loss': 0.7673, 'grad_norm': 2.324352502822876, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:11,  2.49it/s]  7%|▋         | 2/30 [00:00<00:13,  2.11it/s]                                              {'loss': 0.3848, 'grad_norm': 1.7231159210205078, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:13,  2.11it/s]                                              {'loss': 0.1671, 'grad_norm': 1.46430242061615, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:12,  2.11it/s] 13%|█▎        | 4/30 [00:01<00:05,  4.38it/s]                                              {'loss': 0.2607, 'grad_norm': 0.6336352825164795, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:05,  4.38it/s]                                              {'loss': 0.0207, 'grad_norm': 0.3102528154850006, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.38it/s] 20%|██        | 6/30 [00:01<00:03,  6.19it/s]                                              {'loss': 0.0114, 'grad_norm': 0.13145917654037476, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:03,  6.19it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.63it/s]                                              {'loss': 0.0065, 'grad_norm': 0.08149825036525726, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.63it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.84it/s]                                              {'loss': 0.0049, 'grad_norm': 0.06351232528686523, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.84it/s] 30%|███       | 9/30 [00:01<00:03,  6.94it/s]                                              {'loss': 0.0048, 'grad_norm': 0.07212049514055252, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.94it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.61it/s]                                               {'loss': 0.2965, 'grad_norm': 2.6444661617279053, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.61it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.03it/s]                                               {'loss': 0.0035, 'grad_norm': 0.05659119412302971, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.03it/s]                                               {'loss': 0.0061, 'grad_norm': 0.1054021567106247, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  7.03it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.62it/s]                                               {'loss': 0.0048, 'grad_norm': 0.11867555975914001, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.62it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.86it/s]                                               {'loss': 0.007, 'grad_norm': 0.16971291601657867, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.86it/s]                                               {'loss': 0.3646, 'grad_norm': 1.2432451248168945, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.86it/s] 53%|█████▎    | 16/30 [00:02<00:01,  8.32it/s]                                               {'loss': 0.0091, 'grad_norm': 0.2549802362918854, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.32it/s]                                               {'loss': 0.0236, 'grad_norm': 0.5511981844902039, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  8.32it/s] 60%|██████    | 18/30 [00:02<00:01, 10.10it/s]                                               {'loss': 0.0104, 'grad_norm': 0.3215343654155731, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 10.10it/s]                                               {'loss': 0.0063, 'grad_norm': 0.17571087181568146, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01, 10.10it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.20it/s]                                               {'loss': 0.0101, 'grad_norm': 0.26096969842910767, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.20it/s] 70%|███████   | 21/30 [00:03<00:01,  8.31it/s]                                               {'loss': 0.0079, 'grad_norm': 0.1875367909669876, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  8.31it/s] 73%|███████▎  | 22/30 [00:03<00:00,  8.50it/s]                                               {'loss': 0.0052, 'grad_norm': 0.10664251446723938, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00,  8.50it/s] 77%|███████▋  | 23/30 [00:03<00:00,  8.51it/s]                                               {'loss': 0.4217, 'grad_norm': 2.1280486583709717, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  8.51it/s]                                               {'loss': 0.003, 'grad_norm': 0.05316421389579773, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  8.51it/s] 83%|████████▎ | 25/30 [00:03<00:00,  9.38it/s]                                               {'loss': 0.0096, 'grad_norm': 0.20647259056568146, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  9.38it/s] 87%|████████▋ | 26/30 [00:03<00:00,  8.54it/s]                                               {'loss': 0.0043, 'grad_norm': 0.08533080667257309, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  8.54it/s]                                               {'loss': 0.3469, 'grad_norm': 0.673567533493042, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  8.54it/s] 93%|█████████▎| 28/30 [00:03<00:00,  8.98it/s]                                               {'loss': 0.0038, 'grad_norm': 0.08484795689582825, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  8.98it/s]                                               {'loss': 0.0048, 'grad_norm': 0.10135497897863388, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  8.98it/s]100%|██████████| 30/30 [00:03<00:00, 10.86it/s]                                               {'loss': 0.0079, 'grad_norm': 0.18344295024871826, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 10.86it/s]                                               {'train_runtime': 4.0848, 'train_samples_per_second': 104.045, 'train_steps_per_second': 7.344, 'train_loss': 0.10617401854445536, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00, 10.86it/s]100%|██████████| 30/30 [00:04<00:00,  7.35it/s]
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  7.98it/s]                                              {'loss': 0.6263, 'grad_norm': 1.0468419790267944, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  7.98it/s]                                              {'loss': 0.684, 'grad_norm': 0.6869457960128784, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.98it/s] 10%|█         | 3/30 [00:00<00:02,  9.22it/s]                                              {'loss': 0.6924, 'grad_norm': 0.6657269597053528, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02,  9.22it/s]                                              {'loss': 0.6559, 'grad_norm': 0.7111095786094666, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  9.22it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.87it/s]                                              {'loss': 0.6274, 'grad_norm': 0.823758065700531, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  9.87it/s]                                              {'loss': 0.627, 'grad_norm': 0.8856129050254822, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02,  9.87it/s] 23%|██▎       | 7/30 [00:00<00:02, 11.39it/s]                                              {'loss': 0.6485, 'grad_norm': 0.43002671003341675, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02, 11.39it/s]                                              {'loss': 0.639, 'grad_norm': 0.6486412882804871, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 11.39it/s] 30%|███       | 9/30 [00:00<00:02,  8.76it/s]                                              {'loss': 0.6485, 'grad_norm': 2.1813833713531494, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:02,  8.76it/s] 33%|███▎      | 10/30 [00:01<00:02,  8.95it/s]                                               {'loss': 0.7036, 'grad_norm': 0.48045614361763, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.95it/s] 37%|███▋      | 11/30 [00:01<00:02,  8.39it/s]                                               {'loss': 0.6883, 'grad_norm': 0.5351964831352234, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.39it/s]                                               {'loss': 0.6425, 'grad_norm': 1.212218165397644, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.39it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.33it/s]                                               {'loss': 0.7368, 'grad_norm': 0.8535208702087402, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  9.33it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.49it/s]                                               {'loss': 0.7019, 'grad_norm': 0.44101929664611816, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.49it/s]                                               {'loss': 0.4866, 'grad_norm': 0.7004134058952332, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  8.49it/s] 53%|█████▎    | 16/30 [00:01<00:01,  8.72it/s]                                               {'loss': 0.6997, 'grad_norm': 2.301762580871582, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  8.72it/s] 57%|█████▋    | 17/30 [00:01<00:01,  8.19it/s]                                               {'loss': 0.723, 'grad_norm': 1.4142217636108398, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01,  8.19it/s]                                               {'loss': 0.6453, 'grad_norm': 0.5581639409065247, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01,  8.19it/s] 63%|██████▎   | 19/30 [00:02<00:01, 10.45it/s]                                               {'loss': 0.6934, 'grad_norm': 1.3990886211395264, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01, 10.45it/s]                                               {'loss': 0.6314, 'grad_norm': 0.3939177691936493, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 10.45it/s] 70%|███████   | 21/30 [00:02<00:00, 11.22it/s]                                               {'loss': 0.6487, 'grad_norm': 1.0332586765289307, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.22it/s]                                               {'loss': 0.6413, 'grad_norm': 0.39341577887535095, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.22it/s] 77%|███████▋  | 23/30 [00:02<00:00, 11.56it/s]                                               {'loss': 0.6188, 'grad_norm': 0.8500677347183228, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 11.56it/s]                                               {'loss': 0.5978, 'grad_norm': 0.738731861114502, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 11.56it/s] 83%|████████▎ | 25/30 [00:02<00:00, 12.59it/s]                                               {'loss': 0.6155, 'grad_norm': 0.8232938647270203, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.59it/s]                                               {'loss': 0.6087, 'grad_norm': 0.48797738552093506, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 12.59it/s] 90%|█████████ | 27/30 [00:02<00:00, 12.37it/s]                                               {'loss': 0.6065, 'grad_norm': 0.49070680141448975, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 12.37it/s]                                               {'loss': 0.6477, 'grad_norm': 1.5408146381378174, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.37it/s] 97%|█████████▋| 29/30 [00:02<00:00, 12.46it/s]                                               {'loss': 0.5356, 'grad_norm': 0.5971949100494385, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.46it/s]                                               {'loss': 0.7108, 'grad_norm': 1.4759728908538818, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.46it/s]                                               {'train_runtime': 2.9852, 'train_samples_per_second': 142.368, 'train_steps_per_second': 10.05, 'train_loss': 0.647764723499616, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.46it/s]100%|██████████| 30/30 [00:02<00:00, 10.05it/s]
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.72it/s]                                              {'loss': 0.643, 'grad_norm': 1.5970455408096313, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.72it/s]  7%|▋         | 2/30 [00:00<00:07,  3.66it/s]                                              {'loss': 0.4229, 'grad_norm': 1.7091164588928223, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.66it/s] 10%|█         | 3/30 [00:00<00:07,  3.65it/s]                                              {'loss': 0.3196, 'grad_norm': 0.7565642595291138, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.65it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.64it/s]                                              {'loss': 0.0639, 'grad_norm': 0.7495003342628479, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.64it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.67it/s]                                              {'loss': 0.4324, 'grad_norm': 1.523667812347412, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.67it/s] 20%|██        | 6/30 [00:01<00:05,  4.57it/s]                                              {'loss': 0.0068, 'grad_norm': 0.09678620100021362, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.57it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.24it/s]                                              {'loss': 0.3248, 'grad_norm': 0.7622584104537964, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.24it/s] 27%|██▋       | 8/30 [00:01<00:05,  4.34it/s]                                              {'loss': 1.2126, 'grad_norm': 4.061354160308838, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  4.34it/s] 30%|███       | 9/30 [00:02<00:04,  4.46it/s]                                              {'loss': 0.0238, 'grad_norm': 0.31969091296195984, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.46it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.59it/s]                                               {'loss': 0.2988, 'grad_norm': 0.7521668672561646, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.59it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.51it/s]                                               {'loss': 0.0272, 'grad_norm': 0.5433248281478882, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.51it/s]                                               {'loss': 0.0165, 'grad_norm': 0.38696008920669556, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.51it/s] 43%|████▎     | 13/30 [00:02<00:02,  5.71it/s]                                               {'loss': 0.5138, 'grad_norm': 2.050790786743164, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  5.71it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.06it/s]                                               {'loss': 0.0306, 'grad_norm': 0.46110522747039795, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:02,  6.06it/s] 50%|█████     | 15/30 [00:03<00:02,  5.84it/s]                                               {'loss': 0.5322, 'grad_norm': 1.8206273317337036, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.84it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.98it/s]                                               {'loss': 0.2419, 'grad_norm': 1.5645745992660522, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.98it/s] 57%|█████▋    | 17/30 [00:03<00:02,  6.30it/s]                                               {'loss': 0.0611, 'grad_norm': 0.8691020607948303, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  6.30it/s] 60%|██████    | 18/30 [00:03<00:01,  6.43it/s]                                               {'loss': 0.6232, 'grad_norm': 3.64039945602417, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  6.43it/s] 63%|██████▎   | 19/30 [00:03<00:02,  5.42it/s]                                               {'loss': 0.0405, 'grad_norm': 0.6387591361999512, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  5.42it/s] 67%|██████▋   | 20/30 [00:04<00:01,  5.44it/s]                                               {'loss': 0.7082, 'grad_norm': 5.317303657531738, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  5.44it/s] 70%|███████   | 21/30 [00:04<00:01,  5.23it/s]                                               {'loss': 0.2359, 'grad_norm': 0.8276389241218567, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.23it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.97it/s]                                               {'loss': 0.2155, 'grad_norm': 1.6714872121810913, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.97it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.70it/s]                                               {'loss': 0.1837, 'grad_norm': 0.9432709217071533, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.70it/s] 80%|████████  | 24/30 [00:04<00:01,  4.89it/s]                                               {'loss': 0.0658, 'grad_norm': 1.2393639087677002, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  4.89it/s] 83%|████████▎ | 25/30 [00:05<00:01,  4.27it/s]                                               {'loss': 0.24, 'grad_norm': 1.0687609910964966, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  4.27it/s] 87%|████████▋ | 26/30 [00:05<00:00,  4.35it/s]                                               {'loss': 0.0632, 'grad_norm': 0.8865447044372559, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  4.35it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.39it/s]                                               {'loss': 0.1792, 'grad_norm': 1.004350185394287, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.39it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.61it/s]                                               {'loss': 0.3074, 'grad_norm': 2.9014227390289307, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.61it/s] 97%|█████████▋| 29/30 [00:05<00:00,  5.15it/s]                                               {'loss': 0.2004, 'grad_norm': 1.6762412786483765, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  5.15it/s]100%|██████████| 30/30 [00:06<00:00,  5.91it/s]                                               {'loss': 0.5764, 'grad_norm': 3.2097575664520264, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.91it/s]                                               {'train_runtime': 6.3797, 'train_samples_per_second': 66.618, 'train_steps_per_second': 4.702, 'train_loss': 0.2937250392821928, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.91it/s]100%|██████████| 30/30 [00:06<00:00,  4.70it/s]
CLIENT:5
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.76it/s]                                              {'loss': 0.6929, 'grad_norm': 1.0028884410858154, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.76it/s]  7%|▋         | 2/30 [00:00<00:06,  4.38it/s]                                              {'loss': 0.6982, 'grad_norm': 0.5244592428207397, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.38it/s] 10%|█         | 3/30 [00:00<00:06,  4.14it/s]                                              {'loss': 0.7089, 'grad_norm': 0.423352986574173, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.14it/s] 13%|█▎        | 4/30 [00:00<00:06,  4.32it/s]                                              {'loss': 0.6395, 'grad_norm': 0.4967537820339203, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:06,  4.32it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.62it/s]                                              {'loss': 0.652, 'grad_norm': 0.4646725654602051, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.62it/s]                                              {'loss': 0.6643, 'grad_norm': 1.3605492115020752, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.62it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.18it/s]                                              {'loss': 0.6704, 'grad_norm': 0.5549304485321045, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.18it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.26it/s]                                              {'loss': 0.63, 'grad_norm': 0.48023685812950134, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.26it/s] 30%|███       | 9/30 [00:01<00:03,  5.42it/s]                                              {'loss': 0.6238, 'grad_norm': 0.41079288721084595, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.42it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.84it/s]                                               {'loss': 0.6871, 'grad_norm': 0.2954423129558563, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.84it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.79it/s]                                               {'loss': 0.6681, 'grad_norm': 0.35877060890197754, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.79it/s] 40%|████      | 12/30 [00:02<00:02,  6.46it/s]                                               {'loss': 0.8282, 'grad_norm': 2.8842461109161377, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.46it/s] 43%|████▎     | 13/30 [00:02<00:02,  5.77it/s]                                               {'loss': 0.6543, 'grad_norm': 0.5069740414619446, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  5.77it/s] 47%|████▋     | 14/30 [00:02<00:03,  4.96it/s]                                               {'loss': 0.7087, 'grad_norm': 0.7037930488586426, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  4.96it/s] 50%|█████     | 15/30 [00:02<00:03,  4.80it/s]                                               {'loss': 0.5118, 'grad_norm': 0.6839377284049988, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:03,  4.80it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.44it/s]                                               {'loss': 0.6364, 'grad_norm': 0.6477299332618713, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.44it/s] 57%|█████▋    | 17/30 [00:03<00:03,  4.19it/s]                                               {'loss': 0.685, 'grad_norm': 0.6652712821960449, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  4.19it/s]                                               {'loss': 0.6269, 'grad_norm': 0.5966200232505798, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.19it/s] 63%|██████▎   | 19/30 [00:03<00:02,  5.13it/s]                                               {'loss': 0.7463, 'grad_norm': 1.1851773262023926, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  5.13it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.82it/s]                                               {'loss': 0.6168, 'grad_norm': 0.3174856901168823, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.82it/s] 70%|███████   | 21/30 [00:04<00:01,  4.69it/s]                                               {'loss': 0.6527, 'grad_norm': 0.3945375382900238, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  4.69it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.56it/s]                                               {'loss': 0.6216, 'grad_norm': 0.49768248200416565, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.56it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.54it/s]                                               {'loss': 0.5862, 'grad_norm': 0.5041768550872803, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.54it/s]                                               {'loss': 0.6338, 'grad_norm': 0.7137818932533264, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  4.54it/s] 83%|████████▎ | 25/30 [00:05<00:00,  5.31it/s]                                               {'loss': 0.6794, 'grad_norm': 0.8032237887382507, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  5.31it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.07it/s]                                               {'loss': 0.5916, 'grad_norm': 0.4647916257381439, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.07it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.89it/s]                                               {'loss': 0.5878, 'grad_norm': 0.4998962879180908, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.89it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.51it/s]                                               {'loss': 0.6098, 'grad_norm': 0.4846521317958832, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.51it/s] 97%|█████████▋| 29/30 [00:06<00:00,  4.19it/s]                                               {'loss': 0.5671, 'grad_norm': 0.538491427898407, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  4.19it/s]100%|██████████| 30/30 [00:06<00:00,  4.91it/s]                                               {'loss': 0.5593, 'grad_norm': 0.746777355670929, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.91it/s]                                               {'train_runtime': 6.2242, 'train_samples_per_second': 68.282, 'train_steps_per_second': 4.82, 'train_loss': 0.6479575475056966, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.91it/s]100%|██████████| 30/30 [00:06<00:00,  4.83it/s]
CLIENT:52
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.38it/s]                                              {'loss': 0.6299, 'grad_norm': 0.938681423664093, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.38it/s]  7%|▋         | 2/30 [00:00<00:05,  5.49it/s]                                              {'loss': 0.6606, 'grad_norm': 0.5809785723686218, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.49it/s] 10%|█         | 3/30 [00:00<00:04,  6.31it/s]                                              {'loss': 0.7143, 'grad_norm': 0.4294975996017456, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.31it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.90it/s]                                              {'loss': 0.6274, 'grad_norm': 0.7335762977600098, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.90it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.12it/s]                                              {'loss': 0.6327, 'grad_norm': 0.4791067838668823, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.12it/s]                                              {'loss': 0.7204, 'grad_norm': 1.6773371696472168, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.12it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.09it/s]                                              {'loss': 0.6956, 'grad_norm': 0.4336056113243103, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.09it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.62it/s]                                              {'loss': 0.6286, 'grad_norm': 0.5346493721008301, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.62it/s] 30%|███       | 9/30 [00:01<00:02,  7.16it/s]                                              {'loss': 0.6309, 'grad_norm': 0.45465055108070374, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.16it/s] 33%|███▎      | 10/30 [00:01<00:02,  6.97it/s]                                               {'loss': 0.6915, 'grad_norm': 0.5555039048194885, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  6.97it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.38it/s]                                               {'loss': 0.6679, 'grad_norm': 0.3047730624675751, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.38it/s]                                               {'loss': 0.708, 'grad_norm': 2.125380754470825, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.38it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.21it/s]                                               {'loss': 0.6454, 'grad_norm': 0.6523898243904114, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.21it/s] 47%|████▋     | 14/30 [00:01<00:02,  8.00it/s]                                               {'loss': 0.6707, 'grad_norm': 0.4705773890018463, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  8.00it/s] 50%|█████     | 15/30 [00:02<00:01,  7.77it/s]                                               {'loss': 0.5253, 'grad_norm': 0.9008567333221436, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.77it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.42it/s]                                               {'loss': 0.6244, 'grad_norm': 0.5963028073310852, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.42it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.59it/s]                                               {'loss': 0.6882, 'grad_norm': 0.5406887531280518, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.59it/s]                                               {'loss': 0.6171, 'grad_norm': 0.5588102340698242, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.59it/s] 63%|██████▎   | 19/30 [00:02<00:01, 10.13it/s]                                               {'loss': 0.7108, 'grad_norm': 1.7075421810150146, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01, 10.13it/s]                                               {'loss': 0.6139, 'grad_norm': 0.38352155685424805, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 10.13it/s] 70%|███████   | 21/30 [00:02<00:00, 11.08it/s]                                               {'loss': 0.6339, 'grad_norm': 0.37475454807281494, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.08it/s]                                               {'loss': 0.606, 'grad_norm': 0.3555987775325775, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.08it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.92it/s]                                               {'loss': 0.577, 'grad_norm': 0.5976005792617798, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  9.92it/s]                                               {'loss': 0.5596, 'grad_norm': 0.6641820073127747, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  9.92it/s] 83%|████████▎ | 25/30 [00:02<00:00, 10.58it/s]                                               {'loss': 0.6182, 'grad_norm': 0.666619062423706, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 10.58it/s]                                               {'loss': 0.6054, 'grad_norm': 0.5453056693077087, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 10.58it/s] 90%|█████████ | 27/30 [00:03<00:00,  9.23it/s]                                               {'loss': 0.5997, 'grad_norm': 0.6102131605148315, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  9.23it/s] 93%|█████████▎| 28/30 [00:03<00:00,  8.93it/s]                                               {'loss': 0.6004, 'grad_norm': 0.5529793500900269, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  8.93it/s] 97%|█████████▋| 29/30 [00:03<00:00,  8.89it/s]                                               {'loss': 0.5199, 'grad_norm': 0.7548390030860901, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  8.89it/s]100%|██████████| 30/30 [00:03<00:00,  8.82it/s]                                               {'loss': 0.6151, 'grad_norm': 0.8424431681632996, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.82it/s]                                               {'train_runtime': 3.9313, 'train_samples_per_second': 108.108, 'train_steps_per_second': 7.631, 'train_loss': 0.6346327741940816, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.82it/s]100%|██████████| 30/30 [00:03<00:00,  7.63it/s]
CLIENT:41
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.30it/s]                                              {'loss': 0.6337, 'grad_norm': 0.7721432447433472, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.30it/s]  7%|▋         | 2/30 [00:00<00:07,  3.74it/s]                                              {'loss': 0.509, 'grad_norm': 1.061020016670227, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.74it/s] 10%|█         | 3/30 [00:00<00:06,  4.30it/s]                                              {'loss': 0.501, 'grad_norm': 0.9313415884971619, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.30it/s] 13%|█▎        | 4/30 [00:01<00:06,  4.00it/s]                                              {'loss': 0.8415, 'grad_norm': 2.2175180912017822, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:06,  4.00it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.99it/s]                                              {'loss': 0.7738, 'grad_norm': 3.152039051055908, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.99it/s] 20%|██        | 6/30 [00:01<00:05,  4.70it/s]                                              {'loss': 1.0044, 'grad_norm': 3.2953460216522217, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.70it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.48it/s]                                              {'loss': 0.5341, 'grad_norm': 0.7862219214439392, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.48it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.75it/s]                                              {'loss': 0.8018, 'grad_norm': 1.8613382577896118, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.75it/s] 30%|███       | 9/30 [00:02<00:04,  4.39it/s]                                              {'loss': 0.5344, 'grad_norm': 0.6361895203590393, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.39it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.66it/s]                                               {'loss': 0.5162, 'grad_norm': 1.2349812984466553, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.66it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.42it/s]                                               {'loss': 0.5545, 'grad_norm': 1.0335896015167236, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.42it/s]                                               {'loss': 0.3953, 'grad_norm': 1.5422186851501465, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:04,  4.42it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.02it/s]                                               {'loss': 0.5239, 'grad_norm': 1.4523043632507324, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.02it/s] 47%|████▋     | 14/30 [00:03<00:03,  5.00it/s]                                               {'loss': 0.5641, 'grad_norm': 0.7921642661094666, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  5.00it/s] 50%|█████     | 15/30 [00:03<00:03,  4.83it/s]                                               {'loss': 1.045, 'grad_norm': 4.055854320526123, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.83it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.26it/s]                                               {'loss': 0.4753, 'grad_norm': 1.5284425020217896, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.26it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.42it/s]                                               {'loss': 0.5164, 'grad_norm': 1.2128866910934448, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.42it/s]                                               {'loss': 0.4153, 'grad_norm': 1.5911146402359009, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.42it/s] 63%|██████▎   | 19/30 [00:03<00:01,  5.97it/s]                                               {'loss': 0.4946, 'grad_norm': 1.0971540212631226, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  5.97it/s] 67%|██████▋   | 20/30 [00:04<00:01,  6.07it/s]                                               {'loss': 0.5655, 'grad_norm': 1.7483675479888916, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  6.07it/s] 70%|███████   | 21/30 [00:04<00:01,  5.57it/s]                                               {'loss': 0.5476, 'grad_norm': 1.5103119611740112, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.57it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.69it/s]                                               {'loss': 0.6351, 'grad_norm': 1.523566722869873, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.69it/s] 77%|███████▋  | 23/30 [00:04<00:01,  5.39it/s]                                               {'loss': 0.6838, 'grad_norm': 2.2528491020202637, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  5.39it/s]                                               {'loss': 0.3752, 'grad_norm': 3.3950369358062744, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  5.39it/s] 83%|████████▎ | 25/30 [00:04<00:00,  6.97it/s]                                               {'loss': 0.306, 'grad_norm': 1.7592623233795166, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  6.97it/s] 87%|████████▋ | 26/30 [00:04<00:00,  7.43it/s]                                               {'loss': 0.5659, 'grad_norm': 2.2342631816864014, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  7.43it/s] 90%|█████████ | 27/30 [00:05<00:00,  7.61it/s]                                               {'loss': 0.5939, 'grad_norm': 1.7593472003936768, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  7.61it/s] 93%|█████████▎| 28/30 [00:05<00:00,  7.89it/s]                                               {'loss': 0.6674, 'grad_norm': 2.0195364952087402, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  7.89it/s]                                               {'loss': 0.4812, 'grad_norm': 1.2213250398635864, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  7.89it/s]100%|██████████| 30/30 [00:05<00:00, 10.24it/s]                                               {'loss': 0.6272, 'grad_norm': 1.998733639717102, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00, 10.24it/s]                                               {'train_runtime': 5.3928, 'train_samples_per_second': 78.809, 'train_steps_per_second': 5.563, 'train_loss': 0.5894218613704045, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00, 10.24it/s]100%|██████████| 30/30 [00:05<00:00,  5.56it/s]
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  7.24it/s]                                              {'loss': 0.6838, 'grad_norm': 2.5699143409729004, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  7.24it/s]  7%|▋         | 2/30 [00:00<00:04,  6.63it/s]                                              {'loss': 0.397, 'grad_norm': 1.784855604171753, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.63it/s] 10%|█         | 3/30 [00:00<00:03,  7.12it/s]                                              {'loss': 0.1275, 'grad_norm': 1.4599851369857788, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.12it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.82it/s]                                              {'loss': 0.3323, 'grad_norm': 0.7315665483474731, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.82it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.57it/s]                                              {'loss': 0.2803, 'grad_norm': 1.198330044746399, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.57it/s] 20%|██        | 6/30 [00:01<00:05,  4.09it/s]                                              {'loss': 0.0146, 'grad_norm': 0.3057197332382202, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.09it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.69it/s]                                              {'loss': 0.2276, 'grad_norm': 5.106765270233154, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.69it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.25it/s]                                              {'loss': 0.0214, 'grad_norm': 0.6264418363571167, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.25it/s] 30%|███       | 9/30 [00:01<00:03,  5.72it/s]                                              {'loss': 0.3048, 'grad_norm': 1.3988261222839355, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.72it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.00it/s]                                               {'loss': 0.0164, 'grad_norm': 0.4623366892337799, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.00it/s]                                               {'loss': 0.0117, 'grad_norm': 0.1908743977546692, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  6.00it/s] 40%|████      | 12/30 [00:01<00:02,  8.77it/s]                                               {'loss': 0.017, 'grad_norm': 0.25122442841529846, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.77it/s]                                               {'loss': 0.0126, 'grad_norm': 0.16978418827056885, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  8.77it/s] 47%|████▋     | 14/30 [00:02<00:01,  9.37it/s]                                               {'loss': 0.0093, 'grad_norm': 0.13502313196659088, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  9.37it/s]                                               {'loss': 0.3526, 'grad_norm': 0.9722100496292114, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  9.37it/s] 53%|█████▎    | 16/30 [00:02<00:01, 10.04it/s]                                               {'loss': 0.0074, 'grad_norm': 0.08083923906087875, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01, 10.04it/s]                                               {'loss': 0.2992, 'grad_norm': 0.7574546337127686, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01, 10.04it/s] 60%|██████    | 18/30 [00:02<00:01, 11.98it/s]                                               {'loss': 0.0087, 'grad_norm': 0.14262178540229797, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 11.98it/s]                                               {'loss': 0.0087, 'grad_norm': 0.12215401977300644, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 11.98it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.48it/s]                                               {'loss': 0.3105, 'grad_norm': 0.802345335483551, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.48it/s]                                               {'loss': 0.0138, 'grad_norm': 0.2159581035375595, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.48it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.05it/s]                                               {'loss': 0.0131, 'grad_norm': 0.17534801363945007, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.05it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.36it/s]                                               {'loss': 0.2636, 'grad_norm': 0.7634503245353699, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.36it/s] 80%|████████  | 24/30 [00:03<00:01,  5.73it/s]                                               {'loss': 0.0153, 'grad_norm': 0.18972992897033691, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:01,  5.73it/s] 83%|████████▎ | 25/30 [00:03<00:00,  5.30it/s]                                               {'loss': 0.5298, 'grad_norm': 2.1109824180603027, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  5.30it/s] 87%|████████▋ | 26/30 [00:04<00:00,  4.75it/s]                                               {'loss': 0.0188, 'grad_norm': 0.2376701682806015, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  4.75it/s] 90%|█████████ | 27/30 [00:04<00:00,  4.27it/s]                                               {'loss': 0.0211, 'grad_norm': 0.35855334997177124, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  4.27it/s] 93%|█████████▎| 28/30 [00:04<00:00,  4.21it/s]                                               {'loss': 0.0211, 'grad_norm': 0.2959679365158081, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  4.21it/s] 97%|█████████▋| 29/30 [00:04<00:00,  4.31it/s]                                               {'loss': 0.0212, 'grad_norm': 0.3092648684978485, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  4.31it/s]                                               {'loss': 0.0229, 'grad_norm': 0.34484121203422546, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  4.31it/s]                                               {'train_runtime': 5.0927, 'train_samples_per_second': 83.453, 'train_steps_per_second': 5.891, 'train_loss': 0.1461321212972204, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.31it/s]100%|██████████| 30/30 [00:05<00:00,  5.89it/s]
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.11it/s]                                              {'loss': 0.6024, 'grad_norm': 0.7909132242202759, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.11it/s]                                              {'loss': 0.6677, 'grad_norm': 0.7345206141471863, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.11it/s] 10%|█         | 3/30 [00:00<00:03,  8.09it/s]                                              {'loss': 0.6718, 'grad_norm': 0.6035677790641785, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.09it/s] 13%|█▎        | 4/30 [00:00<00:03,  8.16it/s]                                              {'loss': 0.6911, 'grad_norm': 0.7196609973907471, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  8.16it/s] 17%|█▋        | 5/30 [00:00<00:03,  8.31it/s]                                              {'loss': 0.6734, 'grad_norm': 0.6909453868865967, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  8.31it/s] 20%|██        | 6/30 [00:01<00:07,  3.10it/s]                                              {'loss': 0.9528, 'grad_norm': 1.9261449575424194, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:07,  3.10it/s]                                              {'loss': 0.6177, 'grad_norm': 0.6336856484413147, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:07,  3.10it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.76it/s]                                              {'loss': 0.7335, 'grad_norm': 0.8579719662666321, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.76it/s]                                              {'loss': 0.618, 'grad_norm': 0.5814564824104309, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.76it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.04it/s]                                               {'loss': 0.6309, 'grad_norm': 0.5356172919273376, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.04it/s]                                               {'loss': 0.6701, 'grad_norm': 0.42718788981437683, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  6.04it/s] 40%|████      | 12/30 [00:01<00:02,  7.95it/s]                                               {'loss': 0.6202, 'grad_norm': 1.5509998798370361, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.95it/s]                                               {'loss': 0.62, 'grad_norm': 0.7084111571311951, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.95it/s] 47%|████▋     | 14/30 [00:02<00:01,  8.86it/s]                                               {'loss': 0.6082, 'grad_norm': 0.5908825993537903, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  8.86it/s]                                               {'loss': 0.5853, 'grad_norm': 1.0633190870285034, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.86it/s] 53%|█████▎    | 16/30 [00:02<00:01,  9.60it/s]                                               {'loss': 0.626, 'grad_norm': 0.46507716178894043, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  9.60it/s]                                               {'loss': 0.5948, 'grad_norm': 0.8306229114532471, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  9.60it/s] 60%|██████    | 18/30 [00:02<00:01, 11.28it/s]                                               {'loss': 0.6304, 'grad_norm': 1.1671323776245117, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 11.28it/s]                                               {'loss': 0.566, 'grad_norm': 0.9520182013511658, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 11.28it/s] 67%|██████▋   | 20/30 [00:02<00:00, 11.36it/s]                                               {'loss': 0.6221, 'grad_norm': 0.7299205660820007, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 11.36it/s]                                               {'loss': 0.5573, 'grad_norm': 1.1205737590789795, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.36it/s] 73%|███████▎  | 22/30 [00:02<00:00, 11.81it/s]                                               {'loss': 0.6024, 'grad_norm': 0.9621992111206055, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.81it/s]                                               {'loss': 0.5594, 'grad_norm': 1.0180872678756714, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 11.81it/s] 80%|████████  | 24/30 [00:02<00:00, 13.09it/s]                                               {'loss': 0.5671, 'grad_norm': 1.3962172269821167, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 13.09it/s]                                               {'loss': 0.4568, 'grad_norm': 1.004740834236145, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 13.09it/s] 87%|████████▋ | 26/30 [00:02<00:00, 12.43it/s]                                               {'loss': 0.5145, 'grad_norm': 1.1110904216766357, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 12.43it/s]                                               {'loss': 0.4662, 'grad_norm': 1.382872462272644, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 12.43it/s] 93%|█████████▎| 28/30 [00:03<00:00, 12.48it/s]                                               {'loss': 0.4938, 'grad_norm': 1.1142964363098145, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 12.48it/s]                                               {'loss': 0.4767, 'grad_norm': 1.4651069641113281, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 12.48it/s]100%|██████████| 30/30 [00:03<00:00, 13.93it/s]                                               {'loss': 0.4545, 'grad_norm': 2.987980604171753, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.93it/s]                                               {'train_runtime': 3.4578, 'train_samples_per_second': 122.911, 'train_steps_per_second': 8.676, 'train_loss': 0.605039123694102, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.93it/s]100%|██████████| 30/30 [00:03<00:00,  8.68it/s]
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.59it/s]                                              {'loss': 0.5171, 'grad_norm': 1.0800448656082153, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.59it/s]  7%|▋         | 2/30 [00:00<00:06,  4.14it/s]                                              {'loss': 0.395, 'grad_norm': 1.417161226272583, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.14it/s] 10%|█         | 3/30 [00:00<00:05,  4.90it/s]                                              {'loss': 0.3766, 'grad_norm': 0.7506095170974731, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.90it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.55it/s]                                              {'loss': 0.2975, 'grad_norm': 0.8049095273017883, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.55it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.87it/s]                                              {'loss': 0.7689, 'grad_norm': 2.2452056407928467, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.87it/s] 20%|██        | 6/30 [00:01<00:03,  6.75it/s]                                              {'loss': 0.0349, 'grad_norm': 0.6037165522575378, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:03,  6.75it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.23it/s]                                              {'loss': 0.1353, 'grad_norm': 0.4246263802051544, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.23it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.29it/s]                                              {'loss': 0.5542, 'grad_norm': 1.7273057699203491, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.29it/s] 30%|███       | 9/30 [00:01<00:03,  6.63it/s]                                              {'loss': 0.5957, 'grad_norm': 2.4418230056762695, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.63it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.58it/s]                                               {'loss': 0.7387, 'grad_norm': 2.0184872150421143, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.58it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.80it/s]                                               {'loss': 0.2497, 'grad_norm': 1.2759426832199097, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.80it/s]                                               {'loss': 0.5672, 'grad_norm': 5.871062755584717, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.80it/s] 43%|████▎     | 13/30 [00:02<00:02,  8.20it/s]                                               {'loss': 0.27, 'grad_norm': 1.219903588294983, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  8.20it/s] 47%|████▋     | 14/30 [00:02<00:01,  8.04it/s]                                               {'loss': 0.3235, 'grad_norm': 1.4625033140182495, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  8.04it/s] 50%|█████     | 15/30 [00:02<00:01,  8.00it/s]                                               {'loss': 0.3713, 'grad_norm': 1.232708215713501, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.00it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.88it/s]                                               {'loss': 0.3578, 'grad_norm': 1.4639427661895752, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.88it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.90it/s]                                               {'loss': 0.1741, 'grad_norm': 2.3196728229522705, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.90it/s]                                               {'loss': 0.4367, 'grad_norm': 2.270599126815796, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.90it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.55it/s]                                               {'loss': 0.2553, 'grad_norm': 0.9668219685554504, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.55it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.02it/s]                                               {'loss': 0.3751, 'grad_norm': 1.935035228729248, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.02it/s] 70%|███████   | 21/30 [00:03<00:01,  7.69it/s]                                               {'loss': 0.2935, 'grad_norm': 1.5989669561386108, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.69it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.81it/s]                                               {'loss': 0.3759, 'grad_norm': 1.4710689783096313, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.81it/s] 77%|███████▋  | 23/30 [00:03<00:00,  7.60it/s]                                               {'loss': 0.0846, 'grad_norm': 1.227607011795044, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.60it/s]                                               {'loss': 0.2657, 'grad_norm': 1.1276195049285889, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.60it/s] 83%|████████▎ | 25/30 [00:03<00:00,  8.23it/s]                                               {'loss': 0.3007, 'grad_norm': 1.1644104719161987, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  8.23it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.59it/s]                                               {'loss': 0.3369, 'grad_norm': 1.8799281120300293, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.59it/s] 90%|█████████ | 27/30 [00:03<00:00,  7.33it/s]                                               {'loss': 0.3288, 'grad_norm': 1.151631474494934, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  7.33it/s] 93%|█████████▎| 28/30 [00:03<00:00,  7.30it/s]                                               {'loss': 0.0678, 'grad_norm': 0.9395005702972412, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  7.30it/s] 97%|█████████▋| 29/30 [00:04<00:00,  7.13it/s]                                               {'loss': 0.2259, 'grad_norm': 0.6289246678352356, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  7.13it/s]                                               {'loss': 0.3049, 'grad_norm': 2.1481692790985107, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.13it/s]                                               {'train_runtime': 4.3619, 'train_samples_per_second': 97.434, 'train_steps_per_second': 6.878, 'train_loss': 0.34598107474545636, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.13it/s]100%|██████████| 30/30 [00:04<00:00,  6.89it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.43it/s]                                              {'loss': 0.5352, 'grad_norm': 0.48722168803215027, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.43it/s]  7%|▋         | 2/30 [00:00<00:05,  4.80it/s]                                              {'loss': 0.6271, 'grad_norm': 1.1560335159301758, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  4.80it/s] 10%|█         | 3/30 [00:00<00:05,  4.51it/s]                                              {'loss': 0.6512, 'grad_norm': 0.7155287861824036, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.51it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.48it/s]                                              {'loss': 0.7126, 'grad_norm': 1.407245397567749, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.48it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.44it/s]                                              {'loss': 0.6794, 'grad_norm': 1.2589839696884155, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.44it/s] 20%|██        | 6/30 [00:01<00:09,  2.40it/s]                                              {'loss': 0.7856, 'grad_norm': 2.516291856765747, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:09,  2.40it/s] 23%|██▎       | 7/30 [00:02<00:08,  2.65it/s]                                              {'loss': 0.6813, 'grad_norm': 1.221120834350586, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:08,  2.65it/s] 27%|██▋       | 8/30 [00:02<00:07,  2.84it/s]                                              {'loss': 0.6461, 'grad_norm': 0.8736135363578796, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:07,  2.84it/s] 30%|███       | 9/30 [00:02<00:06,  3.01it/s]                                              {'loss': 0.6464, 'grad_norm': 0.9407439231872559, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:06,  3.01it/s] 33%|███▎      | 10/30 [00:03<00:06,  3.27it/s]                                               {'loss': 0.6538, 'grad_norm': 1.3722310066223145, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:06,  3.27it/s] 37%|███▋      | 11/30 [00:03<00:05,  3.36it/s]                                               {'loss': 0.6212, 'grad_norm': 1.209708571434021, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:05,  3.36it/s] 40%|████      | 12/30 [00:03<00:04,  4.01it/s]                                               {'loss': 0.511, 'grad_norm': 2.756938934326172, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:04,  4.01it/s] 43%|████▎     | 13/30 [00:03<00:04,  3.79it/s]                                               {'loss': 0.509, 'grad_norm': 1.5914193391799927, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:04,  3.79it/s] 47%|████▋     | 14/30 [00:04<00:04,  3.67it/s]                                               {'loss': 0.5045, 'grad_norm': 2.8739805221557617, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:04,  3.67it/s] 50%|█████     | 15/30 [00:04<00:04,  3.58it/s]                                               {'loss': 0.9535, 'grad_norm': 7.9080281257629395, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:04,  3.58it/s] 53%|█████▎    | 16/30 [00:04<00:03,  3.77it/s]                                               {'loss': 0.5587, 'grad_norm': 3.990049123764038, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:03,  3.77it/s] 57%|█████▋    | 17/30 [00:04<00:03,  3.64it/s]                                               {'loss': 0.5268, 'grad_norm': 2.6171021461486816, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:03,  3.64it/s] 60%|██████    | 18/30 [00:05<00:02,  4.21it/s]                                               {'loss': 0.4587, 'grad_norm': 7.63893985748291, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:02,  4.21it/s] 63%|██████▎   | 19/30 [00:05<00:02,  3.79it/s]                                               {'loss': 0.4664, 'grad_norm': 1.692406177520752, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  3.79it/s] 67%|██████▋   | 20/30 [00:05<00:02,  3.65it/s]                                               {'loss': 0.5295, 'grad_norm': 1.7591924667358398, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  3.65it/s] 70%|███████   | 21/30 [00:05<00:02,  3.56it/s]                                               {'loss': 0.4836, 'grad_norm': 1.8224360942840576, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:02,  3.56it/s] 73%|███████▎  | 22/30 [00:06<00:02,  3.71it/s]                                               {'loss': 0.5104, 'grad_norm': 2.370846748352051, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:06<00:02,  3.71it/s] 77%|███████▋  | 23/30 [00:06<00:01,  3.85it/s]                                               {'loss': 0.5841, 'grad_norm': 2.861811637878418, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  3.85it/s] 80%|████████  | 24/30 [00:06<00:01,  4.58it/s]                                               {'loss': 0.4337, 'grad_norm': 5.173278331756592, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  4.58it/s] 83%|████████▎ | 25/30 [00:06<00:01,  4.12it/s]                                               {'loss': 0.4422, 'grad_norm': 1.881221055984497, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:01,  4.12it/s] 87%|████████▋ | 26/30 [00:07<00:01,  4.00it/s]                                               {'loss': 0.5332, 'grad_norm': 3.25252628326416, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:07<00:01,  4.00it/s] 90%|█████████ | 27/30 [00:07<00:00,  3.92it/s]                                               {'loss': 0.39, 'grad_norm': 2.2950170040130615, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:07<00:00,  3.92it/s] 93%|█████████▎| 28/30 [00:07<00:00,  3.80it/s]                                               {'loss': 0.4622, 'grad_norm': 3.881718635559082, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  3.80it/s] 97%|█████████▋| 29/30 [00:07<00:00,  3.86it/s]                                               {'loss': 0.4071, 'grad_norm': 1.8289587497711182, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  3.86it/s]100%|██████████| 30/30 [00:08<00:00,  4.61it/s]                                               {'loss': 0.4601, 'grad_norm': 3.398538589477539, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.61it/s]                                               {'train_runtime': 8.1529, 'train_samples_per_second': 52.129, 'train_steps_per_second': 3.68, 'train_loss': 0.5654819518327713, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.61it/s]100%|██████████| 30/30 [00:08<00:00,  3.68it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  5%|▍         | 3/66 [00:00<00:02, 21.72it/s]  9%|▉         | 6/66 [00:00<00:03, 17.88it/s] 12%|█▏        | 8/66 [00:00<00:03, 16.43it/s] 15%|█▌        | 10/66 [00:00<00:03, 15.52it/s] 18%|█▊        | 12/66 [00:00<00:03, 15.34it/s] 21%|██        | 14/66 [00:00<00:03, 15.97it/s] 24%|██▍       | 16/66 [00:00<00:03, 15.97it/s] 27%|██▋       | 18/66 [00:01<00:03, 13.81it/s] 30%|███       | 20/66 [00:01<00:03, 12.96it/s] 33%|███▎      | 22/66 [00:01<00:03, 12.27it/s] 36%|███▋      | 24/66 [00:01<00:03, 11.70it/s] 39%|███▉      | 26/66 [00:01<00:03, 11.17it/s] 42%|████▏     | 28/66 [00:02<00:03, 10.72it/s] 45%|████▌     | 30/66 [00:02<00:03, 11.96it/s] 48%|████▊     | 32/66 [00:02<00:02, 12.56it/s] 52%|█████▏    | 34/66 [00:02<00:02, 12.89it/s] 55%|█████▍    | 36/66 [00:02<00:02, 11.84it/s] 58%|█████▊    | 38/66 [00:02<00:02, 10.14it/s] 61%|██████    | 40/66 [00:03<00:02,  9.22it/s] 62%|██████▏   | 41/66 [00:03<00:02,  8.73it/s] 64%|██████▎   | 42/66 [00:03<00:02,  8.59it/s] 65%|██████▌   | 43/66 [00:03<00:02,  8.56it/s] 67%|██████▋   | 44/66 [00:03<00:02,  8.81it/s] 68%|██████▊   | 45/66 [00:03<00:02,  8.94it/s] 71%|███████   | 47/66 [00:04<00:02,  9.27it/s] 74%|███████▍  | 49/66 [00:04<00:01,  9.45it/s] 77%|███████▋  | 51/66 [00:04<00:01, 11.00it/s] 80%|████████  | 53/66 [00:04<00:01, 10.87it/s] 83%|████████▎ | 55/66 [00:04<00:00, 11.01it/s] 86%|████████▋ | 57/66 [00:04<00:00, 11.42it/s] 89%|████████▉ | 59/66 [00:05<00:00, 11.20it/s] 92%|█████████▏| 61/66 [00:05<00:00, 10.85it/s] 95%|█████████▌| 63/66 [00:05<00:00, 10.68it/s] 98%|█████████▊| 65/66 [00:05<00:00, 10.56it/s]100%|██████████| 66/66 [00:05<00:00, 11.53it/s]
{'eval_loss': 0.6189649105072021, 'eval_model_preparation_time': 0.0132, 'eval_acc': 0.6989453499520614, 'eval_runtime': 5.8231, 'eval_samples_per_second': 179.114, 'eval_steps_per_second': 11.334}

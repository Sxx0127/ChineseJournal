nohup: ignoring input
Map:   0%|          | 0/11314 [00:00<?, ? examples/s]Map:   9%|▉         | 1000/11314 [00:00<00:02, 4536.87 examples/s]Map:  18%|█▊        | 2000/11314 [00:00<00:02, 3962.48 examples/s]Map:  27%|██▋       | 3000/11314 [00:00<00:01, 4350.31 examples/s]Map:  35%|███▌      | 4000/11314 [00:00<00:01, 4036.87 examples/s]Map:  44%|████▍     | 5000/11314 [00:01<00:01, 3805.51 examples/s]Map:  53%|█████▎    | 6000/11314 [00:01<00:01, 3766.45 examples/s]Map:  62%|██████▏   | 7000/11314 [00:01<00:01, 3855.78 examples/s]Map:  71%|███████   | 8000/11314 [00:01<00:00, 4122.47 examples/s]Map:  80%|███████▉  | 9000/11314 [00:02<00:00, 4012.38 examples/s]Map:  88%|████████▊ | 10000/11314 [00:02<00:00, 4006.97 examples/s]Map:  97%|█████████▋| 11000/11314 [00:02<00:00, 4059.17 examples/s]Map: 100%|██████████| 11314/11314 [00:02<00:00, 4023.17 examples/s]
Map:   0%|          | 0/7532 [00:00<?, ? examples/s]Map:  13%|█▎        | 1000/7532 [00:00<00:01, 5223.57 examples/s]Map:  27%|██▋       | 2000/7532 [00:00<00:01, 3283.32 examples/s]Map:  40%|███▉      | 3000/7532 [00:00<00:01, 2916.89 examples/s]Map:  53%|█████▎    | 4000/7532 [00:01<00:00, 3619.09 examples/s]Map:  66%|██████▋   | 5000/7532 [00:01<00:00, 4108.30 examples/s]Map:  80%|███████▉  | 6000/7532 [00:01<00:00, 4468.63 examples/s]Map:  93%|█████████▎| 7000/7532 [00:01<00:00, 4442.37 examples/s]Map: 100%|██████████| 7532/7532 [00:01<00:00, 4540.58 examples/s]Map: 100%|██████████| 7532/7532 [00:01<00:00, 4079.94 examples/s]
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at ./model/models--distilbert-base-multilingual-cased/ and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 589,824 || all params: 135,929,876 || trainable%: 0.4339
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): DistilBertForSequenceClassification(
      (distilbert): DistilBertModel(
        (embeddings): Embeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (transformer): Transformer(
          (layer): ModuleList(
            (0-5): 6 x TransformerBlock(
              (attention): DistilBertSdpaAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (q_lin): lora.Linear(
                  (base_layer): Linear(in_features=768, out_features=768, bias=True)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=768, out_features=32, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=32, out_features=768, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (k_lin): Linear(in_features=768, out_features=768, bias=True)
                (v_lin): lora.Linear(
                  (base_layer): Linear(in_features=768, out_features=768, bias=True)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=768, out_features=32, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=32, out_features=768, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (out_lin): Linear(in_features=768, out_features=768, bias=True)
              )
              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (ffn): FFN(
                (dropout): Dropout(p=0.1, inplace=False)
                (lin1): Linear(in_features=768, out_features=3072, bias=True)
                (lin2): Linear(in_features=3072, out_features=768, bias=True)
                (activation): GELUActivation()
              )
              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            )
          )
        )
      )
      (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
      (classifier): Linear(in_features=768, out_features=20, bias=True)
      (dropout): Dropout(p=0.2, inplace=False)
    )
  )
)
ROUND:0
CLIENT:22
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]  1%|▏         | 1/75 [00:00<00:37,  1.96it/s]                                              {'loss': 2.9818, 'grad_norm': 1.6226356029510498, 'learning_rate': 0.0003, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:37,  1.96it/s]                                              {'loss': 3.0297, 'grad_norm': 1.9346152544021606, 'learning_rate': 0.000296, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:37,  1.96it/s]                                              {'loss': 3.0315, 'grad_norm': 1.6752760410308838, 'learning_rate': 0.000292, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:36,  1.96it/s]  5%|▌         | 4/75 [00:00<00:09,  7.75it/s]                                              {'loss': 3.0868, 'grad_norm': 1.8067033290863037, 'learning_rate': 0.00028799999999999995, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:09,  7.75it/s]                                              {'loss': 2.9751, 'grad_norm': 1.3649283647537231, 'learning_rate': 0.00028399999999999996, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:09,  7.75it/s]                                              {'loss': 3.0019, 'grad_norm': 1.4689372777938843, 'learning_rate': 0.00028, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:08,  7.75it/s]  9%|▉         | 7/75 [00:00<00:05, 12.46it/s]                                              {'loss': 3.0002, 'grad_norm': 1.4849560260772705, 'learning_rate': 0.000276, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:05, 12.46it/s]                                              {'loss': 3.0279, 'grad_norm': 1.5148028135299683, 'learning_rate': 0.00027199999999999994, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:05, 12.46it/s]                                              {'loss': 3.0741, 'grad_norm': 1.7584253549575806, 'learning_rate': 0.00026799999999999995, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:05, 12.46it/s] 13%|█▎        | 10/75 [00:00<00:04, 16.18it/s]                                               {'loss': 2.9624, 'grad_norm': 1.343464970588684, 'learning_rate': 0.00026399999999999997, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:04, 16.18it/s]                                               {'loss': 2.9926, 'grad_norm': 1.667909026145935, 'learning_rate': 0.00026, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 16.18it/s]                                               {'loss': 3.0359, 'grad_norm': 1.8601462841033936, 'learning_rate': 0.000256, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:03, 16.18it/s] 17%|█▋        | 13/75 [00:00<00:03, 19.08it/s]                                               {'loss': 3.0156, 'grad_norm': 1.6055957078933716, 'learning_rate': 0.00025199999999999995, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:03, 19.08it/s]                                               {'loss': 3.0311, 'grad_norm': 1.5466327667236328, 'learning_rate': 0.00024799999999999996, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:01<00:03, 19.08it/s]                                               {'loss': 2.7801, 'grad_norm': 4.416076183319092, 'learning_rate': 0.000244, 'epoch': 1.0}
 20%|██        | 15/75 [00:01<00:03, 19.08it/s]                                               {'loss': 2.8918, 'grad_norm': 1.4291402101516724, 'learning_rate': 0.00023999999999999998, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:01<00:03, 19.08it/s] 23%|██▎       | 17/75 [00:01<00:02, 22.77it/s]                                               {'loss': 2.8969, 'grad_norm': 1.3395137786865234, 'learning_rate': 0.00023599999999999996, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:01<00:02, 22.77it/s]                                               {'loss': 2.9162, 'grad_norm': 1.521956443786621, 'learning_rate': 0.00023199999999999997, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:01<00:02, 22.77it/s]                                               {'loss': 2.8963, 'grad_norm': 1.4098972082138062, 'learning_rate': 0.00022799999999999999, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:01<00:02, 22.77it/s] 27%|██▋       | 20/75 [00:01<00:02, 23.97it/s]                                               {'loss': 2.9294, 'grad_norm': 1.5437532663345337, 'learning_rate': 0.000224, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:01<00:02, 23.97it/s]                                               {'loss': 2.8482, 'grad_norm': 1.5612279176712036, 'learning_rate': 0.00021999999999999995, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:02, 23.97it/s]                                               {'loss': 2.8965, 'grad_norm': 1.4789724349975586, 'learning_rate': 0.00021599999999999996, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 23.97it/s] 31%|███       | 23/75 [00:01<00:02, 24.68it/s]                                               {'loss': 2.8365, 'grad_norm': 1.448038935661316, 'learning_rate': 0.00021199999999999998, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 24.68it/s]                                               {'loss': 2.8986, 'grad_norm': 1.5049265623092651, 'learning_rate': 0.000208, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 24.68it/s]                                               {'loss': 2.8636, 'grad_norm': 1.5212428569793701, 'learning_rate': 0.000204, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.68it/s] 35%|███▍      | 26/75 [00:01<00:01, 25.40it/s]                                               {'loss': 2.8699, 'grad_norm': 1.8956856727600098, 'learning_rate': 0.00019999999999999998, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.40it/s]                                               {'loss': 2.8994, 'grad_norm': 1.8312397003173828, 'learning_rate': 0.00019599999999999997, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.40it/s]                                               {'loss': 2.8432, 'grad_norm': 1.5392693281173706, 'learning_rate': 0.00019199999999999998, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.40it/s] 39%|███▊      | 29/75 [00:01<00:01, 25.74it/s]                                               {'loss': 2.9073, 'grad_norm': 1.7796754837036133, 'learning_rate': 0.000188, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.74it/s]                                               {'loss': 2.7255, 'grad_norm': 5.092189311981201, 'learning_rate': 0.00018399999999999997, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.74it/s]                                               {'loss': 2.8531, 'grad_norm': 1.6955108642578125, 'learning_rate': 0.00017999999999999998, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.74it/s]                                               {'loss': 2.7366, 'grad_norm': 1.641014575958252, 'learning_rate': 0.000176, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.74it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.61it/s]                                               {'loss': 2.7721, 'grad_norm': 2.108766794204712, 'learning_rate': 0.000172, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 27.61it/s]                                               {'loss': 2.6093, 'grad_norm': 2.0536892414093018, 'learning_rate': 0.000168, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 27.61it/s]                                               {'loss': 2.8283, 'grad_norm': 1.9495881795883179, 'learning_rate': 0.00016399999999999997, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 27.61it/s] 48%|████▊     | 36/75 [00:01<00:01, 27.26it/s]                                               {'loss': 2.7156, 'grad_norm': 1.711740255355835, 'learning_rate': 0.00015999999999999999, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 27.26it/s]                                               {'loss': 2.6875, 'grad_norm': 1.718062400817871, 'learning_rate': 0.000156, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 27.26it/s]                                               {'loss': 2.6569, 'grad_norm': 2.0632612705230713, 'learning_rate': 0.000152, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 27.26it/s] 52%|█████▏    | 39/75 [00:01<00:01, 27.06it/s]                                               {'loss': 2.6642, 'grad_norm': 2.1325535774230957, 'learning_rate': 0.000148, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 27.06it/s]                                               {'loss': 2.7208, 'grad_norm': 2.260593891143799, 'learning_rate': 0.00014399999999999998, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 27.06it/s]                                               {'loss': 2.6932, 'grad_norm': 2.1625816822052, 'learning_rate': 0.00014, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 27.06it/s] 56%|█████▌    | 42/75 [00:02<00:01, 27.00it/s]                                               {'loss': 2.6169, 'grad_norm': 2.0544025897979736, 'learning_rate': 0.00013599999999999997, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 27.00it/s]                                               {'loss': 2.5972, 'grad_norm': 2.2694387435913086, 'learning_rate': 0.00013199999999999998, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 27.00it/s]                                               {'loss': 2.6819, 'grad_norm': 1.9883224964141846, 'learning_rate': 0.000128, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 27.00it/s]                                               {'loss': 2.4287, 'grad_norm': 6.504225730895996, 'learning_rate': 0.00012399999999999998, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 27.00it/s] 61%|██████▏   | 46/75 [00:02<00:01, 28.46it/s]                                               {'loss': 2.4268, 'grad_norm': 2.327834129333496, 'learning_rate': 0.00011999999999999999, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 28.46it/s]                                               {'loss': 2.5052, 'grad_norm': 2.4902992248535156, 'learning_rate': 0.00011599999999999999, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:00, 28.46it/s]                                               {'loss': 2.4367, 'grad_norm': 2.3864972591400146, 'learning_rate': 0.000112, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:00, 28.46it/s] 65%|██████▌   | 49/75 [00:02<00:00, 28.00it/s]                                               {'loss': 2.4278, 'grad_norm': 2.7697219848632812, 'learning_rate': 0.00010799999999999998, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:00, 28.00it/s]                                               {'loss': 2.6511, 'grad_norm': 2.8165781497955322, 'learning_rate': 0.000104, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 28.00it/s]                                               {'loss': 2.5068, 'grad_norm': 2.4764325618743896, 'learning_rate': 9.999999999999999e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 28.00it/s] 69%|██████▉   | 52/75 [00:02<00:00, 27.49it/s]                                               {'loss': 2.4853, 'grad_norm': 2.5693891048431396, 'learning_rate': 9.599999999999999e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 27.49it/s]                                               {'loss': 2.5296, 'grad_norm': 2.5765719413757324, 'learning_rate': 9.199999999999999e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 27.49it/s]                                               {'loss': 2.2893, 'grad_norm': 2.8326289653778076, 'learning_rate': 8.8e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 27.49it/s] 73%|███████▎  | 55/75 [00:02<00:00, 27.34it/s]                                               {'loss': 2.4434, 'grad_norm': 2.612626314163208, 'learning_rate': 8.4e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 27.34it/s]                                               {'loss': 2.2647, 'grad_norm': 2.8157029151916504, 'learning_rate': 7.999999999999999e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 27.34it/s]                                               {'loss': 2.373, 'grad_norm': 2.93152117729187, 'learning_rate': 7.6e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 27.34it/s] 77%|███████▋  | 58/75 [00:02<00:00, 27.04it/s]                                               {'loss': 2.313, 'grad_norm': 2.722601890563965, 'learning_rate': 7.199999999999999e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 27.04it/s]                                               {'loss': 2.4603, 'grad_norm': 2.927950620651245, 'learning_rate': 6.799999999999999e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 27.04it/s]                                               {'loss': 2.5644, 'grad_norm': 9.552595138549805, 'learning_rate': 6.4e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 27.04it/s]                                               {'loss': 2.388, 'grad_norm': 3.0629615783691406, 'learning_rate': 5.9999999999999995e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 27.04it/s] 83%|████████▎ | 62/75 [00:02<00:00, 28.62it/s]                                               {'loss': 2.1662, 'grad_norm': 3.071169376373291, 'learning_rate': 5.6e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 28.62it/s]                                               {'loss': 2.3692, 'grad_norm': 3.5939743518829346, 'learning_rate': 5.2e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 28.62it/s]                                               {'loss': 2.1218, 'grad_norm': 3.625410318374634, 'learning_rate': 4.7999999999999994e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 28.62it/s] 87%|████████▋ | 65/75 [00:02<00:00, 27.91it/s]                                               {'loss': 2.1486, 'grad_norm': 3.1667709350585938, 'learning_rate': 4.4e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 27.91it/s]                                               {'loss': 2.1561, 'grad_norm': 3.394055128097534, 'learning_rate': 3.9999999999999996e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 27.91it/s]                                               {'loss': 2.3374, 'grad_norm': 2.975456714630127, 'learning_rate': 3.5999999999999994e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 27.91it/s] 91%|█████████ | 68/75 [00:02<00:00, 27.46it/s]                                               {'loss': 2.3432, 'grad_norm': 3.0796115398406982, 'learning_rate': 3.2e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 27.46it/s]                                               {'loss': 2.1786, 'grad_norm': 2.8619959354400635, 'learning_rate': 2.8e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 27.46it/s]                                               {'loss': 2.2901, 'grad_norm': 3.3251092433929443, 'learning_rate': 2.3999999999999997e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 27.46it/s] 95%|█████████▍| 71/75 [00:03<00:00, 27.35it/s]                                               {'loss': 2.1136, 'grad_norm': 3.5723958015441895, 'learning_rate': 1.9999999999999998e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 27.35it/s]                                               {'loss': 2.1837, 'grad_norm': 3.026376485824585, 'learning_rate': 1.6e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 27.35it/s]                                               {'loss': 2.1504, 'grad_norm': 3.0337560176849365, 'learning_rate': 1.1999999999999999e-05, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 27.35it/s] 99%|█████████▊| 74/75 [00:03<00:00, 27.09it/s]                                               {'loss': 2.2797, 'grad_norm': 3.310637950897217, 'learning_rate': 8e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 27.09it/s]                                               {'loss': 2.4815, 'grad_norm': 9.416845321655273, 'learning_rate': 4e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 27.09it/s]                                               {'train_runtime': 3.2699, 'train_samples_per_second': 345.575, 'train_steps_per_second': 22.936, 'train_loss': 2.6505917867024738, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 27.09it/s]100%|██████████| 75/75 [00:03<00:00, 22.94it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:16
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.9996, 'grad_norm': 0.4522029161453247, 'learning_rate': 0.0003, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:02, 25.61it/s]                                              {'loss': 3.0682, 'grad_norm': 0.7900094985961914, 'learning_rate': 0.000296, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 26.76it/s]  4%|▍         | 3/75 [00:00<00:02, 26.56it/s]                                              {'loss': 2.9075, 'grad_norm': 0.5901237726211548, 'learning_rate': 0.000292, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 26.56it/s]                                              {'loss': 2.9295, 'grad_norm': 0.4962204694747925, 'learning_rate': 0.00028799999999999995, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 26.56it/s]                                              {'loss': 2.8289, 'grad_norm': 0.557219386100769, 'learning_rate': 0.00028399999999999996, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 26.56it/s]  8%|▊         | 6/75 [00:00<00:02, 26.42it/s]                                              {'loss': 2.821, 'grad_norm': 0.4834260046482086, 'learning_rate': 0.00028, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 26.42it/s]                                              {'loss': 2.8183, 'grad_norm': 0.5666890144348145, 'learning_rate': 0.000276, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 26.42it/s]                                              {'loss': 2.8731, 'grad_norm': 0.574941098690033, 'learning_rate': 0.00027199999999999994, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 26.42it/s] 12%|█▏        | 9/75 [00:00<00:02, 26.73it/s]                                              {'loss': 2.7323, 'grad_norm': 0.7768457531929016, 'learning_rate': 0.00026799999999999995, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 26.73it/s]                                              {'loss': 2.7161, 'grad_norm': 0.75858473777771, 'learning_rate': 0.00026399999999999997, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 26.73it/s]                                               {'loss': 2.7204, 'grad_norm': 0.8318959474563599, 'learning_rate': 0.00026, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 26.73it/s] 16%|█▌        | 12/75 [00:00<00:02, 26.54it/s]                                               {'loss': 2.7642, 'grad_norm': 0.7008348703384399, 'learning_rate': 0.000256, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 26.54it/s]                                               {'loss': 2.5452, 'grad_norm': 0.9663372039794922, 'learning_rate': 0.00025199999999999995, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 26.54it/s]                                               {'loss': 2.8047, 'grad_norm': 0.6624065637588501, 'learning_rate': 0.00024799999999999996, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 26.54it/s]                                               {'loss': 2.6935, 'grad_norm': 1.7184854745864868, 'learning_rate': 0.000244, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 26.54it/s] 21%|██▏       | 16/75 [00:00<00:02, 28.51it/s]                                               {'loss': 2.6578, 'grad_norm': 0.8464562296867371, 'learning_rate': 0.00023999999999999998, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 28.51it/s]                                               {'loss': 2.623, 'grad_norm': 1.0976390838623047, 'learning_rate': 0.00023599999999999996, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 28.51it/s]                                               {'loss': 2.534, 'grad_norm': 0.6947893500328064, 'learning_rate': 0.00023199999999999997, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:01, 28.51it/s] 25%|██▌       | 19/75 [00:00<00:02, 27.83it/s]                                               {'loss': 2.5936, 'grad_norm': 1.0929421186447144, 'learning_rate': 0.00022799999999999999, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 27.83it/s]                                               {'loss': 2.5313, 'grad_norm': 1.1549675464630127, 'learning_rate': 0.000224, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:01, 27.83it/s]                                               {'loss': 2.5082, 'grad_norm': 0.9715085625648499, 'learning_rate': 0.00021999999999999995, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:01, 27.83it/s] 29%|██▉       | 22/75 [00:00<00:01, 27.41it/s]                                               {'loss': 2.5626, 'grad_norm': 0.9834229946136475, 'learning_rate': 0.00021599999999999996, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:01, 27.41it/s]                                               {'loss': 2.4211, 'grad_norm': 0.9512569308280945, 'learning_rate': 0.00021199999999999998, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 27.41it/s]                                               {'loss': 2.4875, 'grad_norm': 1.7556312084197998, 'learning_rate': 0.000208, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 27.41it/s] 33%|███▎      | 25/75 [00:00<00:01, 27.40it/s]                                               {'loss': 2.3984, 'grad_norm': 1.1054003238677979, 'learning_rate': 0.000204, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 27.40it/s]                                               {'loss': 2.3403, 'grad_norm': 1.318060040473938, 'learning_rate': 0.00019999999999999998, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:00<00:01, 27.40it/s]                                               {'loss': 2.4881, 'grad_norm': 1.1995325088500977, 'learning_rate': 0.00019599999999999997, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:00<00:01, 27.40it/s] 37%|███▋      | 28/75 [00:01<00:01, 27.04it/s]                                               {'loss': 2.5157, 'grad_norm': 1.2368272542953491, 'learning_rate': 0.00019199999999999998, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 27.04it/s]                                               {'loss': 2.2966, 'grad_norm': 1.0581456422805786, 'learning_rate': 0.000188, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 27.04it/s]                                               {'loss': 1.8933, 'grad_norm': 3.534003496170044, 'learning_rate': 0.00018399999999999997, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 27.04it/s]                                               {'loss': 2.1225, 'grad_norm': 1.5648373365402222, 'learning_rate': 0.00017999999999999998, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 27.04it/s] 43%|████▎     | 32/75 [00:01<00:01, 28.69it/s]                                               {'loss': 2.3743, 'grad_norm': 1.1204116344451904, 'learning_rate': 0.000176, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 28.69it/s]                                               {'loss': 2.3287, 'grad_norm': 1.0510337352752686, 'learning_rate': 0.000172, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 28.69it/s]                                               {'loss': 2.3485, 'grad_norm': 1.6379824876785278, 'learning_rate': 0.000168, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 28.69it/s] 47%|████▋     | 35/75 [00:01<00:01, 27.99it/s]                                               {'loss': 2.4479, 'grad_norm': 1.3022829294204712, 'learning_rate': 0.00016399999999999997, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 27.99it/s]                                               {'loss': 2.4459, 'grad_norm': 0.9840776920318604, 'learning_rate': 0.00015999999999999999, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 27.99it/s]                                               {'loss': 2.3931, 'grad_norm': 1.7000854015350342, 'learning_rate': 0.000156, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 27.99it/s] 51%|█████     | 38/75 [00:01<00:01, 27.71it/s]                                               {'loss': 2.3764, 'grad_norm': 1.2494701147079468, 'learning_rate': 0.000152, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 27.71it/s]                                               {'loss': 2.2987, 'grad_norm': 1.1091557741165161, 'learning_rate': 0.000148, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 27.71it/s]                                               {'loss': 2.4018, 'grad_norm': 1.4319829940795898, 'learning_rate': 0.00014399999999999998, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 27.71it/s] 55%|█████▍    | 41/75 [00:01<00:01, 27.32it/s]                                               {'loss': 2.1346, 'grad_norm': 1.2024669647216797, 'learning_rate': 0.00014, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 27.32it/s]                                               {'loss': 2.5882, 'grad_norm': 1.6427189111709595, 'learning_rate': 0.00013599999999999997, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 27.32it/s]                                               {'loss': 2.3796, 'grad_norm': 1.2167736291885376, 'learning_rate': 0.00013199999999999998, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 27.32it/s] 59%|█████▊    | 44/75 [00:01<00:01, 27.19it/s]                                               {'loss': 2.1487, 'grad_norm': 1.0583933591842651, 'learning_rate': 0.000128, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 27.19it/s]                                               {'loss': 2.0588, 'grad_norm': 5.446762561798096, 'learning_rate': 0.00012399999999999998, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 27.19it/s]                                               {'loss': 2.2838, 'grad_norm': 1.1097350120544434, 'learning_rate': 0.00011999999999999999, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 27.19it/s]                                               {'loss': 2.1954, 'grad_norm': 1.147257685661316, 'learning_rate': 0.00011599999999999999, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 27.19it/s] 64%|██████▍   | 48/75 [00:01<00:00, 28.59it/s]                                               {'loss': 2.3209, 'grad_norm': 1.3779321908950806, 'learning_rate': 0.000112, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 28.59it/s]                                               {'loss': 2.1785, 'grad_norm': 1.092787742614746, 'learning_rate': 0.00010799999999999998, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 28.59it/s]                                               {'loss': 2.4197, 'grad_norm': 1.2262310981750488, 'learning_rate': 0.000104, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 28.59it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.97it/s]                                               {'loss': 2.383, 'grad_norm': 1.2997733354568481, 'learning_rate': 9.999999999999999e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 27.97it/s]                                               {'loss': 2.4774, 'grad_norm': 1.3094596862792969, 'learning_rate': 9.599999999999999e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 27.97it/s]                                               {'loss': 2.1859, 'grad_norm': 1.1245537996292114, 'learning_rate': 9.199999999999999e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:01<00:00, 27.97it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.72it/s]                                               {'loss': 2.2638, 'grad_norm': 1.2431139945983887, 'learning_rate': 8.8e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:01<00:00, 27.72it/s]                                               {'loss': 2.1912, 'grad_norm': 2.0642645359039307, 'learning_rate': 8.4e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:01<00:00, 27.72it/s]                                               {'loss': 2.2003, 'grad_norm': 1.1427805423736572, 'learning_rate': 7.999999999999999e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 27.72it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.25it/s]                                               {'loss': 2.1609, 'grad_norm': 1.1640400886535645, 'learning_rate': 7.6e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 27.25it/s]                                               {'loss': 2.3364, 'grad_norm': 1.2885156869888306, 'learning_rate': 7.199999999999999e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 27.25it/s]                                               {'loss': 2.2581, 'grad_norm': 1.2222949266433716, 'learning_rate': 6.799999999999999e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 27.25it/s]                                               {'loss': 1.7543, 'grad_norm': 2.797576427459717, 'learning_rate': 6.4e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 27.25it/s] 81%|████████▏ | 61/75 [00:02<00:00, 28.54it/s]                                               {'loss': 2.2573, 'grad_norm': 1.1467474699020386, 'learning_rate': 5.9999999999999995e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 28.54it/s]                                               {'loss': 2.1845, 'grad_norm': 1.1467578411102295, 'learning_rate': 5.6e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 28.54it/s]                                               {'loss': 1.9882, 'grad_norm': 1.1972227096557617, 'learning_rate': 5.2e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 28.54it/s] 85%|████████▌ | 64/75 [00:02<00:00, 27.95it/s]                                               {'loss': 2.4945, 'grad_norm': 1.2063151597976685, 'learning_rate': 4.7999999999999994e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 27.95it/s]                                               {'loss': 2.2163, 'grad_norm': 0.947180986404419, 'learning_rate': 4.4e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 27.95it/s]                                               {'loss': 2.3552, 'grad_norm': 1.313854694366455, 'learning_rate': 3.9999999999999996e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 27.95it/s] 89%|████████▉ | 67/75 [00:02<00:00, 27.53it/s]                                               {'loss': 2.4792, 'grad_norm': 1.217203974723816, 'learning_rate': 3.5999999999999994e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 27.53it/s]                                               {'loss': 2.214, 'grad_norm': 1.204940915107727, 'learning_rate': 3.2e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 27.53it/s]                                               {'loss': 2.1857, 'grad_norm': 0.9457188248634338, 'learning_rate': 2.8e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 27.53it/s] 93%|█████████▎| 70/75 [00:02<00:00, 27.38it/s]                                               {'loss': 2.3745, 'grad_norm': 1.7782315015792847, 'learning_rate': 2.3999999999999997e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 27.38it/s]                                               {'loss': 2.2389, 'grad_norm': 1.4466036558151245, 'learning_rate': 1.9999999999999998e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 27.38it/s]                                               {'loss': 2.0224, 'grad_norm': 1.1682775020599365, 'learning_rate': 1.6e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 27.38it/s] 97%|█████████▋| 73/75 [00:02<00:00, 27.01it/s]                                               {'loss': 2.048, 'grad_norm': 1.6923391819000244, 'learning_rate': 1.1999999999999999e-05, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 27.01it/s]                                               {'loss': 2.2808, 'grad_norm': 1.147599220275879, 'learning_rate': 8e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 27.01it/s]                                               {'loss': 2.2365, 'grad_norm': 2.519343137741089, 'learning_rate': 4e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 27.01it/s]                                               {'train_runtime': 2.7724, 'train_samples_per_second': 407.588, 'train_steps_per_second': 27.052, 'train_loss': 2.41476412932078, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 27.01it/s]100%|██████████| 75/75 [00:02<00:00, 27.06it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:26
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 3.0822, 'grad_norm': 0.9085405468940735, 'learning_rate': 0.0003, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:02, 25.80it/s]                                              {'loss': 2.9073, 'grad_norm': 0.3890949785709381, 'learning_rate': 0.000296, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 27.01it/s]  4%|▍         | 3/75 [00:00<00:02, 26.70it/s]                                              {'loss': 2.8761, 'grad_norm': 0.48480647802352905, 'learning_rate': 0.000292, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 26.70it/s]                                              {'loss': 2.8364, 'grad_norm': 0.7164641618728638, 'learning_rate': 0.00028799999999999995, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 26.70it/s]                                              {'loss': 2.8248, 'grad_norm': 0.6048794984817505, 'learning_rate': 0.00028399999999999996, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 26.70it/s]  8%|▊         | 6/75 [00:00<00:02, 26.37it/s]                                              {'loss': 2.8554, 'grad_norm': 0.5703060626983643, 'learning_rate': 0.00028, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 26.37it/s]                                              {'loss': 2.8738, 'grad_norm': 0.7328211665153503, 'learning_rate': 0.000276, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 26.37it/s]                                              {'loss': 2.7548, 'grad_norm': 0.652549147605896, 'learning_rate': 0.00027199999999999994, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 26.37it/s] 12%|█▏        | 9/75 [00:00<00:02, 26.58it/s]                                              {'loss': 2.8282, 'grad_norm': 0.49289241433143616, 'learning_rate': 0.00026799999999999995, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 26.58it/s]                                              {'loss': 2.6396, 'grad_norm': 0.9810715317726135, 'learning_rate': 0.00026399999999999997, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 26.58it/s]                                               {'loss': 2.7755, 'grad_norm': 0.7141614556312561, 'learning_rate': 0.00026, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 26.58it/s] 16%|█▌        | 12/75 [00:00<00:02, 26.42it/s]                                               {'loss': 2.6696, 'grad_norm': 0.7354300618171692, 'learning_rate': 0.000256, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 26.42it/s]                                               {'loss': 2.7782, 'grad_norm': 0.7584421634674072, 'learning_rate': 0.00025199999999999995, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 26.42it/s]                                               {'loss': 2.5226, 'grad_norm': 0.8800535202026367, 'learning_rate': 0.00024799999999999996, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 26.42it/s]                                               {'loss': 2.4851, 'grad_norm': 1.5461485385894775, 'learning_rate': 0.000244, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 26.42it/s] 21%|██▏       | 16/75 [00:00<00:02, 28.21it/s]                                               {'loss': 2.558, 'grad_norm': 0.6613247394561768, 'learning_rate': 0.00023999999999999998, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 28.21it/s]                                               {'loss': 2.5295, 'grad_norm': 0.8344665765762329, 'learning_rate': 0.00023599999999999996, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 28.21it/s]                                               {'loss': 2.647, 'grad_norm': 0.7947829961776733, 'learning_rate': 0.00023199999999999997, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 28.21it/s] 25%|██▌       | 19/75 [00:00<00:02, 27.77it/s]                                               {'loss': 2.5534, 'grad_norm': 0.907448947429657, 'learning_rate': 0.00022799999999999999, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 27.77it/s]                                               {'loss': 2.4242, 'grad_norm': 0.9343144297599792, 'learning_rate': 0.000224, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:01, 27.77it/s]                                               {'loss': 2.4987, 'grad_norm': 1.3140417337417603, 'learning_rate': 0.00021999999999999995, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:01, 27.77it/s] 29%|██▉       | 22/75 [00:00<00:01, 27.18it/s]                                               {'loss': 2.293, 'grad_norm': 1.0659711360931396, 'learning_rate': 0.00021599999999999996, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:01, 27.18it/s]                                               {'loss': 2.5213, 'grad_norm': 0.8879539966583252, 'learning_rate': 0.00021199999999999998, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 27.18it/s]                                               {'loss': 2.3028, 'grad_norm': 0.7920716404914856, 'learning_rate': 0.000208, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 27.18it/s] 33%|███▎      | 25/75 [00:00<00:01, 26.90it/s]                                               {'loss': 2.5304, 'grad_norm': 0.8774518966674805, 'learning_rate': 0.000204, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 26.90it/s]                                               {'loss': 2.7752, 'grad_norm': 1.3385027647018433, 'learning_rate': 0.00019999999999999998, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:00<00:01, 26.90it/s]                                               {'loss': 2.3917, 'grad_norm': 0.9513547420501709, 'learning_rate': 0.00019599999999999997, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:00<00:01, 26.90it/s] 37%|███▋      | 28/75 [00:01<00:01, 26.98it/s]                                               {'loss': 2.4036, 'grad_norm': 1.1539971828460693, 'learning_rate': 0.00019199999999999998, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 26.98it/s]                                               {'loss': 2.3416, 'grad_norm': 0.9309912919998169, 'learning_rate': 0.000188, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 26.98it/s]                                               {'loss': 2.5952, 'grad_norm': 1.8396579027175903, 'learning_rate': 0.00018399999999999997, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 26.98it/s]                                               {'loss': 2.3173, 'grad_norm': 1.2622120380401611, 'learning_rate': 0.00017999999999999998, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.98it/s] 43%|████▎     | 32/75 [00:01<00:01, 28.57it/s]                                               {'loss': 2.4489, 'grad_norm': 1.941947340965271, 'learning_rate': 0.000176, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 28.57it/s]                                               {'loss': 2.3351, 'grad_norm': 1.385559320449829, 'learning_rate': 0.000172, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 28.57it/s]                                               {'loss': 2.5359, 'grad_norm': 1.9488368034362793, 'learning_rate': 0.000168, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 28.57it/s] 47%|████▋     | 35/75 [00:01<00:01, 27.99it/s]                                               {'loss': 2.4646, 'grad_norm': 1.3537416458129883, 'learning_rate': 0.00016399999999999997, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 27.99it/s]                                               {'loss': 2.476, 'grad_norm': 1.1621140241622925, 'learning_rate': 0.00015999999999999999, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 27.99it/s]                                               {'loss': 2.2335, 'grad_norm': 1.2082810401916504, 'learning_rate': 0.000156, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 27.99it/s] 51%|█████     | 38/75 [00:01<00:01, 27.33it/s]                                               {'loss': 2.2705, 'grad_norm': 1.1713775396347046, 'learning_rate': 0.000152, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 27.33it/s]                                               {'loss': 2.2387, 'grad_norm': 1.231204628944397, 'learning_rate': 0.000148, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 27.33it/s]                                               {'loss': 2.2657, 'grad_norm': 1.1802581548690796, 'learning_rate': 0.00014399999999999998, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 27.33it/s] 55%|█████▍    | 41/75 [00:01<00:01, 27.05it/s]                                               {'loss': 2.2729, 'grad_norm': 1.0632597208023071, 'learning_rate': 0.00014, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 27.05it/s]                                               {'loss': 2.4904, 'grad_norm': 1.0519402027130127, 'learning_rate': 0.00013599999999999997, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 27.05it/s]                                               {'loss': 2.2491, 'grad_norm': 1.2831021547317505, 'learning_rate': 0.00013199999999999998, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 27.05it/s] 59%|█████▊    | 44/75 [00:01<00:01, 27.06it/s]                                               {'loss': 2.2211, 'grad_norm': 1.6379752159118652, 'learning_rate': 0.000128, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 27.06it/s]                                               {'loss': 2.4234, 'grad_norm': 2.9766340255737305, 'learning_rate': 0.00012399999999999998, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 27.06it/s]                                               {'loss': 2.3566, 'grad_norm': 1.2806692123413086, 'learning_rate': 0.00011999999999999999, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 27.06it/s]                                               {'loss': 2.2904, 'grad_norm': 1.3121503591537476, 'learning_rate': 0.00011599999999999999, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 27.06it/s] 64%|██████▍   | 48/75 [00:01<00:00, 28.49it/s]                                               {'loss': 2.448, 'grad_norm': 1.2570196390151978, 'learning_rate': 0.000112, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 28.49it/s]                                               {'loss': 2.4915, 'grad_norm': 1.3887031078338623, 'learning_rate': 0.00010799999999999998, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 28.49it/s]                                               {'loss': 2.1072, 'grad_norm': 1.2411701679229736, 'learning_rate': 0.000104, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 28.49it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.90it/s]                                               {'loss': 2.2777, 'grad_norm': 1.4805381298065186, 'learning_rate': 9.999999999999999e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 27.90it/s]                                               {'loss': 2.3033, 'grad_norm': 1.7286860942840576, 'learning_rate': 9.599999999999999e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 27.90it/s]                                               {'loss': 2.1458, 'grad_norm': 1.2130199670791626, 'learning_rate': 9.199999999999999e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:01<00:00, 27.90it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.36it/s]                                               {'loss': 2.1716, 'grad_norm': 1.2456285953521729, 'learning_rate': 8.8e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:01<00:00, 27.36it/s]                                               {'loss': 2.159, 'grad_norm': 1.2081730365753174, 'learning_rate': 8.4e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 27.36it/s]                                               {'loss': 2.1026, 'grad_norm': 0.8522103428840637, 'learning_rate': 7.999999999999999e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 27.36it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.07it/s]                                               {'loss': 2.3107, 'grad_norm': 1.0067988634109497, 'learning_rate': 7.6e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 27.07it/s]                                               {'loss': 2.2861, 'grad_norm': 1.1133909225463867, 'learning_rate': 7.199999999999999e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 27.07it/s]                                               {'loss': 2.1894, 'grad_norm': 0.9508132934570312, 'learning_rate': 6.799999999999999e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 27.07it/s]                                               {'loss': 2.4856, 'grad_norm': 2.6799144744873047, 'learning_rate': 6.4e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 27.07it/s] 81%|████████▏ | 61/75 [00:02<00:00, 28.65it/s]                                               {'loss': 2.2867, 'grad_norm': 1.248620629310608, 'learning_rate': 5.9999999999999995e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 28.65it/s]                                               {'loss': 2.0544, 'grad_norm': 0.9479278922080994, 'learning_rate': 5.6e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 28.65it/s]                                               {'loss': 2.0785, 'grad_norm': 0.9571622014045715, 'learning_rate': 5.2e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 28.65it/s] 85%|████████▌ | 64/75 [00:02<00:00, 28.10it/s]                                               {'loss': 2.1666, 'grad_norm': 1.0614981651306152, 'learning_rate': 4.7999999999999994e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 28.10it/s]                                               {'loss': 2.2953, 'grad_norm': 1.281026005744934, 'learning_rate': 4.4e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 28.10it/s]                                               {'loss': 2.1981, 'grad_norm': 1.2716238498687744, 'learning_rate': 3.9999999999999996e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 28.10it/s] 89%|████████▉ | 67/75 [00:02<00:00, 27.62it/s]                                               {'loss': 2.2682, 'grad_norm': 0.967049241065979, 'learning_rate': 3.5999999999999994e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 27.62it/s]                                               {'loss': 2.1905, 'grad_norm': 0.9762307405471802, 'learning_rate': 3.2e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 27.62it/s]                                               {'loss': 2.4742, 'grad_norm': 1.5306823253631592, 'learning_rate': 2.8e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 27.62it/s] 93%|█████████▎| 70/75 [00:02<00:00, 27.16it/s]                                               {'loss': 2.1002, 'grad_norm': 1.1125187873840332, 'learning_rate': 2.3999999999999997e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 27.16it/s]                                               {'loss': 2.2884, 'grad_norm': 1.3822031021118164, 'learning_rate': 1.9999999999999998e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 27.16it/s]                                               {'loss': 2.3074, 'grad_norm': 1.0078089237213135, 'learning_rate': 1.6e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 27.16it/s] 97%|█████████▋| 73/75 [00:02<00:00, 27.07it/s]                                               {'loss': 2.3033, 'grad_norm': 1.306434988975525, 'learning_rate': 1.1999999999999999e-05, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 27.07it/s]                                               {'loss': 2.2774, 'grad_norm': 1.0397998094558716, 'learning_rate': 8e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 27.07it/s]                                               {'loss': 2.3422, 'grad_norm': 3.874819755554199, 'learning_rate': 4e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 27.07it/s]                                               {'train_runtime': 2.7828, 'train_samples_per_second': 406.069, 'train_steps_per_second': 26.951, 'train_loss': 2.427669537862142, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 27.07it/s]100%|██████████| 75/75 [00:02<00:00, 26.95it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.998, 'grad_norm': 0.33287400007247925, 'learning_rate': 0.0003, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:02, 25.81it/s]                                              {'loss': 2.8052, 'grad_norm': 0.341001033782959, 'learning_rate': 0.000296, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 26.86it/s]  4%|▍         | 3/75 [00:00<00:02, 27.38it/s]                                              {'loss': 2.9886, 'grad_norm': 0.5694117546081543, 'learning_rate': 0.000292, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 27.38it/s]                                              {'loss': 2.9029, 'grad_norm': 0.3890042304992676, 'learning_rate': 0.00028799999999999995, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 27.38it/s]                                              {'loss': 2.9505, 'grad_norm': 0.552703857421875, 'learning_rate': 0.00028399999999999996, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 27.38it/s]  8%|▊         | 6/75 [00:00<00:02, 26.65it/s]                                              {'loss': 2.896, 'grad_norm': 0.4621444344520569, 'learning_rate': 0.00028, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 26.65it/s]                                              {'loss': 2.8811, 'grad_norm': 0.6626129746437073, 'learning_rate': 0.000276, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 26.65it/s]                                              {'loss': 2.8197, 'grad_norm': 0.7784419059753418, 'learning_rate': 0.00027199999999999994, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 26.65it/s] 12%|█▏        | 9/75 [00:00<00:02, 26.63it/s]                                              {'loss': 2.7862, 'grad_norm': 0.6291106939315796, 'learning_rate': 0.00026799999999999995, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 26.63it/s]                                              {'loss': 2.72, 'grad_norm': 0.7491425275802612, 'learning_rate': 0.00026399999999999997, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 26.63it/s]                                               {'loss': 2.8074, 'grad_norm': 0.701319694519043, 'learning_rate': 0.00026, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 26.63it/s] 16%|█▌        | 12/75 [00:00<00:02, 26.99it/s]                                               {'loss': 2.7323, 'grad_norm': 0.8308457732200623, 'learning_rate': 0.000256, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 26.99it/s]                                               {'loss': 2.7399, 'grad_norm': 1.159110188484192, 'learning_rate': 0.00025199999999999995, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 26.99it/s]                                               {'loss': 2.6635, 'grad_norm': 0.7416694164276123, 'learning_rate': 0.00024799999999999996, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 26.99it/s]                                               {'loss': 2.7777, 'grad_norm': 1.8322548866271973, 'learning_rate': 0.000244, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 26.99it/s] 21%|██▏       | 16/75 [00:00<00:02, 28.72it/s]                                               {'loss': 2.6984, 'grad_norm': 0.7072330713272095, 'learning_rate': 0.00023999999999999998, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 28.72it/s]                                               {'loss': 2.6238, 'grad_norm': 0.807845413684845, 'learning_rate': 0.00023599999999999996, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 28.72it/s]                                               {'loss': 2.5331, 'grad_norm': 0.875749409198761, 'learning_rate': 0.00023199999999999997, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:01, 28.72it/s] 25%|██▌       | 19/75 [00:00<00:02, 27.87it/s]                                               {'loss': 2.5382, 'grad_norm': 0.9310247302055359, 'learning_rate': 0.00022799999999999999, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 27.87it/s]                                               {'loss': 2.5405, 'grad_norm': 0.9597575664520264, 'learning_rate': 0.000224, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:01, 27.87it/s]                                               {'loss': 2.5748, 'grad_norm': 0.8982852697372437, 'learning_rate': 0.00021999999999999995, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:01, 27.87it/s] 29%|██▉       | 22/75 [00:00<00:01, 27.32it/s]                                               {'loss': 2.5137, 'grad_norm': 1.0622303485870361, 'learning_rate': 0.00021599999999999996, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:01, 27.32it/s]                                               {'loss': 2.4086, 'grad_norm': 0.8325766324996948, 'learning_rate': 0.00021199999999999998, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 27.32it/s]                                               {'loss': 2.3807, 'grad_norm': 0.9896110892295837, 'learning_rate': 0.000208, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 27.32it/s] 33%|███▎      | 25/75 [00:00<00:01, 27.18it/s]                                               {'loss': 2.4813, 'grad_norm': 1.0739905834197998, 'learning_rate': 0.000204, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 27.18it/s]                                               {'loss': 2.61, 'grad_norm': 1.595109462738037, 'learning_rate': 0.00019999999999999998, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:00<00:01, 27.18it/s]                                               {'loss': 2.2289, 'grad_norm': 0.9546937942504883, 'learning_rate': 0.00019599999999999997, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:00<00:01, 27.18it/s] 37%|███▋      | 28/75 [00:01<00:01, 26.92it/s]                                               {'loss': 2.5121, 'grad_norm': 1.467580795288086, 'learning_rate': 0.00019199999999999998, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 26.92it/s]                                               {'loss': 2.5121, 'grad_norm': 0.9299376010894775, 'learning_rate': 0.000188, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 26.92it/s]                                               {'loss': 2.5331, 'grad_norm': 3.1895394325256348, 'learning_rate': 0.00018399999999999997, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 26.92it/s]                                               {'loss': 2.3079, 'grad_norm': 1.4923611879348755, 'learning_rate': 0.00017999999999999998, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.92it/s] 43%|████▎     | 32/75 [00:01<00:01, 28.56it/s]                                               {'loss': 2.3116, 'grad_norm': 1.1020499467849731, 'learning_rate': 0.000176, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 28.56it/s]                                               {'loss': 2.4473, 'grad_norm': 1.1697109937667847, 'learning_rate': 0.000172, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 28.56it/s]                                               {'loss': 2.3484, 'grad_norm': 1.2928187847137451, 'learning_rate': 0.000168, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 28.56it/s] 47%|████▋     | 35/75 [00:01<00:01, 27.84it/s]                                               {'loss': 2.3199, 'grad_norm': 1.0066808462142944, 'learning_rate': 0.00016399999999999997, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 27.84it/s]                                               {'loss': 2.2274, 'grad_norm': 1.3960247039794922, 'learning_rate': 0.00015999999999999999, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 27.84it/s]                                               {'loss': 2.2077, 'grad_norm': 1.3128560781478882, 'learning_rate': 0.000156, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 27.84it/s] 51%|█████     | 38/75 [00:01<00:01, 27.32it/s]                                               {'loss': 2.2872, 'grad_norm': 1.611132025718689, 'learning_rate': 0.000152, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 27.32it/s]                                               {'loss': 2.3436, 'grad_norm': 1.2877196073532104, 'learning_rate': 0.000148, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 27.32it/s]                                               {'loss': 2.4985, 'grad_norm': 1.4598809480667114, 'learning_rate': 0.00014399999999999998, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 27.32it/s] 55%|█████▍    | 41/75 [00:01<00:01, 27.18it/s]                                               {'loss': 2.3921, 'grad_norm': 1.1086801290512085, 'learning_rate': 0.00014, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 27.18it/s]                                               {'loss': 2.4766, 'grad_norm': 1.298497200012207, 'learning_rate': 0.00013599999999999997, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 27.18it/s]                                               {'loss': 2.5045, 'grad_norm': 1.1873849630355835, 'learning_rate': 0.00013199999999999998, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 27.18it/s] 59%|█████▊    | 44/75 [00:01<00:01, 27.05it/s]                                               {'loss': 2.1914, 'grad_norm': 1.0534472465515137, 'learning_rate': 0.000128, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 27.05it/s]                                               {'loss': 2.3604, 'grad_norm': 3.8130781650543213, 'learning_rate': 0.00012399999999999998, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 27.05it/s]                                               {'loss': 2.2343, 'grad_norm': 1.150444507598877, 'learning_rate': 0.00011999999999999999, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 27.05it/s]                                               {'loss': 2.1788, 'grad_norm': 1.4059576988220215, 'learning_rate': 0.00011599999999999999, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 27.05it/s] 64%|██████▍   | 48/75 [00:01<00:00, 28.64it/s]                                               {'loss': 2.2577, 'grad_norm': 1.4203033447265625, 'learning_rate': 0.000112, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 28.64it/s]                                               {'loss': 2.2099, 'grad_norm': 0.9701762199401855, 'learning_rate': 0.00010799999999999998, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 28.64it/s]                                               {'loss': 2.3073, 'grad_norm': 0.9535826444625854, 'learning_rate': 0.000104, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 28.64it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.92it/s]                                               {'loss': 2.3588, 'grad_norm': 1.227797508239746, 'learning_rate': 9.999999999999999e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 27.92it/s]                                               {'loss': 2.3792, 'grad_norm': 1.367453694343567, 'learning_rate': 9.599999999999999e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 27.92it/s]                                               {'loss': 2.3707, 'grad_norm': 1.6610726118087769, 'learning_rate': 9.199999999999999e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:01<00:00, 27.92it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.22it/s]                                               {'loss': 2.1491, 'grad_norm': 1.1296342611312866, 'learning_rate': 8.8e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:01<00:00, 27.22it/s]                                               {'loss': 2.1205, 'grad_norm': 0.9377990961074829, 'learning_rate': 8.4e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 27.22it/s]                                               {'loss': 2.3583, 'grad_norm': 1.1994891166687012, 'learning_rate': 7.999999999999999e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 27.22it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.09it/s]                                               {'loss': 2.315, 'grad_norm': 1.0408748388290405, 'learning_rate': 7.6e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 27.09it/s]                                               {'loss': 2.2496, 'grad_norm': 1.0891475677490234, 'learning_rate': 7.199999999999999e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 27.09it/s]                                               {'loss': 2.4206, 'grad_norm': 1.9168769121170044, 'learning_rate': 6.799999999999999e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 27.09it/s]                                               {'loss': 2.2475, 'grad_norm': 2.5262341499328613, 'learning_rate': 6.4e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 27.09it/s] 81%|████████▏ | 61/75 [00:02<00:00, 28.67it/s]                                               {'loss': 2.3114, 'grad_norm': 1.5095735788345337, 'learning_rate': 5.9999999999999995e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 28.67it/s]                                               {'loss': 2.2683, 'grad_norm': 1.1792280673980713, 'learning_rate': 5.6e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 28.67it/s]                                               {'loss': 2.1326, 'grad_norm': 1.0329010486602783, 'learning_rate': 5.2e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 28.67it/s] 85%|████████▌ | 64/75 [00:02<00:00, 28.22it/s]                                               {'loss': 2.1982, 'grad_norm': 1.266608715057373, 'learning_rate': 4.7999999999999994e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 28.22it/s]                                               {'loss': 2.3356, 'grad_norm': 1.3283321857452393, 'learning_rate': 4.4e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 28.22it/s]                                               {'loss': 2.2892, 'grad_norm': 1.1976895332336426, 'learning_rate': 3.9999999999999996e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 28.22it/s] 89%|████████▉ | 67/75 [00:02<00:00, 27.56it/s]                                               {'loss': 2.3065, 'grad_norm': 1.091292381286621, 'learning_rate': 3.5999999999999994e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 27.56it/s]                                               {'loss': 2.2863, 'grad_norm': 1.124869465827942, 'learning_rate': 3.2e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 27.56it/s]                                               {'loss': 2.3325, 'grad_norm': 1.0544402599334717, 'learning_rate': 2.8e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 27.56it/s] 93%|█████████▎| 70/75 [00:02<00:00, 27.15it/s]                                               {'loss': 2.0932, 'grad_norm': 1.1331297159194946, 'learning_rate': 2.3999999999999997e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 27.15it/s]                                               {'loss': 2.0323, 'grad_norm': 0.9146566987037659, 'learning_rate': 1.9999999999999998e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 27.15it/s]                                               {'loss': 2.3775, 'grad_norm': 1.5274989604949951, 'learning_rate': 1.6e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 27.15it/s] 97%|█████████▋| 73/75 [00:02<00:00, 27.08it/s]                                               {'loss': 2.121, 'grad_norm': 0.9930986166000366, 'learning_rate': 1.1999999999999999e-05, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 27.08it/s]                                               {'loss': 2.262, 'grad_norm': 0.917411208152771, 'learning_rate': 8e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 27.08it/s]                                               {'loss': 2.6127, 'grad_norm': 4.918824195861816, 'learning_rate': 4e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 27.08it/s]                                               {'train_runtime': 2.7743, 'train_samples_per_second': 407.308, 'train_steps_per_second': 27.034, 'train_loss': 2.4466620095570883, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 27.08it/s]100%|██████████| 75/75 [00:02<00:00, 27.04it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:32
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 3.0513, 'grad_norm': 0.4896029233932495, 'learning_rate': 0.0003, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.57it/s]                                              {'loss': 2.955, 'grad_norm': 0.4002465605735779, 'learning_rate': 0.000296, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 25.65it/s]  4%|▍         | 3/75 [00:00<00:02, 26.51it/s]                                              {'loss': 2.7631, 'grad_norm': 0.410407692193985, 'learning_rate': 0.000292, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 26.51it/s]                                              {'loss': 2.8078, 'grad_norm': 0.43068426847457886, 'learning_rate': 0.00028799999999999995, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 26.51it/s]                                              {'loss': 2.8339, 'grad_norm': 0.49408140778541565, 'learning_rate': 0.00028399999999999996, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 26.51it/s]  8%|▊         | 6/75 [00:00<00:02, 26.74it/s]                                              {'loss': 2.8655, 'grad_norm': 0.8458051681518555, 'learning_rate': 0.00028, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 26.74it/s]                                              {'loss': 2.7723, 'grad_norm': 0.8253248929977417, 'learning_rate': 0.000276, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 26.74it/s]                                              {'loss': 2.8208, 'grad_norm': 0.6588025093078613, 'learning_rate': 0.00027199999999999994, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 26.74it/s] 12%|█▏        | 9/75 [00:00<00:02, 26.65it/s]                                              {'loss': 2.7617, 'grad_norm': 0.6813689470291138, 'learning_rate': 0.00026799999999999995, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 26.65it/s]                                              {'loss': 2.8084, 'grad_norm': 0.9585041403770447, 'learning_rate': 0.00026399999999999997, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 26.65it/s]                                               {'loss': 2.6963, 'grad_norm': 0.7773921489715576, 'learning_rate': 0.00026, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 26.65it/s] 16%|█▌        | 12/75 [00:00<00:02, 26.39it/s]                                               {'loss': 2.7703, 'grad_norm': 0.6009793281555176, 'learning_rate': 0.000256, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 26.39it/s]                                               {'loss': 2.7626, 'grad_norm': 0.9498465657234192, 'learning_rate': 0.00025199999999999995, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 26.39it/s]                                               {'loss': 2.6412, 'grad_norm': 0.857519805431366, 'learning_rate': 0.00024799999999999996, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 26.39it/s]                                               {'loss': 2.25, 'grad_norm': 2.845005512237549, 'learning_rate': 0.000244, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 26.39it/s] 21%|██▏       | 16/75 [00:00<00:02, 28.38it/s]                                               {'loss': 2.534, 'grad_norm': 0.7849456667900085, 'learning_rate': 0.00023999999999999998, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 28.38it/s]                                               {'loss': 2.5675, 'grad_norm': 0.8536257147789001, 'learning_rate': 0.00023599999999999996, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 28.38it/s]                                               {'loss': 2.5425, 'grad_norm': 1.235679268836975, 'learning_rate': 0.00023199999999999997, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 28.38it/s] 25%|██▌       | 19/75 [00:00<00:02, 27.54it/s]                                               {'loss': 2.597, 'grad_norm': 1.0422173738479614, 'learning_rate': 0.00022799999999999999, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 27.54it/s]                                               {'loss': 2.5812, 'grad_norm': 0.8950054049491882, 'learning_rate': 0.000224, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:01, 27.54it/s]                                               {'loss': 2.4306, 'grad_norm': 1.209798812866211, 'learning_rate': 0.00021999999999999995, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:01, 27.54it/s] 29%|██▉       | 22/75 [00:00<00:01, 27.11it/s]                                               {'loss': 2.323, 'grad_norm': 1.0951416492462158, 'learning_rate': 0.00021599999999999996, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:01, 27.11it/s]                                               {'loss': 2.4467, 'grad_norm': 0.860066294670105, 'learning_rate': 0.00021199999999999998, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 27.11it/s]                                               {'loss': 2.5793, 'grad_norm': 1.0837881565093994, 'learning_rate': 0.000208, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 27.11it/s] 33%|███▎      | 25/75 [00:00<00:01, 27.05it/s]                                               {'loss': 2.5125, 'grad_norm': 0.9692090749740601, 'learning_rate': 0.000204, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 27.05it/s]                                               {'loss': 2.5684, 'grad_norm': 1.1102783679962158, 'learning_rate': 0.00019999999999999998, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:00<00:01, 27.05it/s]                                               {'loss': 2.4052, 'grad_norm': 1.5465774536132812, 'learning_rate': 0.00019599999999999997, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:00<00:01, 27.05it/s] 37%|███▋      | 28/75 [00:01<00:01, 26.74it/s]                                               {'loss': 2.3404, 'grad_norm': 1.681026816368103, 'learning_rate': 0.00019199999999999998, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 26.74it/s]                                               {'loss': 2.3734, 'grad_norm': 1.0217630863189697, 'learning_rate': 0.000188, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 26.74it/s]                                               {'loss': 1.9284, 'grad_norm': 2.433237075805664, 'learning_rate': 0.00018399999999999997, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 26.74it/s]                                               {'loss': 2.4749, 'grad_norm': 1.0087158679962158, 'learning_rate': 0.00017999999999999998, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.74it/s] 43%|████▎     | 32/75 [00:01<00:01, 28.35it/s]                                               {'loss': 2.2052, 'grad_norm': 1.2721401453018188, 'learning_rate': 0.000176, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 28.35it/s]                                               {'loss': 2.5057, 'grad_norm': 1.1559842824935913, 'learning_rate': 0.000172, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 28.35it/s]                                               {'loss': 2.3179, 'grad_norm': 1.507531762123108, 'learning_rate': 0.000168, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 28.35it/s] 47%|████▋     | 35/75 [00:01<00:01, 27.76it/s]                                               {'loss': 2.2465, 'grad_norm': 1.252897024154663, 'learning_rate': 0.00016399999999999997, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 27.76it/s]                                               {'loss': 2.2461, 'grad_norm': 1.1859045028686523, 'learning_rate': 0.00015999999999999999, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 27.76it/s]                                               {'loss': 2.1693, 'grad_norm': 1.2733479738235474, 'learning_rate': 0.000156, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 27.76it/s] 51%|█████     | 38/75 [00:01<00:01, 27.29it/s]                                               {'loss': 2.423, 'grad_norm': 1.2916183471679688, 'learning_rate': 0.000152, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 27.29it/s]                                               {'loss': 2.3428, 'grad_norm': 1.7683463096618652, 'learning_rate': 0.000148, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 27.29it/s]                                               {'loss': 2.4856, 'grad_norm': 1.2307177782058716, 'learning_rate': 0.00014399999999999998, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 27.29it/s] 55%|█████▍    | 41/75 [00:01<00:01, 27.19it/s]                                               {'loss': 2.2377, 'grad_norm': 1.2244906425476074, 'learning_rate': 0.00014, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 27.19it/s]                                               {'loss': 2.2049, 'grad_norm': 1.5786566734313965, 'learning_rate': 0.00013599999999999997, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 27.19it/s]                                               {'loss': 2.3138, 'grad_norm': 1.3946844339370728, 'learning_rate': 0.00013199999999999998, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 27.19it/s] 59%|█████▊    | 44/75 [00:01<00:01, 26.93it/s]                                               {'loss': 2.2554, 'grad_norm': 0.9041936993598938, 'learning_rate': 0.000128, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 26.93it/s]                                               {'loss': 2.3743, 'grad_norm': 3.15748929977417, 'learning_rate': 0.00012399999999999998, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 26.93it/s]                                               {'loss': 2.1769, 'grad_norm': 1.1586304903030396, 'learning_rate': 0.00011999999999999999, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.93it/s]                                               {'loss': 2.2019, 'grad_norm': 0.9604124426841736, 'learning_rate': 0.00011599999999999999, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.93it/s] 64%|██████▍   | 48/75 [00:01<00:00, 28.39it/s]                                               {'loss': 2.2827, 'grad_norm': 1.2204545736312866, 'learning_rate': 0.000112, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 28.39it/s]                                               {'loss': 2.3341, 'grad_norm': 1.143956184387207, 'learning_rate': 0.00010799999999999998, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 28.39it/s]                                               {'loss': 2.2702, 'grad_norm': 1.0066685676574707, 'learning_rate': 0.000104, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 28.39it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.84it/s]                                               {'loss': 2.1921, 'grad_norm': 1.6161463260650635, 'learning_rate': 9.999999999999999e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 27.84it/s]                                               {'loss': 2.1225, 'grad_norm': 1.144453763961792, 'learning_rate': 9.599999999999999e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 27.84it/s]                                               {'loss': 2.1635, 'grad_norm': 1.36649489402771, 'learning_rate': 9.199999999999999e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:01<00:00, 27.84it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.32it/s]                                               {'loss': 2.5005, 'grad_norm': 1.2711150646209717, 'learning_rate': 8.8e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:01<00:00, 27.32it/s]                                               {'loss': 2.2362, 'grad_norm': 1.2218172550201416, 'learning_rate': 8.4e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 27.32it/s]                                               {'loss': 2.5118, 'grad_norm': 1.114341139793396, 'learning_rate': 7.999999999999999e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 27.32it/s] 76%|███████▌  | 57/75 [00:02<00:00, 27.12it/s]                                               {'loss': 2.1022, 'grad_norm': 1.355168342590332, 'learning_rate': 7.6e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 27.12it/s]                                               {'loss': 2.2036, 'grad_norm': 1.0885987281799316, 'learning_rate': 7.199999999999999e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 27.12it/s]                                               {'loss': 2.2622, 'grad_norm': 1.248515248298645, 'learning_rate': 6.799999999999999e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 27.12it/s]                                               {'loss': 2.4876, 'grad_norm': 3.9151172637939453, 'learning_rate': 6.4e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 27.12it/s] 81%|████████▏ | 61/75 [00:02<00:00, 28.43it/s]                                               {'loss': 2.4547, 'grad_norm': 1.5316758155822754, 'learning_rate': 5.9999999999999995e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 28.43it/s]                                               {'loss': 2.1374, 'grad_norm': 1.0822248458862305, 'learning_rate': 5.6e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 28.43it/s]                                               {'loss': 2.1536, 'grad_norm': 0.9782738089561462, 'learning_rate': 5.2e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 28.43it/s] 85%|████████▌ | 64/75 [00:02<00:00, 27.86it/s]                                               {'loss': 2.1454, 'grad_norm': 1.3540713787078857, 'learning_rate': 4.7999999999999994e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 27.86it/s]                                               {'loss': 2.5634, 'grad_norm': 1.4227901697158813, 'learning_rate': 4.4e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 27.86it/s]                                               {'loss': 2.224, 'grad_norm': 1.234864354133606, 'learning_rate': 3.9999999999999996e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 27.86it/s] 89%|████████▉ | 67/75 [00:02<00:00, 27.56it/s]                                               {'loss': 2.3201, 'grad_norm': 1.0886389017105103, 'learning_rate': 3.5999999999999994e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 27.56it/s]                                               {'loss': 2.0816, 'grad_norm': 1.1314592361450195, 'learning_rate': 3.2e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 27.56it/s]                                               {'loss': 2.295, 'grad_norm': 1.0836066007614136, 'learning_rate': 2.8e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 27.56it/s] 93%|█████████▎| 70/75 [00:02<00:00, 27.05it/s]                                               {'loss': 2.1648, 'grad_norm': 1.1307743787765503, 'learning_rate': 2.3999999999999997e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 27.05it/s]                                               {'loss': 2.059, 'grad_norm': 1.0224746465682983, 'learning_rate': 1.9999999999999998e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 27.05it/s]                                               {'loss': 2.2277, 'grad_norm': 0.9463680386543274, 'learning_rate': 1.6e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 27.05it/s] 97%|█████████▋| 73/75 [00:02<00:00, 26.76it/s]                                               {'loss': 2.0952, 'grad_norm': 0.8937111496925354, 'learning_rate': 1.1999999999999999e-05, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 26.76it/s]                                               {'loss': 2.2078, 'grad_norm': 1.4172124862670898, 'learning_rate': 8e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 26.76it/s]                                               {'loss': 1.9652, 'grad_norm': 2.3775668144226074, 'learning_rate': 4e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.76it/s]                                               {'train_runtime': 2.7933, 'train_samples_per_second': 404.543, 'train_steps_per_second': 26.85, 'train_loss': 2.4031555032730103, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.76it/s]100%|██████████| 75/75 [00:02<00:00, 26.85it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(2074, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1703, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1537, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1552, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1491, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1548, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1460, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1450, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1276, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1361, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1325, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1573, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 65.03it/s]  3%|▎         | 14/471 [00:00<00:07, 58.05it/s]  4%|▍         | 20/471 [00:00<00:08, 56.07it/s]  6%|▌         | 26/471 [00:00<00:08, 55.25it/s]  7%|▋         | 32/471 [00:00<00:08, 54.67it/s]  8%|▊         | 38/471 [00:00<00:07, 54.37it/s]  9%|▉         | 44/471 [00:00<00:07, 54.17it/s] 11%|█         | 50/471 [00:00<00:07, 53.95it/s] 12%|█▏        | 56/471 [00:01<00:07, 53.97it/s] 13%|█▎        | 62/471 [00:01<00:07, 53.75it/s] 14%|█▍        | 68/471 [00:01<00:07, 53.69it/s] 16%|█▌        | 74/471 [00:01<00:07, 53.62it/s] 17%|█▋        | 80/471 [00:01<00:07, 53.48it/s] 18%|█▊        | 86/471 [00:01<00:07, 53.60it/s] 20%|█▉        | 92/471 [00:01<00:07, 53.44it/s] 21%|██        | 98/471 [00:01<00:06, 53.42it/s] 22%|██▏       | 104/471 [00:01<00:06, 53.54it/s] 23%|██▎       | 110/471 [00:02<00:06, 53.41it/s] 25%|██▍       | 116/471 [00:02<00:06, 53.52it/s] 26%|██▌       | 122/471 [00:02<00:06, 53.42it/s] 27%|██▋       | 128/471 [00:02<00:06, 53.32it/s] 28%|██▊       | 134/471 [00:02<00:06, 53.53it/s] 30%|██▉       | 140/471 [00:02<00:06, 53.40it/s] 31%|███       | 146/471 [00:02<00:06, 53.40it/s] 32%|███▏      | 152/471 [00:02<00:05, 53.41it/s] 34%|███▎      | 158/471 [00:02<00:05, 53.29it/s] 35%|███▍      | 164/471 [00:03<00:05, 53.46it/s] 36%|███▌      | 170/471 [00:03<00:05, 53.36it/s] 37%|███▋      | 176/471 [00:03<00:05, 53.22it/s] 39%|███▊      | 182/471 [00:03<00:05, 53.38it/s] 40%|███▉      | 188/471 [00:03<00:05, 53.21it/s] 41%|████      | 194/471 [00:03<00:05, 52.96it/s] 42%|████▏     | 200/471 [00:03<00:05, 53.20it/s] 44%|████▎     | 206/471 [00:03<00:04, 53.16it/s] 45%|████▌     | 212/471 [00:03<00:04, 53.20it/s] 46%|████▋     | 218/471 [00:04<00:04, 53.32it/s] 48%|████▊     | 224/471 [00:04<00:04, 53.22it/s] 49%|████▉     | 230/471 [00:04<00:04, 53.33it/s] 50%|█████     | 236/471 [00:04<00:04, 53.24it/s] 51%|█████▏    | 242/471 [00:04<00:04, 53.12it/s] 53%|█████▎    | 248/471 [00:04<00:04, 53.30it/s] 54%|█████▍    | 254/471 [00:04<00:04, 53.19it/s] 55%|█████▌    | 260/471 [00:04<00:03, 53.12it/s] 56%|█████▋    | 266/471 [00:04<00:03, 53.27it/s] 58%|█████▊    | 272/471 [00:05<00:03, 53.16it/s] 59%|█████▉    | 278/471 [00:05<00:03, 53.13it/s] 60%|██████    | 284/471 [00:05<00:03, 53.25it/s] 62%|██████▏   | 290/471 [00:05<00:03, 53.11it/s] 63%|██████▎   | 296/471 [00:05<00:03, 53.17it/s] 64%|██████▍   | 302/471 [00:05<00:03, 53.24it/s] 65%|██████▌   | 308/471 [00:05<00:03, 53.15it/s] 67%|██████▋   | 314/471 [00:05<00:02, 53.12it/s] 68%|██████▊   | 320/471 [00:05<00:02, 53.16it/s] 69%|██████▉   | 326/471 [00:06<00:02, 53.04it/s] 70%|███████   | 332/471 [00:06<00:02, 53.19it/s] 72%|███████▏  | 338/471 [00:06<00:02, 53.10it/s] 73%|███████▎  | 344/471 [00:06<00:02, 52.99it/s] 74%|███████▍  | 350/471 [00:06<00:02, 53.14it/s] 76%|███████▌  | 356/471 [00:06<00:02, 53.07it/s] 77%|███████▋  | 362/471 [00:06<00:02, 52.98it/s] 78%|███████▊  | 368/471 [00:06<00:01, 53.14it/s] 79%|███████▉  | 374/471 [00:06<00:01, 52.99it/s] 81%|████████  | 380/471 [00:07<00:01, 52.91it/s] 82%|████████▏ | 386/471 [00:07<00:01, 53.13it/s] 83%|████████▎ | 392/471 [00:07<00:01, 53.01it/s] 85%|████████▍ | 398/471 [00:07<00:01, 52.91it/s] 86%|████████▌ | 404/471 [00:07<00:01, 53.10it/s] 87%|████████▋ | 410/471 [00:07<00:01, 52.99it/s] 88%|████████▊ | 416/471 [00:07<00:01, 52.92it/s] 90%|████████▉ | 422/471 [00:07<00:00, 53.13it/s] 91%|█████████ | 428/471 [00:08<00:00, 53.02it/s] 92%|█████████▏| 434/471 [00:08<00:00, 52.94it/s] 93%|█████████▎| 440/471 [00:08<00:00, 53.08it/s] 95%|█████████▍| 446/471 [00:08<00:00, 52.96it/s] 96%|█████████▌| 452/471 [00:08<00:00, 52.90it/s] 97%|█████████▋| 458/471 [00:08<00:00, 53.04it/s] 99%|█████████▊| 464/471 [00:08<00:00, 52.92it/s]100%|█████████▉| 470/471 [00:08<00:00, 52.89it/s]100%|██████████| 471/471 [00:08<00:00, 53.41it/s]
{'eval_loss': 2.841414451599121, 'eval_model_preparation_time': 0.0021, 'eval_acc': 0.12692511949017526, 'eval_runtime': 8.8383, 'eval_samples_per_second': 852.202, 'eval_steps_per_second': 53.291}
ROUND:1
CLIENT:14
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.6963, 'grad_norm': 5.845194339752197, 'learning_rate': 0.00022792407799438732, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:02, 25.53it/s]                                              {'loss': 2.6874, 'grad_norm': 3.753948926925659, 'learning_rate': 0.0002248850902877955, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 26.16it/s]  4%|▍         | 3/75 [00:00<00:02, 26.11it/s]                                              {'loss': 2.6089, 'grad_norm': 4.210590362548828, 'learning_rate': 0.00022184610258120367, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 26.11it/s]                                              {'loss': 2.5979, 'grad_norm': 4.374686241149902, 'learning_rate': 0.0002188071148746118, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 26.11it/s]                                              {'loss': 2.5372, 'grad_norm': 5.1274919509887695, 'learning_rate': 0.00021576812716802, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 26.11it/s]  8%|▊         | 6/75 [00:00<00:02, 25.90it/s]                                              {'loss': 2.5702, 'grad_norm': 4.863040447235107, 'learning_rate': 0.00021272913946142818, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.90it/s]                                              {'loss': 2.5142, 'grad_norm': 4.225435256958008, 'learning_rate': 0.00020969015175483635, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.90it/s]                                              {'loss': 2.4279, 'grad_norm': 4.116603851318359, 'learning_rate': 0.0002066511640482445, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.90it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.85it/s]                                              {'loss': 2.4162, 'grad_norm': 3.3448917865753174, 'learning_rate': 0.00020361217634165267, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.85it/s]                                              {'loss': 2.4223, 'grad_norm': 3.544029951095581, 'learning_rate': 0.00020057318863506084, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.85it/s]                                               {'loss': 2.3667, 'grad_norm': 4.715819835662842, 'learning_rate': 0.000197534200928469, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.85it/s] 16%|█▌        | 12/75 [00:00<00:02, 26.19it/s]                                               {'loss': 2.3996, 'grad_norm': 4.779453754425049, 'learning_rate': 0.0001944952132218772, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 26.19it/s]                                               {'loss': 2.4288, 'grad_norm': 4.333915710449219, 'learning_rate': 0.00019145622551528535, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 26.19it/s]                                               {'loss': 2.4446, 'grad_norm': 4.772586822509766, 'learning_rate': 0.00018841723780869352, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 26.19it/s]                                               {'loss': 2.6417, 'grad_norm': 20.044010162353516, 'learning_rate': 0.0001853782501021017, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 26.19it/s] 21%|██▏       | 16/75 [00:00<00:02, 27.81it/s]                                               {'loss': 2.3141, 'grad_norm': 4.795694828033447, 'learning_rate': 0.00018233926239550986, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 27.81it/s]                                               {'loss': 2.2461, 'grad_norm': 3.6625101566314697, 'learning_rate': 0.000179300274688918, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 27.81it/s]                                               {'loss': 2.1423, 'grad_norm': 3.499988555908203, 'learning_rate': 0.0001762612869823262, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 27.81it/s] 25%|██▌       | 19/75 [00:00<00:02, 27.37it/s]                                               {'loss': 2.3278, 'grad_norm': 4.102872848510742, 'learning_rate': 0.00017322229927573438, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 27.37it/s]                                               {'loss': 2.318, 'grad_norm': 4.614943981170654, 'learning_rate': 0.00017018331156914255, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 27.37it/s]                                               {'loss': 2.1724, 'grad_norm': 4.29987907409668, 'learning_rate': 0.0001671443238625507, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:01, 27.37it/s] 29%|██▉       | 22/75 [00:00<00:01, 27.10it/s]                                               {'loss': 2.4314, 'grad_norm': 3.948939561843872, 'learning_rate': 0.00016410533615595886, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:01, 27.10it/s]                                               {'loss': 2.3584, 'grad_norm': 3.9834282398223877, 'learning_rate': 0.00016106634844936704, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 27.10it/s]                                               {'loss': 2.3445, 'grad_norm': 3.607342004776001, 'learning_rate': 0.0001580273607427752, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 27.10it/s] 33%|███▎      | 25/75 [00:00<00:01, 26.85it/s]                                               {'loss': 2.1124, 'grad_norm': 4.7289814949035645, 'learning_rate': 0.00015498837303618338, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 26.85it/s]                                               {'loss': 2.0908, 'grad_norm': 3.403238296508789, 'learning_rate': 0.00015194938532959155, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:00<00:01, 26.85it/s]                                               {'loss': 2.0208, 'grad_norm': 3.687041997909546, 'learning_rate': 0.00014891039762299972, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 26.85it/s] 37%|███▋      | 28/75 [00:01<00:01, 26.56it/s]                                               {'loss': 2.3116, 'grad_norm': 3.768951654434204, 'learning_rate': 0.0001458714099164079, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 26.56it/s]                                               {'loss': 2.3244, 'grad_norm': 4.103758335113525, 'learning_rate': 0.00014283242220981606, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 26.56it/s]                                               {'loss': 2.0745, 'grad_norm': 13.24643611907959, 'learning_rate': 0.0001397934345032242, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 26.56it/s]                                               {'loss': 2.319, 'grad_norm': 3.473329544067383, 'learning_rate': 0.00013675444679663238, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.56it/s] 43%|████▎     | 32/75 [00:01<00:01, 28.12it/s]                                               {'loss': 2.0829, 'grad_norm': 3.2750179767608643, 'learning_rate': 0.00013371545909004058, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 28.12it/s]                                               {'loss': 2.173, 'grad_norm': 3.5631937980651855, 'learning_rate': 0.00013067647138344875, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 28.12it/s]                                               {'loss': 2.122, 'grad_norm': 3.3394970893859863, 'learning_rate': 0.00012763748367685692, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 28.12it/s] 47%|████▋     | 35/75 [00:01<00:01, 27.47it/s]                                               {'loss': 2.3732, 'grad_norm': 6.251216411590576, 'learning_rate': 0.00012459849597026506, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 27.47it/s]                                               {'loss': 2.0082, 'grad_norm': 3.755851984024048, 'learning_rate': 0.00012155950826367323, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 27.47it/s]                                               {'loss': 2.1784, 'grad_norm': 4.295971393585205, 'learning_rate': 0.00011852052055708142, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 27.47it/s] 51%|█████     | 38/75 [00:01<00:01, 27.18it/s]                                               {'loss': 2.3675, 'grad_norm': 3.8417625427246094, 'learning_rate': 0.00011548153285048959, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 27.18it/s]                                               {'loss': 2.2307, 'grad_norm': 3.2681455612182617, 'learning_rate': 0.00011244254514389775, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 27.18it/s]                                               {'loss': 2.2805, 'grad_norm': 3.8298416137695312, 'learning_rate': 0.0001094035574373059, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 27.18it/s] 55%|█████▍    | 41/75 [00:01<00:01, 26.93it/s]                                               {'loss': 2.1557, 'grad_norm': 5.454148769378662, 'learning_rate': 0.00010636456973071409, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 26.93it/s]                                               {'loss': 2.0557, 'grad_norm': 3.5463130474090576, 'learning_rate': 0.00010332558202412225, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 26.93it/s]                                               {'loss': 2.2118, 'grad_norm': 4.056927680969238, 'learning_rate': 0.00010028659431753042, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 26.93it/s] 59%|█████▊    | 44/75 [00:01<00:01, 26.58it/s]                                               {'loss': 2.1982, 'grad_norm': 3.9975433349609375, 'learning_rate': 9.72476066109386e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 26.58it/s]                                               {'loss': 2.5054, 'grad_norm': 18.203365325927734, 'learning_rate': 9.420861890434676e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 26.58it/s]                                               {'loss': 2.1599, 'grad_norm': 3.4582130908966064, 'learning_rate': 9.116963119775493e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.58it/s]                                               {'loss': 2.1384, 'grad_norm': 4.174638271331787, 'learning_rate': 8.81306434911631e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.58it/s] 64%|██████▍   | 48/75 [00:01<00:00, 28.08it/s]                                               {'loss': 2.1094, 'grad_norm': 3.507664680480957, 'learning_rate': 8.509165578457127e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 28.08it/s]                                               {'loss': 2.1066, 'grad_norm': 4.615678787231445, 'learning_rate': 8.205266807797943e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 28.08it/s]                                               {'loss': 2.1959, 'grad_norm': 3.2873244285583496, 'learning_rate': 7.90136803713876e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 28.08it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.52it/s]                                               {'loss': 2.2217, 'grad_norm': 3.1040005683898926, 'learning_rate': 7.597469266479577e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 27.52it/s]                                               {'loss': 2.1112, 'grad_norm': 4.360052585601807, 'learning_rate': 7.293570495820395e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 27.52it/s]                                               {'loss': 2.1759, 'grad_norm': 4.081393718719482, 'learning_rate': 6.98967172516121e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:01<00:00, 27.52it/s] 72%|███████▏  | 54/75 [00:01<00:00, 26.95it/s]                                               {'loss': 2.0158, 'grad_norm': 3.5840539932250977, 'learning_rate': 6.685772954502029e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:01<00:00, 26.95it/s]                                               {'loss': 2.1846, 'grad_norm': 3.2366414070129395, 'learning_rate': 6.381874183842846e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 26.95it/s]                                               {'loss': 2.22, 'grad_norm': 3.6869585514068604, 'learning_rate': 6.077975413183662e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 26.95it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.66it/s]                                               {'loss': 2.2702, 'grad_norm': 3.407548666000366, 'learning_rate': 5.7740766425244795e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 26.66it/s]                                               {'loss': 2.0029, 'grad_norm': 3.6714789867401123, 'learning_rate': 5.470177871865295e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 26.66it/s]                                               {'loss': 2.1752, 'grad_norm': 2.5455000400543213, 'learning_rate': 5.1662791012061124e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 26.66it/s]                                               {'loss': 1.9762, 'grad_norm': 8.396158218383789, 'learning_rate': 4.86238033054693e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 26.66it/s] 81%|████████▏ | 61/75 [00:02<00:00, 28.04it/s]                                               {'loss': 2.1813, 'grad_norm': 3.0679569244384766, 'learning_rate': 4.5584815598877466e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 28.04it/s]                                               {'loss': 2.3179, 'grad_norm': 4.439636707305908, 'learning_rate': 4.254582789228564e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 28.04it/s]                                               {'loss': 2.0151, 'grad_norm': 3.6706619262695312, 'learning_rate': 3.95068401856938e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 28.04it/s] 85%|████████▌ | 64/75 [00:02<00:00, 27.42it/s]                                               {'loss': 2.4166, 'grad_norm': 5.688992977142334, 'learning_rate': 3.646785247910197e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 27.42it/s]                                               {'loss': 2.0566, 'grad_norm': 3.082038164138794, 'learning_rate': 3.3428864772510144e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 27.42it/s]                                               {'loss': 1.9225, 'grad_norm': 2.6378536224365234, 'learning_rate': 3.038987706591831e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 27.42it/s] 89%|████████▉ | 67/75 [00:02<00:00, 27.01it/s]                                               {'loss': 2.0619, 'grad_norm': 3.1956777572631836, 'learning_rate': 2.7350889359326476e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 27.01it/s]                                               {'loss': 1.9959, 'grad_norm': 3.6276473999023438, 'learning_rate': 2.431190165273465e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 27.01it/s]                                               {'loss': 2.1515, 'grad_norm': 3.45546817779541, 'learning_rate': 2.127291394614282e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 27.01it/s] 93%|█████████▎| 70/75 [00:02<00:00, 26.84it/s]                                               {'loss': 2.2782, 'grad_norm': 3.4895148277282715, 'learning_rate': 1.8233926239550986e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 26.84it/s]                                               {'loss': 2.0796, 'grad_norm': 4.2746357917785645, 'learning_rate': 1.5194938532959154e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 26.84it/s]                                               {'loss': 1.9919, 'grad_norm': 2.6614015102386475, 'learning_rate': 1.2155950826367325e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 26.84it/s] 97%|█████████▋| 73/75 [00:02<00:00, 26.65it/s]                                               {'loss': 1.9158, 'grad_norm': 3.652297258377075, 'learning_rate': 9.116963119775493e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 26.65it/s]                                               {'loss': 2.1213, 'grad_norm': 3.6370372772216797, 'learning_rate': 6.077975413183663e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 26.65it/s]                                               {'loss': 2.0626, 'grad_norm': 6.19185733795166, 'learning_rate': 3.0389877065918314e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.65it/s]                                               {'train_runtime': 2.8282, 'train_samples_per_second': 399.552, 'train_steps_per_second': 26.519, 'train_loss': 2.24325536886851, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.65it/s]100%|██████████| 75/75 [00:02<00:00, 26.52it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:18
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.8048, 'grad_norm': 3.197927713394165, 'learning_rate': 0.00022792407799438732, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:02, 25.49it/s]                                              {'loss': 2.8815, 'grad_norm': 3.206334352493286, 'learning_rate': 0.0002248850902877955, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 26.65it/s]  4%|▍         | 3/75 [00:00<00:02, 26.40it/s]                                              {'loss': 2.7866, 'grad_norm': 7.022216796875, 'learning_rate': 0.00022184610258120367, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 26.40it/s]                                              {'loss': 2.6243, 'grad_norm': 4.108603477478027, 'learning_rate': 0.0002188071148746118, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 26.40it/s]                                              {'loss': 2.6875, 'grad_norm': 3.8645851612091064, 'learning_rate': 0.00021576812716802, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 26.40it/s]  8%|▊         | 6/75 [00:00<00:02, 26.08it/s]                                              {'loss': 2.6135, 'grad_norm': 3.8950841426849365, 'learning_rate': 0.00021272913946142818, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 26.08it/s]                                              {'loss': 2.5621, 'grad_norm': 3.2917072772979736, 'learning_rate': 0.00020969015175483635, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 26.08it/s]                                              {'loss': 2.5222, 'grad_norm': 4.084153652191162, 'learning_rate': 0.0002066511640482445, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 26.08it/s] 12%|█▏        | 9/75 [00:00<00:02, 26.15it/s]                                              {'loss': 2.3698, 'grad_norm': 3.2730562686920166, 'learning_rate': 0.00020361217634165267, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 26.15it/s]                                              {'loss': 2.4271, 'grad_norm': 4.258556842803955, 'learning_rate': 0.00020057318863506084, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 26.15it/s]                                               {'loss': 2.4663, 'grad_norm': 3.6690022945404053, 'learning_rate': 0.000197534200928469, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 26.15it/s] 16%|█▌        | 12/75 [00:00<00:02, 26.34it/s]                                               {'loss': 2.4499, 'grad_norm': 4.507583141326904, 'learning_rate': 0.0001944952132218772, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 26.34it/s]                                               {'loss': 2.4732, 'grad_norm': 4.593304634094238, 'learning_rate': 0.00019145622551528535, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 26.34it/s]                                               {'loss': 2.3396, 'grad_norm': 4.153873920440674, 'learning_rate': 0.00018841723780869352, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 26.34it/s]                                               {'loss': 2.571, 'grad_norm': 8.433100700378418, 'learning_rate': 0.0001853782501021017, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 26.34it/s] 21%|██▏       | 16/75 [00:00<00:02, 28.03it/s]                                               {'loss': 2.3033, 'grad_norm': 3.6086785793304443, 'learning_rate': 0.00018233926239550986, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 28.03it/s]                                               {'loss': 2.3149, 'grad_norm': 4.750779151916504, 'learning_rate': 0.000179300274688918, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 28.03it/s]                                               {'loss': 2.5121, 'grad_norm': 6.095987319946289, 'learning_rate': 0.0001762612869823262, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 28.03it/s] 25%|██▌       | 19/75 [00:00<00:02, 27.62it/s]                                               {'loss': 2.4114, 'grad_norm': 3.2304720878601074, 'learning_rate': 0.00017322229927573438, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 27.62it/s]                                               {'loss': 2.4111, 'grad_norm': 3.3151986598968506, 'learning_rate': 0.00017018331156914255, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:01, 27.62it/s]                                               {'loss': 2.1855, 'grad_norm': 3.314291477203369, 'learning_rate': 0.0001671443238625507, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:01, 27.62it/s] 29%|██▉       | 22/75 [00:00<00:01, 27.07it/s]                                               {'loss': 2.2855, 'grad_norm': 3.4292116165161133, 'learning_rate': 0.00016410533615595886, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:01, 27.07it/s]                                               {'loss': 2.2278, 'grad_norm': 3.789358139038086, 'learning_rate': 0.00016106634844936704, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 27.07it/s]                                               {'loss': 2.2203, 'grad_norm': 4.151757717132568, 'learning_rate': 0.0001580273607427752, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 27.07it/s] 33%|███▎      | 25/75 [00:00<00:01, 26.65it/s]                                               {'loss': 2.242, 'grad_norm': 4.057344913482666, 'learning_rate': 0.00015498837303618338, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 26.65it/s]                                               {'loss': 2.3608, 'grad_norm': 3.6089327335357666, 'learning_rate': 0.00015194938532959155, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:00<00:01, 26.65it/s]                                               {'loss': 2.5546, 'grad_norm': 3.497992992401123, 'learning_rate': 0.00014891039762299972, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 26.65it/s] 37%|███▋      | 28/75 [00:01<00:01, 26.44it/s]                                               {'loss': 2.3373, 'grad_norm': 4.4792704582214355, 'learning_rate': 0.0001458714099164079, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 26.44it/s]                                               {'loss': 2.4386, 'grad_norm': 4.154373645782471, 'learning_rate': 0.00014283242220981606, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 26.44it/s]                                               {'loss': 2.6796, 'grad_norm': 10.172109603881836, 'learning_rate': 0.0001397934345032242, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 26.44it/s]                                               {'loss': 2.2918, 'grad_norm': 3.6393837928771973, 'learning_rate': 0.00013675444679663238, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.44it/s] 43%|████▎     | 32/75 [00:01<00:01, 28.04it/s]                                               {'loss': 2.2198, 'grad_norm': 2.8272979259490967, 'learning_rate': 0.00013371545909004058, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 28.04it/s]                                               {'loss': 2.3643, 'grad_norm': 3.088397979736328, 'learning_rate': 0.00013067647138344875, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 28.04it/s]                                               {'loss': 2.1432, 'grad_norm': 3.287156343460083, 'learning_rate': 0.00012763748367685692, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 28.04it/s] 47%|████▋     | 35/75 [00:01<00:01, 27.34it/s]                                               {'loss': 2.4111, 'grad_norm': 3.355292320251465, 'learning_rate': 0.00012459849597026506, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 27.34it/s]                                               {'loss': 2.345, 'grad_norm': 3.864123582839966, 'learning_rate': 0.00012155950826367323, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 27.34it/s]                                               {'loss': 2.0554, 'grad_norm': 3.06418776512146, 'learning_rate': 0.00011852052055708142, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 27.34it/s] 51%|█████     | 38/75 [00:01<00:01, 26.85it/s]                                               {'loss': 2.2058, 'grad_norm': 4.080657958984375, 'learning_rate': 0.00011548153285048959, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.85it/s]                                               {'loss': 2.2751, 'grad_norm': 3.508668899536133, 'learning_rate': 0.00011244254514389775, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.85it/s]                                               {'loss': 2.223, 'grad_norm': 4.044849395751953, 'learning_rate': 0.0001094035574373059, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 26.85it/s] 55%|█████▍    | 41/75 [00:01<00:01, 26.75it/s]                                               {'loss': 2.2439, 'grad_norm': 3.6724345684051514, 'learning_rate': 0.00010636456973071409, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 26.75it/s]                                               {'loss': 2.3834, 'grad_norm': 3.467738389968872, 'learning_rate': 0.00010332558202412225, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 26.75it/s]                                               {'loss': 2.3669, 'grad_norm': 3.038383722305298, 'learning_rate': 0.00010028659431753042, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 26.75it/s] 59%|█████▊    | 44/75 [00:01<00:01, 26.74it/s]                                               {'loss': 2.1574, 'grad_norm': 3.806098461151123, 'learning_rate': 9.72476066109386e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 26.74it/s]                                               {'loss': 2.433, 'grad_norm': 11.34114933013916, 'learning_rate': 9.420861890434676e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 26.74it/s]                                               {'loss': 2.061, 'grad_norm': 3.8348236083984375, 'learning_rate': 9.116963119775493e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.74it/s]                                               {'loss': 2.1043, 'grad_norm': 3.0374560356140137, 'learning_rate': 8.81306434911631e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.74it/s] 64%|██████▍   | 48/75 [00:01<00:00, 28.11it/s]                                               {'loss': 2.1531, 'grad_norm': 3.2258880138397217, 'learning_rate': 8.509165578457127e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 28.11it/s]                                               {'loss': 2.2729, 'grad_norm': 3.5501914024353027, 'learning_rate': 8.205266807797943e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 28.11it/s]                                               {'loss': 2.2719, 'grad_norm': 3.532085657119751, 'learning_rate': 7.90136803713876e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 28.11it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.64it/s]                                               {'loss': 1.9919, 'grad_norm': 3.9175221920013428, 'learning_rate': 7.597469266479577e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 27.64it/s]                                               {'loss': 2.3345, 'grad_norm': 4.608180046081543, 'learning_rate': 7.293570495820395e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 27.64it/s]                                               {'loss': 2.3826, 'grad_norm': 4.744741439819336, 'learning_rate': 6.98967172516121e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:01<00:00, 27.64it/s] 72%|███████▏  | 54/75 [00:01<00:00, 27.01it/s]                                               {'loss': 2.4079, 'grad_norm': 4.048411846160889, 'learning_rate': 6.685772954502029e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:01<00:00, 27.01it/s]                                               {'loss': 2.3526, 'grad_norm': 3.9843268394470215, 'learning_rate': 6.381874183842846e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 27.01it/s]                                               {'loss': 2.2037, 'grad_norm': 3.1945810317993164, 'learning_rate': 6.077975413183662e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 27.01it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.66it/s]                                               {'loss': 2.1806, 'grad_norm': 3.751537799835205, 'learning_rate': 5.7740766425244795e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 26.66it/s]                                               {'loss': 2.1504, 'grad_norm': 3.754181385040283, 'learning_rate': 5.470177871865295e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 26.66it/s]                                               {'loss': 2.1519, 'grad_norm': 4.155888080596924, 'learning_rate': 5.1662791012061124e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 26.66it/s]                                               {'loss': 2.5082, 'grad_norm': 8.516098022460938, 'learning_rate': 4.86238033054693e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 26.66it/s] 81%|████████▏ | 61/75 [00:02<00:00, 28.09it/s]                                               {'loss': 2.1953, 'grad_norm': 3.2733237743377686, 'learning_rate': 4.5584815598877466e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 28.09it/s]                                               {'loss': 2.2311, 'grad_norm': 3.7950892448425293, 'learning_rate': 4.254582789228564e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 28.09it/s]                                               {'loss': 2.1512, 'grad_norm': 3.268303394317627, 'learning_rate': 3.95068401856938e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 28.09it/s] 85%|████████▌ | 64/75 [00:02<00:00, 27.45it/s]                                               {'loss': 2.1937, 'grad_norm': 3.5108134746551514, 'learning_rate': 3.646785247910197e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 27.45it/s]                                               {'loss': 2.2031, 'grad_norm': 4.209275245666504, 'learning_rate': 3.3428864772510144e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 27.45it/s]                                               {'loss': 2.3161, 'grad_norm': 3.5085582733154297, 'learning_rate': 3.038987706591831e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 27.45it/s] 89%|████████▉ | 67/75 [00:02<00:00, 26.93it/s]                                               {'loss': 2.1125, 'grad_norm': 2.8157594203948975, 'learning_rate': 2.7350889359326476e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.93it/s]                                               {'loss': 2.1908, 'grad_norm': 3.037426471710205, 'learning_rate': 2.431190165273465e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 26.93it/s]                                               {'loss': 2.2418, 'grad_norm': 3.507680892944336, 'learning_rate': 2.127291394614282e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 26.93it/s] 93%|█████████▎| 70/75 [00:02<00:00, 26.69it/s]                                               {'loss': 2.1467, 'grad_norm': 3.117600917816162, 'learning_rate': 1.8233926239550986e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 26.69it/s]                                               {'loss': 2.2712, 'grad_norm': 3.8921735286712646, 'learning_rate': 1.5194938532959154e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 26.69it/s]                                               {'loss': 2.1824, 'grad_norm': 3.5863492488861084, 'learning_rate': 1.2155950826367325e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 26.69it/s] 97%|█████████▋| 73/75 [00:02<00:00, 26.69it/s]                                               {'loss': 2.2129, 'grad_norm': 3.0494136810302734, 'learning_rate': 9.116963119775493e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 26.69it/s]                                               {'loss': 2.1723, 'grad_norm': 3.244002103805542, 'learning_rate': 6.077975413183663e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 26.69it/s]                                               {'loss': 2.3488, 'grad_norm': 9.58558177947998, 'learning_rate': 3.0389877065918314e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.69it/s]                                               {'train_runtime': 2.8218, 'train_samples_per_second': 400.451, 'train_steps_per_second': 26.579, 'train_loss': 2.331738551457723, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.69it/s]100%|██████████| 75/75 [00:02<00:00, 26.58it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.9579, 'grad_norm': 3.398022174835205, 'learning_rate': 0.00022792407799438732, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:02, 25.83it/s]                                              {'loss': 2.7859, 'grad_norm': 5.509798049926758, 'learning_rate': 0.0002248850902877955, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 26.73it/s]  4%|▍         | 3/75 [00:00<00:02, 27.08it/s]                                              {'loss': 2.7338, 'grad_norm': 4.0773138999938965, 'learning_rate': 0.00022184610258120367, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 27.08it/s]                                              {'loss': 2.7085, 'grad_norm': 4.0157060623168945, 'learning_rate': 0.0002188071148746118, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 27.08it/s]                                              {'loss': 2.4571, 'grad_norm': 5.338004112243652, 'learning_rate': 0.00021576812716802, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 27.08it/s]  8%|▊         | 6/75 [00:00<00:02, 26.67it/s]                                              {'loss': 2.4937, 'grad_norm': 4.709381580352783, 'learning_rate': 0.00021272913946142818, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 26.67it/s]                                              {'loss': 2.6407, 'grad_norm': 5.207286834716797, 'learning_rate': 0.00020969015175483635, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 26.67it/s]                                              {'loss': 2.5873, 'grad_norm': 4.5383100509643555, 'learning_rate': 0.0002066511640482445, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 26.67it/s] 12%|█▏        | 9/75 [00:00<00:02, 26.28it/s]                                              {'loss': 2.5405, 'grad_norm': 3.214735746383667, 'learning_rate': 0.00020361217634165267, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 26.28it/s]                                              {'loss': 2.4397, 'grad_norm': 3.1098482608795166, 'learning_rate': 0.00020057318863506084, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 26.28it/s]                                               {'loss': 2.2743, 'grad_norm': 3.671293020248413, 'learning_rate': 0.000197534200928469, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 26.28it/s] 16%|█▌        | 12/75 [00:00<00:02, 26.41it/s]                                               {'loss': 2.3713, 'grad_norm': 4.177114963531494, 'learning_rate': 0.0001944952132218772, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 26.41it/s]                                               {'loss': 2.4745, 'grad_norm': 4.710851192474365, 'learning_rate': 0.00019145622551528535, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 26.41it/s]                                               {'loss': 2.3564, 'grad_norm': 3.759859323501587, 'learning_rate': 0.00018841723780869352, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 26.41it/s]                                               {'loss': 2.3527, 'grad_norm': 8.813798904418945, 'learning_rate': 0.0001853782501021017, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 26.41it/s] 21%|██▏       | 16/75 [00:00<00:02, 28.13it/s]                                               {'loss': 2.3166, 'grad_norm': 4.120223522186279, 'learning_rate': 0.00018233926239550986, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 28.13it/s]                                               {'loss': 2.5357, 'grad_norm': 3.8342623710632324, 'learning_rate': 0.000179300274688918, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 28.13it/s]                                               {'loss': 2.387, 'grad_norm': 4.00712776184082, 'learning_rate': 0.0001762612869823262, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 28.13it/s] 25%|██▌       | 19/75 [00:00<00:02, 27.42it/s]                                               {'loss': 2.0687, 'grad_norm': 3.4278059005737305, 'learning_rate': 0.00017322229927573438, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 27.42it/s]                                               {'loss': 2.1168, 'grad_norm': 5.743239402770996, 'learning_rate': 0.00017018331156914255, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 27.42it/s]                                               {'loss': 2.1433, 'grad_norm': 3.889493703842163, 'learning_rate': 0.0001671443238625507, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:01, 27.42it/s] 29%|██▉       | 22/75 [00:00<00:01, 27.12it/s]                                               {'loss': 2.3226, 'grad_norm': 3.872666835784912, 'learning_rate': 0.00016410533615595886, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:01, 27.12it/s]                                               {'loss': 2.5122, 'grad_norm': 4.659146785736084, 'learning_rate': 0.00016106634844936704, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 27.12it/s]                                               {'loss': 2.4097, 'grad_norm': 4.386773586273193, 'learning_rate': 0.0001580273607427752, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 27.12it/s] 33%|███▎      | 25/75 [00:00<00:01, 26.54it/s]                                               {'loss': 2.4005, 'grad_norm': 4.596691608428955, 'learning_rate': 0.00015498837303618338, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 26.54it/s]                                               {'loss': 2.3836, 'grad_norm': 3.6562349796295166, 'learning_rate': 0.00015194938532959155, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:00<00:01, 26.54it/s]                                               {'loss': 2.278, 'grad_norm': 5.2718939781188965, 'learning_rate': 0.00014891039762299972, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 26.54it/s] 37%|███▋      | 28/75 [00:01<00:01, 26.23it/s]                                               {'loss': 2.1999, 'grad_norm': 3.3793892860412598, 'learning_rate': 0.0001458714099164079, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 26.23it/s]                                               {'loss': 2.2143, 'grad_norm': 3.3768553733825684, 'learning_rate': 0.00014283242220981606, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 26.23it/s]                                               {'loss': 2.6061, 'grad_norm': 10.08989429473877, 'learning_rate': 0.0001397934345032242, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 26.23it/s]                                               {'loss': 2.4517, 'grad_norm': 3.7918293476104736, 'learning_rate': 0.00013675444679663238, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.23it/s] 43%|████▎     | 32/75 [00:01<00:01, 27.76it/s]                                               {'loss': 2.0813, 'grad_norm': 3.627746343612671, 'learning_rate': 0.00013371545909004058, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 27.76it/s]                                               {'loss': 2.2628, 'grad_norm': 4.002057075500488, 'learning_rate': 0.00013067647138344875, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 27.76it/s]                                               {'loss': 2.1171, 'grad_norm': 3.1164886951446533, 'learning_rate': 0.00012763748367685692, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 27.76it/s] 47%|████▋     | 35/75 [00:01<00:01, 27.40it/s]                                               {'loss': 2.0835, 'grad_norm': 3.539137363433838, 'learning_rate': 0.00012459849597026506, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 27.40it/s]                                               {'loss': 2.17, 'grad_norm': 2.834031581878662, 'learning_rate': 0.00012155950826367323, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 27.40it/s]                                               {'loss': 2.2488, 'grad_norm': 4.444528102874756, 'learning_rate': 0.00011852052055708142, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 27.40it/s] 51%|█████     | 38/75 [00:01<00:01, 26.79it/s]                                               {'loss': 2.0374, 'grad_norm': 2.53100323677063, 'learning_rate': 0.00011548153285048959, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.79it/s]                                               {'loss': 2.2666, 'grad_norm': 3.618878126144409, 'learning_rate': 0.00011244254514389775, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.79it/s]                                               {'loss': 2.2938, 'grad_norm': 5.879350662231445, 'learning_rate': 0.0001094035574373059, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 26.79it/s] 55%|█████▍    | 41/75 [00:01<00:01, 26.41it/s]                                               {'loss': 2.4201, 'grad_norm': 4.730101585388184, 'learning_rate': 0.00010636456973071409, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 26.41it/s]                                               {'loss': 2.0077, 'grad_norm': 3.2780346870422363, 'learning_rate': 0.00010332558202412225, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 26.41it/s]                                               {'loss': 2.2335, 'grad_norm': 3.4873342514038086, 'learning_rate': 0.00010028659431753042, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 26.41it/s] 59%|█████▊    | 44/75 [00:01<00:01, 26.29it/s]                                               {'loss': 2.6206, 'grad_norm': 4.8112263679504395, 'learning_rate': 9.72476066109386e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 26.29it/s]                                               {'loss': 1.938, 'grad_norm': 11.246305465698242, 'learning_rate': 9.420861890434676e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 26.29it/s]                                               {'loss': 2.265, 'grad_norm': 3.502612829208374, 'learning_rate': 9.116963119775493e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.29it/s]                                               {'loss': 2.3059, 'grad_norm': 4.449577331542969, 'learning_rate': 8.81306434911631e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.29it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.80it/s]                                               {'loss': 2.3143, 'grad_norm': 4.5440850257873535, 'learning_rate': 8.509165578457127e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 27.80it/s]                                               {'loss': 2.3793, 'grad_norm': 4.117414951324463, 'learning_rate': 8.205266807797943e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 27.80it/s]                                               {'loss': 2.3405, 'grad_norm': 3.4350388050079346, 'learning_rate': 7.90136803713876e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 27.80it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.09it/s]                                               {'loss': 2.0868, 'grad_norm': 3.8692896366119385, 'learning_rate': 7.597469266479577e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 27.09it/s]                                               {'loss': 2.3061, 'grad_norm': 4.834315776824951, 'learning_rate': 7.293570495820395e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 27.09it/s]                                               {'loss': 2.1153, 'grad_norm': 3.0588948726654053, 'learning_rate': 6.98967172516121e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:01<00:00, 27.09it/s] 72%|███████▏  | 54/75 [00:02<00:00, 26.64it/s]                                               {'loss': 2.2558, 'grad_norm': 3.1519644260406494, 'learning_rate': 6.685772954502029e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 26.64it/s]                                               {'loss': 1.9781, 'grad_norm': 4.028961181640625, 'learning_rate': 6.381874183842846e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 26.64it/s]                                               {'loss': 2.2394, 'grad_norm': 3.8155441284179688, 'learning_rate': 6.077975413183662e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 26.64it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.37it/s]                                               {'loss': 2.337, 'grad_norm': 3.6648240089416504, 'learning_rate': 5.7740766425244795e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 26.37it/s]                                               {'loss': 1.9355, 'grad_norm': 3.3744828701019287, 'learning_rate': 5.470177871865295e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 26.37it/s]                                               {'loss': 2.1605, 'grad_norm': 3.3790814876556396, 'learning_rate': 5.1662791012061124e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 26.37it/s]                                               {'loss': 1.8639, 'grad_norm': 13.342196464538574, 'learning_rate': 4.86238033054693e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 26.37it/s] 81%|████████▏ | 61/75 [00:02<00:00, 27.83it/s]                                               {'loss': 2.161, 'grad_norm': 5.394124507904053, 'learning_rate': 4.5584815598877466e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 27.83it/s]                                               {'loss': 2.285, 'grad_norm': 3.5370571613311768, 'learning_rate': 4.254582789228564e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 27.83it/s]                                               {'loss': 2.4969, 'grad_norm': 4.287405014038086, 'learning_rate': 3.95068401856938e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 27.83it/s] 85%|████████▌ | 64/75 [00:02<00:00, 27.19it/s]                                               {'loss': 2.1349, 'grad_norm': 4.127047538757324, 'learning_rate': 3.646785247910197e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 27.19it/s]                                               {'loss': 2.0555, 'grad_norm': 3.4712326526641846, 'learning_rate': 3.3428864772510144e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 27.19it/s]                                               {'loss': 2.0779, 'grad_norm': 4.836477756500244, 'learning_rate': 3.038987706591831e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 27.19it/s] 89%|████████▉ | 67/75 [00:02<00:00, 26.70it/s]                                               {'loss': 2.1705, 'grad_norm': 3.969512462615967, 'learning_rate': 2.7350889359326476e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.70it/s]                                               {'loss': 2.2862, 'grad_norm': 3.662750005722046, 'learning_rate': 2.431190165273465e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 26.70it/s]                                               {'loss': 2.1783, 'grad_norm': 4.515961170196533, 'learning_rate': 2.127291394614282e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 26.70it/s] 93%|█████████▎| 70/75 [00:02<00:00, 26.42it/s]                                               {'loss': 2.0871, 'grad_norm': 3.504791498184204, 'learning_rate': 1.8233926239550986e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 26.42it/s]                                               {'loss': 2.1023, 'grad_norm': 3.7363688945770264, 'learning_rate': 1.5194938532959154e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 26.42it/s]                                               {'loss': 2.019, 'grad_norm': 3.2239770889282227, 'learning_rate': 1.2155950826367325e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 26.42it/s] 97%|█████████▋| 73/75 [00:02<00:00, 26.22it/s]                                               {'loss': 1.9205, 'grad_norm': 2.9223368167877197, 'learning_rate': 9.116963119775493e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 26.22it/s]                                               {'loss': 2.1722, 'grad_norm': 4.145425319671631, 'learning_rate': 6.077975413183663e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 26.22it/s]                                               {'loss': 1.8271, 'grad_norm': 11.727197647094727, 'learning_rate': 3.0389877065918314e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.22it/s]                                               {'train_runtime': 2.8445, 'train_samples_per_second': 397.257, 'train_steps_per_second': 26.367, 'train_loss': 2.2821338192621865, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.22it/s]100%|██████████| 75/75 [00:02<00:00, 26.37it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:32
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.9403, 'grad_norm': 5.781045913696289, 'learning_rate': 0.00022792407799438732, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:02, 25.73it/s]                                              {'loss': 2.8252, 'grad_norm': 3.895517587661743, 'learning_rate': 0.0002248850902877955, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 26.00it/s]  4%|▍         | 3/75 [00:00<00:02, 26.06it/s]                                              {'loss': 2.6415, 'grad_norm': 4.098789215087891, 'learning_rate': 0.00022184610258120367, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 26.06it/s]                                              {'loss': 2.6271, 'grad_norm': 3.798441171646118, 'learning_rate': 0.0002188071148746118, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 26.06it/s]                                              {'loss': 2.6064, 'grad_norm': 3.2123990058898926, 'learning_rate': 0.00021576812716802, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 26.06it/s]  8%|▊         | 6/75 [00:00<00:02, 26.28it/s]                                              {'loss': 2.5376, 'grad_norm': 4.025411605834961, 'learning_rate': 0.00021272913946142818, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 26.28it/s]                                              {'loss': 2.5424, 'grad_norm': 3.807572364807129, 'learning_rate': 0.00020969015175483635, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 26.28it/s]                                              {'loss': 2.6516, 'grad_norm': 4.121856212615967, 'learning_rate': 0.0002066511640482445, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 26.28it/s] 12%|█▏        | 9/75 [00:00<00:02, 26.19it/s]                                              {'loss': 2.5354, 'grad_norm': 4.981429576873779, 'learning_rate': 0.00020361217634165267, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 26.19it/s]                                              {'loss': 2.4762, 'grad_norm': 3.687920570373535, 'learning_rate': 0.00020057318863506084, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 26.19it/s]                                               {'loss': 2.4228, 'grad_norm': 3.6332318782806396, 'learning_rate': 0.000197534200928469, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 26.19it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.91it/s]                                               {'loss': 2.5652, 'grad_norm': 3.3571903705596924, 'learning_rate': 0.0001944952132218772, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.91it/s]                                               {'loss': 2.4375, 'grad_norm': 4.805797100067139, 'learning_rate': 0.00019145622551528535, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.91it/s]                                               {'loss': 2.3473, 'grad_norm': 4.158149242401123, 'learning_rate': 0.00018841723780869352, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.91it/s]                                               {'loss': 1.876, 'grad_norm': 10.62347412109375, 'learning_rate': 0.0001853782501021017, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.91it/s] 21%|██▏       | 16/75 [00:00<00:02, 27.88it/s]                                               {'loss': 2.3337, 'grad_norm': 4.719108581542969, 'learning_rate': 0.00018233926239550986, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 27.88it/s]                                               {'loss': 2.3241, 'grad_norm': 3.8761956691741943, 'learning_rate': 0.000179300274688918, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 27.88it/s]                                               {'loss': 2.2498, 'grad_norm': 3.547335624694824, 'learning_rate': 0.0001762612869823262, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 27.88it/s] 25%|██▌       | 19/75 [00:00<00:02, 27.36it/s]                                               {'loss': 2.413, 'grad_norm': 5.251928806304932, 'learning_rate': 0.00017322229927573438, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 27.36it/s]                                               {'loss': 2.4154, 'grad_norm': 4.737971782684326, 'learning_rate': 0.00017018331156914255, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 27.36it/s]                                               {'loss': 2.1549, 'grad_norm': 3.596013307571411, 'learning_rate': 0.0001671443238625507, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:01, 27.36it/s] 29%|██▉       | 22/75 [00:00<00:01, 26.84it/s]                                               {'loss': 2.1142, 'grad_norm': 3.1891064643859863, 'learning_rate': 0.00016410533615595886, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:01, 26.84it/s]                                               {'loss': 2.2397, 'grad_norm': 3.8820836544036865, 'learning_rate': 0.00016106634844936704, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 26.84it/s]                                               {'loss': 2.4555, 'grad_norm': 4.269333839416504, 'learning_rate': 0.0001580273607427752, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 26.84it/s] 33%|███▎      | 25/75 [00:00<00:01, 26.56it/s]                                               {'loss': 2.4139, 'grad_norm': 3.925227165222168, 'learning_rate': 0.00015498837303618338, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 26.56it/s]                                               {'loss': 2.431, 'grad_norm': 3.9456090927124023, 'learning_rate': 0.00015194938532959155, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:00<00:01, 26.56it/s]                                               {'loss': 2.2959, 'grad_norm': 5.517219066619873, 'learning_rate': 0.00014891039762299972, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 26.56it/s] 37%|███▋      | 28/75 [00:01<00:01, 26.43it/s]                                               {'loss': 2.2438, 'grad_norm': 4.915066719055176, 'learning_rate': 0.0001458714099164079, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 26.43it/s]                                               {'loss': 2.2573, 'grad_norm': 3.4327900409698486, 'learning_rate': 0.00014283242220981606, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 26.43it/s]                                               {'loss': 1.7826, 'grad_norm': 8.958707809448242, 'learning_rate': 0.0001397934345032242, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 26.43it/s]                                               {'loss': 2.37, 'grad_norm': 3.1107065677642822, 'learning_rate': 0.00013675444679663238, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.43it/s] 43%|████▎     | 32/75 [00:01<00:01, 28.03it/s]                                               {'loss': 2.1195, 'grad_norm': 3.343730926513672, 'learning_rate': 0.00013371545909004058, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 28.03it/s]                                               {'loss': 2.3905, 'grad_norm': 3.623971939086914, 'learning_rate': 0.00013067647138344875, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 28.03it/s]                                               {'loss': 2.2271, 'grad_norm': 4.128823280334473, 'learning_rate': 0.00012763748367685692, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 28.03it/s] 47%|████▋     | 35/75 [00:01<00:01, 27.46it/s]                                               {'loss': 2.1093, 'grad_norm': 3.3295438289642334, 'learning_rate': 0.00012459849597026506, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 27.46it/s]                                               {'loss': 2.1648, 'grad_norm': 3.430402994155884, 'learning_rate': 0.00012155950826367323, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 27.46it/s]                                               {'loss': 2.034, 'grad_norm': 3.331773519515991, 'learning_rate': 0.00011852052055708142, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 27.46it/s] 51%|█████     | 38/75 [00:01<00:01, 27.25it/s]                                               {'loss': 2.3255, 'grad_norm': 4.204324245452881, 'learning_rate': 0.00011548153285048959, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 27.25it/s]                                               {'loss': 2.3022, 'grad_norm': 5.7281036376953125, 'learning_rate': 0.00011244254514389775, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 27.25it/s]                                               {'loss': 2.3732, 'grad_norm': 3.562228202819824, 'learning_rate': 0.0001094035574373059, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 27.25it/s] 55%|█████▍    | 41/75 [00:01<00:01, 26.78it/s]                                               {'loss': 2.147, 'grad_norm': 3.5515031814575195, 'learning_rate': 0.00010636456973071409, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 26.78it/s]                                               {'loss': 2.1197, 'grad_norm': 4.628310203552246, 'learning_rate': 0.00010332558202412225, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 26.78it/s]                                               {'loss': 2.2155, 'grad_norm': 4.379910469055176, 'learning_rate': 0.00010028659431753042, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 26.78it/s] 59%|█████▊    | 44/75 [00:01<00:01, 26.44it/s]                                               {'loss': 2.1894, 'grad_norm': 2.907935380935669, 'learning_rate': 9.72476066109386e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 26.44it/s]                                               {'loss': 2.2275, 'grad_norm': 10.323746681213379, 'learning_rate': 9.420861890434676e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 26.44it/s]                                               {'loss': 2.0618, 'grad_norm': 3.4040043354034424, 'learning_rate': 9.116963119775493e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.44it/s]                                               {'loss': 2.1287, 'grad_norm': 3.3715121746063232, 'learning_rate': 8.81306434911631e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.44it/s] 64%|██████▍   | 48/75 [00:01<00:00, 28.02it/s]                                               {'loss': 2.192, 'grad_norm': 3.5727555751800537, 'learning_rate': 8.509165578457127e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 28.02it/s]                                               {'loss': 2.2497, 'grad_norm': 3.6837542057037354, 'learning_rate': 8.205266807797943e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 28.02it/s]                                               {'loss': 2.1746, 'grad_norm': 3.519444704055786, 'learning_rate': 7.90136803713876e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 28.02it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.37it/s]                                               {'loss': 2.1009, 'grad_norm': 4.527193069458008, 'learning_rate': 7.597469266479577e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 27.37it/s]                                               {'loss': 2.0846, 'grad_norm': 4.037466526031494, 'learning_rate': 7.293570495820395e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 27.37it/s]                                               {'loss': 2.1088, 'grad_norm': 3.849259614944458, 'learning_rate': 6.98967172516121e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:01<00:00, 27.37it/s] 72%|███████▏  | 54/75 [00:02<00:00, 26.82it/s]                                               {'loss': 2.4275, 'grad_norm': 3.9207146167755127, 'learning_rate': 6.685772954502029e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 26.82it/s]                                               {'loss': 2.1625, 'grad_norm': 3.6283459663391113, 'learning_rate': 6.381874183842846e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 26.82it/s]                                               {'loss': 2.4318, 'grad_norm': 3.4874649047851562, 'learning_rate': 6.077975413183662e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 26.82it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.54it/s]                                               {'loss': 2.0264, 'grad_norm': 4.265710353851318, 'learning_rate': 5.7740766425244795e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 26.54it/s]                                               {'loss': 2.1349, 'grad_norm': 3.388716697692871, 'learning_rate': 5.470177871865295e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 26.54it/s]                                               {'loss': 2.1938, 'grad_norm': 3.3995418548583984, 'learning_rate': 5.1662791012061124e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 26.54it/s]                                               {'loss': 2.339, 'grad_norm': 12.912920951843262, 'learning_rate': 4.86238033054693e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 26.54it/s] 81%|████████▏ | 61/75 [00:02<00:00, 27.93it/s]                                               {'loss': 2.3421, 'grad_norm': 4.653438568115234, 'learning_rate': 4.5584815598877466e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 27.93it/s]                                               {'loss': 2.0363, 'grad_norm': 3.349147081375122, 'learning_rate': 4.254582789228564e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 27.93it/s]                                               {'loss': 2.1049, 'grad_norm': 3.4687416553497314, 'learning_rate': 3.95068401856938e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 27.93it/s] 85%|████████▌ | 64/75 [00:02<00:00, 27.31it/s]                                               {'loss': 2.0588, 'grad_norm': 3.8703184127807617, 'learning_rate': 3.646785247910197e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 27.31it/s]                                               {'loss': 2.4845, 'grad_norm': 4.583519458770752, 'learning_rate': 3.3428864772510144e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 27.31it/s]                                               {'loss': 2.1726, 'grad_norm': 3.4098057746887207, 'learning_rate': 3.038987706591831e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 27.31it/s] 89%|████████▉ | 67/75 [00:02<00:00, 26.83it/s]                                               {'loss': 2.2419, 'grad_norm': 3.6502749919891357, 'learning_rate': 2.7350889359326476e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.83it/s]                                               {'loss': 2.0509, 'grad_norm': 3.6904187202453613, 'learning_rate': 2.431190165273465e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 26.83it/s]                                               {'loss': 2.2204, 'grad_norm': 3.4446821212768555, 'learning_rate': 2.127291394614282e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 26.83it/s] 93%|█████████▎| 70/75 [00:02<00:00, 26.52it/s]                                               {'loss': 2.1226, 'grad_norm': 3.264331817626953, 'learning_rate': 1.8233926239550986e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 26.52it/s]                                               {'loss': 1.9754, 'grad_norm': 3.7808845043182373, 'learning_rate': 1.5194938532959154e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 26.52it/s]                                               {'loss': 2.1894, 'grad_norm': 2.7339277267456055, 'learning_rate': 1.2155950826367325e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 26.52it/s] 97%|█████████▋| 73/75 [00:02<00:00, 26.36it/s]                                               {'loss': 2.0262, 'grad_norm': 3.0421197414398193, 'learning_rate': 9.116963119775493e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 26.36it/s]                                               {'loss': 2.1362, 'grad_norm': 4.47770881652832, 'learning_rate': 6.077975413183663e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 26.36it/s]                                               {'loss': 1.9122, 'grad_norm': 7.54203987121582, 'learning_rate': 3.0389877065918314e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.36it/s]                                               {'train_runtime': 2.835, 'train_samples_per_second': 398.587, 'train_steps_per_second': 26.455, 'train_loss': 2.2715150769551595, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.36it/s]100%|██████████| 75/75 [00:02<00:00, 26.46it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:13
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.8136, 'grad_norm': 3.447760581970215, 'learning_rate': 0.00022792407799438732, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:02, 25.18it/s]                                              {'loss': 2.8385, 'grad_norm': 6.1150078773498535, 'learning_rate': 0.0002248850902877955, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 26.63it/s]  4%|▍         | 3/75 [00:00<00:02, 26.63it/s]                                              {'loss': 2.6805, 'grad_norm': 4.671422481536865, 'learning_rate': 0.00022184610258120367, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 26.63it/s]                                              {'loss': 2.5946, 'grad_norm': 4.332900047302246, 'learning_rate': 0.0002188071148746118, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 26.63it/s]                                              {'loss': 2.7869, 'grad_norm': 2.795013427734375, 'learning_rate': 0.00021576812716802, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 26.63it/s]  8%|▊         | 6/75 [00:00<00:02, 26.11it/s]                                              {'loss': 2.7311, 'grad_norm': 3.413391351699829, 'learning_rate': 0.00021272913946142818, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 26.11it/s]                                              {'loss': 2.6759, 'grad_norm': 4.2123942375183105, 'learning_rate': 0.00020969015175483635, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 26.11it/s]                                              {'loss': 2.4517, 'grad_norm': 3.9413280487060547, 'learning_rate': 0.0002066511640482445, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 26.11it/s] 12%|█▏        | 9/75 [00:00<00:02, 26.07it/s]                                              {'loss': 2.6519, 'grad_norm': 3.68332576751709, 'learning_rate': 0.00020361217634165267, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 26.07it/s]                                              {'loss': 2.3711, 'grad_norm': 3.0783774852752686, 'learning_rate': 0.00020057318863506084, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 26.07it/s]                                               {'loss': 2.4816, 'grad_norm': 4.091403961181641, 'learning_rate': 0.000197534200928469, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 26.07it/s] 16%|█▌        | 12/75 [00:00<00:02, 26.26it/s]                                               {'loss': 2.3676, 'grad_norm': 3.5479860305786133, 'learning_rate': 0.0001944952132218772, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 26.26it/s]                                               {'loss': 2.403, 'grad_norm': 4.056704044342041, 'learning_rate': 0.00019145622551528535, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 26.26it/s]                                               {'loss': 2.4574, 'grad_norm': 4.253188610076904, 'learning_rate': 0.00018841723780869352, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 26.26it/s]                                               {'loss': 2.7462, 'grad_norm': 10.740445137023926, 'learning_rate': 0.0001853782501021017, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 26.26it/s] 21%|██▏       | 16/75 [00:00<00:02, 28.11it/s]                                               {'loss': 2.3795, 'grad_norm': 3.9321346282958984, 'learning_rate': 0.00018233926239550986, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 28.11it/s]                                               {'loss': 2.379, 'grad_norm': 3.664997100830078, 'learning_rate': 0.000179300274688918, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 28.11it/s]                                               {'loss': 2.3857, 'grad_norm': 4.232318878173828, 'learning_rate': 0.0001762612869823262, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 28.11it/s] 25%|██▌       | 19/75 [00:00<00:02, 27.62it/s]                                               {'loss': 2.3376, 'grad_norm': 3.9696030616760254, 'learning_rate': 0.00017322229927573438, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 27.62it/s]                                               {'loss': 2.3396, 'grad_norm': 3.7196881771087646, 'learning_rate': 0.00017018331156914255, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:01, 27.62it/s]                                               {'loss': 2.1972, 'grad_norm': 4.054074764251709, 'learning_rate': 0.0001671443238625507, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:01, 27.62it/s] 29%|██▉       | 22/75 [00:00<00:01, 27.07it/s]                                               {'loss': 2.542, 'grad_norm': 3.9144484996795654, 'learning_rate': 0.00016410533615595886, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:01, 27.07it/s]                                               {'loss': 2.4635, 'grad_norm': 4.155642509460449, 'learning_rate': 0.00016106634844936704, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 27.07it/s]                                               {'loss': 2.2523, 'grad_norm': 3.172239303588867, 'learning_rate': 0.0001580273607427752, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 27.07it/s] 33%|███▎      | 25/75 [00:00<00:01, 26.62it/s]                                               {'loss': 2.3069, 'grad_norm': 4.3780670166015625, 'learning_rate': 0.00015498837303618338, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 26.62it/s]                                               {'loss': 2.4281, 'grad_norm': 4.925826549530029, 'learning_rate': 0.00015194938532959155, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:00<00:01, 26.62it/s]                                               {'loss': 2.2882, 'grad_norm': 3.8529059886932373, 'learning_rate': 0.00014891039762299972, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 26.62it/s] 37%|███▋      | 28/75 [00:01<00:01, 26.39it/s]                                               {'loss': 2.1866, 'grad_norm': 3.481900453567505, 'learning_rate': 0.0001458714099164079, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 26.39it/s]                                               {'loss': 2.4125, 'grad_norm': 4.281772136688232, 'learning_rate': 0.00014283242220981606, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 26.39it/s]                                               {'loss': 2.6687, 'grad_norm': 10.049250602722168, 'learning_rate': 0.0001397934345032242, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 26.39it/s]                                               {'loss': 2.1731, 'grad_norm': 2.8562278747558594, 'learning_rate': 0.00013675444679663238, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.39it/s] 43%|████▎     | 32/75 [00:01<00:01, 27.92it/s]                                               {'loss': 2.1859, 'grad_norm': 3.160339832305908, 'learning_rate': 0.00013371545909004058, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 27.92it/s]                                               {'loss': 2.3208, 'grad_norm': 5.109374046325684, 'learning_rate': 0.00013067647138344875, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 27.92it/s]                                               {'loss': 2.1958, 'grad_norm': 3.919585704803467, 'learning_rate': 0.00012763748367685692, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 27.92it/s] 47%|████▋     | 35/75 [00:01<00:01, 27.18it/s]                                               {'loss': 2.6361, 'grad_norm': 4.550304889678955, 'learning_rate': 0.00012459849597026506, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 27.18it/s]                                               {'loss': 2.3378, 'grad_norm': 3.7510600090026855, 'learning_rate': 0.00012155950826367323, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 27.18it/s]                                               {'loss': 2.2891, 'grad_norm': 3.690218448638916, 'learning_rate': 0.00011852052055708142, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 27.18it/s] 51%|█████     | 38/75 [00:01<00:01, 26.73it/s]                                               {'loss': 2.1828, 'grad_norm': 3.8017947673797607, 'learning_rate': 0.00011548153285048959, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.73it/s]                                               {'loss': 2.1016, 'grad_norm': 2.8843605518341064, 'learning_rate': 0.00011244254514389775, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.73it/s]                                               {'loss': 2.227, 'grad_norm': 3.6043269634246826, 'learning_rate': 0.0001094035574373059, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 26.73it/s] 55%|█████▍    | 41/75 [00:01<00:01, 26.43it/s]                                               {'loss': 2.3563, 'grad_norm': 3.62092924118042, 'learning_rate': 0.00010636456973071409, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 26.43it/s]                                               {'loss': 2.1251, 'grad_norm': 4.3442606925964355, 'learning_rate': 0.00010332558202412225, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 26.43it/s]                                               {'loss': 2.1986, 'grad_norm': 2.906146287918091, 'learning_rate': 0.00010028659431753042, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 26.43it/s] 59%|█████▊    | 44/75 [00:01<00:01, 26.38it/s]                                               {'loss': 2.2853, 'grad_norm': 3.6406025886535645, 'learning_rate': 9.72476066109386e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 26.38it/s]                                               {'loss': 2.0712, 'grad_norm': 8.85219955444336, 'learning_rate': 9.420861890434676e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 26.38it/s]                                               {'loss': 2.151, 'grad_norm': 3.823153257369995, 'learning_rate': 9.116963119775493e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.38it/s]                                               {'loss': 2.3267, 'grad_norm': 4.205376148223877, 'learning_rate': 8.81306434911631e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.38it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.75it/s]                                               {'loss': 2.2874, 'grad_norm': 3.1906418800354004, 'learning_rate': 8.509165578457127e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 27.75it/s]                                               {'loss': 2.3193, 'grad_norm': 4.24411678314209, 'learning_rate': 8.205266807797943e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 27.75it/s]                                               {'loss': 2.1506, 'grad_norm': 2.8020191192626953, 'learning_rate': 7.90136803713876e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 27.75it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.17it/s]                                               {'loss': 2.2807, 'grad_norm': 4.500738620758057, 'learning_rate': 7.597469266479577e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 27.17it/s]                                               {'loss': 2.3034, 'grad_norm': 3.8248705863952637, 'learning_rate': 7.293570495820395e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 27.17it/s]                                               {'loss': 2.1547, 'grad_norm': 2.6603262424468994, 'learning_rate': 6.98967172516121e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:01<00:00, 27.17it/s] 72%|███████▏  | 54/75 [00:02<00:00, 26.83it/s]                                               {'loss': 2.3833, 'grad_norm': 4.089776515960693, 'learning_rate': 6.685772954502029e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 26.83it/s]                                               {'loss': 2.2292, 'grad_norm': 3.786064624786377, 'learning_rate': 6.381874183842846e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 26.83it/s]                                               {'loss': 2.0776, 'grad_norm': 2.988769054412842, 'learning_rate': 6.077975413183662e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 26.83it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.77it/s]                                               {'loss': 2.2092, 'grad_norm': 3.789536476135254, 'learning_rate': 5.7740766425244795e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 26.77it/s]                                               {'loss': 2.0589, 'grad_norm': 3.523925304412842, 'learning_rate': 5.470177871865295e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 26.77it/s]                                               {'loss': 2.2595, 'grad_norm': 3.457303047180176, 'learning_rate': 5.1662791012061124e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 26.77it/s]                                               {'loss': 1.6375, 'grad_norm': 6.2319416999816895, 'learning_rate': 4.86238033054693e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 26.77it/s] 81%|████████▏ | 61/75 [00:02<00:00, 28.06it/s]                                               {'loss': 2.0919, 'grad_norm': 3.4305708408355713, 'learning_rate': 4.5584815598877466e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 28.06it/s]                                               {'loss': 2.3525, 'grad_norm': 3.0364253520965576, 'learning_rate': 4.254582789228564e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 28.06it/s]                                               {'loss': 2.3764, 'grad_norm': 3.9593288898468018, 'learning_rate': 3.95068401856938e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 28.06it/s] 85%|████████▌ | 64/75 [00:02<00:00, 27.49it/s]                                               {'loss': 2.1529, 'grad_norm': 3.2431671619415283, 'learning_rate': 3.646785247910197e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 27.49it/s]                                               {'loss': 2.1369, 'grad_norm': 4.535808086395264, 'learning_rate': 3.3428864772510144e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 27.49it/s]                                               {'loss': 2.101, 'grad_norm': 3.4769511222839355, 'learning_rate': 3.038987706591831e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 27.49it/s] 89%|████████▉ | 67/75 [00:02<00:00, 27.25it/s]                                               {'loss': 2.2307, 'grad_norm': 3.487957239151001, 'learning_rate': 2.7350889359326476e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 27.25it/s]                                               {'loss': 2.2118, 'grad_norm': 3.6032040119171143, 'learning_rate': 2.431190165273465e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 27.25it/s]                                               {'loss': 2.2933, 'grad_norm': 3.517904043197632, 'learning_rate': 2.127291394614282e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 27.25it/s] 93%|█████████▎| 70/75 [00:02<00:00, 26.70it/s]                                               {'loss': 2.0056, 'grad_norm': 2.776912212371826, 'learning_rate': 1.8233926239550986e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 26.70it/s]                                               {'loss': 2.1685, 'grad_norm': 3.3039119243621826, 'learning_rate': 1.5194938532959154e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 26.70it/s]                                               {'loss': 2.2167, 'grad_norm': 3.6429803371429443, 'learning_rate': 1.2155950826367325e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 26.70it/s] 97%|█████████▋| 73/75 [00:02<00:00, 26.35it/s]                                               {'loss': 2.0589, 'grad_norm': 4.297829627990723, 'learning_rate': 9.116963119775493e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 26.35it/s]                                               {'loss': 2.0628, 'grad_norm': 3.218813419342041, 'learning_rate': 6.077975413183663e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 26.35it/s]                                               {'loss': 1.9741, 'grad_norm': 8.002127647399902, 'learning_rate': 3.0389877065918314e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.35it/s]                                               {'train_runtime': 2.8354, 'train_samples_per_second': 398.534, 'train_steps_per_second': 26.451, 'train_loss': 2.314242836634318, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.35it/s]100%|██████████| 75/75 [00:02<00:00, 26.45it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1920, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1584, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1508, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1323, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1542, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1315, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1557, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1494, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1370, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1273, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1471, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1098, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 64.15it/s]  3%|▎         | 14/471 [00:00<00:08, 56.60it/s]  4%|▍         | 20/471 [00:00<00:08, 55.02it/s]  6%|▌         | 26/471 [00:00<00:08, 53.95it/s]  7%|▋         | 32/471 [00:00<00:08, 53.33it/s]  8%|▊         | 38/471 [00:00<00:08, 53.18it/s]  9%|▉         | 44/471 [00:00<00:08, 52.99it/s] 11%|█         | 50/471 [00:00<00:07, 52.79it/s] 12%|█▏        | 56/471 [00:01<00:07, 52.60it/s] 13%|█▎        | 62/471 [00:01<00:07, 52.73it/s] 14%|█▍        | 68/471 [00:01<00:07, 52.54it/s] 16%|█▌        | 74/471 [00:01<00:07, 52.46it/s] 17%|█▋        | 80/471 [00:01<00:07, 52.50it/s] 18%|█▊        | 86/471 [00:01<00:07, 52.56it/s] 20%|█▉        | 92/471 [00:01<00:07, 52.48it/s] 21%|██        | 98/471 [00:01<00:07, 52.43it/s] 22%|██▏       | 104/471 [00:01<00:06, 52.57it/s] 23%|██▎       | 110/471 [00:02<00:06, 52.43it/s] 25%|██▍       | 116/471 [00:02<00:06, 52.35it/s] 26%|██▌       | 122/471 [00:02<00:06, 52.34it/s] 27%|██▋       | 128/471 [00:02<00:06, 52.47it/s] 28%|██▊       | 134/471 [00:02<00:06, 52.42it/s] 30%|██▉       | 140/471 [00:02<00:06, 52.30it/s] 31%|███       | 146/471 [00:02<00:06, 52.39it/s] 32%|███▏      | 152/471 [00:02<00:06, 52.39it/s] 34%|███▎      | 158/471 [00:02<00:05, 52.29it/s] 35%|███▍      | 164/471 [00:03<00:05, 52.23it/s] 36%|███▌      | 170/471 [00:03<00:05, 52.39it/s] 37%|███▋      | 176/471 [00:03<00:05, 52.41it/s] 39%|███▊      | 182/471 [00:03<00:05, 52.32it/s] 40%|███▉      | 188/471 [00:03<00:05, 52.19it/s] 41%|████      | 194/471 [00:03<00:05, 52.39it/s] 42%|████▏     | 200/471 [00:03<00:05, 52.32it/s] 44%|████▎     | 206/471 [00:03<00:05, 52.25it/s] 45%|████▌     | 212/471 [00:04<00:04, 52.22it/s] 46%|████▋     | 218/471 [00:04<00:04, 52.42it/s] 48%|████▊     | 224/471 [00:04<00:04, 52.33it/s] 49%|████▉     | 230/471 [00:04<00:04, 52.21it/s] 50%|█████     | 236/471 [00:04<00:04, 52.21it/s] 51%|█████▏    | 242/471 [00:04<00:04, 52.38it/s] 53%|█████▎    | 248/471 [00:04<00:04, 52.22it/s] 54%|█████▍    | 254/471 [00:04<00:04, 52.13it/s] 55%|█████▌    | 260/471 [00:04<00:04, 51.96it/s] 56%|█████▋    | 266/471 [00:05<00:03, 52.11it/s] 58%|█████▊    | 272/471 [00:05<00:03, 52.03it/s] 59%|█████▉    | 278/471 [00:05<00:03, 52.01it/s] 60%|██████    | 284/471 [00:05<00:03, 51.76it/s] 62%|██████▏   | 290/471 [00:05<00:03, 51.92it/s] 63%|██████▎   | 296/471 [00:05<00:03, 52.13it/s] 64%|██████▍   | 302/471 [00:05<00:03, 51.89it/s] 65%|██████▌   | 308/471 [00:05<00:03, 51.92it/s] 67%|██████▋   | 314/471 [00:05<00:03, 51.90it/s] 68%|██████▊   | 320/471 [00:06<00:02, 52.07it/s] 69%|██████▉   | 326/471 [00:06<00:02, 52.07it/s] 70%|███████   | 332/471 [00:06<00:02, 52.03it/s] 72%|███████▏  | 338/471 [00:06<00:02, 51.96it/s] 73%|███████▎  | 344/471 [00:06<00:02, 52.13it/s] 74%|███████▍  | 350/471 [00:06<00:02, 52.10it/s] 76%|███████▌  | 356/471 [00:06<00:02, 52.02it/s] 77%|███████▋  | 362/471 [00:06<00:02, 51.95it/s] 78%|███████▊  | 368/471 [00:07<00:01, 52.05it/s] 79%|███████▉  | 374/471 [00:07<00:01, 52.11it/s] 81%|████████  | 380/471 [00:07<00:01, 51.98it/s] 82%|████████▏ | 386/471 [00:07<00:01, 51.98it/s] 83%|████████▎ | 392/471 [00:07<00:01, 52.03it/s] 85%|████████▍ | 398/471 [00:07<00:01, 52.21it/s] 86%|████████▌ | 404/471 [00:07<00:01, 52.19it/s] 87%|████████▋ | 410/471 [00:07<00:01, 52.09it/s] 88%|████████▊ | 416/471 [00:07<00:01, 51.88it/s] 90%|████████▉ | 422/471 [00:08<00:00, 52.01it/s] 91%|█████████ | 428/471 [00:08<00:00, 51.90it/s] 92%|█████████▏| 434/471 [00:08<00:00, 51.86it/s] 93%|█████████▎| 440/471 [00:08<00:00, 51.86it/s] 95%|█████████▍| 446/471 [00:08<00:00, 51.76it/s] 96%|█████████▌| 452/471 [00:08<00:00, 52.08it/s] 97%|█████████▋| 458/471 [00:08<00:00, 52.03it/s] 99%|█████████▊| 464/471 [00:08<00:00, 51.97it/s]100%|█████████▉| 470/471 [00:08<00:00, 51.99it/s]100%|██████████| 471/471 [00:08<00:00, 52.36it/s]
{'eval_loss': 2.6721787452697754, 'eval_model_preparation_time': 0.0025, 'eval_acc': 0.23473181093998938, 'eval_runtime': 9.015, 'eval_samples_per_second': 835.499, 'eval_steps_per_second': 52.246}
ROUND:2
CLIENT:21
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.685, 'grad_norm': 6.021395206451416, 'learning_rate': 0.00020729490168751576, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:02, 25.09it/s]                                              {'loss': 2.423, 'grad_norm': 6.243534088134766, 'learning_rate': 0.00020453096966501557, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 25.90it/s]  4%|▍         | 3/75 [00:00<00:02, 26.35it/s]                                              {'loss': 2.7061, 'grad_norm': 3.469827890396118, 'learning_rate': 0.00020176703764251535, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 26.35it/s]                                              {'loss': 2.438, 'grad_norm': 4.220912933349609, 'learning_rate': 0.00019900310562001513, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 26.35it/s]                                              {'loss': 2.4215, 'grad_norm': 3.998840570449829, 'learning_rate': 0.0001962391735975149, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 26.35it/s]  8%|▊         | 6/75 [00:00<00:02, 26.63it/s]                                              {'loss': 2.2923, 'grad_norm': 3.900404691696167, 'learning_rate': 0.0001934752415750147, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 26.63it/s]                                              {'loss': 2.4424, 'grad_norm': 3.765650510787964, 'learning_rate': 0.00019071130955251452, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 26.63it/s]                                              {'loss': 2.4551, 'grad_norm': 5.0746989250183105, 'learning_rate': 0.00018794737753001427, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 26.63it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.86it/s]                                              {'loss': 2.1075, 'grad_norm': 3.8695266246795654, 'learning_rate': 0.00018518344550751408, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.86it/s]                                              {'loss': 2.5932, 'grad_norm': 7.106784820556641, 'learning_rate': 0.00018241951348501388, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.86it/s]                                               {'loss': 2.2697, 'grad_norm': 4.167245864868164, 'learning_rate': 0.00017965558146251366, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.86it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.91it/s]                                               {'loss': 2.1468, 'grad_norm': 4.082938194274902, 'learning_rate': 0.00017689164944001347, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.91it/s]                                               {'loss': 2.2923, 'grad_norm': 3.0291569232940674, 'learning_rate': 0.00017412771741751322, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.91it/s]                                               {'loss': 2.4301, 'grad_norm': 4.627497673034668, 'learning_rate': 0.00017136378539501303, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.91it/s]                                               {'loss': 1.6443, 'grad_norm': 8.767975807189941, 'learning_rate': 0.00016859985337251284, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.91it/s] 21%|██▏       | 16/75 [00:00<00:02, 27.78it/s]                                               {'loss': 2.4266, 'grad_norm': 4.930126190185547, 'learning_rate': 0.00016583592135001261, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 27.78it/s]                                               {'loss': 2.2726, 'grad_norm': 5.169094085693359, 'learning_rate': 0.0001630719893275124, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 27.78it/s]                                               {'loss': 2.0901, 'grad_norm': 3.871077060699463, 'learning_rate': 0.00016030805730501217, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 27.78it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.97it/s]                                               {'loss': 2.2002, 'grad_norm': 3.2718420028686523, 'learning_rate': 0.00015754412528251198, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.97it/s]                                               {'loss': 2.1096, 'grad_norm': 4.009253025054932, 'learning_rate': 0.00015478019326001179, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.97it/s]                                               {'loss': 2.2093, 'grad_norm': 3.925166606903076, 'learning_rate': 0.00015201626123751154, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.97it/s] 29%|██▉       | 22/75 [00:00<00:02, 26.33it/s]                                               {'loss': 2.3148, 'grad_norm': 4.434101581573486, 'learning_rate': 0.00014925232921501134, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 26.33it/s]                                               {'loss': 2.2809, 'grad_norm': 4.1397294998168945, 'learning_rate': 0.00014648839719251112, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 26.33it/s]                                               {'loss': 2.2811, 'grad_norm': 3.345543146133423, 'learning_rate': 0.00014372446517001093, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 26.33it/s] 33%|███▎      | 25/75 [00:00<00:01, 26.00it/s]                                               {'loss': 2.0472, 'grad_norm': 3.808758020401001, 'learning_rate': 0.00014096053314751074, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 26.00it/s]                                               {'loss': 2.1849, 'grad_norm': 3.379906177520752, 'learning_rate': 0.0001381966011250105, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:00<00:01, 26.00it/s]                                               {'loss': 2.1273, 'grad_norm': 3.6561601161956787, 'learning_rate': 0.0001354326691025103, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 26.00it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.82it/s]                                               {'loss': 2.2289, 'grad_norm': 3.5159058570861816, 'learning_rate': 0.0001326687370800101, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.82it/s]                                               {'loss': 2.2774, 'grad_norm': 3.9792234897613525, 'learning_rate': 0.00012990480505750988, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.82it/s]                                               {'loss': 1.8198, 'grad_norm': 11.364967346191406, 'learning_rate': 0.00012714087303500966, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.82it/s]                                               {'loss': 1.9861, 'grad_norm': 3.451673984527588, 'learning_rate': 0.00012437694101250944, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.82it/s] 43%|████▎     | 32/75 [00:01<00:01, 27.41it/s]                                               {'loss': 2.21, 'grad_norm': 4.096028804779053, 'learning_rate': 0.00012161300899000925, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 27.41it/s]                                               {'loss': 2.1733, 'grad_norm': 2.732541561126709, 'learning_rate': 0.00011884907696750904, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 27.41it/s]                                               {'loss': 2.0997, 'grad_norm': 3.5244390964508057, 'learning_rate': 0.00011608514494500883, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 27.41it/s] 47%|████▋     | 35/75 [00:01<00:01, 27.01it/s]                                               {'loss': 1.9405, 'grad_norm': 3.1970572471618652, 'learning_rate': 0.00011332121292250861, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 27.01it/s]                                               {'loss': 2.2379, 'grad_norm': 5.373228073120117, 'learning_rate': 0.0001105572809000084, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 27.01it/s]                                               {'loss': 2.1448, 'grad_norm': 3.203918933868408, 'learning_rate': 0.0001077933488775082, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 27.01it/s] 51%|█████     | 38/75 [00:01<00:01, 26.28it/s]                                               {'loss': 2.1886, 'grad_norm': 4.049821853637695, 'learning_rate': 0.00010502941685500799, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.28it/s]                                               {'loss': 2.1646, 'grad_norm': 3.244936227798462, 'learning_rate': 0.00010226548483250778, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.28it/s]                                               {'loss': 2.1284, 'grad_norm': 3.918642044067383, 'learning_rate': 9.950155281000756e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 26.28it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.87it/s]                                               {'loss': 2.2574, 'grad_norm': 3.7178711891174316, 'learning_rate': 9.673762078750736e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.87it/s]                                               {'loss': 2.2325, 'grad_norm': 3.6403443813323975, 'learning_rate': 9.397368876500714e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.87it/s]                                               {'loss': 2.1744, 'grad_norm': 3.649216413497925, 'learning_rate': 9.120975674250694e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.87it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.78it/s]                                               {'loss': 2.1775, 'grad_norm': 4.40879487991333, 'learning_rate': 8.844582472000673e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.78it/s]                                               {'loss': 2.1844, 'grad_norm': 13.41536808013916, 'learning_rate': 8.568189269750651e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.78it/s]                                               {'loss': 2.1005, 'grad_norm': 3.153670310974121, 'learning_rate': 8.291796067500631e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.78it/s]                                               {'loss': 2.1303, 'grad_norm': 3.549682855606079, 'learning_rate': 8.015402865250609e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.78it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.24it/s]                                               {'loss': 2.0742, 'grad_norm': 3.1422080993652344, 'learning_rate': 7.739009663000589e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 27.24it/s]                                               {'loss': 1.9949, 'grad_norm': 3.459557056427002, 'learning_rate': 7.462616460750567e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 27.24it/s]                                               {'loss': 2.2095, 'grad_norm': 4.113431453704834, 'learning_rate': 7.186223258500547e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 27.24it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.77it/s]                                               {'loss': 2.0983, 'grad_norm': 3.6865978240966797, 'learning_rate': 6.909830056250524e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 26.77it/s]                                               {'loss': 2.2277, 'grad_norm': 3.839362621307373, 'learning_rate': 6.633436854000505e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 26.77it/s]                                               {'loss': 2.147, 'grad_norm': 3.8440568447113037, 'learning_rate': 6.357043651750483e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:01<00:00, 26.77it/s] 72%|███████▏  | 54/75 [00:02<00:00, 26.57it/s]                                               {'loss': 2.1823, 'grad_norm': 3.5543055534362793, 'learning_rate': 6.0806504495004623e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 26.57it/s]                                               {'loss': 2.0224, 'grad_norm': 3.6040258407592773, 'learning_rate': 5.8042572472504416e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 26.57it/s]                                               {'loss': 2.1287, 'grad_norm': 3.3679404258728027, 'learning_rate': 5.52786404500042e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 26.57it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.29it/s]                                               {'loss': 2.0685, 'grad_norm': 3.2680249214172363, 'learning_rate': 5.2514708427503995e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 26.29it/s]                                               {'loss': 2.0123, 'grad_norm': 3.5576095581054688, 'learning_rate': 4.975077640500378e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 26.29it/s]                                               {'loss': 2.1466, 'grad_norm': 4.337093830108643, 'learning_rate': 4.698684438250357e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 26.29it/s]                                               {'loss': 2.1492, 'grad_norm': 7.719354152679443, 'learning_rate': 4.422291236000337e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 26.29it/s] 81%|████████▏ | 61/75 [00:02<00:00, 27.23it/s]                                               {'loss': 2.3099, 'grad_norm': 5.029235363006592, 'learning_rate': 4.1458980337503154e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 27.23it/s]                                               {'loss': 2.0037, 'grad_norm': 3.507683515548706, 'learning_rate': 3.8695048315002947e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 27.23it/s]                                               {'loss': 2.051, 'grad_norm': 3.639183759689331, 'learning_rate': 3.593111629250273e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 27.23it/s] 85%|████████▌ | 64/75 [00:02<00:00, 26.64it/s]                                               {'loss': 2.2393, 'grad_norm': 3.209028720855713, 'learning_rate': 3.3167184270002526e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.64it/s]                                               {'loss': 2.1942, 'grad_norm': 4.07296085357666, 'learning_rate': 3.0403252247502312e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.64it/s]                                               {'loss': 2.0085, 'grad_norm': 2.9196903705596924, 'learning_rate': 2.76393202250021e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.64it/s] 89%|████████▉ | 67/75 [00:02<00:00, 26.20it/s]                                               {'loss': 1.919, 'grad_norm': 4.279717922210693, 'learning_rate': 2.487538820250189e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.20it/s]                                               {'loss': 1.9569, 'grad_norm': 3.5513248443603516, 'learning_rate': 2.2111456180001684e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 26.20it/s]                                               {'loss': 1.93, 'grad_norm': 3.225517511367798, 'learning_rate': 1.9347524157501473e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 26.20it/s] 93%|█████████▎| 70/75 [00:02<00:00, 25.94it/s]                                               {'loss': 1.9754, 'grad_norm': 3.027407646179199, 'learning_rate': 1.6583592135001263e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.94it/s]                                               {'loss': 2.1406, 'grad_norm': 3.3280692100524902, 'learning_rate': 1.381966011250105e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.94it/s]                                               {'loss': 2.3135, 'grad_norm': 4.431565761566162, 'learning_rate': 1.1055728090000842e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.94it/s] 97%|█████████▋| 73/75 [00:02<00:00, 25.81it/s]                                               {'loss': 2.2806, 'grad_norm': 3.786379337310791, 'learning_rate': 8.291796067500631e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.81it/s]                                               {'loss': 2.1419, 'grad_norm': 3.3160347938537598, 'learning_rate': 5.527864045000421e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.81it/s]                                               {'loss': 1.7827, 'grad_norm': 10.538195610046387, 'learning_rate': 2.7639320225002105e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.81it/s]                                               {'train_runtime': 2.9005, 'train_samples_per_second': 389.581, 'train_steps_per_second': 25.857, 'train_loss': 2.1794345394770303, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.81it/s]100%|██████████| 75/75 [00:02<00:00, 25.86it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.6305, 'grad_norm': 5.156942367553711, 'learning_rate': 0.00020729490168751576, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:02, 25.41it/s]                                              {'loss': 2.5789, 'grad_norm': 5.806811332702637, 'learning_rate': 0.00020453096966501557, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 26.26it/s]  4%|▍         | 3/75 [00:00<00:02, 25.09it/s]                                              {'loss': 2.5455, 'grad_norm': 4.413576126098633, 'learning_rate': 0.00020176703764251535, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.09it/s]                                              {'loss': 2.3885, 'grad_norm': 4.9803595542907715, 'learning_rate': 0.00019900310562001513, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.09it/s]                                              {'loss': 2.5299, 'grad_norm': 5.2514967918396, 'learning_rate': 0.0001962391735975149, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.09it/s]  8%|▊         | 6/75 [00:00<00:02, 24.69it/s]                                              {'loss': 2.6383, 'grad_norm': 4.290030479431152, 'learning_rate': 0.0001934752415750147, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.69it/s]                                              {'loss': 2.3945, 'grad_norm': 4.427065372467041, 'learning_rate': 0.00019071130955251452, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.69it/s]                                              {'loss': 2.3644, 'grad_norm': 4.527737140655518, 'learning_rate': 0.00018794737753001427, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.69it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.24it/s]                                              {'loss': 2.6142, 'grad_norm': 4.808636665344238, 'learning_rate': 0.00018518344550751408, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.24it/s]                                              {'loss': 2.5428, 'grad_norm': 4.691483974456787, 'learning_rate': 0.00018241951348501388, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.24it/s]                                               {'loss': 2.2548, 'grad_norm': 4.224063873291016, 'learning_rate': 0.00017965558146251366, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.24it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.46it/s]                                               {'loss': 2.2253, 'grad_norm': 3.4892165660858154, 'learning_rate': 0.00017689164944001347, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.46it/s]                                               {'loss': 2.6121, 'grad_norm': 4.829139232635498, 'learning_rate': 0.00017412771741751322, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.46it/s]                                               {'loss': 2.3753, 'grad_norm': 5.0746870040893555, 'learning_rate': 0.00017136378539501303, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.46it/s]                                               {'loss': 2.3927, 'grad_norm': 19.830751419067383, 'learning_rate': 0.00016859985337251284, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.46it/s] 21%|██▏       | 16/75 [00:00<00:02, 27.35it/s]                                               {'loss': 2.3239, 'grad_norm': 5.451004505157471, 'learning_rate': 0.00016583592135001261, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 27.35it/s]                                               {'loss': 2.2702, 'grad_norm': 4.872158527374268, 'learning_rate': 0.0001630719893275124, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 27.35it/s]                                               {'loss': 2.2239, 'grad_norm': 4.140322685241699, 'learning_rate': 0.00016030805730501217, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 27.35it/s] 25%|██▌       | 19/75 [00:00<00:02, 27.08it/s]                                               {'loss': 2.194, 'grad_norm': 4.058452129364014, 'learning_rate': 0.00015754412528251198, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 27.08it/s]                                               {'loss': 2.5224, 'grad_norm': 3.5816495418548584, 'learning_rate': 0.00015478019326001179, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 27.08it/s]                                               {'loss': 2.1912, 'grad_norm': 3.59094500541687, 'learning_rate': 0.00015201626123751154, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:01, 27.08it/s] 29%|██▉       | 22/75 [00:00<00:02, 26.39it/s]                                               {'loss': 2.3856, 'grad_norm': 3.9855315685272217, 'learning_rate': 0.00014925232921501134, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 26.39it/s]                                               {'loss': 2.4854, 'grad_norm': 3.7746849060058594, 'learning_rate': 0.00014648839719251112, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 26.39it/s]                                               {'loss': 2.4084, 'grad_norm': 4.274120807647705, 'learning_rate': 0.00014372446517001093, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 26.39it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.36it/s]                                               {'loss': 2.3005, 'grad_norm': 4.364236354827881, 'learning_rate': 0.00014096053314751074, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.36it/s]                                               {'loss': 2.2503, 'grad_norm': 4.256170749664307, 'learning_rate': 0.0001381966011250105, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.36it/s]                                               {'loss': 2.2286, 'grad_norm': 3.1339595317840576, 'learning_rate': 0.0001354326691025103, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.36it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.14it/s]                                               {'loss': 2.5113, 'grad_norm': 4.6674699783325195, 'learning_rate': 0.0001326687370800101, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.14it/s]                                               {'loss': 2.2801, 'grad_norm': 3.638188362121582, 'learning_rate': 0.00012990480505750988, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.14it/s]                                               {'loss': 1.7776, 'grad_norm': 10.030579566955566, 'learning_rate': 0.00012714087303500966, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.14it/s]                                               {'loss': 2.3924, 'grad_norm': 3.9229488372802734, 'learning_rate': 0.00012437694101250944, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.14it/s] 43%|████▎     | 32/75 [00:01<00:01, 25.99it/s]                                               {'loss': 2.3047, 'grad_norm': 5.137661933898926, 'learning_rate': 0.00012161300899000925, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.99it/s]                                               {'loss': 2.2966, 'grad_norm': 5.172427177429199, 'learning_rate': 0.00011884907696750904, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.99it/s]                                               {'loss': 2.3142, 'grad_norm': 4.046605587005615, 'learning_rate': 0.00011608514494500883, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.99it/s] 47%|████▋     | 35/75 [00:01<00:01, 25.09it/s]                                               {'loss': 2.0722, 'grad_norm': 3.5722124576568604, 'learning_rate': 0.00011332121292250861, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.09it/s]                                               {'loss': 2.1007, 'grad_norm': 2.8901774883270264, 'learning_rate': 0.0001105572809000084, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.09it/s]                                               {'loss': 2.0588, 'grad_norm': 3.9974958896636963, 'learning_rate': 0.0001077933488775082, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.09it/s] 51%|█████     | 38/75 [00:01<00:01, 23.91it/s]                                               {'loss': 2.1582, 'grad_norm': 4.055912971496582, 'learning_rate': 0.00010502941685500799, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 23.91it/s]                                               {'loss': 2.2982, 'grad_norm': 4.557745933532715, 'learning_rate': 0.00010226548483250778, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.91it/s]                                               {'loss': 2.303, 'grad_norm': 3.553576946258545, 'learning_rate': 9.950155281000756e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.91it/s] 55%|█████▍    | 41/75 [00:01<00:01, 24.25it/s]                                               {'loss': 2.2733, 'grad_norm': 5.2668538093566895, 'learning_rate': 9.673762078750736e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.25it/s]                                               {'loss': 2.3364, 'grad_norm': 4.792301177978516, 'learning_rate': 9.397368876500714e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.25it/s]                                               {'loss': 2.2045, 'grad_norm': 4.170775413513184, 'learning_rate': 9.120975674250694e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.25it/s] 59%|█████▊    | 44/75 [00:01<00:01, 24.66it/s]                                               {'loss': 2.5065, 'grad_norm': 4.611301898956299, 'learning_rate': 8.844582472000673e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.66it/s]                                               {'loss': 1.4568, 'grad_norm': 5.868827819824219, 'learning_rate': 8.568189269750651e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.66it/s]                                               {'loss': 2.174, 'grad_norm': 4.010499000549316, 'learning_rate': 8.291796067500631e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.66it/s]                                               {'loss': 2.1925, 'grad_norm': 3.9614033699035645, 'learning_rate': 8.015402865250609e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 24.66it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.02it/s]                                               {'loss': 2.2753, 'grad_norm': 3.7828822135925293, 'learning_rate': 7.739009663000589e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.02it/s]                                               {'loss': 2.3484, 'grad_norm': 3.7148025035858154, 'learning_rate': 7.462616460750567e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.02it/s]                                               {'loss': 2.1557, 'grad_norm': 4.302737712860107, 'learning_rate': 7.186223258500547e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.02it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.10it/s]                                               {'loss': 2.3708, 'grad_norm': 3.863105058670044, 'learning_rate': 6.909830056250524e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 26.10it/s]                                               {'loss': 2.0956, 'grad_norm': 4.0084757804870605, 'learning_rate': 6.633436854000505e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 26.10it/s]                                               {'loss': 2.247, 'grad_norm': 4.238337516784668, 'learning_rate': 6.357043651750483e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 26.10it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.62it/s]                                               {'loss': 2.2914, 'grad_norm': 4.178211212158203, 'learning_rate': 6.0806504495004623e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.62it/s]                                               {'loss': 2.3012, 'grad_norm': 3.5600087642669678, 'learning_rate': 5.8042572472504416e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.62it/s]                                               {'loss': 2.2218, 'grad_norm': 3.5206334590911865, 'learning_rate': 5.52786404500042e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.62it/s] 76%|███████▌  | 57/75 [00:02<00:00, 25.27it/s]                                               {'loss': 2.0868, 'grad_norm': 3.7671515941619873, 'learning_rate': 5.2514708427503995e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.27it/s]                                               {'loss': 2.0833, 'grad_norm': 3.4673192501068115, 'learning_rate': 4.975077640500378e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.27it/s]                                               {'loss': 2.3248, 'grad_norm': 3.7201664447784424, 'learning_rate': 4.698684438250357e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.27it/s] 80%|████████  | 60/75 [00:02<00:00, 21.20it/s]                                               {'loss': 1.6556, 'grad_norm': 9.046873092651367, 'learning_rate': 4.422291236000337e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 21.20it/s]                                               {'loss': 2.1258, 'grad_norm': 3.2130985260009766, 'learning_rate': 4.1458980337503154e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 21.20it/s]                                               {'loss': 2.1353, 'grad_norm': 4.137611389160156, 'learning_rate': 3.8695048315002947e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 21.20it/s] 84%|████████▍ | 63/75 [00:02<00:00, 17.21it/s]                                               {'loss': 2.1398, 'grad_norm': 3.1485538482666016, 'learning_rate': 3.593111629250273e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 17.21it/s]                                               {'loss': 2.2933, 'grad_norm': 3.280691623687744, 'learning_rate': 3.3167184270002526e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 17.21it/s] 87%|████████▋ | 65/75 [00:02<00:00, 17.07it/s]                                               {'loss': 2.155, 'grad_norm': 4.574990749359131, 'learning_rate': 3.0403252247502312e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 17.07it/s]                                               {'loss': 2.327, 'grad_norm': 4.014019966125488, 'learning_rate': 2.76393202250021e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 17.07it/s] 89%|████████▉ | 67/75 [00:02<00:00, 17.12it/s]                                               {'loss': 2.2271, 'grad_norm': 3.3144187927246094, 'learning_rate': 2.487538820250189e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 17.12it/s]                                               {'loss': 2.0495, 'grad_norm': 3.078080415725708, 'learning_rate': 2.2111456180001684e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 17.12it/s]                                               {'loss': 2.1754, 'grad_norm': 4.174831390380859, 'learning_rate': 1.9347524157501473e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 17.12it/s] 93%|█████████▎| 70/75 [00:03<00:00, 17.31it/s]                                               {'loss': 2.4575, 'grad_norm': 4.901393413543701, 'learning_rate': 1.6583592135001263e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 17.31it/s]                                               {'loss': 2.1751, 'grad_norm': 4.134588241577148, 'learning_rate': 1.381966011250105e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 17.31it/s] 96%|█████████▌| 72/75 [00:03<00:00, 17.01it/s]                                               {'loss': 2.2497, 'grad_norm': 3.8903183937072754, 'learning_rate': 1.1055728090000842e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 17.01it/s]                                               {'loss': 2.0822, 'grad_norm': 4.405562400817871, 'learning_rate': 8.291796067500631e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 17.01it/s]                                               {'loss': 2.3951, 'grad_norm': 4.055103778839111, 'learning_rate': 5.527864045000421e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 17.01it/s]                                               {'loss': 2.0634, 'grad_norm': 12.863259315490723, 'learning_rate': 2.7639320225002105e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 17.01it/s]                                               {'train_runtime': 3.4071, 'train_samples_per_second': 331.663, 'train_steps_per_second': 22.013, 'train_loss': 2.271908330917358, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 17.01it/s]100%|██████████| 75/75 [00:03<00:00, 22.01it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:2
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]  1%|▏         | 1/75 [00:00<00:09,  7.49it/s]                                              {'loss': 2.5718, 'grad_norm': 5.747073173522949, 'learning_rate': 0.00020729490168751576, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:09,  7.49it/s]                                              {'loss': 2.6099, 'grad_norm': 4.581268310546875, 'learning_rate': 0.00020453096966501557, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:09,  7.49it/s]  4%|▍         | 3/75 [00:00<00:05, 12.61it/s]                                              {'loss': 2.3629, 'grad_norm': 5.475912570953369, 'learning_rate': 0.00020176703764251535, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:05, 12.61it/s]                                              {'loss': 2.4998, 'grad_norm': 4.177952766418457, 'learning_rate': 0.00019900310562001513, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:05, 12.61it/s]                                              {'loss': 2.3955, 'grad_norm': 4.7318830490112305, 'learning_rate': 0.0001962391735975149, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:05, 12.61it/s]  8%|▊         | 6/75 [00:00<00:04, 16.83it/s]                                              {'loss': 2.4891, 'grad_norm': 5.311194896697998, 'learning_rate': 0.0001934752415750147, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:04, 16.83it/s]                                              {'loss': 2.5612, 'grad_norm': 3.9203875064849854, 'learning_rate': 0.00019071130955251452, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:04, 16.83it/s]                                              {'loss': 2.2122, 'grad_norm': 4.360802173614502, 'learning_rate': 0.00018794737753001427, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 16.83it/s] 12%|█▏        | 9/75 [00:00<00:03, 19.74it/s]                                              {'loss': 2.4778, 'grad_norm': 4.0119123458862305, 'learning_rate': 0.00018518344550751408, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 19.74it/s]                                              {'loss': 2.4179, 'grad_norm': 4.053442001342773, 'learning_rate': 0.00018241951348501388, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 19.74it/s]                                               {'loss': 2.284, 'grad_norm': 4.173444747924805, 'learning_rate': 0.00017965558146251366, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 19.74it/s] 16%|█▌        | 12/75 [00:00<00:02, 21.42it/s]                                               {'loss': 2.571, 'grad_norm': 4.747979164123535, 'learning_rate': 0.00017689164944001347, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 21.42it/s]                                               {'loss': 2.324, 'grad_norm': 3.7062225341796875, 'learning_rate': 0.00017412771741751322, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 21.42it/s]                                               {'loss': 2.3637, 'grad_norm': 5.970532417297363, 'learning_rate': 0.00017136378539501303, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 21.42it/s]                                               {'loss': 2.2292, 'grad_norm': 7.812813758850098, 'learning_rate': 0.00016859985337251284, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 21.42it/s] 21%|██▏       | 16/75 [00:00<00:02, 24.28it/s]                                               {'loss': 2.3952, 'grad_norm': 4.986184120178223, 'learning_rate': 0.00016583592135001261, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 24.28it/s]                                               {'loss': 2.3478, 'grad_norm': 4.250979423522949, 'learning_rate': 0.0001630719893275124, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.28it/s]                                               {'loss': 2.3322, 'grad_norm': 3.376509428024292, 'learning_rate': 0.00016030805730501217, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.28it/s] 25%|██▌       | 19/75 [00:00<00:02, 21.19it/s]                                               {'loss': 2.3585, 'grad_norm': 4.200650215148926, 'learning_rate': 0.00015754412528251198, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 21.19it/s]                                               {'loss': 2.2963, 'grad_norm': 3.9485721588134766, 'learning_rate': 0.00015478019326001179, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:01<00:02, 21.19it/s]                                               {'loss': 2.3137, 'grad_norm': 4.457719326019287, 'learning_rate': 0.00015201626123751154, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:02, 21.19it/s] 29%|██▉       | 22/75 [00:01<00:02, 20.83it/s]                                               {'loss': 2.5032, 'grad_norm': 4.7838568687438965, 'learning_rate': 0.00014925232921501134, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 20.83it/s]                                               {'loss': 2.3371, 'grad_norm': 4.083846569061279, 'learning_rate': 0.00014648839719251112, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 20.83it/s]                                               {'loss': 2.3749, 'grad_norm': 4.225738525390625, 'learning_rate': 0.00014372446517001093, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 20.83it/s] 33%|███▎      | 25/75 [00:01<00:02, 21.58it/s]                                               {'loss': 2.1568, 'grad_norm': 4.286378860473633, 'learning_rate': 0.00014096053314751074, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 21.58it/s]                                               {'loss': 2.3252, 'grad_norm': 5.3930535316467285, 'learning_rate': 0.0001381966011250105, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 21.58it/s]                                               {'loss': 2.1825, 'grad_norm': 3.838268518447876, 'learning_rate': 0.0001354326691025103, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 21.58it/s] 37%|███▋      | 28/75 [00:01<00:02, 21.76it/s]                                               {'loss': 2.2255, 'grad_norm': 4.887998580932617, 'learning_rate': 0.0001326687370800101, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 21.76it/s]                                               {'loss': 2.2515, 'grad_norm': 3.5095791816711426, 'learning_rate': 0.00012990480505750988, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 21.76it/s]                                               {'loss': 2.5583, 'grad_norm': 9.414892196655273, 'learning_rate': 0.00012714087303500966, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 21.76it/s] 41%|████▏     | 31/75 [00:01<00:01, 22.71it/s]                                               {'loss': 2.1434, 'grad_norm': 3.764141798019409, 'learning_rate': 0.00012437694101250944, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 22.71it/s]                                               {'loss': 2.1866, 'grad_norm': 4.044419765472412, 'learning_rate': 0.00012161300899000925, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 22.71it/s]                                               {'loss': 2.2765, 'grad_norm': 4.017584323883057, 'learning_rate': 0.00011884907696750904, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 22.71it/s] 45%|████▌     | 34/75 [00:01<00:01, 22.66it/s]                                               {'loss': 2.1147, 'grad_norm': 3.4711201190948486, 'learning_rate': 0.00011608514494500883, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 22.66it/s]                                               {'loss': 2.2164, 'grad_norm': 3.2969553470611572, 'learning_rate': 0.00011332121292250861, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 22.66it/s]                                               {'loss': 2.2501, 'grad_norm': 3.4261255264282227, 'learning_rate': 0.0001105572809000084, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 22.66it/s] 49%|████▉     | 37/75 [00:01<00:01, 19.84it/s]                                               {'loss': 2.3684, 'grad_norm': 3.986499786376953, 'learning_rate': 0.0001077933488775082, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 19.84it/s]                                               {'loss': 2.2941, 'grad_norm': 3.2038891315460205, 'learning_rate': 0.00010502941685500799, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 19.84it/s]                                               {'loss': 2.2452, 'grad_norm': 4.267024040222168, 'learning_rate': 0.00010226548483250778, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 19.84it/s] 53%|█████▎    | 40/75 [00:01<00:01, 21.24it/s]                                               {'loss': 2.3, 'grad_norm': 4.373471260070801, 'learning_rate': 9.950155281000756e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 21.24it/s]                                               {'loss': 2.258, 'grad_norm': 4.766195297241211, 'learning_rate': 9.673762078750736e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 21.24it/s]                                               {'loss': 2.2351, 'grad_norm': 3.98449444770813, 'learning_rate': 9.397368876500714e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 21.24it/s] 57%|█████▋    | 43/75 [00:02<00:01, 22.03it/s]                                               {'loss': 2.2892, 'grad_norm': 4.1590399742126465, 'learning_rate': 9.120975674250694e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 22.03it/s]                                               {'loss': 2.2543, 'grad_norm': 4.026753902435303, 'learning_rate': 8.844582472000673e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 22.03it/s]                                               {'loss': 2.9833, 'grad_norm': 21.035140991210938, 'learning_rate': 8.568189269750651e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 22.03it/s]                                               {'loss': 2.2482, 'grad_norm': 4.0746541023254395, 'learning_rate': 8.291796067500631e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 22.03it/s] 63%|██████▎   | 47/75 [00:02<00:01, 24.34it/s]                                               {'loss': 2.3076, 'grad_norm': 3.932701349258423, 'learning_rate': 8.015402865250609e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 24.34it/s]                                               {'loss': 2.2827, 'grad_norm': 4.893627643585205, 'learning_rate': 7.739009663000589e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 24.34it/s]                                               {'loss': 2.2238, 'grad_norm': 4.611788749694824, 'learning_rate': 7.462616460750567e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 24.34it/s] 67%|██████▋   | 50/75 [00:02<00:01, 23.32it/s]                                               {'loss': 2.0526, 'grad_norm': 2.907517671585083, 'learning_rate': 7.186223258500547e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 23.32it/s]                                               {'loss': 2.1217, 'grad_norm': 4.796947479248047, 'learning_rate': 6.909830056250524e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 23.32it/s]                                               {'loss': 2.3025, 'grad_norm': 3.692495584487915, 'learning_rate': 6.633436854000505e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 23.32it/s] 71%|███████   | 53/75 [00:02<00:00, 22.14it/s]                                               {'loss': 2.2677, 'grad_norm': 3.764986991882324, 'learning_rate': 6.357043651750483e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 22.14it/s]                                               {'loss': 2.3282, 'grad_norm': 4.334539413452148, 'learning_rate': 6.0806504495004623e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 22.14it/s]                                               {'loss': 2.1849, 'grad_norm': 4.144547939300537, 'learning_rate': 5.8042572472504416e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 22.14it/s] 75%|███████▍  | 56/75 [00:02<00:00, 19.79it/s]                                               {'loss': 2.3066, 'grad_norm': 3.209869623184204, 'learning_rate': 5.52786404500042e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 19.79it/s]                                               {'loss': 2.2759, 'grad_norm': 4.729040145874023, 'learning_rate': 5.2514708427503995e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 19.79it/s]                                               {'loss': 2.1576, 'grad_norm': 3.4265999794006348, 'learning_rate': 4.975077640500378e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 19.79it/s] 79%|███████▊  | 59/75 [00:02<00:00, 21.06it/s]                                               {'loss': 2.1648, 'grad_norm': 3.840837240219116, 'learning_rate': 4.698684438250357e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 21.06it/s]                                               {'loss': 2.2748, 'grad_norm': 9.940441131591797, 'learning_rate': 4.422291236000337e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 21.06it/s]                                               {'loss': 2.1635, 'grad_norm': 4.670301914215088, 'learning_rate': 4.1458980337503154e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 21.06it/s] 83%|████████▎ | 62/75 [00:02<00:00, 22.33it/s]                                               {'loss': 2.3364, 'grad_norm': 4.783156394958496, 'learning_rate': 3.8695048315002947e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 22.33it/s]                                               {'loss': 1.9893, 'grad_norm': 3.3273370265960693, 'learning_rate': 3.593111629250273e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 22.33it/s]                                               {'loss': 2.2559, 'grad_norm': 3.724787712097168, 'learning_rate': 3.3167184270002526e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 22.33it/s] 87%|████████▋ | 65/75 [00:03<00:00, 22.73it/s]                                               {'loss': 2.2814, 'grad_norm': 3.689818859100342, 'learning_rate': 3.0403252247502312e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 22.73it/s]                                               {'loss': 2.0585, 'grad_norm': 2.807814359664917, 'learning_rate': 2.76393202250021e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 22.73it/s]                                               {'loss': 2.1168, 'grad_norm': 3.8242642879486084, 'learning_rate': 2.487538820250189e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 22.73it/s] 91%|█████████ | 68/75 [00:03<00:00, 23.43it/s]                                               {'loss': 2.2042, 'grad_norm': 2.7030930519104004, 'learning_rate': 2.2111456180001684e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 23.43it/s]                                               {'loss': 2.2775, 'grad_norm': 4.258860111236572, 'learning_rate': 1.9347524157501473e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 23.43it/s]                                               {'loss': 2.1167, 'grad_norm': 3.968400716781616, 'learning_rate': 1.6583592135001263e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 23.43it/s] 95%|█████████▍| 71/75 [00:03<00:00, 23.69it/s]                                               {'loss': 2.3363, 'grad_norm': 3.848884344100952, 'learning_rate': 1.381966011250105e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 23.69it/s]                                               {'loss': 2.2232, 'grad_norm': 5.522170066833496, 'learning_rate': 1.1055728090000842e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 23.69it/s]                                               {'loss': 2.1821, 'grad_norm': 3.294024705886841, 'learning_rate': 8.291796067500631e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 23.69it/s] 99%|█████████▊| 74/75 [00:03<00:00, 24.36it/s]                                               {'loss': 2.1385, 'grad_norm': 3.759183645248413, 'learning_rate': 5.527864045000421e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.36it/s]                                               {'loss': 2.0749, 'grad_norm': 9.826026916503906, 'learning_rate': 2.7639320225002105e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.36it/s]                                               {'train_runtime': 3.5151, 'train_samples_per_second': 321.471, 'train_steps_per_second': 21.337, 'train_loss': 2.293323178291321, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.36it/s]100%|██████████| 75/75 [00:03<00:00, 21.34it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:14
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.5015, 'grad_norm': 7.439428329467773, 'learning_rate': 0.00020729490168751576, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 17.34it/s]  3%|▎         | 2/75 [00:00<00:05, 14.43it/s]                                              {'loss': 2.5186, 'grad_norm': 4.376843452453613, 'learning_rate': 0.00020453096966501557, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:05, 14.43it/s]                                              {'loss': 2.4534, 'grad_norm': 4.636559963226318, 'learning_rate': 0.00020176703764251535, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 14.43it/s]  5%|▌         | 4/75 [00:00<00:04, 16.91it/s]                                              {'loss': 2.4214, 'grad_norm': 4.578778266906738, 'learning_rate': 0.00019900310562001513, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:04, 16.91it/s]                                              {'loss': 2.3548, 'grad_norm': 4.763696193695068, 'learning_rate': 0.0001962391735975149, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:04, 16.91it/s]  8%|▊         | 6/75 [00:00<00:03, 17.25it/s]                                              {'loss': 2.3477, 'grad_norm': 5.173515319824219, 'learning_rate': 0.0001934752415750147, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 17.25it/s]                                              {'loss': 2.3474, 'grad_norm': 4.476475715637207, 'learning_rate': 0.00019071130955251452, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 17.25it/s] 11%|█         | 8/75 [00:00<00:04, 16.14it/s]                                              {'loss': 2.2843, 'grad_norm': 4.476346015930176, 'learning_rate': 0.00018794737753001427, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:04, 16.14it/s]                                              {'loss': 2.3066, 'grad_norm': 3.6504290103912354, 'learning_rate': 0.00018518344550751408, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:04, 16.14it/s] 13%|█▎        | 10/75 [00:00<00:03, 16.48it/s]                                               {'loss': 2.3262, 'grad_norm': 3.866729259490967, 'learning_rate': 0.00018241951348501388, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 16.48it/s]                                               {'loss': 2.2361, 'grad_norm': 4.643081188201904, 'learning_rate': 0.00017965558146251366, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 16.48it/s]                                               {'loss': 2.3197, 'grad_norm': 5.193037509918213, 'learning_rate': 0.00017689164944001347, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:03, 16.48it/s] 17%|█▋        | 13/75 [00:00<00:03, 18.27it/s]                                               {'loss': 2.4066, 'grad_norm': 5.01809024810791, 'learning_rate': 0.00017412771741751322, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:03, 18.27it/s]                                               {'loss': 2.3748, 'grad_norm': 5.185093879699707, 'learning_rate': 0.00017136378539501303, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:03, 18.27it/s]                                               {'loss': 2.4327, 'grad_norm': 20.50313949584961, 'learning_rate': 0.00016859985337251284, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:03, 18.27it/s] 21%|██▏       | 16/75 [00:00<00:03, 18.10it/s]                                               {'loss': 2.3058, 'grad_norm': 4.821680545806885, 'learning_rate': 0.00016583592135001261, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:03, 18.10it/s]                                               {'loss': 2.2348, 'grad_norm': 4.319529056549072, 'learning_rate': 0.0001630719893275124, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:03, 18.10it/s] 24%|██▍       | 18/75 [00:01<00:03, 16.09it/s]                                               {'loss': 2.1039, 'grad_norm': 3.7776105403900146, 'learning_rate': 0.00016030805730501217, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:01<00:03, 16.09it/s]                                               {'loss': 2.2846, 'grad_norm': 4.27315616607666, 'learning_rate': 0.00015754412528251198, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:01<00:03, 16.09it/s] 27%|██▋       | 20/75 [00:01<00:03, 16.59it/s]                                               {'loss': 2.2544, 'grad_norm': 4.714568138122559, 'learning_rate': 0.00015478019326001179, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:01<00:03, 16.59it/s]                                               {'loss': 2.1191, 'grad_norm': 3.9958860874176025, 'learning_rate': 0.00015201626123751154, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:03, 16.59it/s]                                               {'loss': 2.3861, 'grad_norm': 3.939600706100464, 'learning_rate': 0.00014925232921501134, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:03, 16.59it/s] 31%|███       | 23/75 [00:01<00:02, 17.87it/s]                                               {'loss': 2.343, 'grad_norm': 4.171154975891113, 'learning_rate': 0.00014648839719251112, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 17.87it/s]                                               {'loss': 2.3066, 'grad_norm': 3.8541598320007324, 'learning_rate': 0.00014372446517001093, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 17.87it/s] 33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]                                               {'loss': 2.0036, 'grad_norm': 4.371207237243652, 'learning_rate': 0.00014096053314751074, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 17.60it/s]                                               {'loss': 2.0612, 'grad_norm': 3.7551727294921875, 'learning_rate': 0.0001381966011250105, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 17.60it/s] 36%|███▌      | 27/75 [00:01<00:02, 17.20it/s]                                               {'loss': 1.9933, 'grad_norm': 3.879941701889038, 'learning_rate': 0.0001354326691025103, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 17.20it/s]                                               {'loss': 2.3203, 'grad_norm': 4.481751918792725, 'learning_rate': 0.0001326687370800101, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 17.20it/s] 39%|███▊      | 29/75 [00:01<00:02, 17.84it/s]                                               {'loss': 2.274, 'grad_norm': 3.868034601211548, 'learning_rate': 0.00012990480505750988, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 17.84it/s]                                               {'loss': 2.0311, 'grad_norm': 13.660124778747559, 'learning_rate': 0.00012714087303500966, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 17.84it/s] 41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]                                               {'loss': 2.3017, 'grad_norm': 3.57710862159729, 'learning_rate': 0.00012437694101250944, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]                                               {'loss': 2.0465, 'grad_norm': 3.1027989387512207, 'learning_rate': 0.00012161300899000925, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:02, 17.66it/s]                                               {'loss': 2.1264, 'grad_norm': 3.481391191482544, 'learning_rate': 0.00011884907696750904, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:02, 17.66it/s] 45%|████▌     | 34/75 [00:01<00:02, 19.92it/s]                                               {'loss': 2.1005, 'grad_norm': 3.1920347213745117, 'learning_rate': 0.00011608514494500883, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:02, 19.92it/s]                                               {'loss': 2.3134, 'grad_norm': 6.198673248291016, 'learning_rate': 0.00011332121292250861, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:02, 19.92it/s] 48%|████▊     | 36/75 [00:02<00:02, 17.42it/s]                                               {'loss': 1.9784, 'grad_norm': 3.9306280612945557, 'learning_rate': 0.0001105572809000084, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:02<00:02, 17.42it/s]                                               {'loss': 2.1459, 'grad_norm': 4.42931604385376, 'learning_rate': 0.0001077933488775082, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:02<00:02, 17.42it/s] 51%|█████     | 38/75 [00:02<00:02, 15.47it/s]                                               {'loss': 2.3252, 'grad_norm': 3.8782236576080322, 'learning_rate': 0.00010502941685500799, 'epoch': 2.53}
 51%|█████     | 38/75 [00:02<00:02, 15.47it/s]                                               {'loss': 2.2217, 'grad_norm': 3.5315661430358887, 'learning_rate': 0.00010226548483250778, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:02<00:02, 15.47it/s] 53%|█████▎    | 40/75 [00:02<00:02, 15.48it/s]                                               {'loss': 2.267, 'grad_norm': 3.9532687664031982, 'learning_rate': 9.950155281000756e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:02<00:02, 15.48it/s]                                               {'loss': 2.1422, 'grad_norm': 5.2809038162231445, 'learning_rate': 9.673762078750736e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:02<00:02, 15.48it/s] 56%|█████▌    | 42/75 [00:02<00:02, 15.85it/s]                                               {'loss': 2.0176, 'grad_norm': 3.5766563415527344, 'learning_rate': 9.397368876500714e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:02, 15.85it/s]                                               {'loss': 2.1893, 'grad_norm': 3.8131258487701416, 'learning_rate': 9.120975674250694e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:02, 15.85it/s] 59%|█████▊    | 44/75 [00:02<00:01, 16.01it/s]                                               {'loss': 2.1686, 'grad_norm': 3.9155266284942627, 'learning_rate': 8.844582472000673e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 16.01it/s]                                               {'loss': 2.4105, 'grad_norm': 17.279685974121094, 'learning_rate': 8.568189269750651e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 16.01it/s]                                               {'loss': 2.1205, 'grad_norm': 3.6153316497802734, 'learning_rate': 8.291796067500631e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 16.01it/s] 63%|██████▎   | 47/75 [00:02<00:01, 17.12it/s]                                               {'loss': 2.1117, 'grad_norm': 4.175616264343262, 'learning_rate': 8.015402865250609e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 17.12it/s]                                               {'loss': 2.0598, 'grad_norm': 3.6441521644592285, 'learning_rate': 7.739009663000589e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 17.12it/s] 65%|██████▌   | 49/75 [00:02<00:01, 15.25it/s]                                               {'loss': 2.101, 'grad_norm': 4.897040367126465, 'learning_rate': 7.462616460750567e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 15.25it/s]                                               {'loss': 2.1719, 'grad_norm': 3.3445982933044434, 'learning_rate': 7.186223258500547e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 15.25it/s] 68%|██████▊   | 51/75 [00:03<00:01, 16.00it/s]                                               {'loss': 2.2247, 'grad_norm': 3.50736927986145, 'learning_rate': 6.909830056250524e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:03<00:01, 16.00it/s]                                               {'loss': 2.0962, 'grad_norm': 4.407102584838867, 'learning_rate': 6.633436854000505e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:03<00:01, 16.00it/s] 71%|███████   | 53/75 [00:03<00:01, 14.09it/s]                                               {'loss': 2.1593, 'grad_norm': 4.31479549407959, 'learning_rate': 6.357043651750483e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:03<00:01, 14.09it/s]                                               {'loss': 1.999, 'grad_norm': 3.5134010314941406, 'learning_rate': 6.0806504495004623e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:03<00:01, 14.09it/s] 73%|███████▎  | 55/75 [00:03<00:01, 13.25it/s]                                               {'loss': 2.1807, 'grad_norm': 3.480799674987793, 'learning_rate': 5.8042572472504416e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:03<00:01, 13.25it/s]                                               {'loss': 2.203, 'grad_norm': 3.5970261096954346, 'learning_rate': 5.52786404500042e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:03<00:01, 13.25it/s]                                               {'loss': 2.2494, 'grad_norm': 3.61576247215271, 'learning_rate': 5.2514708427503995e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:03<00:01, 13.25it/s] 77%|███████▋  | 58/75 [00:03<00:01, 16.08it/s]                                               {'loss': 1.9796, 'grad_norm': 3.7798850536346436, 'learning_rate': 4.975077640500378e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:03<00:01, 16.08it/s]                                               {'loss': 2.1683, 'grad_norm': 2.8051810264587402, 'learning_rate': 4.698684438250357e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:03<00:00, 16.08it/s]                                               {'loss': 1.9243, 'grad_norm': 8.436517715454102, 'learning_rate': 4.422291236000337e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:03<00:00, 16.08it/s] 81%|████████▏ | 61/75 [00:03<00:00, 18.25it/s]                                               {'loss': 2.1734, 'grad_norm': 3.330350399017334, 'learning_rate': 4.1458980337503154e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:03<00:00, 18.25it/s]                                               {'loss': 2.3092, 'grad_norm': 4.602315425872803, 'learning_rate': 3.8695048315002947e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:03<00:00, 18.25it/s] 84%|████████▍ | 63/75 [00:03<00:00, 16.82it/s]                                               {'loss': 1.9988, 'grad_norm': 3.9560203552246094, 'learning_rate': 3.593111629250273e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 16.82it/s]                                               {'loss': 2.3947, 'grad_norm': 6.1752166748046875, 'learning_rate': 3.3167184270002526e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 16.82it/s] 87%|████████▋ | 65/75 [00:03<00:00, 15.41it/s]                                               {'loss': 2.033, 'grad_norm': 3.275599479675293, 'learning_rate': 3.0403252247502312e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 15.41it/s]                                               {'loss': 1.9124, 'grad_norm': 2.652080535888672, 'learning_rate': 2.76393202250021e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 15.41it/s]                                               {'loss': 2.0525, 'grad_norm': 3.258197069168091, 'learning_rate': 2.487538820250189e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:04<00:00, 15.41it/s] 91%|█████████ | 68/75 [00:04<00:00, 16.94it/s]                                               {'loss': 1.9837, 'grad_norm': 3.789827585220337, 'learning_rate': 2.2111456180001684e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:04<00:00, 16.94it/s]                                               {'loss': 2.1461, 'grad_norm': 3.515062093734741, 'learning_rate': 1.9347524157501473e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:04<00:00, 16.94it/s] 93%|█████████▎| 70/75 [00:04<00:00, 16.81it/s]                                               {'loss': 2.2753, 'grad_norm': 3.638021469116211, 'learning_rate': 1.6583592135001263e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:04<00:00, 16.81it/s]                                               {'loss': 2.0735, 'grad_norm': 4.277567386627197, 'learning_rate': 1.381966011250105e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:04<00:00, 16.81it/s]                                               {'loss': 1.9815, 'grad_norm': 2.743417739868164, 'learning_rate': 1.1055728090000842e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:04<00:00, 16.81it/s] 97%|█████████▋| 73/75 [00:04<00:00, 19.23it/s]                                               {'loss': 1.8964, 'grad_norm': 3.7272791862487793, 'learning_rate': 8.291796067500631e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:04<00:00, 19.23it/s]                                               {'loss': 2.0813, 'grad_norm': 3.3968634605407715, 'learning_rate': 5.527864045000421e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:04<00:00, 19.23it/s]                                               {'loss': 2.0705, 'grad_norm': 6.90833854675293, 'learning_rate': 2.7639320225002105e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 19.23it/s]                                               {'train_runtime': 4.4987, 'train_samples_per_second': 251.186, 'train_steps_per_second': 16.672, 'train_loss': 2.193865316708883, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 19.23it/s]100%|██████████| 75/75 [00:04<00:00, 16.67it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:44
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.6385, 'grad_norm': 5.853964805603027, 'learning_rate': 0.00020729490168751576, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:05, 13.39it/s]  3%|▎         | 2/75 [00:00<00:04, 15.24it/s]                                              {'loss': 2.5838, 'grad_norm': 4.871135711669922, 'learning_rate': 0.00020453096966501557, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:04, 15.24it/s]                                              {'loss': 2.6177, 'grad_norm': 4.047178268432617, 'learning_rate': 0.00020176703764251535, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 15.24it/s]                                              {'loss': 2.551, 'grad_norm': 3.9861907958984375, 'learning_rate': 0.00019900310562001513, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:04, 15.24it/s]  7%|▋         | 5/75 [00:00<00:03, 19.28it/s]                                              {'loss': 2.4796, 'grad_norm': 3.5855958461761475, 'learning_rate': 0.0001962391735975149, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 19.28it/s]                                              {'loss': 2.2995, 'grad_norm': 5.010005474090576, 'learning_rate': 0.0001934752415750147, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 19.28it/s]                                              {'loss': 2.3306, 'grad_norm': 4.405104160308838, 'learning_rate': 0.00019071130955251452, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 19.28it/s] 11%|█         | 8/75 [00:00<00:03, 20.27it/s]                                              {'loss': 2.546, 'grad_norm': 3.4735682010650635, 'learning_rate': 0.00018794737753001427, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 20.27it/s]                                              {'loss': 2.6311, 'grad_norm': 4.3141865730285645, 'learning_rate': 0.00018518344550751408, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 20.27it/s]                                              {'loss': 2.2721, 'grad_norm': 4.124885559082031, 'learning_rate': 0.00018241951348501388, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 20.27it/s] 15%|█▍        | 11/75 [00:00<00:03, 21.25it/s]                                               {'loss': 2.5412, 'grad_norm': 4.9975409507751465, 'learning_rate': 0.00017965558146251366, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 21.25it/s]                                               {'loss': 2.4216, 'grad_norm': 3.5323128700256348, 'learning_rate': 0.00017689164944001347, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 21.25it/s]                                               {'loss': 2.3436, 'grad_norm': 4.9860405921936035, 'learning_rate': 0.00017412771741751322, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 21.25it/s] 19%|█▊        | 14/75 [00:00<00:02, 22.65it/s]                                               {'loss': 2.4146, 'grad_norm': 4.441071033477783, 'learning_rate': 0.00017136378539501303, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 22.65it/s]                                               {'loss': 2.5265, 'grad_norm': 10.364110946655273, 'learning_rate': 0.00016859985337251284, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 22.65it/s]                                               {'loss': 2.4734, 'grad_norm': 4.0829291343688965, 'learning_rate': 0.00016583592135001261, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 22.65it/s] 23%|██▎       | 17/75 [00:00<00:02, 24.02it/s]                                               {'loss': 2.4118, 'grad_norm': 3.8556923866271973, 'learning_rate': 0.0001630719893275124, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.02it/s]                                               {'loss': 2.2337, 'grad_norm': 3.769702434539795, 'learning_rate': 0.00016030805730501217, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.02it/s]                                               {'loss': 2.2929, 'grad_norm': 3.7532708644866943, 'learning_rate': 0.00015754412528251198, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.02it/s] 27%|██▋       | 20/75 [00:00<00:02, 23.81it/s]                                               {'loss': 2.3312, 'grad_norm': 5.852333068847656, 'learning_rate': 0.00015478019326001179, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 23.81it/s]                                               {'loss': 2.4755, 'grad_norm': 4.042457580566406, 'learning_rate': 0.00015201626123751154, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 23.81it/s]                                               {'loss': 2.4177, 'grad_norm': 4.776461124420166, 'learning_rate': 0.00014925232921501134, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 23.81it/s] 31%|███       | 23/75 [00:01<00:02, 22.96it/s]                                               {'loss': 2.2086, 'grad_norm': 3.791649341583252, 'learning_rate': 0.00014648839719251112, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 22.96it/s]                                               {'loss': 2.3106, 'grad_norm': 4.437660217285156, 'learning_rate': 0.00014372446517001093, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 22.96it/s]                                               {'loss': 2.4691, 'grad_norm': 5.8500776290893555, 'learning_rate': 0.00014096053314751074, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 22.96it/s] 35%|███▍      | 26/75 [00:01<00:02, 20.68it/s]                                               {'loss': 2.0927, 'grad_norm': 3.7170422077178955, 'learning_rate': 0.0001381966011250105, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 20.68it/s]                                               {'loss': 2.2258, 'grad_norm': 3.6774706840515137, 'learning_rate': 0.0001354326691025103, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 20.68it/s]                                               {'loss': 2.3443, 'grad_norm': 4.169781684875488, 'learning_rate': 0.0001326687370800101, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 20.68it/s] 39%|███▊      | 29/75 [00:01<00:02, 19.92it/s]                                               {'loss': 2.3019, 'grad_norm': 4.9122748374938965, 'learning_rate': 0.00012990480505750988, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 19.92it/s]                                               {'loss': 1.8156, 'grad_norm': 7.91093635559082, 'learning_rate': 0.00012714087303500966, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 19.92it/s]                                               {'loss': 2.1864, 'grad_norm': 3.727708101272583, 'learning_rate': 0.00012437694101250944, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:02, 19.92it/s] 43%|████▎     | 32/75 [00:01<00:02, 21.08it/s]                                               {'loss': 2.4715, 'grad_norm': 4.0300493240356445, 'learning_rate': 0.00012161300899000925, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:02, 21.08it/s]                                               {'loss': 2.0454, 'grad_norm': 3.700752019882202, 'learning_rate': 0.00011884907696750904, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 21.08it/s]                                               {'loss': 2.3173, 'grad_norm': 3.202305316925049, 'learning_rate': 0.00011608514494500883, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 21.08it/s] 47%|████▋     | 35/75 [00:01<00:02, 18.17it/s]                                               {'loss': 2.2549, 'grad_norm': 4.87232780456543, 'learning_rate': 0.00011332121292250861, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:02, 18.17it/s]                                               {'loss': 2.2161, 'grad_norm': 3.8831918239593506, 'learning_rate': 0.0001105572809000084, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:02, 18.17it/s]                                               {'loss': 2.6793, 'grad_norm': 6.351160049438477, 'learning_rate': 0.0001077933488775082, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:02, 18.17it/s] 51%|█████     | 38/75 [00:01<00:01, 19.89it/s]                                               {'loss': 2.216, 'grad_norm': 4.826168537139893, 'learning_rate': 0.00010502941685500799, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 19.89it/s]                                               {'loss': 2.3243, 'grad_norm': 4.413513660430908, 'learning_rate': 0.00010226548483250778, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 19.89it/s]                                               {'loss': 2.2972, 'grad_norm': 4.06549072265625, 'learning_rate': 9.950155281000756e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 19.89it/s] 55%|█████▍    | 41/75 [00:01<00:01, 21.21it/s]                                               {'loss': 2.2264, 'grad_norm': 4.0085859298706055, 'learning_rate': 9.673762078750736e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 21.21it/s]                                               {'loss': 2.2987, 'grad_norm': 3.5939109325408936, 'learning_rate': 9.397368876500714e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 21.21it/s]                                               {'loss': 2.2119, 'grad_norm': 4.372387886047363, 'learning_rate': 9.120975674250694e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 21.21it/s] 59%|█████▊    | 44/75 [00:02<00:01, 22.43it/s]                                               {'loss': 2.1011, 'grad_norm': 3.6296818256378174, 'learning_rate': 8.844582472000673e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 22.43it/s]                                               {'loss': 2.7393, 'grad_norm': 14.760196685791016, 'learning_rate': 8.568189269750651e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 22.43it/s]                                               {'loss': 2.1168, 'grad_norm': 4.817161560058594, 'learning_rate': 8.291796067500631e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 22.43it/s]                                               {'loss': 2.0727, 'grad_norm': 3.5966391563415527, 'learning_rate': 8.015402865250609e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 22.43it/s] 64%|██████▍   | 48/75 [00:02<00:01, 24.71it/s]                                               {'loss': 2.2974, 'grad_norm': 3.885617256164551, 'learning_rate': 7.739009663000589e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 24.71it/s]                                               {'loss': 2.2423, 'grad_norm': 3.587203025817871, 'learning_rate': 7.462616460750567e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 24.71it/s]                                               {'loss': 2.166, 'grad_norm': 3.3300116062164307, 'learning_rate': 7.186223258500547e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 24.71it/s] 68%|██████▊   | 51/75 [00:02<00:00, 24.93it/s]                                               {'loss': 2.4284, 'grad_norm': 3.6528382301330566, 'learning_rate': 6.909830056250524e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 24.93it/s]                                               {'loss': 1.939, 'grad_norm': 3.6898486614227295, 'learning_rate': 6.633436854000505e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.93it/s]                                               {'loss': 2.1572, 'grad_norm': 5.824521541595459, 'learning_rate': 6.357043651750483e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.93it/s] 72%|███████▏  | 54/75 [00:02<00:00, 24.85it/s]                                               {'loss': 2.0663, 'grad_norm': 2.7292966842651367, 'learning_rate': 6.0806504495004623e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.85it/s]                                               {'loss': 2.3216, 'grad_norm': 3.7040090560913086, 'learning_rate': 5.8042572472504416e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.85it/s]                                               {'loss': 2.2189, 'grad_norm': 3.5353736877441406, 'learning_rate': 5.52786404500042e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.85it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.73it/s]                                               {'loss': 2.5393, 'grad_norm': 4.940736293792725, 'learning_rate': 5.2514708427503995e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.73it/s]                                               {'loss': 2.376, 'grad_norm': 4.6302385330200195, 'learning_rate': 4.975077640500378e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.73it/s]                                               {'loss': 2.1345, 'grad_norm': 3.7341885566711426, 'learning_rate': 4.698684438250357e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.73it/s]                                               {'loss': 2.2658, 'grad_norm': 7.893640518188477, 'learning_rate': 4.422291236000337e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.73it/s] 81%|████████▏ | 61/75 [00:02<00:00, 26.15it/s]                                               {'loss': 2.1673, 'grad_norm': 3.7755486965179443, 'learning_rate': 4.1458980337503154e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.15it/s]                                               {'loss': 2.1375, 'grad_norm': 5.298735618591309, 'learning_rate': 3.8695048315002947e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.15it/s]                                               {'loss': 1.8378, 'grad_norm': 3.253613233566284, 'learning_rate': 3.593111629250273e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.15it/s] 85%|████████▌ | 64/75 [00:02<00:00, 24.26it/s]                                               {'loss': 2.1482, 'grad_norm': 3.161181926727295, 'learning_rate': 3.3167184270002526e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 24.26it/s]                                               {'loss': 2.2145, 'grad_norm': 3.637465000152588, 'learning_rate': 3.0403252247502312e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.26it/s]                                               {'loss': 2.3348, 'grad_norm': 3.929579257965088, 'learning_rate': 2.76393202250021e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.26it/s] 89%|████████▉ | 67/75 [00:02<00:00, 23.77it/s]                                               {'loss': 2.186, 'grad_norm': 3.6016156673431396, 'learning_rate': 2.487538820250189e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 23.77it/s]                                               {'loss': 2.3082, 'grad_norm': 4.341535568237305, 'learning_rate': 2.2111456180001684e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 23.77it/s]                                               {'loss': 2.2963, 'grad_norm': 3.29753041267395, 'learning_rate': 1.9347524157501473e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 23.77it/s] 93%|█████████▎| 70/75 [00:03<00:00, 23.96it/s]                                               {'loss': 2.3169, 'grad_norm': 3.29868745803833, 'learning_rate': 1.6583592135001263e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 23.96it/s]                                               {'loss': 2.0029, 'grad_norm': 3.7465169429779053, 'learning_rate': 1.381966011250105e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 23.96it/s]                                               {'loss': 2.3301, 'grad_norm': 3.846412420272827, 'learning_rate': 1.1055728090000842e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 23.96it/s] 97%|█████████▋| 73/75 [00:03<00:00, 24.58it/s]                                               {'loss': 2.0731, 'grad_norm': 4.5046210289001465, 'learning_rate': 8.291796067500631e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 24.58it/s]                                               {'loss': 2.3981, 'grad_norm': 4.480280876159668, 'learning_rate': 5.527864045000421e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.58it/s]                                               {'loss': 2.6701, 'grad_norm': 10.189260482788086, 'learning_rate': 2.7639320225002105e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.58it/s]                                               {'train_runtime': 3.468, 'train_samples_per_second': 325.839, 'train_steps_per_second': 21.626, 'train_loss': 2.306028890609741, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.58it/s]100%|██████████| 75/75 [00:03<00:00, 21.63it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1730, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1334, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1341, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1407, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1667, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1348, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1493, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1312, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1330, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1216, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1436, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1132, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 6/471 [00:00<00:08, 56.95it/s]  3%|▎         | 12/471 [00:00<00:10, 43.65it/s]  4%|▎         | 17/471 [00:00<00:10, 44.40it/s]  5%|▍         | 23/471 [00:00<00:09, 47.11it/s]  6%|▌         | 29/471 [00:00<00:09, 48.86it/s]  7%|▋         | 35/471 [00:00<00:08, 49.81it/s]  9%|▊         | 41/471 [00:00<00:09, 47.27it/s] 10%|▉         | 47/471 [00:00<00:08, 48.01it/s] 11%|█▏        | 53/471 [00:01<00:08, 49.77it/s] 13%|█▎        | 59/471 [00:01<00:08, 50.94it/s] 14%|█▍        | 65/471 [00:01<00:08, 48.27it/s] 15%|█▍        | 70/471 [00:01<00:08, 44.97it/s] 16%|█▌        | 76/471 [00:01<00:08, 47.62it/s] 17%|█▋        | 81/471 [00:01<00:08, 45.88it/s] 18%|█▊        | 86/471 [00:01<00:08, 44.60it/s] 19%|█▉        | 91/471 [00:01<00:08, 45.40it/s] 20%|██        | 96/471 [00:02<00:09, 41.64it/s] 21%|██▏       | 101/471 [00:02<00:10, 36.61it/s] 22%|██▏       | 105/471 [00:02<00:10, 34.50it/s] 23%|██▎       | 110/471 [00:02<00:09, 37.97it/s] 24%|██▍       | 115/471 [00:02<00:08, 39.80it/s] 25%|██▌       | 120/471 [00:02<00:08, 42.03it/s] 27%|██▋       | 125/471 [00:02<00:08, 42.57it/s] 28%|██▊       | 130/471 [00:02<00:08, 42.21it/s] 29%|██▊       | 135/471 [00:03<00:08, 38.90it/s] 30%|██▉       | 139/471 [00:03<00:08, 38.02it/s] 30%|███       | 143/471 [00:03<00:08, 37.03it/s] 31%|███▏      | 148/471 [00:03<00:08, 38.05it/s] 32%|███▏      | 152/471 [00:03<00:08, 36.12it/s] 33%|███▎      | 157/471 [00:03<00:08, 38.02it/s] 34%|███▍      | 161/471 [00:03<00:08, 34.82it/s] 35%|███▌      | 166/471 [00:03<00:08, 36.78it/s] 36%|███▋      | 171/471 [00:04<00:07, 39.43it/s] 37%|███▋      | 176/471 [00:04<00:07, 39.95it/s] 39%|███▊      | 182/471 [00:04<00:06, 42.40it/s] 40%|███▉      | 187/471 [00:04<00:06, 43.94it/s] 41%|████      | 192/471 [00:04<00:07, 38.40it/s] 42%|████▏     | 196/471 [00:04<00:07, 35.87it/s] 42%|████▏     | 200/471 [00:04<00:07, 35.91it/s] 43%|████▎     | 204/471 [00:04<00:08, 32.23it/s] 45%|████▍     | 210/471 [00:05<00:07, 36.63it/s] 45%|████▌     | 214/471 [00:05<00:07, 36.24it/s] 46%|████▋     | 219/471 [00:05<00:06, 39.21it/s] 48%|████▊     | 224/471 [00:05<00:06, 38.34it/s] 48%|████▊     | 228/471 [00:05<00:06, 38.36it/s] 49%|████▉     | 232/471 [00:05<00:06, 36.91it/s] 50%|█████     | 236/471 [00:05<00:06, 36.78it/s] 51%|█████     | 241/471 [00:05<00:06, 38.08it/s] 52%|█████▏    | 246/471 [00:06<00:05, 38.99it/s] 53%|█████▎    | 250/471 [00:06<00:05, 38.73it/s] 54%|█████▍    | 255/471 [00:06<00:05, 39.48it/s] 55%|█████▌    | 261/471 [00:06<00:04, 42.60it/s] 56%|█████▋    | 266/471 [00:06<00:05, 38.03it/s] 57%|█████▋    | 270/471 [00:06<00:05, 35.59it/s] 58%|█████▊    | 274/471 [00:06<00:05, 36.02it/s] 59%|█████▉    | 278/471 [00:06<00:05, 34.44it/s] 60%|█████▉    | 282/471 [00:07<00:05, 35.71it/s] 61%|██████    | 287/471 [00:07<00:04, 36.89it/s] 62%|██████▏   | 292/471 [00:07<00:04, 39.93it/s] 63%|██████▎   | 297/471 [00:07<00:04, 39.00it/s] 64%|██████▍   | 301/471 [00:07<00:04, 38.26it/s] 65%|██████▌   | 307/471 [00:07<00:03, 41.90it/s] 66%|██████▌   | 312/471 [00:07<00:03, 40.26it/s] 67%|██████▋   | 317/471 [00:07<00:04, 35.73it/s] 68%|██████▊   | 321/471 [00:08<00:04, 33.72it/s] 69%|██████▉   | 325/471 [00:08<00:04, 31.50it/s] 70%|██████▉   | 329/471 [00:08<00:04, 31.91it/s] 71%|███████   | 333/471 [00:08<00:04, 31.24it/s] 72%|███████▏  | 337/471 [00:08<00:04, 30.56it/s] 73%|███████▎  | 343/471 [00:08<00:03, 36.31it/s] 74%|███████▍  | 349/471 [00:08<00:02, 40.73it/s] 75%|███████▌  | 354/471 [00:08<00:02, 41.33it/s] 76%|███████▋  | 360/471 [00:09<00:02, 45.10it/s] 77%|███████▋  | 365/471 [00:09<00:02, 41.40it/s] 79%|███████▊  | 370/471 [00:09<00:02, 40.66it/s] 80%|███████▉  | 375/471 [00:09<00:02, 38.72it/s] 80%|████████  | 379/471 [00:09<00:02, 37.74it/s] 82%|████████▏ | 384/471 [00:09<00:02, 39.51it/s] 83%|████████▎ | 390/471 [00:09<00:01, 42.61it/s] 84%|████████▍ | 395/471 [00:09<00:01, 42.27it/s] 85%|████████▍ | 400/471 [00:10<00:01, 42.48it/s] 86%|████████▌ | 406/471 [00:10<00:01, 44.46it/s] 87%|████████▋ | 411/471 [00:10<00:01, 45.17it/s] 88%|████████▊ | 416/471 [00:10<00:01, 45.96it/s] 89%|████████▉ | 421/471 [00:10<00:01, 44.48it/s] 90%|█████████ | 426/471 [00:10<00:01, 41.01it/s] 92%|█████████▏| 431/471 [00:10<00:01, 39.69it/s] 93%|█████████▎| 436/471 [00:10<00:00, 40.79it/s] 94%|█████████▎| 441/471 [00:11<00:00, 42.00it/s] 95%|█████████▍| 446/471 [00:11<00:00, 43.81it/s] 96%|█████████▌| 451/471 [00:11<00:00, 42.20it/s] 97%|█████████▋| 456/471 [00:11<00:00, 42.03it/s] 98%|█████████▊| 461/471 [00:11<00:00, 38.99it/s] 99%|█████████▊| 465/471 [00:11<00:00, 36.00it/s]100%|█████████▉| 469/471 [00:11<00:00, 34.92it/s]100%|██████████| 471/471 [00:11<00:00, 39.82it/s]
{'eval_loss': 2.553142547607422, 'eval_model_preparation_time': 0.0149, 'eval_acc': 0.2956718003186405, 'eval_runtime': 11.8557, 'eval_samples_per_second': 635.308, 'eval_steps_per_second': 39.728}
ROUND:3
CLIENT:32
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.5275, 'grad_norm': 7.070445537567139, 'learning_rate': 0.00019383318964064309, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 16.29it/s]  3%|▎         | 2/75 [00:00<00:04, 15.93it/s]                                              {'loss': 2.6249, 'grad_norm': 4.546407699584961, 'learning_rate': 0.00019124874711210117, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:04, 15.93it/s]                                              {'loss': 2.4441, 'grad_norm': 4.47484827041626, 'learning_rate': 0.00018866430458355928, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 15.93it/s]  5%|▌         | 4/75 [00:00<00:04, 17.08it/s]                                              {'loss': 2.4394, 'grad_norm': 4.437933921813965, 'learning_rate': 0.00018607986205501737, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:04, 17.08it/s]                                              {'loss': 2.4528, 'grad_norm': 3.780930995941162, 'learning_rate': 0.00018349541952647545, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:04, 17.08it/s]                                              {'loss': 2.2791, 'grad_norm': 4.7792181968688965, 'learning_rate': 0.00018091097699793354, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:04, 17.08it/s]  9%|▉         | 7/75 [00:00<00:03, 20.66it/s]                                              {'loss': 2.4045, 'grad_norm': 4.640314102172852, 'learning_rate': 0.00017832653446939165, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 20.66it/s]                                              {'loss': 2.5088, 'grad_norm': 4.386795997619629, 'learning_rate': 0.00017574209194084973, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 20.66it/s]                                              {'loss': 2.3436, 'grad_norm': 3.894200086593628, 'learning_rate': 0.00017315764941230782, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 20.66it/s] 13%|█▎        | 10/75 [00:00<00:02, 22.35it/s]                                               {'loss': 2.315, 'grad_norm': 3.984236240386963, 'learning_rate': 0.0001705732068837659, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 22.35it/s]                                               {'loss': 2.3343, 'grad_norm': 3.8959691524505615, 'learning_rate': 0.00016798876435522402, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 22.35it/s]                                               {'loss': 2.4741, 'grad_norm': 4.019785404205322, 'learning_rate': 0.0001654043218266821, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 22.35it/s] 17%|█▋        | 13/75 [00:00<00:02, 20.77it/s]                                               {'loss': 2.3163, 'grad_norm': 4.682477951049805, 'learning_rate': 0.0001628198792981402, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 20.77it/s]                                               {'loss': 2.2518, 'grad_norm': 4.058688640594482, 'learning_rate': 0.00016023543676959827, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 20.77it/s]                                               {'loss': 1.76, 'grad_norm': 8.523381233215332, 'learning_rate': 0.00015765099424105638, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 20.77it/s] 21%|██▏       | 16/75 [00:00<00:02, 21.71it/s]                                               {'loss': 2.262, 'grad_norm': 3.6179256439208984, 'learning_rate': 0.00015506655171251447, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 21.71it/s]                                               {'loss': 2.247, 'grad_norm': 3.897871494293213, 'learning_rate': 0.00015248210918397255, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 21.71it/s]                                               {'loss': 2.2053, 'grad_norm': 4.0584306716918945, 'learning_rate': 0.00014989766665543064, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 21.71it/s] 25%|██▌       | 19/75 [00:00<00:02, 21.11it/s]                                               {'loss': 2.3329, 'grad_norm': 5.543560981750488, 'learning_rate': 0.00014731322412688875, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 21.11it/s]                                               {'loss': 2.3689, 'grad_norm': 4.207897663116455, 'learning_rate': 0.00014472878159834684, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 21.11it/s]                                               {'loss': 2.1005, 'grad_norm': 3.8731203079223633, 'learning_rate': 0.00014214433906980492, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:02, 21.11it/s] 29%|██▉       | 22/75 [00:01<00:02, 19.92it/s]                                               {'loss': 2.1024, 'grad_norm': 3.9260246753692627, 'learning_rate': 0.000139559896541263, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 19.92it/s]                                               {'loss': 2.2245, 'grad_norm': 4.382892608642578, 'learning_rate': 0.00013697545401272112, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 19.92it/s]                                               {'loss': 2.3849, 'grad_norm': 4.36869478225708, 'learning_rate': 0.0001343910114841792, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 19.92it/s] 33%|███▎      | 25/75 [00:01<00:02, 19.03it/s]                                               {'loss': 2.3441, 'grad_norm': 4.077167987823486, 'learning_rate': 0.00013180656895563731, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 19.03it/s]                                               {'loss': 2.381, 'grad_norm': 4.096471309661865, 'learning_rate': 0.00012922212642709537, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 19.03it/s]                                               {'loss': 2.1956, 'grad_norm': 4.832612037658691, 'learning_rate': 0.00012663768389855348, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 19.03it/s] 37%|███▋      | 28/75 [00:01<00:02, 19.52it/s]                                               {'loss': 2.2307, 'grad_norm': 6.182471752166748, 'learning_rate': 0.00012405324137001157, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 19.52it/s]                                               {'loss': 2.241, 'grad_norm': 3.9734342098236084, 'learning_rate': 0.00012146879884146967, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 19.52it/s] 40%|████      | 30/75 [00:01<00:02, 19.45it/s]                                               {'loss': 1.7677, 'grad_norm': 9.611099243164062, 'learning_rate': 0.00011888435631292775, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 19.45it/s]                                               {'loss': 2.3179, 'grad_norm': 3.6344826221466064, 'learning_rate': 0.00011629991378438585, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:02, 19.45it/s]                                               {'loss': 2.0832, 'grad_norm': 3.5576281547546387, 'learning_rate': 0.00011371547125584395, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:02, 19.45it/s] 44%|████▍     | 33/75 [00:01<00:02, 20.31it/s]                                               {'loss': 2.3973, 'grad_norm': 4.032688140869141, 'learning_rate': 0.00011113102872730204, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:02, 20.31it/s]                                               {'loss': 2.1851, 'grad_norm': 4.180553436279297, 'learning_rate': 0.00010854658619876013, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:02, 20.31it/s]                                               {'loss': 2.0867, 'grad_norm': 3.598137617111206, 'learning_rate': 0.00010596214367021822, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 20.31it/s] 48%|████▊     | 36/75 [00:01<00:01, 21.01it/s]                                               {'loss': 2.1691, 'grad_norm': 4.433925151824951, 'learning_rate': 0.00010337770114167632, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 21.01it/s]                                               {'loss': 2.0258, 'grad_norm': 3.8988847732543945, 'learning_rate': 0.0001007932586131344, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 21.01it/s]                                               {'loss': 2.2946, 'grad_norm': 4.450451850891113, 'learning_rate': 9.82088160845925e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 21.01it/s] 52%|█████▏    | 39/75 [00:01<00:01, 20.17it/s]                                               {'loss': 2.2698, 'grad_norm': 6.147627353668213, 'learning_rate': 9.562437355605059e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 20.17it/s]                                               {'loss': 2.326, 'grad_norm': 3.7706727981567383, 'learning_rate': 9.303993102750868e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 20.17it/s]                                               {'loss': 2.1051, 'grad_norm': 3.672487497329712, 'learning_rate': 9.045548849896677e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:02<00:01, 20.17it/s] 56%|█████▌    | 42/75 [00:02<00:01, 19.41it/s]                                               {'loss': 2.0842, 'grad_norm': 5.1438140869140625, 'learning_rate': 8.787104597042487e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 19.41it/s]                                               {'loss': 2.1726, 'grad_norm': 4.679656982421875, 'learning_rate': 8.528660344188295e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 19.41it/s]                                               {'loss': 2.1681, 'grad_norm': 3.0726702213287354, 'learning_rate': 8.270216091334105e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 19.41it/s] 60%|██████    | 45/75 [00:02<00:01, 20.94it/s]                                               {'loss': 2.1437, 'grad_norm': 11.492032051086426, 'learning_rate': 8.011771838479914e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 20.94it/s]                                               {'loss': 2.0628, 'grad_norm': 3.903662919998169, 'learning_rate': 7.753327585625723e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 20.94it/s]                                               {'loss': 2.1256, 'grad_norm': 3.8323771953582764, 'learning_rate': 7.494883332771532e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 20.94it/s] 64%|██████▍   | 48/75 [00:02<00:01, 21.72it/s]                                               {'loss': 2.1802, 'grad_norm': 3.8674895763397217, 'learning_rate': 7.236439079917342e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 21.72it/s]                                               {'loss': 2.2341, 'grad_norm': 4.004711627960205, 'learning_rate': 6.97799482706315e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 21.72it/s]                                               {'loss': 2.1632, 'grad_norm': 3.918285608291626, 'learning_rate': 6.71955057420896e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 21.72it/s] 68%|██████▊   | 51/75 [00:02<00:01, 21.27it/s]                                               {'loss': 2.0786, 'grad_norm': 4.662990570068359, 'learning_rate': 6.461106321354769e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 21.27it/s]                                               {'loss': 2.0466, 'grad_norm': 4.518353462219238, 'learning_rate': 6.202662068500578e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 21.27it/s]                                               {'loss': 2.0938, 'grad_norm': 3.8860526084899902, 'learning_rate': 5.9442178156463877e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:01, 21.27it/s] 72%|███████▏  | 54/75 [00:02<00:00, 21.34it/s]                                               {'loss': 2.4388, 'grad_norm': 4.504673957824707, 'learning_rate': 5.6857735627921975e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 21.34it/s]                                               {'loss': 2.1463, 'grad_norm': 4.069897174835205, 'learning_rate': 5.427329309938007e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 21.34it/s]                                               {'loss': 2.4258, 'grad_norm': 3.961012840270996, 'learning_rate': 5.168885057083816e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 21.34it/s] 76%|███████▌  | 57/75 [00:02<00:00, 22.04it/s]                                               {'loss': 2.0149, 'grad_norm': 4.99828577041626, 'learning_rate': 4.910440804229625e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 22.04it/s]                                               {'loss': 2.1424, 'grad_norm': 3.8393378257751465, 'learning_rate': 4.651996551375434e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 22.04it/s]                                               {'loss': 2.1756, 'grad_norm': 3.868654251098633, 'learning_rate': 4.3935522985212434e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 22.04it/s] 80%|████████  | 60/75 [00:02<00:00, 20.78it/s]                                               {'loss': 2.3414, 'grad_norm': 15.139444351196289, 'learning_rate': 4.1351080456670525e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 20.78it/s]                                               {'loss': 2.3258, 'grad_norm': 5.103237152099609, 'learning_rate': 3.876663792812862e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 20.78it/s]                                               {'loss': 2.0299, 'grad_norm': 3.823646068572998, 'learning_rate': 3.618219539958671e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:03<00:00, 20.78it/s] 84%|████████▍ | 63/75 [00:03<00:00, 20.08it/s]                                               {'loss': 2.1037, 'grad_norm': 3.7970056533813477, 'learning_rate': 3.35977528710448e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 20.08it/s]                                               {'loss': 2.0489, 'grad_norm': 4.415109157562256, 'learning_rate': 3.101331034250289e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 20.08it/s]                                               {'loss': 2.5012, 'grad_norm': 5.2758097648620605, 'learning_rate': 2.8428867813960988e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 20.08it/s] 88%|████████▊ | 66/75 [00:03<00:00, 20.93it/s]                                               {'loss': 2.1947, 'grad_norm': 4.130645275115967, 'learning_rate': 2.584442528541908e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 20.93it/s]                                               {'loss': 2.2179, 'grad_norm': 3.942105293273926, 'learning_rate': 2.325998275687717e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 20.93it/s]                                               {'loss': 2.0209, 'grad_norm': 3.6856284141540527, 'learning_rate': 2.0675540228335263e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 20.93it/s] 92%|█████████▏| 69/75 [00:03<00:00, 18.80it/s]                                               {'loss': 2.2245, 'grad_norm': 3.711608409881592, 'learning_rate': 1.8091097699793354e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 18.80it/s]                                               {'loss': 2.1311, 'grad_norm': 3.611797332763672, 'learning_rate': 1.5506655171251446e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 18.80it/s] 95%|█████████▍| 71/75 [00:03<00:00, 17.65it/s]                                               {'loss': 1.9669, 'grad_norm': 3.935471296310425, 'learning_rate': 1.292221264270954e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 17.65it/s]                                               {'loss': 2.1808, 'grad_norm': 3.1756913661956787, 'learning_rate': 1.0337770114167631e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 17.65it/s] 97%|█████████▋| 73/75 [00:03<00:00, 17.99it/s]                                               {'loss': 2.0195, 'grad_norm': 3.3160488605499268, 'learning_rate': 7.753327585625723e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 17.99it/s]                                               {'loss': 2.1151, 'grad_norm': 4.782780647277832, 'learning_rate': 5.168885057083816e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 17.99it/s]                                               {'loss': 1.9197, 'grad_norm': 8.691636085510254, 'learning_rate': 2.584442528541908e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 17.99it/s]                                               {'train_runtime': 3.954, 'train_samples_per_second': 285.784, 'train_steps_per_second': 18.968, 'train_loss': 2.2181939713160195, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 17.99it/s]100%|██████████| 75/75 [00:03<00:00, 18.97it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.6264, 'grad_norm': 4.497663974761963, 'learning_rate': 0.00019383318964064309, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 17.26it/s]                                              {'loss': 2.2969, 'grad_norm': 5.807457447052002, 'learning_rate': 0.00019124874711210117, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 20.91it/s]  4%|▍         | 3/75 [00:00<00:03, 21.22it/s]                                              {'loss': 2.4336, 'grad_norm': 4.907882213592529, 'learning_rate': 0.00018866430458355928, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 21.22it/s]                                              {'loss': 2.6421, 'grad_norm': 5.938896656036377, 'learning_rate': 0.00018607986205501737, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 21.22it/s]                                              {'loss': 2.6136, 'grad_norm': 4.21735143661499, 'learning_rate': 0.00018349541952647545, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 21.22it/s]  8%|▊         | 6/75 [00:00<00:03, 21.73it/s]                                              {'loss': 2.3801, 'grad_norm': 3.9974234104156494, 'learning_rate': 0.00018091097699793354, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 21.73it/s]                                              {'loss': 2.5908, 'grad_norm': 5.104437828063965, 'learning_rate': 0.00017832653446939165, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 21.73it/s]                                              {'loss': 2.2023, 'grad_norm': 5.6899847984313965, 'learning_rate': 0.00017574209194084973, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 21.73it/s] 12%|█▏        | 9/75 [00:00<00:02, 22.72it/s]                                              {'loss': 2.1277, 'grad_norm': 4.696434020996094, 'learning_rate': 0.00017315764941230782, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 22.72it/s]                                              {'loss': 2.3228, 'grad_norm': 4.454465866088867, 'learning_rate': 0.0001705732068837659, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 22.72it/s]                                               {'loss': 2.3702, 'grad_norm': 5.390120983123779, 'learning_rate': 0.00016798876435522402, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 22.72it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.72it/s]                                               {'loss': 2.4393, 'grad_norm': 5.409359931945801, 'learning_rate': 0.0001654043218266821, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.72it/s]                                               {'loss': 2.4934, 'grad_norm': 5.557392120361328, 'learning_rate': 0.0001628198792981402, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.72it/s]                                               {'loss': 2.3502, 'grad_norm': 5.261143207550049, 'learning_rate': 0.00016023543676959827, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.72it/s]                                               {'loss': 2.1262, 'grad_norm': 8.02823543548584, 'learning_rate': 0.00015765099424105638, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.72it/s] 21%|██▏       | 16/75 [00:00<00:02, 25.96it/s]                                               {'loss': 2.0709, 'grad_norm': 3.6405270099639893, 'learning_rate': 0.00015506655171251447, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.96it/s]                                               {'loss': 2.2065, 'grad_norm': 5.285851955413818, 'learning_rate': 0.00015248210918397255, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.96it/s]                                               {'loss': 2.5251, 'grad_norm': 4.305933475494385, 'learning_rate': 0.00014989766665543064, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.96it/s] 25%|██▌       | 19/75 [00:00<00:02, 24.09it/s]                                               {'loss': 2.2053, 'grad_norm': 4.828457355499268, 'learning_rate': 0.00014731322412688875, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.09it/s]                                               {'loss': 2.3975, 'grad_norm': 4.810001850128174, 'learning_rate': 0.00014472878159834684, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.09it/s]                                               {'loss': 2.2553, 'grad_norm': 3.9113030433654785, 'learning_rate': 0.00014214433906980492, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.09it/s] 29%|██▉       | 22/75 [00:00<00:02, 24.36it/s]                                               {'loss': 2.2501, 'grad_norm': 4.380904674530029, 'learning_rate': 0.000139559896541263, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.36it/s]                                               {'loss': 2.1362, 'grad_norm': 4.637429237365723, 'learning_rate': 0.00013697545401272112, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.36it/s]                                               {'loss': 2.3043, 'grad_norm': 5.136254787445068, 'learning_rate': 0.0001343910114841792, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 24.36it/s] 33%|███▎      | 25/75 [00:01<00:02, 24.21it/s]                                               {'loss': 2.4917, 'grad_norm': 4.016072750091553, 'learning_rate': 0.00013180656895563731, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.21it/s]                                               {'loss': 2.4142, 'grad_norm': 4.2500224113464355, 'learning_rate': 0.00012922212642709537, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.21it/s]                                               {'loss': 2.2736, 'grad_norm': 6.34328031539917, 'learning_rate': 0.00012663768389855348, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.21it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.20it/s]                                               {'loss': 2.2096, 'grad_norm': 4.584570407867432, 'learning_rate': 0.00012405324137001157, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.20it/s]                                               {'loss': 2.3591, 'grad_norm': 3.8172647953033447, 'learning_rate': 0.00012146879884146967, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.20it/s]                                               {'loss': 1.5971, 'grad_norm': 8.521843910217285, 'learning_rate': 0.00011888435631292775, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.20it/s]                                               {'loss': 2.3871, 'grad_norm': 4.061069011688232, 'learning_rate': 0.00011629991378438585, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.20it/s] 43%|████▎     | 32/75 [00:01<00:01, 25.56it/s]                                               {'loss': 2.335, 'grad_norm': 3.7934749126434326, 'learning_rate': 0.00011371547125584395, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.56it/s]                                               {'loss': 2.1071, 'grad_norm': 3.9164085388183594, 'learning_rate': 0.00011113102872730204, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.56it/s]                                               {'loss': 2.2094, 'grad_norm': 4.140353679656982, 'learning_rate': 0.00010854658619876013, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.56it/s] 47%|████▋     | 35/75 [00:01<00:01, 25.45it/s]                                               {'loss': 2.2208, 'grad_norm': 4.240538120269775, 'learning_rate': 0.00010596214367021822, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.45it/s]                                               {'loss': 2.2309, 'grad_norm': 3.869793176651001, 'learning_rate': 0.00010337770114167632, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.45it/s]                                               {'loss': 2.1936, 'grad_norm': 3.8129005432128906, 'learning_rate': 0.0001007932586131344, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.45it/s] 51%|█████     | 38/75 [00:01<00:01, 25.18it/s]                                               {'loss': 2.3362, 'grad_norm': 3.9522299766540527, 'learning_rate': 9.82088160845925e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.18it/s]                                               {'loss': 2.2569, 'grad_norm': 4.250301361083984, 'learning_rate': 9.562437355605059e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.18it/s]                                               {'loss': 2.2155, 'grad_norm': 4.5406084060668945, 'learning_rate': 9.303993102750868e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.18it/s] 55%|█████▍    | 41/75 [00:01<00:01, 24.80it/s]                                               {'loss': 2.4394, 'grad_norm': 6.435173511505127, 'learning_rate': 9.045548849896677e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.80it/s]                                               {'loss': 2.2075, 'grad_norm': 4.290956497192383, 'learning_rate': 8.787104597042487e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.80it/s]                                               {'loss': 2.0954, 'grad_norm': 4.4774346351623535, 'learning_rate': 8.528660344188295e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.80it/s] 59%|█████▊    | 44/75 [00:01<00:01, 24.28it/s]                                               {'loss': 2.1759, 'grad_norm': 4.406124114990234, 'learning_rate': 8.270216091334105e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.28it/s]                                               {'loss': 1.4532, 'grad_norm': 11.637775421142578, 'learning_rate': 8.011771838479914e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.28it/s]                                               {'loss': 2.3231, 'grad_norm': 3.580383062362671, 'learning_rate': 7.753327585625723e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.28it/s] 63%|██████▎   | 47/75 [00:01<00:01, 25.49it/s]                                               {'loss': 2.3366, 'grad_norm': 3.925333023071289, 'learning_rate': 7.494883332771532e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.49it/s]                                               {'loss': 1.999, 'grad_norm': 3.304325580596924, 'learning_rate': 7.236439079917342e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.49it/s]                                               {'loss': 2.2171, 'grad_norm': 5.07201623916626, 'learning_rate': 6.97799482706315e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 25.49it/s] 67%|██████▋   | 50/75 [00:02<00:01, 21.92it/s]                                               {'loss': 2.0314, 'grad_norm': 5.6301493644714355, 'learning_rate': 6.71955057420896e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 21.92it/s]                                               {'loss': 2.1133, 'grad_norm': 4.044380187988281, 'learning_rate': 6.461106321354769e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 21.92it/s]                                               {'loss': 2.4362, 'grad_norm': 5.189453125, 'learning_rate': 6.202662068500578e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 21.92it/s] 71%|███████   | 53/75 [00:02<00:01, 20.79it/s]                                               {'loss': 2.2867, 'grad_norm': 3.364501953125, 'learning_rate': 5.9442178156463877e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:01, 20.79it/s]                                               {'loss': 2.1544, 'grad_norm': 4.356050491333008, 'learning_rate': 5.6857735627921975e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:01, 20.79it/s]                                               {'loss': 2.2016, 'grad_norm': 3.9249722957611084, 'learning_rate': 5.427329309938007e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 20.79it/s] 75%|███████▍  | 56/75 [00:02<00:00, 19.63it/s]                                               {'loss': 2.3726, 'grad_norm': 5.572439670562744, 'learning_rate': 5.168885057083816e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 19.63it/s]                                               {'loss': 2.0839, 'grad_norm': 3.6099538803100586, 'learning_rate': 4.910440804229625e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 19.63it/s]                                               {'loss': 2.357, 'grad_norm': 3.8886313438415527, 'learning_rate': 4.651996551375434e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 19.63it/s] 79%|███████▊  | 59/75 [00:02<00:00, 17.60it/s]                                               {'loss': 2.1076, 'grad_norm': 4.241919040679932, 'learning_rate': 4.3935522985212434e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 17.60it/s]                                               {'loss': 2.0686, 'grad_norm': 10.2328519821167, 'learning_rate': 4.1351080456670525e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 17.60it/s] 81%|████████▏ | 61/75 [00:02<00:00, 16.78it/s]                                               {'loss': 2.1456, 'grad_norm': 4.31271505355835, 'learning_rate': 3.876663792812862e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 16.78it/s]                                               {'loss': 2.278, 'grad_norm': 4.149609565734863, 'learning_rate': 3.618219539958671e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 16.78it/s] 84%|████████▍ | 63/75 [00:02<00:00, 16.61it/s]                                               {'loss': 2.3225, 'grad_norm': 4.082272529602051, 'learning_rate': 3.35977528710448e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 16.61it/s]                                               {'loss': 2.2763, 'grad_norm': 6.3715620040893555, 'learning_rate': 3.101331034250289e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 16.61it/s]                                               {'loss': 2.1936, 'grad_norm': 3.4003162384033203, 'learning_rate': 2.8428867813960988e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 16.61it/s] 88%|████████▊ | 66/75 [00:03<00:00, 18.52it/s]                                               {'loss': 2.2136, 'grad_norm': 4.4795966148376465, 'learning_rate': 2.584442528541908e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 18.52it/s]                                               {'loss': 2.2178, 'grad_norm': 3.9684438705444336, 'learning_rate': 2.325998275687717e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 18.52it/s]                                               {'loss': 2.3323, 'grad_norm': 5.2004523277282715, 'learning_rate': 2.0675540228335263e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 18.52it/s] 92%|█████████▏| 69/75 [00:03<00:00, 18.53it/s]                                               {'loss': 2.1162, 'grad_norm': 3.9108622074127197, 'learning_rate': 1.8091097699793354e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 18.53it/s]                                               {'loss': 2.1468, 'grad_norm': 4.02687931060791, 'learning_rate': 1.5506655171251446e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 18.53it/s] 95%|█████████▍| 71/75 [00:03<00:00, 18.44it/s]                                               {'loss': 2.0954, 'grad_norm': 3.5694098472595215, 'learning_rate': 1.292221264270954e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 18.44it/s]                                               {'loss': 1.9276, 'grad_norm': 3.3687744140625, 'learning_rate': 1.0337770114167631e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 18.44it/s]                                               {'loss': 2.3076, 'grad_norm': 4.276375770568848, 'learning_rate': 7.753327585625723e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 18.44it/s] 99%|█████████▊| 74/75 [00:03<00:00, 19.64it/s]                                               {'loss': 2.1177, 'grad_norm': 3.833799123764038, 'learning_rate': 5.168885057083816e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 19.64it/s]                                               {'loss': 1.6446, 'grad_norm': 9.258281707763672, 'learning_rate': 2.584442528541908e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 19.64it/s]                                               {'train_runtime': 3.5748, 'train_samples_per_second': 316.099, 'train_steps_per_second': 20.98, 'train_loss': 2.240013640721639, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 19.64it/s]100%|██████████| 75/75 [00:03<00:00, 20.98it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:5
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.6694, 'grad_norm': 10.197918891906738, 'learning_rate': 0.00019383318964064309, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.58it/s]                                              {'loss': 2.4767, 'grad_norm': 5.670928478240967, 'learning_rate': 0.00019124874711210117, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 24.32it/s]  4%|▍         | 3/75 [00:00<00:03, 23.04it/s]                                              {'loss': 2.5085, 'grad_norm': 4.503518581390381, 'learning_rate': 0.00018866430458355928, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.04it/s]                                              {'loss': 2.6234, 'grad_norm': 4.143093109130859, 'learning_rate': 0.00018607986205501737, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 23.04it/s]                                              {'loss': 2.1679, 'grad_norm': 4.171919822692871, 'learning_rate': 0.00018349541952647545, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 23.04it/s]  8%|▊         | 6/75 [00:00<00:03, 22.35it/s]                                              {'loss': 2.3898, 'grad_norm': 5.395651340484619, 'learning_rate': 0.00018091097699793354, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 22.35it/s]                                              {'loss': 2.2973, 'grad_norm': 6.0679521560668945, 'learning_rate': 0.00017832653446939165, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 22.35it/s]                                              {'loss': 2.3013, 'grad_norm': 3.783947229385376, 'learning_rate': 0.00017574209194084973, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 22.35it/s] 12%|█▏        | 9/75 [00:00<00:03, 20.64it/s]                                              {'loss': 2.2385, 'grad_norm': 4.21459436416626, 'learning_rate': 0.00017315764941230782, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 20.64it/s]                                              {'loss': 2.373, 'grad_norm': 3.861299514770508, 'learning_rate': 0.0001705732068837659, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 20.64it/s]                                               {'loss': 2.1683, 'grad_norm': 5.904845237731934, 'learning_rate': 0.00016798876435522402, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 20.64it/s] 16%|█▌        | 12/75 [00:00<00:02, 21.40it/s]                                               {'loss': 2.231, 'grad_norm': 5.041346073150635, 'learning_rate': 0.0001654043218266821, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 21.40it/s]                                               {'loss': 2.3496, 'grad_norm': 4.469065189361572, 'learning_rate': 0.0001628198792981402, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 21.40it/s]                                               {'loss': 2.3712, 'grad_norm': 5.64110803604126, 'learning_rate': 0.00016023543676959827, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 21.40it/s] 20%|██        | 15/75 [00:00<00:02, 20.19it/s]                                               {'loss': 3.1155, 'grad_norm': 14.161787986755371, 'learning_rate': 0.00015765099424105638, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 20.19it/s]                                               {'loss': 2.3145, 'grad_norm': 4.255274772644043, 'learning_rate': 0.00015506655171251447, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 20.19it/s]                                               {'loss': 2.243, 'grad_norm': 3.8625924587249756, 'learning_rate': 0.00015248210918397255, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 20.19it/s] 24%|██▍       | 18/75 [00:00<00:02, 19.37it/s]                                               {'loss': 2.233, 'grad_norm': 3.1638309955596924, 'learning_rate': 0.00014989766665543064, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 19.37it/s]                                               {'loss': 2.2221, 'grad_norm': 4.413321495056152, 'learning_rate': 0.00014731322412688875, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 19.37it/s]                                               {'loss': 2.2528, 'grad_norm': 4.100887775421143, 'learning_rate': 0.00014472878159834684, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 19.37it/s] 28%|██▊       | 21/75 [00:01<00:02, 20.24it/s]                                               {'loss': 2.2331, 'grad_norm': 5.2388691902160645, 'learning_rate': 0.00014214433906980492, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:02, 20.24it/s]                                               {'loss': 2.2542, 'grad_norm': 4.546246528625488, 'learning_rate': 0.000139559896541263, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 20.24it/s]                                               {'loss': 2.2427, 'grad_norm': 4.108851432800293, 'learning_rate': 0.00013697545401272112, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 20.24it/s] 32%|███▏      | 24/75 [00:01<00:02, 17.83it/s]                                               {'loss': 2.2004, 'grad_norm': 3.7842342853546143, 'learning_rate': 0.0001343910114841792, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 17.83it/s]                                               {'loss': 2.2708, 'grad_norm': 5.234349250793457, 'learning_rate': 0.00013180656895563731, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 17.83it/s] 35%|███▍      | 26/75 [00:01<00:02, 17.40it/s]                                               {'loss': 2.2149, 'grad_norm': 4.729963302612305, 'learning_rate': 0.00012922212642709537, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 17.40it/s]                                               {'loss': 2.2188, 'grad_norm': 3.74668288230896, 'learning_rate': 0.00012663768389855348, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 17.40it/s] 37%|███▋      | 28/75 [00:01<00:02, 16.58it/s]                                               {'loss': 2.1454, 'grad_norm': 3.731022596359253, 'learning_rate': 0.00012405324137001157, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 16.58it/s]                                               {'loss': 2.0172, 'grad_norm': 3.577399730682373, 'learning_rate': 0.00012146879884146967, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 16.58it/s] 40%|████      | 30/75 [00:01<00:02, 17.19it/s]                                               {'loss': 2.6712, 'grad_norm': 14.757079124450684, 'learning_rate': 0.00011888435631292775, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 17.19it/s]                                               {'loss': 2.0018, 'grad_norm': 4.616191387176514, 'learning_rate': 0.00011629991378438585, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:02, 17.19it/s] 43%|████▎     | 32/75 [00:01<00:02, 16.77it/s]                                               {'loss': 2.2827, 'grad_norm': 3.628596544265747, 'learning_rate': 0.00011371547125584395, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:02, 16.77it/s]                                               {'loss': 2.1678, 'grad_norm': 3.821082830429077, 'learning_rate': 0.00011113102872730204, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:02, 16.77it/s] 45%|████▌     | 34/75 [00:01<00:02, 16.95it/s]                                               {'loss': 2.2973, 'grad_norm': 3.8135409355163574, 'learning_rate': 0.00010854658619876013, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:02, 16.95it/s]                                               {'loss': 1.9264, 'grad_norm': 4.689126014709473, 'learning_rate': 0.00010596214367021822, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:02, 16.95it/s] 48%|████▊     | 36/75 [00:01<00:02, 17.26it/s]                                               {'loss': 2.3749, 'grad_norm': 4.885115146636963, 'learning_rate': 0.00010337770114167632, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:02, 17.26it/s]                                               {'loss': 2.0492, 'grad_norm': 3.2887730598449707, 'learning_rate': 0.0001007932586131344, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:02<00:02, 17.26it/s] 51%|█████     | 38/75 [00:02<00:02, 17.46it/s]                                               {'loss': 2.1649, 'grad_norm': 5.302145004272461, 'learning_rate': 9.82088160845925e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:02<00:02, 17.46it/s]                                               {'loss': 2.2778, 'grad_norm': 3.6688952445983887, 'learning_rate': 9.562437355605059e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:02<00:02, 17.46it/s] 53%|█████▎    | 40/75 [00:02<00:02, 17.45it/s]                                               {'loss': 2.1271, 'grad_norm': 3.6482584476470947, 'learning_rate': 9.303993102750868e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:02<00:02, 17.45it/s]                                               {'loss': 2.2655, 'grad_norm': 5.561398029327393, 'learning_rate': 9.045548849896677e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:02<00:01, 17.45it/s] 56%|█████▌    | 42/75 [00:02<00:01, 17.34it/s]                                               {'loss': 2.2878, 'grad_norm': 4.472371578216553, 'learning_rate': 8.787104597042487e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 17.34it/s]                                               {'loss': 2.2487, 'grad_norm': 3.7106642723083496, 'learning_rate': 8.528660344188295e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 17.34it/s] 59%|█████▊    | 44/75 [00:02<00:01, 16.98it/s]                                               {'loss': 2.0175, 'grad_norm': 3.9470701217651367, 'learning_rate': 8.270216091334105e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 16.98it/s]                                               {'loss': 2.3895, 'grad_norm': 9.299642562866211, 'learning_rate': 8.011771838479914e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 16.98it/s] 61%|██████▏   | 46/75 [00:02<00:01, 16.30it/s]                                               {'loss': 1.9864, 'grad_norm': 3.6126718521118164, 'learning_rate': 7.753327585625723e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 16.30it/s]                                               {'loss': 2.3017, 'grad_norm': 3.5165963172912598, 'learning_rate': 7.494883332771532e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 16.30it/s] 64%|██████▍   | 48/75 [00:02<00:01, 16.01it/s]                                               {'loss': 2.1563, 'grad_norm': 2.974560022354126, 'learning_rate': 7.236439079917342e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 16.01it/s]                                               {'loss': 2.0938, 'grad_norm': 3.193366050720215, 'learning_rate': 6.97799482706315e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 16.01it/s] 67%|██████▋   | 50/75 [00:02<00:01, 16.45it/s]                                               {'loss': 2.1085, 'grad_norm': 3.7192513942718506, 'learning_rate': 6.71955057420896e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 16.45it/s]                                               {'loss': 2.2916, 'grad_norm': 4.781747341156006, 'learning_rate': 6.461106321354769e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 16.45it/s] 69%|██████▉   | 52/75 [00:02<00:01, 16.31it/s]                                               {'loss': 2.0492, 'grad_norm': 2.835869789123535, 'learning_rate': 6.202662068500578e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 16.31it/s]                                               {'loss': 2.5589, 'grad_norm': 4.376375198364258, 'learning_rate': 5.9442178156463877e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:03<00:01, 16.31it/s] 72%|███████▏  | 54/75 [00:03<00:01, 14.63it/s]                                               {'loss': 2.1272, 'grad_norm': 3.3380725383758545, 'learning_rate': 5.6857735627921975e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:03<00:01, 14.63it/s]                                               {'loss': 2.205, 'grad_norm': 4.629867076873779, 'learning_rate': 5.427329309938007e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:03<00:01, 14.63it/s] 75%|███████▍  | 56/75 [00:03<00:01, 13.84it/s]                                               {'loss': 2.0593, 'grad_norm': 3.2946975231170654, 'learning_rate': 5.168885057083816e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:03<00:01, 13.84it/s]                                               {'loss': 2.1007, 'grad_norm': 3.6933789253234863, 'learning_rate': 4.910440804229625e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:03<00:01, 13.84it/s] 77%|███████▋  | 58/75 [00:03<00:01, 13.36it/s]                                               {'loss': 1.9003, 'grad_norm': 5.529930591583252, 'learning_rate': 4.651996551375434e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:03<00:01, 13.36it/s]                                               {'loss': 2.2902, 'grad_norm': 3.087923765182495, 'learning_rate': 4.3935522985212434e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:03<00:01, 13.36it/s] 80%|████████  | 60/75 [00:03<00:01, 13.91it/s]                                               {'loss': 1.8667, 'grad_norm': 10.067996978759766, 'learning_rate': 4.1351080456670525e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:03<00:01, 13.91it/s]                                               {'loss': 2.1281, 'grad_norm': 3.3531384468078613, 'learning_rate': 3.876663792812862e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:03<00:01, 13.91it/s] 83%|████████▎ | 62/75 [00:03<00:00, 14.68it/s]                                               {'loss': 2.2951, 'grad_norm': 3.9553115367889404, 'learning_rate': 3.618219539958671e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:03<00:00, 14.68it/s]                                               {'loss': 2.2178, 'grad_norm': 4.1778693199157715, 'learning_rate': 3.35977528710448e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 14.68it/s]                                               {'loss': 2.3534, 'grad_norm': 3.155928134918213, 'learning_rate': 3.101331034250289e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 14.68it/s] 87%|████████▋ | 65/75 [00:03<00:00, 17.38it/s]                                               {'loss': 2.0432, 'grad_norm': 4.001861095428467, 'learning_rate': 2.8428867813960988e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 17.38it/s]                                               {'loss': 2.0644, 'grad_norm': 3.777676820755005, 'learning_rate': 2.584442528541908e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 17.38it/s]                                               {'loss': 2.0001, 'grad_norm': 4.568610191345215, 'learning_rate': 2.325998275687717e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 17.38it/s] 91%|█████████ | 68/75 [00:03<00:00, 19.55it/s]                                               {'loss': 2.1577, 'grad_norm': 5.604966163635254, 'learning_rate': 2.0675540228335263e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 19.55it/s]                                               {'loss': 2.1069, 'grad_norm': 3.220176935195923, 'learning_rate': 1.8091097699793354e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 19.55it/s]                                               {'loss': 2.0497, 'grad_norm': 3.3276004791259766, 'learning_rate': 1.5506655171251446e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 19.55it/s] 95%|█████████▍| 71/75 [00:04<00:00, 20.78it/s]                                               {'loss': 2.0459, 'grad_norm': 3.976546049118042, 'learning_rate': 1.292221264270954e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:04<00:00, 20.78it/s]                                               {'loss': 2.3276, 'grad_norm': 3.6917173862457275, 'learning_rate': 1.0337770114167631e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:04<00:00, 20.78it/s]                                               {'loss': 2.0404, 'grad_norm': 3.1411943435668945, 'learning_rate': 7.753327585625723e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:04<00:00, 20.78it/s] 99%|█████████▊| 74/75 [00:04<00:00, 20.12it/s]                                               {'loss': 2.1804, 'grad_norm': 4.008387565612793, 'learning_rate': 5.168885057083816e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:04<00:00, 20.12it/s]                                               {'loss': 2.5238, 'grad_norm': 9.024521827697754, 'learning_rate': 2.584442528541908e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 20.12it/s]                                               {'train_runtime': 4.3782, 'train_samples_per_second': 258.094, 'train_steps_per_second': 17.13, 'train_loss': 2.2336663023630776, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 20.12it/s]100%|██████████| 75/75 [00:04<00:00, 17.12it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.5902, 'grad_norm': 5.01265811920166, 'learning_rate': 0.00019383318964064309, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.51it/s]                                              {'loss': 2.4938, 'grad_norm': 6.569277286529541, 'learning_rate': 0.00019124874711210117, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 25.15it/s]  4%|▍         | 3/75 [00:00<00:02, 25.48it/s]                                              {'loss': 2.4814, 'grad_norm': 4.648618221282959, 'learning_rate': 0.00018866430458355928, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.48it/s]                                              {'loss': 2.3634, 'grad_norm': 4.6775736808776855, 'learning_rate': 0.00018607986205501737, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.48it/s]                                              {'loss': 2.4914, 'grad_norm': 5.322579860687256, 'learning_rate': 0.00018349541952647545, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.48it/s]  8%|▊         | 6/75 [00:00<00:02, 23.32it/s]                                              {'loss': 2.2974, 'grad_norm': 5.337192535400391, 'learning_rate': 0.00018091097699793354, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.32it/s]                                              {'loss': 2.3115, 'grad_norm': 4.455667018890381, 'learning_rate': 0.00017832653446939165, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.32it/s]                                              {'loss': 2.3382, 'grad_norm': 3.812004566192627, 'learning_rate': 0.00017574209194084973, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.32it/s] 12%|█▏        | 9/75 [00:00<00:02, 22.49it/s]                                              {'loss': 2.1554, 'grad_norm': 4.3354949951171875, 'learning_rate': 0.00017315764941230782, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 22.49it/s]                                              {'loss': 2.1843, 'grad_norm': 4.59375, 'learning_rate': 0.0001705732068837659, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 22.49it/s]                                               {'loss': 2.4172, 'grad_norm': 3.970449686050415, 'learning_rate': 0.00016798876435522402, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 22.49it/s] 16%|█▌        | 12/75 [00:00<00:02, 22.66it/s]                                               {'loss': 2.4175, 'grad_norm': 4.072237014770508, 'learning_rate': 0.0001654043218266821, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 22.66it/s]                                               {'loss': 2.509, 'grad_norm': 7.104918479919434, 'learning_rate': 0.0001628198792981402, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 22.66it/s]                                               {'loss': 2.4212, 'grad_norm': 5.948530197143555, 'learning_rate': 0.00016023543676959827, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 22.66it/s] 20%|██        | 15/75 [00:00<00:02, 21.68it/s]                                               {'loss': 2.8748, 'grad_norm': 22.756994247436523, 'learning_rate': 0.00015765099424105638, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 21.68it/s]                                               {'loss': 2.1443, 'grad_norm': 5.003809928894043, 'learning_rate': 0.00015506655171251447, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 21.68it/s]                                               {'loss': 2.4851, 'grad_norm': 5.0779571533203125, 'learning_rate': 0.00015248210918397255, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 21.68it/s] 24%|██▍       | 18/75 [00:00<00:02, 20.52it/s]                                               {'loss': 2.1899, 'grad_norm': 3.969956874847412, 'learning_rate': 0.00014989766665543064, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 20.52it/s]                                               {'loss': 2.0463, 'grad_norm': 3.981764316558838, 'learning_rate': 0.00014731322412688875, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 20.52it/s]                                               {'loss': 2.2437, 'grad_norm': 3.2708323001861572, 'learning_rate': 0.00014472878159834684, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 20.52it/s] 28%|██▊       | 21/75 [00:00<00:02, 19.93it/s]                                               {'loss': 2.4318, 'grad_norm': 5.916080474853516, 'learning_rate': 0.00014214433906980492, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 19.93it/s]                                               {'loss': 2.3429, 'grad_norm': 4.62503719329834, 'learning_rate': 0.000139559896541263, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 19.93it/s]                                               {'loss': 2.2892, 'grad_norm': 4.435944080352783, 'learning_rate': 0.00013697545401272112, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 19.93it/s] 32%|███▏      | 24/75 [00:01<00:02, 19.33it/s]                                               {'loss': 2.1682, 'grad_norm': 4.3259429931640625, 'learning_rate': 0.0001343910114841792, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 19.33it/s]                                               {'loss': 2.3474, 'grad_norm': 3.9969241619110107, 'learning_rate': 0.00013180656895563731, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 19.33it/s]                                               {'loss': 2.3449, 'grad_norm': 3.5593390464782715, 'learning_rate': 0.00012922212642709537, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 19.33it/s] 36%|███▌      | 27/75 [00:01<00:02, 19.67it/s]                                               {'loss': 2.1562, 'grad_norm': 3.3793206214904785, 'learning_rate': 0.00012663768389855348, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 19.67it/s]                                               {'loss': 2.0216, 'grad_norm': 4.40893030166626, 'learning_rate': 0.00012405324137001157, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 19.67it/s] 39%|███▊      | 29/75 [00:01<00:02, 18.59it/s]                                               {'loss': 2.181, 'grad_norm': 4.079698085784912, 'learning_rate': 0.00012146879884146967, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 18.59it/s]                                               {'loss': 2.4507, 'grad_norm': 12.035571098327637, 'learning_rate': 0.00011888435631292775, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 18.59it/s]                                               {'loss': 2.1953, 'grad_norm': 3.537242889404297, 'learning_rate': 0.00011629991378438585, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:02, 18.59it/s] 43%|████▎     | 32/75 [00:01<00:02, 19.08it/s]                                               {'loss': 2.3146, 'grad_norm': 4.1077728271484375, 'learning_rate': 0.00011371547125584395, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:02, 19.08it/s]                                               {'loss': 2.199, 'grad_norm': 3.693743944168091, 'learning_rate': 0.00011113102872730204, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:02, 19.08it/s] 45%|████▌     | 34/75 [00:01<00:02, 19.04it/s]                                               {'loss': 2.0425, 'grad_norm': 3.785433053970337, 'learning_rate': 0.00010854658619876013, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:02, 19.04it/s]                                               {'loss': 2.183, 'grad_norm': 4.732474327087402, 'learning_rate': 0.00010596214367021822, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:02, 19.04it/s]                                               {'loss': 2.0785, 'grad_norm': 3.164480447769165, 'learning_rate': 0.00010337770114167632, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:02, 19.04it/s] 49%|████▉     | 37/75 [00:01<00:01, 20.77it/s]                                               {'loss': 2.0808, 'grad_norm': 5.596251010894775, 'learning_rate': 0.0001007932586131344, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 20.77it/s]                                               {'loss': 2.0415, 'grad_norm': 3.488386869430542, 'learning_rate': 9.82088160845925e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 20.77it/s]                                               {'loss': 2.2305, 'grad_norm': 4.760412693023682, 'learning_rate': 9.562437355605059e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 20.77it/s] 53%|█████▎    | 40/75 [00:01<00:01, 21.62it/s]                                               {'loss': 2.3294, 'grad_norm': 4.51318359375, 'learning_rate': 9.303993102750868e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 21.62it/s]                                               {'loss': 2.3346, 'grad_norm': 4.8152756690979, 'learning_rate': 9.045548849896677e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 21.62it/s]                                               {'loss': 2.2487, 'grad_norm': 3.740347146987915, 'learning_rate': 8.787104597042487e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 21.62it/s] 57%|█████▋    | 43/75 [00:02<00:01, 19.72it/s]                                               {'loss': 2.3076, 'grad_norm': 3.7748217582702637, 'learning_rate': 8.528660344188295e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 19.72it/s]                                               {'loss': 2.1141, 'grad_norm': 4.507851600646973, 'learning_rate': 8.270216091334105e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 19.72it/s]                                               {'loss': 2.6876, 'grad_norm': 12.460184097290039, 'learning_rate': 8.011771838479914e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 19.72it/s] 61%|██████▏   | 46/75 [00:02<00:01, 19.35it/s]                                               {'loss': 2.0071, 'grad_norm': 3.1869709491729736, 'learning_rate': 7.753327585625723e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 19.35it/s]                                               {'loss': 2.2387, 'grad_norm': 4.094280242919922, 'learning_rate': 7.494883332771532e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 19.35it/s]                                               {'loss': 2.0041, 'grad_norm': 3.481828212738037, 'learning_rate': 7.236439079917342e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 19.35it/s] 65%|██████▌   | 49/75 [00:02<00:01, 20.14it/s]                                               {'loss': 2.1765, 'grad_norm': 3.5520801544189453, 'learning_rate': 6.97799482706315e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 20.14it/s]                                               {'loss': 2.1811, 'grad_norm': 3.8573203086853027, 'learning_rate': 6.71955057420896e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 20.14it/s]                                               {'loss': 2.1475, 'grad_norm': 3.5904922485351562, 'learning_rate': 6.461106321354769e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 20.14it/s] 69%|██████▉   | 52/75 [00:02<00:01, 17.80it/s]                                               {'loss': 2.0628, 'grad_norm': 3.218292474746704, 'learning_rate': 6.202662068500578e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 17.80it/s]                                               {'loss': 2.2276, 'grad_norm': 4.476682186126709, 'learning_rate': 5.9442178156463877e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:01, 17.80it/s] 72%|███████▏  | 54/75 [00:02<00:01, 16.78it/s]                                               {'loss': 2.3127, 'grad_norm': 5.954934120178223, 'learning_rate': 5.6857735627921975e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:01, 16.78it/s]                                               {'loss': 2.2018, 'grad_norm': 4.056460380554199, 'learning_rate': 5.427329309938007e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:01, 16.78it/s] 75%|███████▍  | 56/75 [00:02<00:01, 17.41it/s]                                               {'loss': 2.2438, 'grad_norm': 3.7522826194763184, 'learning_rate': 5.168885057083816e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:01, 17.41it/s]                                               {'loss': 2.1146, 'grad_norm': 4.682547092437744, 'learning_rate': 4.910440804229625e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:01, 17.41it/s] 77%|███████▋  | 58/75 [00:02<00:00, 17.92it/s]                                               {'loss': 2.1978, 'grad_norm': 5.167601585388184, 'learning_rate': 4.651996551375434e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 17.92it/s]                                               {'loss': 2.0725, 'grad_norm': 4.790196895599365, 'learning_rate': 4.3935522985212434e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:03<00:00, 17.92it/s]                                               {'loss': 2.2419, 'grad_norm': 11.438667297363281, 'learning_rate': 4.1351080456670525e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:03<00:00, 17.92it/s] 81%|████████▏ | 61/75 [00:03<00:00, 19.40it/s]                                               {'loss': 2.2582, 'grad_norm': 3.938394546508789, 'learning_rate': 3.876663792812862e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:03<00:00, 19.40it/s]                                               {'loss': 2.2357, 'grad_norm': 3.10721755027771, 'learning_rate': 3.618219539958671e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:03<00:00, 19.40it/s] 84%|████████▍ | 63/75 [00:03<00:00, 19.20it/s]                                               {'loss': 2.288, 'grad_norm': 3.724992036819458, 'learning_rate': 3.35977528710448e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 19.20it/s]                                               {'loss': 2.0526, 'grad_norm': 4.0965895652771, 'learning_rate': 3.101331034250289e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 19.20it/s]                                               {'loss': 2.1311, 'grad_norm': 4.194405555725098, 'learning_rate': 2.8428867813960988e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 19.20it/s] 88%|████████▊ | 66/75 [00:03<00:00, 19.28it/s]                                               {'loss': 2.0041, 'grad_norm': 3.0567595958709717, 'learning_rate': 2.584442528541908e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 19.28it/s]                                               {'loss': 2.1999, 'grad_norm': 4.110681056976318, 'learning_rate': 2.325998275687717e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 19.28it/s] 91%|█████████ | 68/75 [00:03<00:00, 17.73it/s]                                               {'loss': 2.0866, 'grad_norm': 3.9021525382995605, 'learning_rate': 2.0675540228335263e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 17.73it/s]                                               {'loss': 2.2072, 'grad_norm': 4.94057035446167, 'learning_rate': 1.8091097699793354e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 17.73it/s] 93%|█████████▎| 70/75 [00:03<00:00, 17.10it/s]                                               {'loss': 2.1609, 'grad_norm': 3.571272134780884, 'learning_rate': 1.5506655171251446e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 17.10it/s]                                               {'loss': 2.2422, 'grad_norm': 4.115259170532227, 'learning_rate': 1.292221264270954e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 17.10it/s] 96%|█████████▌| 72/75 [00:03<00:00, 16.66it/s]                                               {'loss': 2.0095, 'grad_norm': 4.171494960784912, 'learning_rate': 1.0337770114167631e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 16.66it/s]                                               {'loss': 2.0583, 'grad_norm': 3.425018787384033, 'learning_rate': 7.753327585625723e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 16.66it/s]                                               {'loss': 2.1489, 'grad_norm': 3.554070234298706, 'learning_rate': 5.168885057083816e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 16.66it/s]100%|██████████| 75/75 [00:03<00:00, 19.06it/s]                                               {'loss': 2.0734, 'grad_norm': 9.116778373718262, 'learning_rate': 2.584442528541908e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 19.06it/s]                                               {'train_runtime': 4.1809, 'train_samples_per_second': 270.279, 'train_steps_per_second': 17.939, 'train_loss': 2.2421969985961914, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 19.06it/s]100%|██████████| 75/75 [00:04<00:00, 17.93it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:2
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]  1%|▏         | 1/75 [00:00<00:08,  9.19it/s]                                              {'loss': 2.4503, 'grad_norm': 4.871280670166016, 'learning_rate': 0.00019383318964064309, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:08,  9.19it/s]                                              {'loss': 2.538, 'grad_norm': 4.966102123260498, 'learning_rate': 0.00019124874711210117, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:07,  9.19it/s]  4%|▍         | 3/75 [00:00<00:06, 11.95it/s]                                              {'loss': 2.3012, 'grad_norm': 6.5193257331848145, 'learning_rate': 0.00018866430458355928, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:06, 11.95it/s]                                              {'loss': 2.4343, 'grad_norm': 4.564032077789307, 'learning_rate': 0.00018607986205501737, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:05, 11.95it/s]  7%|▋         | 5/75 [00:00<00:05, 13.29it/s]                                              {'loss': 2.343, 'grad_norm': 4.875445365905762, 'learning_rate': 0.00018349541952647545, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:05, 13.29it/s]                                              {'loss': 2.4719, 'grad_norm': 5.276886940002441, 'learning_rate': 0.00018091097699793354, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:05, 13.29it/s]  9%|▉         | 7/75 [00:00<00:04, 14.66it/s]                                              {'loss': 2.5203, 'grad_norm': 4.179258346557617, 'learning_rate': 0.00017832653446939165, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:04, 14.66it/s]                                              {'loss': 2.1683, 'grad_norm': 4.384972095489502, 'learning_rate': 0.00017574209194084973, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:04, 14.66it/s] 12%|█▏        | 9/75 [00:00<00:04, 15.93it/s]                                              {'loss': 2.4656, 'grad_norm': 4.046039581298828, 'learning_rate': 0.00017315764941230782, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:04, 15.93it/s]                                              {'loss': 2.3853, 'grad_norm': 4.11387300491333, 'learning_rate': 0.0001705732068837659, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:04, 15.93it/s]                                               {'loss': 2.2727, 'grad_norm': 4.449922561645508, 'learning_rate': 0.00016798876435522402, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:04, 15.93it/s] 16%|█▌        | 12/75 [00:00<00:03, 17.74it/s]                                               {'loss': 2.5624, 'grad_norm': 4.960662364959717, 'learning_rate': 0.0001654043218266821, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:03, 17.74it/s]                                               {'loss': 2.3174, 'grad_norm': 3.9340827465057373, 'learning_rate': 0.0001628198792981402, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:03, 17.74it/s] 19%|█▊        | 14/75 [00:00<00:03, 17.78it/s]                                               {'loss': 2.3059, 'grad_norm': 5.6288652420043945, 'learning_rate': 0.00016023543676959827, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:03, 17.78it/s]                                               {'loss': 2.2484, 'grad_norm': 8.398584365844727, 'learning_rate': 0.00015765099424105638, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:03, 17.78it/s] 21%|██▏       | 16/75 [00:00<00:03, 17.44it/s]                                               {'loss': 2.3633, 'grad_norm': 4.893949508666992, 'learning_rate': 0.00015506655171251447, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:03, 17.44it/s]                                               {'loss': 2.3019, 'grad_norm': 4.0417280197143555, 'learning_rate': 0.00015248210918397255, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:01<00:03, 17.44it/s] 24%|██▍       | 18/75 [00:01<00:03, 17.42it/s]                                               {'loss': 2.3059, 'grad_norm': 3.0493268966674805, 'learning_rate': 0.00014989766665543064, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:01<00:03, 17.42it/s]                                               {'loss': 2.3141, 'grad_norm': 4.179266452789307, 'learning_rate': 0.00014731322412688875, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:01<00:03, 17.42it/s] 27%|██▋       | 20/75 [00:01<00:03, 16.58it/s]                                               {'loss': 2.2601, 'grad_norm': 3.75434947013855, 'learning_rate': 0.00014472878159834684, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:01<00:03, 16.58it/s]                                               {'loss': 2.2856, 'grad_norm': 4.724589824676514, 'learning_rate': 0.00014214433906980492, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:03, 16.58it/s] 29%|██▉       | 22/75 [00:01<00:03, 15.50it/s]                                               {'loss': 2.4891, 'grad_norm': 5.2751970291137695, 'learning_rate': 0.000139559896541263, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:03, 15.50it/s]                                               {'loss': 2.3366, 'grad_norm': 4.0072922706604, 'learning_rate': 0.00013697545401272112, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:03, 15.50it/s] 32%|███▏      | 24/75 [00:01<00:03, 14.37it/s]                                               {'loss': 2.3782, 'grad_norm': 4.309770107269287, 'learning_rate': 0.0001343910114841792, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:03, 14.37it/s]                                               {'loss': 2.1617, 'grad_norm': 4.445003986358643, 'learning_rate': 0.00013180656895563731, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:03, 14.37it/s] 35%|███▍      | 26/75 [00:01<00:03, 15.07it/s]                                               {'loss': 2.3049, 'grad_norm': 5.311141490936279, 'learning_rate': 0.00012922212642709537, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:03, 15.07it/s]                                               {'loss': 2.1598, 'grad_norm': 3.768120288848877, 'learning_rate': 0.00012663768389855348, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:03, 15.07it/s] 37%|███▋      | 28/75 [00:01<00:02, 16.06it/s]                                               {'loss': 2.2224, 'grad_norm': 5.087138652801514, 'learning_rate': 0.00012405324137001157, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 16.06it/s]                                               {'loss': 2.2482, 'grad_norm': 3.621575117111206, 'learning_rate': 0.00012146879884146967, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 16.06it/s]                                               {'loss': 2.5191, 'grad_norm': 10.408658981323242, 'learning_rate': 0.00011888435631292775, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 16.06it/s] 41%|████▏     | 31/75 [00:01<00:02, 19.61it/s]                                               {'loss': 2.1399, 'grad_norm': 3.9295740127563477, 'learning_rate': 0.00011629991378438585, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:02, 19.61it/s]                                               {'loss': 2.1909, 'grad_norm': 4.154735088348389, 'learning_rate': 0.00011371547125584395, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:02, 19.61it/s]                                               {'loss': 2.2677, 'grad_norm': 4.2120137214660645, 'learning_rate': 0.00011113102872730204, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:02, 19.61it/s] 45%|████▌     | 34/75 [00:02<00:01, 20.69it/s]                                               {'loss': 2.1063, 'grad_norm': 3.765840530395508, 'learning_rate': 0.00010854658619876013, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:02<00:01, 20.69it/s]                                               {'loss': 2.2033, 'grad_norm': 3.2723629474639893, 'learning_rate': 0.00010596214367021822, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:02<00:01, 20.69it/s]                                               {'loss': 2.2398, 'grad_norm': 3.3796424865722656, 'learning_rate': 0.00010337770114167632, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:02<00:01, 20.69it/s] 49%|████▉     | 37/75 [00:02<00:01, 21.16it/s]                                               {'loss': 2.3577, 'grad_norm': 4.226377964019775, 'learning_rate': 0.0001007932586131344, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:02<00:01, 21.16it/s]                                               {'loss': 2.2764, 'grad_norm': 3.233863592147827, 'learning_rate': 9.82088160845925e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:02<00:01, 21.16it/s]                                               {'loss': 2.2557, 'grad_norm': 4.814887523651123, 'learning_rate': 9.562437355605059e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:02<00:01, 21.16it/s] 53%|█████▎    | 40/75 [00:02<00:01, 21.46it/s]                                               {'loss': 2.3001, 'grad_norm': 4.698576927185059, 'learning_rate': 9.303993102750868e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:02<00:01, 21.46it/s]                                               {'loss': 2.2597, 'grad_norm': 5.071063041687012, 'learning_rate': 9.045548849896677e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:02<00:01, 21.46it/s]                                               {'loss': 2.2251, 'grad_norm': 4.182321071624756, 'learning_rate': 8.787104597042487e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 21.46it/s] 57%|█████▋    | 43/75 [00:02<00:01, 20.07it/s]                                               {'loss': 2.2872, 'grad_norm': 4.303408622741699, 'learning_rate': 8.528660344188295e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 20.07it/s]                                               {'loss': 2.2546, 'grad_norm': 4.165660381317139, 'learning_rate': 8.270216091334105e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 20.07it/s]                                               {'loss': 3.0254, 'grad_norm': 22.298534393310547, 'learning_rate': 8.011771838479914e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 20.07it/s]                                               {'loss': 2.2548, 'grad_norm': 4.248053550720215, 'learning_rate': 7.753327585625723e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 20.07it/s] 63%|██████▎   | 47/75 [00:02<00:01, 23.11it/s]                                               {'loss': 2.2899, 'grad_norm': 4.102392196655273, 'learning_rate': 7.494883332771532e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 23.11it/s]                                               {'loss': 2.2894, 'grad_norm': 5.101099967956543, 'learning_rate': 7.236439079917342e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 23.11it/s]                                               {'loss': 2.2423, 'grad_norm': 5.278055191040039, 'learning_rate': 6.97799482706315e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 23.11it/s] 67%|██████▋   | 50/75 [00:02<00:01, 22.69it/s]                                               {'loss': 2.0519, 'grad_norm': 3.149014711380005, 'learning_rate': 6.71955057420896e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 22.69it/s]                                               {'loss': 2.1415, 'grad_norm': 5.035574913024902, 'learning_rate': 6.461106321354769e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 22.69it/s]                                               {'loss': 2.3128, 'grad_norm': 3.9023020267486572, 'learning_rate': 6.202662068500578e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 22.69it/s] 71%|███████   | 53/75 [00:02<00:00, 22.57it/s]                                               {'loss': 2.2713, 'grad_norm': 3.8120806217193604, 'learning_rate': 5.9442178156463877e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 22.57it/s]                                               {'loss': 2.3117, 'grad_norm': 4.520114898681641, 'learning_rate': 5.6857735627921975e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 22.57it/s]                                               {'loss': 2.2009, 'grad_norm': 4.395004749298096, 'learning_rate': 5.427329309938007e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 22.57it/s] 75%|███████▍  | 56/75 [00:02<00:00, 22.07it/s]                                               {'loss': 2.3153, 'grad_norm': 3.321507453918457, 'learning_rate': 5.168885057083816e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 22.07it/s]                                               {'loss': 2.2956, 'grad_norm': 4.873109340667725, 'learning_rate': 4.910440804229625e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:03<00:00, 22.07it/s]                                               {'loss': 2.155, 'grad_norm': 3.6716129779815674, 'learning_rate': 4.651996551375434e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:03<00:00, 22.07it/s] 79%|███████▊  | 59/75 [00:03<00:00, 22.07it/s]                                               {'loss': 2.1655, 'grad_norm': 3.9976091384887695, 'learning_rate': 4.3935522985212434e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:03<00:00, 22.07it/s]                                               {'loss': 2.2835, 'grad_norm': 10.669933319091797, 'learning_rate': 4.1351080456670525e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:03<00:00, 22.07it/s]                                               {'loss': 2.1563, 'grad_norm': 4.540092468261719, 'learning_rate': 3.876663792812862e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:03<00:00, 22.07it/s] 83%|████████▎ | 62/75 [00:03<00:00, 22.34it/s]                                               {'loss': 2.3345, 'grad_norm': 4.827710151672363, 'learning_rate': 3.618219539958671e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:03<00:00, 22.34it/s]                                               {'loss': 1.9904, 'grad_norm': 3.415184497833252, 'learning_rate': 3.35977528710448e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 22.34it/s]                                               {'loss': 2.2559, 'grad_norm': 3.900141716003418, 'learning_rate': 3.101331034250289e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 22.34it/s] 87%|████████▋ | 65/75 [00:03<00:00, 21.43it/s]                                               {'loss': 2.2995, 'grad_norm': 3.811835527420044, 'learning_rate': 2.8428867813960988e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 21.43it/s]                                               {'loss': 2.0529, 'grad_norm': 2.879650592803955, 'learning_rate': 2.584442528541908e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 21.43it/s]                                               {'loss': 2.118, 'grad_norm': 4.045896053314209, 'learning_rate': 2.325998275687717e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 21.43it/s] 91%|█████████ | 68/75 [00:03<00:00, 20.71it/s]                                               {'loss': 2.2137, 'grad_norm': 2.9057767391204834, 'learning_rate': 2.0675540228335263e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 20.71it/s]                                               {'loss': 2.2792, 'grad_norm': 4.44663667678833, 'learning_rate': 1.8091097699793354e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 20.71it/s]                                               {'loss': 2.133, 'grad_norm': 3.951242685317993, 'learning_rate': 1.5506655171251446e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 20.71it/s] 95%|█████████▍| 71/75 [00:03<00:00, 21.24it/s]                                               {'loss': 2.3429, 'grad_norm': 4.0243682861328125, 'learning_rate': 1.292221264270954e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 21.24it/s]                                               {'loss': 2.2549, 'grad_norm': 5.7956647872924805, 'learning_rate': 1.0337770114167631e-05, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 21.24it/s]                                               {'loss': 2.1827, 'grad_norm': 3.4882147312164307, 'learning_rate': 7.753327585625723e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 21.24it/s] 99%|█████████▊| 74/75 [00:03<00:00, 21.28it/s]                                               {'loss': 2.1572, 'grad_norm': 4.049525260925293, 'learning_rate': 5.168885057083816e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 21.28it/s]                                               {'loss': 2.083, 'grad_norm': 10.2615327835083, 'learning_rate': 2.584442528541908e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 21.28it/s]                                               {'train_runtime': 4.2099, 'train_samples_per_second': 268.413, 'train_steps_per_second': 17.815, 'train_loss': 2.2834817282358806, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 21.28it/s]100%|██████████| 75/75 [00:04<00:00, 17.82it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1604, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1229, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1182, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1367, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1560, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1268, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1524, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1253, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1367, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1208, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1421, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1108, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 64.22it/s]  3%|▎         | 14/471 [00:00<00:08, 56.78it/s]  4%|▍         | 20/471 [00:00<00:08, 55.43it/s]  6%|▌         | 26/471 [00:00<00:08, 52.94it/s]  7%|▋         | 32/471 [00:00<00:09, 47.64it/s]  8%|▊         | 37/471 [00:00<00:09, 45.22it/s]  9%|▉         | 42/471 [00:00<00:09, 43.07it/s] 10%|▉         | 47/471 [00:01<00:10, 40.69it/s] 11%|█         | 52/471 [00:01<00:09, 42.71it/s] 12%|█▏        | 57/471 [00:01<00:10, 38.87it/s] 13%|█▎        | 61/471 [00:01<00:10, 37.40it/s] 14%|█▍        | 66/471 [00:01<00:10, 40.38it/s] 15%|█▌        | 71/471 [00:01<00:09, 41.95it/s] 16%|█▌        | 76/471 [00:01<00:09, 42.95it/s] 17%|█▋        | 82/471 [00:01<00:08, 45.80it/s] 18%|█▊        | 87/471 [00:01<00:08, 44.24it/s] 20%|█▉        | 92/471 [00:02<00:08, 42.36it/s] 21%|██        | 97/471 [00:02<00:09, 41.25it/s] 22%|██▏       | 102/471 [00:02<00:08, 42.92it/s] 23%|██▎       | 107/471 [00:02<00:08, 43.62it/s] 24%|██▍       | 112/471 [00:02<00:08, 44.08it/s] 25%|██▌       | 118/471 [00:02<00:07, 46.86it/s] 26%|██▌       | 123/471 [00:02<00:07, 45.63it/s] 27%|██▋       | 128/471 [00:02<00:07, 45.33it/s] 28%|██▊       | 134/471 [00:02<00:07, 46.94it/s] 30%|██▉       | 139/471 [00:03<00:07, 46.24it/s] 31%|███       | 144/471 [00:03<00:07, 44.58it/s] 32%|███▏      | 149/471 [00:03<00:07, 44.82it/s] 33%|███▎      | 154/471 [00:03<00:07, 45.06it/s] 34%|███▍      | 159/471 [00:03<00:06, 45.10it/s] 35%|███▍      | 164/471 [00:03<00:06, 45.22it/s] 36%|███▌      | 170/471 [00:03<00:06, 46.92it/s] 37%|███▋      | 175/471 [00:03<00:07, 39.74it/s] 38%|███▊      | 180/471 [00:04<00:07, 36.78it/s] 39%|███▉      | 184/471 [00:04<00:08, 35.19it/s] 40%|███▉      | 188/471 [00:04<00:08, 31.62it/s] 41%|████      | 192/471 [00:04<00:08, 32.00it/s] 42%|████▏     | 196/471 [00:04<00:08, 33.38it/s] 42%|████▏     | 200/471 [00:04<00:08, 31.62it/s] 43%|████▎     | 204/471 [00:04<00:07, 33.38it/s] 44%|████▍     | 208/471 [00:04<00:07, 34.52it/s] 45%|████▌     | 212/471 [00:05<00:08, 30.59it/s] 46%|████▌     | 217/471 [00:05<00:07, 34.39it/s] 47%|████▋     | 221/471 [00:05<00:07, 33.21it/s] 48%|████▊     | 225/471 [00:05<00:07, 33.76it/s] 49%|████▊     | 229/471 [00:05<00:07, 32.49it/s] 49%|████▉     | 233/471 [00:05<00:07, 31.52it/s] 50%|█████     | 237/471 [00:05<00:07, 32.49it/s] 51%|█████     | 241/471 [00:06<00:06, 34.32it/s] 52%|█████▏    | 245/471 [00:06<00:06, 32.46it/s] 53%|█████▎    | 249/471 [00:06<00:07, 31.49it/s] 54%|█████▎    | 253/471 [00:06<00:06, 31.18it/s] 55%|█████▍    | 257/471 [00:06<00:06, 30.58it/s] 55%|█████▌    | 261/471 [00:06<00:07, 29.80it/s] 56%|█████▋    | 266/471 [00:06<00:06, 34.13it/s] 57%|█████▋    | 270/471 [00:06<00:05, 35.49it/s] 58%|█████▊    | 274/471 [00:07<00:05, 34.42it/s] 59%|█████▉    | 278/471 [00:07<00:05, 34.27it/s] 60%|██████    | 283/471 [00:07<00:05, 37.06it/s] 61%|██████    | 288/471 [00:07<00:04, 39.89it/s] 62%|██████▏   | 293/471 [00:07<00:04, 42.28it/s] 63%|██████▎   | 299/471 [00:07<00:03, 44.40it/s] 65%|██████▍   | 304/471 [00:07<00:03, 45.57it/s] 66%|██████▌   | 310/471 [00:07<00:03, 47.54it/s] 67%|██████▋   | 315/471 [00:07<00:03, 41.80it/s] 68%|██████▊   | 320/471 [00:08<00:04, 34.53it/s] 69%|██████▉   | 325/471 [00:08<00:04, 36.44it/s] 70%|██████▉   | 329/471 [00:08<00:03, 36.42it/s] 71%|███████   | 334/471 [00:08<00:03, 37.18it/s] 72%|███████▏  | 339/471 [00:08<00:03, 38.78it/s] 73%|███████▎  | 344/471 [00:08<00:03, 39.41it/s] 74%|███████▍  | 349/471 [00:08<00:03, 37.13it/s] 75%|███████▌  | 354/471 [00:09<00:03, 38.71it/s] 76%|███████▋  | 360/471 [00:09<00:02, 42.40it/s] 77%|███████▋  | 365/471 [00:09<00:02, 43.80it/s] 79%|███████▊  | 370/471 [00:09<00:02, 39.85it/s] 80%|███████▉  | 375/471 [00:09<00:02, 37.58it/s] 80%|████████  | 379/471 [00:09<00:02, 33.79it/s] 81%|████████▏ | 383/471 [00:09<00:02, 34.13it/s] 82%|████████▏ | 388/471 [00:09<00:02, 36.14it/s] 83%|████████▎ | 393/471 [00:10<00:02, 37.67it/s] 85%|████████▍ | 399/471 [00:10<00:01, 41.89it/s] 86%|████████▌ | 404/471 [00:10<00:01, 42.66it/s] 87%|████████▋ | 409/471 [00:10<00:01, 41.13it/s] 88%|████████▊ | 414/471 [00:10<00:01, 40.76it/s] 89%|████████▉ | 419/471 [00:10<00:01, 37.79it/s] 90%|████████▉ | 423/471 [00:10<00:01, 37.09it/s] 91%|█████████ | 428/471 [00:10<00:01, 36.96it/s] 92%|█████████▏| 434/471 [00:11<00:00, 39.79it/s] 93%|█████████▎| 438/471 [00:11<00:00, 36.79it/s] 94%|█████████▍| 442/471 [00:11<00:00, 36.69it/s] 95%|█████████▍| 446/471 [00:11<00:00, 36.19it/s] 96%|█████████▌| 450/471 [00:11<00:00, 34.94it/s] 96%|█████████▋| 454/471 [00:11<00:00, 34.42it/s] 97%|█████████▋| 459/471 [00:11<00:00, 35.82it/s] 98%|█████████▊| 463/471 [00:11<00:00, 35.60it/s] 99%|█████████▉| 468/471 [00:12<00:00, 38.08it/s]100%|██████████| 471/471 [00:12<00:00, 38.90it/s]
{'eval_loss': 2.479754686355591, 'eval_model_preparation_time': 0.0029, 'eval_acc': 0.3270047796070101, 'eval_runtime': 12.13, 'eval_samples_per_second': 620.941, 'eval_steps_per_second': 38.829}
ROUND:4
CLIENT:38
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.397, 'grad_norm': 5.323314666748047, 'learning_rate': 0.00018377223398316204, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:05, 13.01it/s]  3%|▎         | 2/75 [00:00<00:04, 16.40it/s]                                              {'loss': 2.327, 'grad_norm': 5.226836681365967, 'learning_rate': 0.00018132193753005323, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:04, 16.40it/s]                                              {'loss': 2.61, 'grad_norm': 5.442015171051025, 'learning_rate': 0.0001788716410769444, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 16.40it/s]  5%|▌         | 4/75 [00:00<00:04, 17.49it/s]                                              {'loss': 2.4859, 'grad_norm': 4.192148208618164, 'learning_rate': 0.00017642134462383556, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:04, 17.49it/s]                                              {'loss': 2.3121, 'grad_norm': 4.835198879241943, 'learning_rate': 0.00017397104817072674, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:04, 17.49it/s]                                              {'loss': 2.4979, 'grad_norm': 4.966601848602295, 'learning_rate': 0.0001715207517176179, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 17.49it/s]  9%|▉         | 7/75 [00:00<00:03, 19.07it/s]                                              {'loss': 2.2707, 'grad_norm': 5.305566310882568, 'learning_rate': 0.0001690704552645091, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 19.07it/s]                                              {'loss': 2.3958, 'grad_norm': 4.389123439788818, 'learning_rate': 0.00016662015881140026, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 19.07it/s] 12%|█▏        | 9/75 [00:00<00:03, 18.91it/s]                                              {'loss': 2.2659, 'grad_norm': 4.000494956970215, 'learning_rate': 0.00016416986235829142, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 18.91it/s]                                              {'loss': 2.2183, 'grad_norm': 3.615145683288574, 'learning_rate': 0.0001617195659051826, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 18.91it/s] 15%|█▍        | 11/75 [00:00<00:03, 18.30it/s]                                               {'loss': 2.5286, 'grad_norm': 3.978137254714966, 'learning_rate': 0.00015926926945207377, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 18.30it/s]                                               {'loss': 2.3138, 'grad_norm': 7.67459774017334, 'learning_rate': 0.00015681897299896496, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:03, 18.30it/s]                                               {'loss': 2.5118, 'grad_norm': 7.825214862823486, 'learning_rate': 0.00015436867654585612, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:03, 18.30it/s] 19%|█▊        | 14/75 [00:00<00:03, 19.51it/s]                                               {'loss': 2.7614, 'grad_norm': 6.034158706665039, 'learning_rate': 0.00015191838009274728, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:03, 19.51it/s]                                               {'loss': 2.412, 'grad_norm': 14.472515106201172, 'learning_rate': 0.00014946808363963847, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:03, 19.51it/s]                                               {'loss': 2.4949, 'grad_norm': 3.7750303745269775, 'learning_rate': 0.00014701778718652966, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:03, 19.51it/s] 23%|██▎       | 17/75 [00:00<00:02, 21.81it/s]                                               {'loss': 2.2772, 'grad_norm': 4.738430976867676, 'learning_rate': 0.0001445674907334208, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 21.81it/s]                                               {'loss': 2.2497, 'grad_norm': 4.8611297607421875, 'learning_rate': 0.00014211719428031198, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 21.81it/s]                                               {'loss': 2.3714, 'grad_norm': 5.057580471038818, 'learning_rate': 0.00013966689782720317, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 21.81it/s] 27%|██▋       | 20/75 [00:00<00:02, 22.25it/s]                                               {'loss': 2.34, 'grad_norm': 5.755120754241943, 'learning_rate': 0.00013721660137409433, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 22.25it/s]                                               {'loss': 2.5045, 'grad_norm': 4.538991928100586, 'learning_rate': 0.0001347663049209855, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:02, 22.25it/s]                                               {'loss': 2.3313, 'grad_norm': 4.889319896697998, 'learning_rate': 0.00013231600846787665, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 22.25it/s] 31%|███       | 23/75 [00:01<00:02, 18.79it/s]                                               {'loss': 2.2601, 'grad_norm': 2.9821672439575195, 'learning_rate': 0.00012986571201476784, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 18.79it/s]                                               {'loss': 2.2237, 'grad_norm': 4.161498546600342, 'learning_rate': 0.00012741541556165903, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 18.79it/s] 33%|███▎      | 25/75 [00:01<00:02, 16.86it/s]                                               {'loss': 2.2592, 'grad_norm': 4.528845310211182, 'learning_rate': 0.0001249651191085502, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 16.86it/s]                                               {'loss': 2.4451, 'grad_norm': 4.671521186828613, 'learning_rate': 0.00012251482265544135, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 16.86it/s] 36%|███▌      | 27/75 [00:01<00:02, 16.72it/s]                                               {'loss': 2.2548, 'grad_norm': 5.197055816650391, 'learning_rate': 0.00012006452620233253, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 16.72it/s]                                               {'loss': 2.141, 'grad_norm': 3.831928014755249, 'learning_rate': 0.00011761422974922372, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 16.72it/s]                                               {'loss': 2.3252, 'grad_norm': 5.209415435791016, 'learning_rate': 0.00011516393329611489, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 16.72it/s] 40%|████      | 30/75 [00:01<00:02, 19.16it/s]                                               {'loss': 2.6502, 'grad_norm': 12.43082046508789, 'learning_rate': 0.00011271363684300604, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 19.16it/s]                                               {'loss': 2.3522, 'grad_norm': 4.51461935043335, 'learning_rate': 0.00011026334038989722, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:02, 19.16it/s] 43%|████▎     | 32/75 [00:01<00:02, 18.07it/s]                                               {'loss': 2.1531, 'grad_norm': 3.5488665103912354, 'learning_rate': 0.0001078130439367884, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:02, 18.07it/s]                                               {'loss': 2.176, 'grad_norm': 4.363351345062256, 'learning_rate': 0.00010536274748367958, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:02, 18.07it/s] 45%|████▌     | 34/75 [00:01<00:02, 18.01it/s]                                               {'loss': 2.2628, 'grad_norm': 3.4520819187164307, 'learning_rate': 0.00010291245103057075, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:02, 18.01it/s]                                               {'loss': 2.4398, 'grad_norm': 5.1082892417907715, 'learning_rate': 0.00010046215457746192, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:02, 18.01it/s] 48%|████▊     | 36/75 [00:01<00:02, 16.42it/s]                                               {'loss': 2.244, 'grad_norm': 5.621996879577637, 'learning_rate': 9.801185812435309e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:02, 16.42it/s]                                               {'loss': 2.2868, 'grad_norm': 4.3135552406311035, 'learning_rate': 9.556156167124427e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:02<00:02, 16.42it/s] 51%|█████     | 38/75 [00:02<00:02, 16.48it/s]                                               {'loss': 2.2856, 'grad_norm': 3.5924222469329834, 'learning_rate': 9.311126521813544e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:02<00:02, 16.48it/s]                                               {'loss': 2.2091, 'grad_norm': 3.869520425796509, 'learning_rate': 9.066096876502662e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:02<00:02, 16.48it/s] 53%|█████▎    | 40/75 [00:02<00:02, 15.83it/s]                                               {'loss': 2.2047, 'grad_norm': 4.532750606536865, 'learning_rate': 8.821067231191778e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:02<00:02, 15.83it/s]                                               {'loss': 2.1731, 'grad_norm': 3.6841957569122314, 'learning_rate': 8.576037585880895e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:02<00:02, 15.83it/s] 56%|█████▌    | 42/75 [00:02<00:02, 16.17it/s]                                               {'loss': 2.2464, 'grad_norm': 4.343660354614258, 'learning_rate': 8.331007940570013e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:02, 16.17it/s]                                               {'loss': 2.1496, 'grad_norm': 4.497203826904297, 'learning_rate': 8.08597829525913e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 16.17it/s]                                               {'loss': 2.2243, 'grad_norm': 4.191167831420898, 'learning_rate': 7.840948649948248e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 16.17it/s] 60%|██████    | 45/75 [00:02<00:01, 18.47it/s]                                               {'loss': 2.7291, 'grad_norm': 11.8517427444458, 'learning_rate': 7.595919004637364e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 18.47it/s]                                               {'loss': 2.311, 'grad_norm': 4.146159648895264, 'learning_rate': 7.350889359326483e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 18.47it/s] 63%|██████▎   | 47/75 [00:02<00:01, 18.05it/s]                                               {'loss': 2.1674, 'grad_norm': 4.109485626220703, 'learning_rate': 7.105859714015599e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 18.05it/s]                                               {'loss': 2.2556, 'grad_norm': 3.747715711593628, 'learning_rate': 6.860830068704717e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 18.05it/s] 65%|██████▌   | 49/75 [00:02<00:01, 16.82it/s]                                               {'loss': 2.1005, 'grad_norm': 3.864713430404663, 'learning_rate': 6.615800423393833e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 16.82it/s]                                               {'loss': 2.1368, 'grad_norm': 4.062019348144531, 'learning_rate': 6.370770778082952e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 16.82it/s]                                               {'loss': 2.1938, 'grad_norm': 4.310461044311523, 'learning_rate': 6.125741132772068e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 16.82it/s] 69%|██████▉   | 52/75 [00:02<00:01, 18.24it/s]                                               {'loss': 2.4772, 'grad_norm': 4.017439365386963, 'learning_rate': 5.880711487461186e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 18.24it/s]                                               {'loss': 2.3197, 'grad_norm': 4.540535926818848, 'learning_rate': 5.635681842150302e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:01, 18.24it/s]                                               {'loss': 2.1878, 'grad_norm': 4.219433307647705, 'learning_rate': 5.39065219683942e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:01, 18.24it/s] 73%|███████▎  | 55/75 [00:03<00:01, 19.28it/s]                                               {'loss': 2.1842, 'grad_norm': 3.4705071449279785, 'learning_rate': 5.145622551528538e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:03<00:01, 19.28it/s]                                               {'loss': 2.2425, 'grad_norm': 4.00681734085083, 'learning_rate': 4.9005929062176545e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:03<00:00, 19.28it/s] 76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]                                               {'loss': 2.1534, 'grad_norm': 3.8141262531280518, 'learning_rate': 4.655563260906772e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:03<00:01, 17.71it/s]                                               {'loss': 2.2386, 'grad_norm': 3.446228265762329, 'learning_rate': 4.410533615595889e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:03<00:00, 17.71it/s] 79%|███████▊  | 59/75 [00:03<00:00, 16.83it/s]                                               {'loss': 2.1442, 'grad_norm': 4.118337631225586, 'learning_rate': 4.1655039702850064e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:03<00:00, 16.83it/s]                                               {'loss': 2.2919, 'grad_norm': 12.3633394241333, 'learning_rate': 3.920474324974124e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:03<00:00, 16.83it/s] 81%|████████▏ | 61/75 [00:03<00:00, 16.13it/s]                                               {'loss': 2.2373, 'grad_norm': 3.6202900409698486, 'learning_rate': 3.6754446796632414e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:03<00:00, 16.13it/s]                                               {'loss': 2.247, 'grad_norm': 5.368489742279053, 'learning_rate': 3.430415034352358e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:03<00:00, 16.13it/s] 84%|████████▍ | 63/75 [00:03<00:00, 16.02it/s]                                               {'loss': 2.192, 'grad_norm': 3.8908331394195557, 'learning_rate': 3.185385389041476e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 16.02it/s]                                               {'loss': 2.2957, 'grad_norm': 3.8505871295928955, 'learning_rate': 2.940355743730593e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 16.02it/s] 87%|████████▋ | 65/75 [00:03<00:00, 15.55it/s]                                               {'loss': 2.1181, 'grad_norm': 4.076578140258789, 'learning_rate': 2.69532609841971e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 15.55it/s]                                               {'loss': 2.1533, 'grad_norm': 3.958021640777588, 'learning_rate': 2.4502964531088273e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 15.55it/s] 89%|████████▉ | 67/75 [00:03<00:00, 16.05it/s]                                               {'loss': 2.3081, 'grad_norm': 4.496936798095703, 'learning_rate': 2.2052668077979444e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 16.05it/s]                                               {'loss': 2.0961, 'grad_norm': 3.59226655960083, 'learning_rate': 1.960237162487062e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 16.05it/s] 92%|█████████▏| 69/75 [00:03<00:00, 16.34it/s]                                               {'loss': 2.1309, 'grad_norm': 4.895518779754639, 'learning_rate': 1.715207517176179e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 16.34it/s]                                               {'loss': 2.1197, 'grad_norm': 3.280693769454956, 'learning_rate': 1.4701778718652965e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:04<00:00, 16.34it/s] 95%|█████████▍| 71/75 [00:04<00:00, 15.05it/s]                                               {'loss': 2.2196, 'grad_norm': 4.934140205383301, 'learning_rate': 1.2251482265544136e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:04<00:00, 15.05it/s]                                               {'loss': 2.1848, 'grad_norm': 3.986907482147217, 'learning_rate': 9.80118581243531e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:04<00:00, 15.05it/s] 97%|█████████▋| 73/75 [00:04<00:00, 15.29it/s]                                               {'loss': 2.4463, 'grad_norm': 4.025482177734375, 'learning_rate': 7.350889359326482e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:04<00:00, 15.29it/s]                                               {'loss': 2.1149, 'grad_norm': 4.1596245765686035, 'learning_rate': 4.900592906217655e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:04<00:00, 15.29it/s]100%|██████████| 75/75 [00:04<00:00, 16.21it/s]                                               {'loss': 2.2918, 'grad_norm': 13.36487102508545, 'learning_rate': 2.4502964531088274e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 16.21it/s]                                               {'train_runtime': 4.5318, 'train_samples_per_second': 249.346, 'train_steps_per_second': 16.55, 'train_loss': 2.295112075805664, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 16.21it/s]100%|██████████| 75/75 [00:04<00:00, 16.55it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:20
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]  1%|▏         | 1/75 [00:00<00:10,  7.15it/s]                                              {'loss': 2.6527, 'grad_norm': 5.0316386222839355, 'learning_rate': 0.00018377223398316204, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:10,  7.15it/s]                                              {'loss': 2.5196, 'grad_norm': 6.075386047363281, 'learning_rate': 0.00018132193753005323, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:10,  7.15it/s]  4%|▍         | 3/75 [00:00<00:06, 11.02it/s]                                              {'loss': 2.5696, 'grad_norm': 4.643937587738037, 'learning_rate': 0.0001788716410769444, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:06, 11.02it/s]                                              {'loss': 2.4225, 'grad_norm': 5.864528656005859, 'learning_rate': 0.00017642134462383556, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:06, 11.02it/s]  7%|▋         | 5/75 [00:00<00:05, 13.20it/s]                                              {'loss': 2.4857, 'grad_norm': 4.047101020812988, 'learning_rate': 0.00017397104817072674, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:05, 13.20it/s]                                              {'loss': 2.5363, 'grad_norm': 3.2873013019561768, 'learning_rate': 0.0001715207517176179, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:05, 13.20it/s]  9%|▉         | 7/75 [00:00<00:04, 14.78it/s]                                              {'loss': 2.4732, 'grad_norm': 4.396599769592285, 'learning_rate': 0.0001690704552645091, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:04, 14.78it/s]                                              {'loss': 2.5307, 'grad_norm': 4.788567066192627, 'learning_rate': 0.00016662015881140026, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:04, 14.78it/s]                                              {'loss': 2.2021, 'grad_norm': 3.4875760078430176, 'learning_rate': 0.00016416986235829142, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:04, 14.78it/s] 13%|█▎        | 10/75 [00:00<00:03, 17.37it/s]                                               {'loss': 2.4216, 'grad_norm': 4.982986927032471, 'learning_rate': 0.0001617195659051826, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 17.37it/s]                                               {'loss': 2.6381, 'grad_norm': 4.4253339767456055, 'learning_rate': 0.00015926926945207377, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 17.37it/s]                                               {'loss': 2.4222, 'grad_norm': 4.409005641937256, 'learning_rate': 0.00015681897299896496, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:03, 17.37it/s] 17%|█▋        | 13/75 [00:00<00:03, 19.08it/s]                                               {'loss': 2.1418, 'grad_norm': 6.247572898864746, 'learning_rate': 0.00015436867654585612, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:03, 19.08it/s]                                               {'loss': 2.13, 'grad_norm': 3.445988178253174, 'learning_rate': 0.00015191838009274728, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:03, 19.08it/s]                                               {'loss': 2.733, 'grad_norm': 11.895674705505371, 'learning_rate': 0.00014946808363963847, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:03, 19.08it/s] 21%|██▏       | 16/75 [00:00<00:02, 20.46it/s]                                               {'loss': 2.2424, 'grad_norm': 4.139621257781982, 'learning_rate': 0.00014701778718652966, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 20.46it/s]                                               {'loss': 2.3251, 'grad_norm': 4.471752643585205, 'learning_rate': 0.0001445674907334208, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 20.46it/s]                                               {'loss': 2.1013, 'grad_norm': 4.076648712158203, 'learning_rate': 0.00014211719428031198, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:01<00:02, 20.46it/s] 25%|██▌       | 19/75 [00:01<00:02, 19.93it/s]                                               {'loss': 2.467, 'grad_norm': 4.453429698944092, 'learning_rate': 0.00013966689782720317, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:01<00:02, 19.93it/s]                                               {'loss': 2.3991, 'grad_norm': 4.600826263427734, 'learning_rate': 0.00013721660137409433, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:01<00:02, 19.93it/s]                                               {'loss': 2.3611, 'grad_norm': 4.520106315612793, 'learning_rate': 0.0001347663049209855, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:02, 19.93it/s] 29%|██▉       | 22/75 [00:01<00:02, 20.38it/s]                                               {'loss': 2.307, 'grad_norm': 4.322727680206299, 'learning_rate': 0.00013231600846787665, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 20.38it/s]                                               {'loss': 2.2173, 'grad_norm': 4.046529293060303, 'learning_rate': 0.00012986571201476784, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 20.38it/s]                                               {'loss': 2.2681, 'grad_norm': 4.700232028961182, 'learning_rate': 0.00012741541556165903, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 20.38it/s] 33%|███▎      | 25/75 [00:01<00:02, 21.17it/s]                                               {'loss': 2.1922, 'grad_norm': 5.473613262176514, 'learning_rate': 0.0001249651191085502, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 21.17it/s]                                               {'loss': 2.4562, 'grad_norm': 3.202531099319458, 'learning_rate': 0.00012251482265544135, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 21.17it/s]                                               {'loss': 2.2813, 'grad_norm': 4.638868808746338, 'learning_rate': 0.00012006452620233253, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 21.17it/s] 37%|███▋      | 28/75 [00:01<00:02, 21.69it/s]                                               {'loss': 2.4248, 'grad_norm': 4.155519962310791, 'learning_rate': 0.00011761422974922372, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 21.69it/s]                                               {'loss': 2.2228, 'grad_norm': 3.731458902359009, 'learning_rate': 0.00011516393329611489, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 21.69it/s]                                               {'loss': 2.8322, 'grad_norm': 15.676802635192871, 'learning_rate': 0.00011271363684300604, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 21.69it/s] 41%|████▏     | 31/75 [00:01<00:01, 22.16it/s]                                               {'loss': 2.2428, 'grad_norm': 4.8448052406311035, 'learning_rate': 0.00011026334038989722, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 22.16it/s]                                               {'loss': 2.3938, 'grad_norm': 4.72713565826416, 'learning_rate': 0.0001078130439367884, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 22.16it/s]                                               {'loss': 2.1507, 'grad_norm': 3.582052707672119, 'learning_rate': 0.00010536274748367958, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 22.16it/s] 45%|████▌     | 34/75 [00:01<00:01, 21.70it/s]                                               {'loss': 2.5009, 'grad_norm': 4.697969436645508, 'learning_rate': 0.00010291245103057075, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 21.70it/s]                                               {'loss': 2.0763, 'grad_norm': 3.8632097244262695, 'learning_rate': 0.00010046215457746192, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 21.70it/s]                                               {'loss': 2.2232, 'grad_norm': 3.3854222297668457, 'learning_rate': 9.801185812435309e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 21.70it/s] 49%|████▉     | 37/75 [00:01<00:01, 20.34it/s]                                               {'loss': 2.4252, 'grad_norm': 4.653655529022217, 'learning_rate': 9.556156167124427e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 20.34it/s]                                               {'loss': 2.2776, 'grad_norm': 3.227631092071533, 'learning_rate': 9.311126521813544e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 20.34it/s]                                               {'loss': 2.1074, 'grad_norm': 4.2690534591674805, 'learning_rate': 9.066096876502662e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:02<00:01, 20.34it/s] 53%|█████▎    | 40/75 [00:02<00:01, 19.50it/s]                                               {'loss': 2.4794, 'grad_norm': 4.250419616699219, 'learning_rate': 8.821067231191778e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:02<00:01, 19.50it/s]                                               {'loss': 2.1427, 'grad_norm': 4.4168477058410645, 'learning_rate': 8.576037585880895e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:02<00:01, 19.50it/s] 56%|█████▌    | 42/75 [00:02<00:01, 18.32it/s]                                               {'loss': 2.398, 'grad_norm': 4.895138740539551, 'learning_rate': 8.331007940570013e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 18.32it/s]                                               {'loss': 2.1732, 'grad_norm': 3.620638370513916, 'learning_rate': 8.08597829525913e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 18.32it/s] 59%|█████▊    | 44/75 [00:02<00:01, 18.17it/s]                                               {'loss': 2.1402, 'grad_norm': 4.256476402282715, 'learning_rate': 7.840948649948248e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 18.17it/s]                                               {'loss': 2.3538, 'grad_norm': 15.34235954284668, 'learning_rate': 7.595919004637364e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 18.17it/s] 61%|██████▏   | 46/75 [00:02<00:01, 18.35it/s]                                               {'loss': 2.2332, 'grad_norm': 4.839726448059082, 'learning_rate': 7.350889359326483e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 18.35it/s]                                               {'loss': 2.198, 'grad_norm': 4.463377475738525, 'learning_rate': 7.105859714015599e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 18.35it/s]                                               {'loss': 2.4207, 'grad_norm': 4.452963352203369, 'learning_rate': 6.860830068704717e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 18.35it/s] 65%|██████▌   | 49/75 [00:02<00:01, 19.80it/s]                                               {'loss': 2.3192, 'grad_norm': 4.0503764152526855, 'learning_rate': 6.615800423393833e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 19.80it/s]                                               {'loss': 2.4236, 'grad_norm': 6.394303798675537, 'learning_rate': 6.370770778082952e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 19.80it/s]                                               {'loss': 2.0561, 'grad_norm': 4.054241180419922, 'learning_rate': 6.125741132772068e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 19.80it/s] 69%|██████▉   | 52/75 [00:02<00:01, 20.91it/s]                                               {'loss': 2.1569, 'grad_norm': 3.87860107421875, 'learning_rate': 5.880711487461186e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 20.91it/s]                                               {'loss': 2.1766, 'grad_norm': 4.5983147621154785, 'learning_rate': 5.635681842150302e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:01, 20.91it/s]                                               {'loss': 2.2968, 'grad_norm': 3.450683832168579, 'learning_rate': 5.39065219683942e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:01, 20.91it/s] 73%|███████▎  | 55/75 [00:02<00:00, 20.68it/s]                                               {'loss': 2.1755, 'grad_norm': 4.084984302520752, 'learning_rate': 5.145622551528538e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 20.68it/s]                                               {'loss': 2.2468, 'grad_norm': 3.857114553451538, 'learning_rate': 4.9005929062176545e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 20.68it/s]                                               {'loss': 2.32, 'grad_norm': 4.501126766204834, 'learning_rate': 4.655563260906772e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 20.68it/s] 77%|███████▋  | 58/75 [00:02<00:00, 20.98it/s]                                               {'loss': 2.1766, 'grad_norm': 5.035345077514648, 'learning_rate': 4.410533615595889e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 20.98it/s]                                               {'loss': 2.0924, 'grad_norm': 4.212580680847168, 'learning_rate': 4.1655039702850064e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:03<00:00, 20.98it/s]                                               {'loss': 2.6568, 'grad_norm': 13.749923706054688, 'learning_rate': 3.920474324974124e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:03<00:00, 20.98it/s] 81%|████████▏ | 61/75 [00:03<00:00, 21.80it/s]                                               {'loss': 2.33, 'grad_norm': 3.530524969100952, 'learning_rate': 3.6754446796632414e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:03<00:00, 21.80it/s]                                               {'loss': 2.309, 'grad_norm': 4.978810787200928, 'learning_rate': 3.430415034352358e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:03<00:00, 21.80it/s]                                               {'loss': 1.9888, 'grad_norm': 3.993736505508423, 'learning_rate': 3.185385389041476e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 21.80it/s] 85%|████████▌ | 64/75 [00:03<00:00, 21.11it/s]                                               {'loss': 2.1647, 'grad_norm': 3.884120225906372, 'learning_rate': 2.940355743730593e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 21.11it/s]                                               {'loss': 1.8789, 'grad_norm': 3.83249831199646, 'learning_rate': 2.69532609841971e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 21.11it/s]                                               {'loss': 2.1562, 'grad_norm': 3.8552634716033936, 'learning_rate': 2.4502964531088273e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 21.11it/s] 89%|████████▉ | 67/75 [00:03<00:00, 21.89it/s]                                               {'loss': 2.3693, 'grad_norm': 4.284162998199463, 'learning_rate': 2.2052668077979444e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 21.89it/s]                                               {'loss': 2.1396, 'grad_norm': 4.223491668701172, 'learning_rate': 1.960237162487062e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 21.89it/s]                                               {'loss': 2.2661, 'grad_norm': 4.3609747886657715, 'learning_rate': 1.715207517176179e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 21.89it/s] 93%|█████████▎| 70/75 [00:03<00:00, 22.42it/s]                                               {'loss': 2.3533, 'grad_norm': 3.4502294063568115, 'learning_rate': 1.4701778718652965e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 22.42it/s]                                               {'loss': 2.242, 'grad_norm': 4.531551361083984, 'learning_rate': 1.2251482265544136e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 22.42it/s]                                               {'loss': 2.2036, 'grad_norm': 3.942697763442993, 'learning_rate': 9.80118581243531e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 22.42it/s] 97%|█████████▋| 73/75 [00:03<00:00, 21.96it/s]                                               {'loss': 2.2893, 'grad_norm': 4.809346675872803, 'learning_rate': 7.350889359326482e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 21.96it/s]                                               {'loss': 2.3244, 'grad_norm': 4.225425720214844, 'learning_rate': 4.900592906217655e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 21.96it/s]                                               {'loss': 2.3572, 'grad_norm': 19.60082244873047, 'learning_rate': 2.4502964531088274e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 21.96it/s]                                               {'train_runtime': 3.9365, 'train_samples_per_second': 287.056, 'train_steps_per_second': 19.052, 'train_loss': 2.3117871872584024, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 21.96it/s]100%|██████████| 75/75 [00:03<00:00, 19.05it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.5762, 'grad_norm': 4.580866813659668, 'learning_rate': 0.00018377223398316204, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:06, 10.87it/s]  3%|▎         | 2/75 [00:00<00:04, 15.63it/s]                                              {'loss': 2.2337, 'grad_norm': 5.375839710235596, 'learning_rate': 0.00018132193753005323, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:04, 15.63it/s]                                              {'loss': 2.3894, 'grad_norm': 4.8700432777404785, 'learning_rate': 0.0001788716410769444, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 15.63it/s]                                              {'loss': 2.6099, 'grad_norm': 6.318325996398926, 'learning_rate': 0.00017642134462383556, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:04, 15.63it/s]  7%|▋         | 5/75 [00:00<00:03, 19.17it/s]                                              {'loss': 2.59, 'grad_norm': 4.363833904266357, 'learning_rate': 0.00017397104817072674, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 19.17it/s]                                              {'loss': 2.363, 'grad_norm': 3.9135866165161133, 'learning_rate': 0.0001715207517176179, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 19.17it/s]                                              {'loss': 2.5701, 'grad_norm': 5.0445942878723145, 'learning_rate': 0.0001690704552645091, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 19.17it/s] 11%|█         | 8/75 [00:00<00:03, 21.36it/s]                                              {'loss': 2.2056, 'grad_norm': 5.958183288574219, 'learning_rate': 0.00016662015881140026, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 21.36it/s]                                              {'loss': 2.1129, 'grad_norm': 4.689342021942139, 'learning_rate': 0.00016416986235829142, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 21.36it/s]                                              {'loss': 2.3123, 'grad_norm': 4.434963226318359, 'learning_rate': 0.0001617195659051826, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 21.36it/s] 15%|█▍        | 11/75 [00:00<00:02, 21.92it/s]                                               {'loss': 2.3574, 'grad_norm': 5.392427921295166, 'learning_rate': 0.00015926926945207377, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 21.92it/s]                                               {'loss': 2.4355, 'grad_norm': 5.561336040496826, 'learning_rate': 0.00015681897299896496, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 21.92it/s]                                               {'loss': 2.4813, 'grad_norm': 5.720265865325928, 'learning_rate': 0.00015436867654585612, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 21.92it/s] 19%|█▊        | 14/75 [00:00<00:02, 22.06it/s]                                               {'loss': 2.3309, 'grad_norm': 5.410440921783447, 'learning_rate': 0.00015191838009274728, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 22.06it/s]                                               {'loss': 2.1037, 'grad_norm': 8.07420539855957, 'learning_rate': 0.00014946808363963847, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 22.06it/s]                                               {'loss': 2.0598, 'grad_norm': 3.6100993156433105, 'learning_rate': 0.00014701778718652966, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 22.06it/s]                                               {'loss': 2.2097, 'grad_norm': 5.412011623382568, 'learning_rate': 0.0001445674907334208, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 22.06it/s] 24%|██▍       | 18/75 [00:00<00:02, 24.28it/s]                                               {'loss': 2.5197, 'grad_norm': 4.338416576385498, 'learning_rate': 0.00014211719428031198, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.28it/s]                                               {'loss': 2.1813, 'grad_norm': 4.6399054527282715, 'learning_rate': 0.00013966689782720317, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.28it/s]                                               {'loss': 2.3838, 'grad_norm': 4.673993110656738, 'learning_rate': 0.00013721660137409433, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.28it/s] 28%|██▊       | 21/75 [00:00<00:02, 22.40it/s]                                               {'loss': 2.2366, 'grad_norm': 3.691110849380493, 'learning_rate': 0.0001347663049209855, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 22.40it/s]                                               {'loss': 2.2275, 'grad_norm': 4.243061065673828, 'learning_rate': 0.00013231600846787665, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 22.40it/s]                                               {'loss': 2.129, 'grad_norm': 4.6750006675720215, 'learning_rate': 0.00012986571201476784, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 22.40it/s] 32%|███▏      | 24/75 [00:01<00:02, 19.95it/s]                                               {'loss': 2.2861, 'grad_norm': 5.138463973999023, 'learning_rate': 0.00012741541556165903, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 19.95it/s]                                               {'loss': 2.4754, 'grad_norm': 4.089526653289795, 'learning_rate': 0.0001249651191085502, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 19.95it/s]                                               {'loss': 2.4116, 'grad_norm': 4.309780120849609, 'learning_rate': 0.00012251482265544135, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 19.95it/s] 36%|███▌      | 27/75 [00:01<00:02, 19.19it/s]                                               {'loss': 2.2661, 'grad_norm': 6.475978374481201, 'learning_rate': 0.00012006452620233253, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 19.19it/s]                                               {'loss': 2.2116, 'grad_norm': 4.6284260749816895, 'learning_rate': 0.00011761422974922372, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 19.19it/s]                                               {'loss': 2.3523, 'grad_norm': 3.948277235031128, 'learning_rate': 0.00011516393329611489, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 19.19it/s] 40%|████      | 30/75 [00:01<00:02, 21.42it/s]                                               {'loss': 1.5974, 'grad_norm': 8.61447811126709, 'learning_rate': 0.00011271363684300604, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 21.42it/s]                                               {'loss': 2.3851, 'grad_norm': 4.155900001525879, 'learning_rate': 0.00011026334038989722, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:02, 21.42it/s]                                               {'loss': 2.3328, 'grad_norm': 3.789154529571533, 'learning_rate': 0.0001078130439367884, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:02, 21.42it/s] 44%|████▍     | 33/75 [00:01<00:01, 21.98it/s]                                               {'loss': 2.1141, 'grad_norm': 4.410503387451172, 'learning_rate': 0.00010536274748367958, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 21.98it/s]                                               {'loss': 2.1889, 'grad_norm': 3.983306646347046, 'learning_rate': 0.00010291245103057075, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 21.98it/s]                                               {'loss': 2.2082, 'grad_norm': 4.343626022338867, 'learning_rate': 0.00010046215457746192, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 21.98it/s] 48%|████▊     | 36/75 [00:01<00:01, 21.85it/s]                                               {'loss': 2.227, 'grad_norm': 4.09589147567749, 'learning_rate': 9.801185812435309e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 21.85it/s]                                               {'loss': 2.1867, 'grad_norm': 3.796649217605591, 'learning_rate': 9.556156167124427e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 21.85it/s]                                               {'loss': 2.333, 'grad_norm': 3.9862687587738037, 'learning_rate': 9.311126521813544e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 21.85it/s] 52%|█████▏    | 39/75 [00:01<00:01, 21.48it/s]                                               {'loss': 2.2491, 'grad_norm': 4.219217777252197, 'learning_rate': 9.066096876502662e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 21.48it/s]                                               {'loss': 2.2017, 'grad_norm': 4.609579086303711, 'learning_rate': 8.821067231191778e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 21.48it/s]                                               {'loss': 2.413, 'grad_norm': 6.41441535949707, 'learning_rate': 8.576037585880895e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 21.48it/s] 56%|█████▌    | 42/75 [00:02<00:01, 19.71it/s]                                               {'loss': 2.2088, 'grad_norm': 4.5038743019104, 'learning_rate': 8.331007940570013e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 19.71it/s]                                               {'loss': 2.0904, 'grad_norm': 4.535524845123291, 'learning_rate': 8.08597829525913e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 19.71it/s]                                               {'loss': 2.1742, 'grad_norm': 4.339197158813477, 'learning_rate': 7.840948649948248e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 19.71it/s] 60%|██████    | 45/75 [00:02<00:01, 20.96it/s]                                               {'loss': 1.4465, 'grad_norm': 11.424154281616211, 'learning_rate': 7.595919004637364e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 20.96it/s]                                               {'loss': 2.3225, 'grad_norm': 3.6967122554779053, 'learning_rate': 7.350889359326483e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 20.96it/s]                                               {'loss': 2.3355, 'grad_norm': 3.9057822227478027, 'learning_rate': 7.105859714015599e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 20.96it/s] 64%|██████▍   | 48/75 [00:02<00:01, 20.71it/s]                                               {'loss': 1.9966, 'grad_norm': 3.4778435230255127, 'learning_rate': 6.860830068704717e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 20.71it/s]                                               {'loss': 2.2059, 'grad_norm': 4.867492198944092, 'learning_rate': 6.615800423393833e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 20.71it/s]                                               {'loss': 2.0323, 'grad_norm': 5.684849262237549, 'learning_rate': 6.370770778082952e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 20.71it/s] 68%|██████▊   | 51/75 [00:02<00:01, 19.44it/s]                                               {'loss': 2.1166, 'grad_norm': 4.2048444747924805, 'learning_rate': 6.125741132772068e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 19.44it/s]                                               {'loss': 2.425, 'grad_norm': 5.2685017585754395, 'learning_rate': 5.880711487461186e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 19.44it/s] 71%|███████   | 53/75 [00:02<00:01, 19.46it/s]                                               {'loss': 2.2874, 'grad_norm': 3.294579029083252, 'learning_rate': 5.635681842150302e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:01, 19.46it/s]                                               {'loss': 2.1607, 'grad_norm': 4.7084126472473145, 'learning_rate': 5.39065219683942e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:01, 19.46it/s]                                               {'loss': 2.1866, 'grad_norm': 3.7280898094177246, 'learning_rate': 5.145622551528538e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:01, 19.46it/s] 75%|███████▍  | 56/75 [00:02<00:00, 19.72it/s]                                               {'loss': 2.3688, 'grad_norm': 5.656732082366943, 'learning_rate': 4.9005929062176545e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 19.72it/s]                                               {'loss': 2.0841, 'grad_norm': 3.61380672454834, 'learning_rate': 4.655563260906772e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 19.72it/s]                                               {'loss': 2.356, 'grad_norm': 3.955803155899048, 'learning_rate': 4.410533615595889e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 19.72it/s] 79%|███████▊  | 59/75 [00:02<00:00, 20.14it/s]                                               {'loss': 2.0981, 'grad_norm': 4.2627410888671875, 'learning_rate': 4.1655039702850064e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 20.14it/s]                                               {'loss': 2.06, 'grad_norm': 10.522159576416016, 'learning_rate': 3.920474324974124e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 20.14it/s]                                               {'loss': 2.1451, 'grad_norm': 4.274492263793945, 'learning_rate': 3.6754446796632414e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 20.14it/s] 83%|████████▎ | 62/75 [00:02<00:00, 21.56it/s]                                               {'loss': 2.2794, 'grad_norm': 4.150989055633545, 'learning_rate': 3.430415034352358e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 21.56it/s]                                               {'loss': 2.3116, 'grad_norm': 4.1382880210876465, 'learning_rate': 3.185385389041476e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 21.56it/s]                                               {'loss': 2.2888, 'grad_norm': 6.681483268737793, 'learning_rate': 2.940355743730593e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 21.56it/s] 87%|████████▋ | 65/75 [00:03<00:00, 21.84it/s]                                               {'loss': 2.1923, 'grad_norm': 3.483548164367676, 'learning_rate': 2.69532609841971e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 21.84it/s]                                               {'loss': 2.2093, 'grad_norm': 4.453792095184326, 'learning_rate': 2.4502964531088273e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 21.84it/s]                                               {'loss': 2.2133, 'grad_norm': 3.967689037322998, 'learning_rate': 2.2052668077979444e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 21.84it/s] 91%|█████████ | 68/75 [00:03<00:00, 22.27it/s]                                               {'loss': 2.3262, 'grad_norm': 5.331818103790283, 'learning_rate': 1.960237162487062e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 22.27it/s]                                               {'loss': 2.1218, 'grad_norm': 3.923917531967163, 'learning_rate': 1.715207517176179e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 22.27it/s]                                               {'loss': 2.1391, 'grad_norm': 4.098943710327148, 'learning_rate': 1.4701778718652965e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 22.27it/s] 95%|█████████▍| 71/75 [00:03<00:00, 22.28it/s]                                               {'loss': 2.0932, 'grad_norm': 3.644442319869995, 'learning_rate': 1.2251482265544136e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 22.28it/s]                                               {'loss': 1.9234, 'grad_norm': 3.446315288543701, 'learning_rate': 9.80118581243531e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 22.28it/s]                                               {'loss': 2.2996, 'grad_norm': 4.410935878753662, 'learning_rate': 7.350889359326482e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 22.28it/s] 99%|█████████▊| 74/75 [00:03<00:00, 20.61it/s]                                               {'loss': 2.1112, 'grad_norm': 3.813199758529663, 'learning_rate': 4.900592906217655e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 20.61it/s]                                               {'loss': 1.646, 'grad_norm': 9.677570343017578, 'learning_rate': 2.4502964531088274e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 20.61it/s]                                               {'train_runtime': 3.7593, 'train_samples_per_second': 300.589, 'train_steps_per_second': 19.951, 'train_loss': 2.230870855649312, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 20.61it/s]100%|██████████| 75/75 [00:03<00:00, 19.99it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:28
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]  1%|▏         | 1/75 [00:00<00:07,  9.84it/s]                                              {'loss': 2.5184, 'grad_norm': 4.748481273651123, 'learning_rate': 0.00018377223398316204, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:07,  9.84it/s]                                              {'loss': 2.4341, 'grad_norm': 7.312705039978027, 'learning_rate': 0.00018132193753005323, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:07,  9.84it/s]  4%|▍         | 3/75 [00:00<00:05, 14.05it/s]                                              {'loss': 2.4235, 'grad_norm': 5.026636600494385, 'learning_rate': 0.0001788716410769444, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:05, 14.05it/s]                                              {'loss': 2.3651, 'grad_norm': 4.3968048095703125, 'learning_rate': 0.00017642134462383556, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:05, 14.05it/s]  7%|▋         | 5/75 [00:00<00:05, 13.87it/s]                                              {'loss': 2.3483, 'grad_norm': 4.77178430557251, 'learning_rate': 0.00017397104817072674, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:05, 13.87it/s]                                              {'loss': 2.4013, 'grad_norm': 4.486502647399902, 'learning_rate': 0.0001715207517176179, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:04, 13.87it/s]  9%|▉         | 7/75 [00:00<00:05, 12.90it/s]                                              {'loss': 2.2913, 'grad_norm': 5.310047626495361, 'learning_rate': 0.0001690704552645091, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:05, 12.90it/s]                                              {'loss': 2.5459, 'grad_norm': 4.610126495361328, 'learning_rate': 0.00016662015881140026, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:05, 12.90it/s] 12%|█▏        | 9/75 [00:00<00:05, 12.93it/s]                                              {'loss': 2.2459, 'grad_norm': 4.067759990692139, 'learning_rate': 0.00016416986235829142, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:05, 12.93it/s]                                              {'loss': 2.0544, 'grad_norm': 3.953735113143921, 'learning_rate': 0.0001617195659051826, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:05, 12.93it/s] 15%|█▍        | 11/75 [00:00<00:04, 13.86it/s]                                               {'loss': 2.269, 'grad_norm': 4.448610305786133, 'learning_rate': 0.00015926926945207377, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:04, 13.86it/s]                                               {'loss': 2.4019, 'grad_norm': 4.070480823516846, 'learning_rate': 0.00015681897299896496, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:04, 13.86it/s] 17%|█▋        | 13/75 [00:00<00:04, 14.21it/s]                                               {'loss': 2.2123, 'grad_norm': 5.527340888977051, 'learning_rate': 0.00015436867654585612, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:04, 14.21it/s]                                               {'loss': 2.4511, 'grad_norm': 5.18073034286499, 'learning_rate': 0.00015191838009274728, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:01<00:04, 14.21it/s] 20%|██        | 15/75 [00:01<00:03, 15.25it/s]                                               {'loss': 2.3141, 'grad_norm': 15.266014099121094, 'learning_rate': 0.00014946808363963847, 'epoch': 1.0}
 20%|██        | 15/75 [00:01<00:03, 15.25it/s]                                               {'loss': 2.1752, 'grad_norm': 4.49263334274292, 'learning_rate': 0.00014701778718652966, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:01<00:03, 15.25it/s] 23%|██▎       | 17/75 [00:01<00:03, 15.67it/s]                                               {'loss': 2.1676, 'grad_norm': 3.8249611854553223, 'learning_rate': 0.0001445674907334208, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:01<00:03, 15.67it/s]                                               {'loss': 2.0553, 'grad_norm': 3.4483377933502197, 'learning_rate': 0.00014211719428031198, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:01<00:03, 15.67it/s] 25%|██▌       | 19/75 [00:01<00:03, 16.46it/s]                                               {'loss': 2.2322, 'grad_norm': 4.181367874145508, 'learning_rate': 0.00013966689782720317, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:01<00:03, 16.46it/s]                                               {'loss': 2.2611, 'grad_norm': 5.067974090576172, 'learning_rate': 0.00013721660137409433, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:01<00:03, 16.46it/s] 28%|██▊       | 21/75 [00:01<00:03, 16.11it/s]                                               {'loss': 2.2664, 'grad_norm': 3.9885799884796143, 'learning_rate': 0.0001347663049209855, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:03, 16.11it/s]                                               {'loss': 2.328, 'grad_norm': 4.615875720977783, 'learning_rate': 0.00013231600846787665, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:03, 16.11it/s] 31%|███       | 23/75 [00:01<00:03, 15.36it/s]                                               {'loss': 2.3199, 'grad_norm': 5.7270002365112305, 'learning_rate': 0.00012986571201476784, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:03, 15.36it/s]                                               {'loss': 2.2025, 'grad_norm': 3.5428335666656494, 'learning_rate': 0.00012741541556165903, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:03, 15.36it/s] 33%|███▎      | 25/75 [00:01<00:03, 14.19it/s]                                               {'loss': 2.1236, 'grad_norm': 3.771693229675293, 'learning_rate': 0.0001249651191085502, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:03, 14.19it/s]                                               {'loss': 2.1953, 'grad_norm': 3.8384509086608887, 'learning_rate': 0.00012251482265544135, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:03, 14.19it/s] 36%|███▌      | 27/75 [00:01<00:03, 14.27it/s]                                               {'loss': 2.3634, 'grad_norm': 5.82023286819458, 'learning_rate': 0.00012006452620233253, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:03, 14.27it/s]                                               {'loss': 2.2078, 'grad_norm': 4.325592517852783, 'learning_rate': 0.00011761422974922372, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:03, 14.27it/s]                                               {'loss': 2.1075, 'grad_norm': 3.0809874534606934, 'learning_rate': 0.00011516393329611489, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:03, 14.27it/s] 40%|████      | 30/75 [00:02<00:02, 16.56it/s]                                               {'loss': 2.4158, 'grad_norm': 12.804662704467773, 'learning_rate': 0.00011271363684300604, 'epoch': 2.0}
 40%|████      | 30/75 [00:02<00:02, 16.56it/s]                                               {'loss': 2.2807, 'grad_norm': 4.857541561126709, 'learning_rate': 0.00011026334038989722, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:02<00:02, 16.56it/s] 43%|████▎     | 32/75 [00:02<00:02, 17.04it/s]                                               {'loss': 2.336, 'grad_norm': 3.8229904174804688, 'learning_rate': 0.0001078130439367884, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:02<00:02, 17.04it/s]                                               {'loss': 2.3444, 'grad_norm': 3.69671630859375, 'learning_rate': 0.00010536274748367958, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:02<00:02, 17.04it/s] 45%|████▌     | 34/75 [00:02<00:02, 17.44it/s]                                               {'loss': 1.9809, 'grad_norm': 3.792346239089966, 'learning_rate': 0.00010291245103057075, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:02<00:02, 17.44it/s]                                               {'loss': 2.1233, 'grad_norm': 3.5746896266937256, 'learning_rate': 0.00010046215457746192, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:02<00:02, 17.44it/s] 48%|████▊     | 36/75 [00:02<00:02, 17.42it/s]                                               {'loss': 2.1733, 'grad_norm': 5.188485622406006, 'learning_rate': 9.801185812435309e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:02<00:02, 17.42it/s]                                               {'loss': 2.1278, 'grad_norm': 3.4064173698425293, 'learning_rate': 9.556156167124427e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:02<00:02, 17.42it/s] 51%|█████     | 38/75 [00:02<00:02, 17.68it/s]                                               {'loss': 1.9719, 'grad_norm': 3.898451566696167, 'learning_rate': 9.311126521813544e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:02<00:02, 17.68it/s]                                               {'loss': 2.1024, 'grad_norm': 4.140170097351074, 'learning_rate': 9.066096876502662e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:02<00:02, 17.68it/s] 53%|█████▎    | 40/75 [00:02<00:01, 17.71it/s]                                               {'loss': 2.3154, 'grad_norm': 3.9089550971984863, 'learning_rate': 8.821067231191778e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:02<00:01, 17.71it/s]                                               {'loss': 2.3629, 'grad_norm': 4.317075252532959, 'learning_rate': 8.576037585880895e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]                                               {'loss': 2.0213, 'grad_norm': 4.43864631652832, 'learning_rate': 8.331007940570013e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 17.71it/s] 57%|█████▋    | 43/75 [00:02<00:01, 18.70it/s]                                               {'loss': 2.3752, 'grad_norm': 3.2047622203826904, 'learning_rate': 8.08597829525913e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 18.70it/s]                                               {'loss': 2.2786, 'grad_norm': 4.0875959396362305, 'learning_rate': 7.840948649948248e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 18.70it/s]                                               {'loss': 2.0986, 'grad_norm': 8.098910331726074, 'learning_rate': 7.595919004637364e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 18.70it/s] 61%|██████▏   | 46/75 [00:02<00:01, 20.35it/s]                                               {'loss': 2.0943, 'grad_norm': 4.298709392547607, 'learning_rate': 7.350889359326483e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 20.35it/s]                                               {'loss': 2.3758, 'grad_norm': 3.074965238571167, 'learning_rate': 7.105859714015599e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 20.35it/s]                                               {'loss': 2.2257, 'grad_norm': 5.085921764373779, 'learning_rate': 6.860830068704717e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 20.35it/s] 65%|██████▌   | 49/75 [00:02<00:01, 21.39it/s]                                               {'loss': 1.9718, 'grad_norm': 5.264214038848877, 'learning_rate': 6.615800423393833e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 21.39it/s]                                               {'loss': 2.1954, 'grad_norm': 3.7172348499298096, 'learning_rate': 6.370770778082952e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 21.39it/s]                                               {'loss': 2.3417, 'grad_norm': 4.4052205085754395, 'learning_rate': 6.125741132772068e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:03<00:01, 21.39it/s] 69%|██████▉   | 52/75 [00:03<00:01, 22.74it/s]                                               {'loss': 2.4419, 'grad_norm': 5.614618301391602, 'learning_rate': 5.880711487461186e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:03<00:01, 22.74it/s]                                               {'loss': 2.2173, 'grad_norm': 4.0985107421875, 'learning_rate': 5.635681842150302e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:03<00:00, 22.74it/s]                                               {'loss': 2.1533, 'grad_norm': 4.592746257781982, 'learning_rate': 5.39065219683942e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:03<00:00, 22.74it/s] 73%|███████▎  | 55/75 [00:03<00:00, 23.24it/s]                                               {'loss': 2.0878, 'grad_norm': 3.278588056564331, 'learning_rate': 5.145622551528538e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:03<00:00, 23.24it/s]                                               {'loss': 2.2242, 'grad_norm': 3.844611883163452, 'learning_rate': 4.9005929062176545e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:03<00:00, 23.24it/s]                                               {'loss': 1.9805, 'grad_norm': 3.626427412033081, 'learning_rate': 4.655563260906772e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:03<00:00, 23.24it/s] 77%|███████▋  | 58/75 [00:03<00:00, 23.64it/s]                                               {'loss': 2.0388, 'grad_norm': 3.272123098373413, 'learning_rate': 4.410533615595889e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:03<00:00, 23.64it/s]                                               {'loss': 2.1404, 'grad_norm': 4.033081531524658, 'learning_rate': 4.1655039702850064e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:03<00:00, 23.64it/s]                                               {'loss': 1.7845, 'grad_norm': 6.68271541595459, 'learning_rate': 3.920474324974124e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:03<00:00, 23.64it/s]                                               {'loss': 1.9181, 'grad_norm': 4.32671594619751, 'learning_rate': 3.6754446796632414e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:03<00:00, 23.64it/s] 83%|████████▎ | 62/75 [00:03<00:00, 25.73it/s]                                               {'loss': 2.0441, 'grad_norm': 4.5590362548828125, 'learning_rate': 3.430415034352358e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:03<00:00, 25.73it/s]                                               {'loss': 2.4232, 'grad_norm': 4.102708339691162, 'learning_rate': 3.185385389041476e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 25.73it/s]                                               {'loss': 2.0433, 'grad_norm': 4.045620441436768, 'learning_rate': 2.940355743730593e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 25.73it/s] 87%|████████▋ | 65/75 [00:03<00:00, 25.42it/s]                                               {'loss': 2.1029, 'grad_norm': 3.96368408203125, 'learning_rate': 2.69532609841971e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 25.42it/s]                                               {'loss': 2.0806, 'grad_norm': 3.1945457458496094, 'learning_rate': 2.4502964531088273e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 25.42it/s]                                               {'loss': 2.1916, 'grad_norm': 4.8374481201171875, 'learning_rate': 2.2052668077979444e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 25.42it/s] 91%|█████████ | 68/75 [00:03<00:00, 25.36it/s]                                               {'loss': 2.1351, 'grad_norm': 4.301885604858398, 'learning_rate': 1.960237162487062e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 25.36it/s]                                               {'loss': 2.3001, 'grad_norm': 3.628718376159668, 'learning_rate': 1.715207517176179e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 25.36it/s]                                               {'loss': 2.2397, 'grad_norm': 4.791633129119873, 'learning_rate': 1.4701778718652965e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 25.36it/s] 95%|█████████▍| 71/75 [00:03<00:00, 25.25it/s]                                               {'loss': 2.3269, 'grad_norm': 3.695636034011841, 'learning_rate': 1.2251482265544136e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 25.25it/s]                                               {'loss': 2.1173, 'grad_norm': 3.4734838008880615, 'learning_rate': 9.80118581243531e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 25.25it/s]                                               {'loss': 1.9638, 'grad_norm': 4.6703572273254395, 'learning_rate': 7.350889359326482e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 25.25it/s] 99%|█████████▊| 74/75 [00:03<00:00, 24.34it/s]                                               {'loss': 2.0144, 'grad_norm': 4.5594482421875, 'learning_rate': 4.900592906217655e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.34it/s]                                               {'loss': 1.7624, 'grad_norm': 6.85484504699707, 'learning_rate': 2.4502964531088274e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.34it/s]                                               {'train_runtime': 4.1136, 'train_samples_per_second': 274.697, 'train_steps_per_second': 18.232, 'train_loss': 2.207038272221883, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 24.34it/s]100%|██████████| 75/75 [00:04<00:00, 18.26it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:40
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4924, 'grad_norm': 6.332103729248047, 'learning_rate': 0.00018377223398316204, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.30it/s]                                              {'loss': 2.6219, 'grad_norm': 6.476021766662598, 'learning_rate': 0.00018132193753005323, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.15it/s]  4%|▍         | 3/75 [00:00<00:03, 20.54it/s]                                              {'loss': 2.3057, 'grad_norm': 6.24640417098999, 'learning_rate': 0.0001788716410769444, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 20.54it/s]                                              {'loss': 2.66, 'grad_norm': 4.401867866516113, 'learning_rate': 0.00017642134462383556, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 20.54it/s]                                              {'loss': 2.3805, 'grad_norm': 4.3263840675354, 'learning_rate': 0.00017397104817072674, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 20.54it/s]  8%|▊         | 6/75 [00:00<00:04, 15.28it/s]                                              {'loss': 2.3987, 'grad_norm': 3.797194719314575, 'learning_rate': 0.0001715207517176179, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:04, 15.28it/s]                                              {'loss': 2.546, 'grad_norm': 5.094162940979004, 'learning_rate': 0.0001690704552645091, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:04, 15.28it/s]                                              {'loss': 2.4216, 'grad_norm': 4.411504745483398, 'learning_rate': 0.00016662015881140026, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:04, 15.28it/s] 12%|█▏        | 9/75 [00:00<00:03, 17.48it/s]                                              {'loss': 2.4561, 'grad_norm': 3.886112928390503, 'learning_rate': 0.00016416986235829142, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 17.48it/s]                                              {'loss': 2.6864, 'grad_norm': 4.178238868713379, 'learning_rate': 0.0001617195659051826, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 17.48it/s] 15%|█▍        | 11/75 [00:00<00:03, 17.87it/s]                                               {'loss': 2.4424, 'grad_norm': 3.5901026725769043, 'learning_rate': 0.00015926926945207377, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 17.87it/s]                                               {'loss': 2.4555, 'grad_norm': 4.432075500488281, 'learning_rate': 0.00015681897299896496, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:03, 17.87it/s] 17%|█▋        | 13/75 [00:00<00:03, 17.76it/s]                                               {'loss': 2.4137, 'grad_norm': 5.63605260848999, 'learning_rate': 0.00015436867654585612, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:03, 17.76it/s]                                               {'loss': 2.1653, 'grad_norm': 3.920212984085083, 'learning_rate': 0.00015191838009274728, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:03, 17.76it/s] 20%|██        | 15/75 [00:00<00:03, 17.52it/s]                                               {'loss': 2.3461, 'grad_norm': 12.970057487487793, 'learning_rate': 0.00014946808363963847, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:03, 17.52it/s]                                               {'loss': 2.3537, 'grad_norm': 4.364680767059326, 'learning_rate': 0.00014701778718652966, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:03, 17.52it/s]                                               {'loss': 2.3555, 'grad_norm': 5.278779029846191, 'learning_rate': 0.0001445674907334208, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:03, 17.52it/s] 24%|██▍       | 18/75 [00:00<00:02, 19.85it/s]                                               {'loss': 2.3441, 'grad_norm': 4.810344219207764, 'learning_rate': 0.00014211719428031198, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 19.85it/s]                                               {'loss': 2.2765, 'grad_norm': 4.105788707733154, 'learning_rate': 0.00013966689782720317, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:01<00:02, 19.85it/s]                                               {'loss': 2.2823, 'grad_norm': 5.305562496185303, 'learning_rate': 0.00013721660137409433, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:01<00:02, 19.85it/s] 28%|██▊       | 21/75 [00:01<00:02, 19.14it/s]                                               {'loss': 2.284, 'grad_norm': 3.743568181991577, 'learning_rate': 0.0001347663049209855, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:02, 19.14it/s]                                               {'loss': 2.391, 'grad_norm': 5.051965713500977, 'learning_rate': 0.00013231600846787665, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 19.14it/s] 31%|███       | 23/75 [00:01<00:02, 18.51it/s]                                               {'loss': 2.2793, 'grad_norm': 4.444276809692383, 'learning_rate': 0.00012986571201476784, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 18.51it/s]                                               {'loss': 2.3362, 'grad_norm': 4.268221855163574, 'learning_rate': 0.00012741541556165903, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 18.51it/s] 33%|███▎      | 25/75 [00:01<00:02, 18.62it/s]                                               {'loss': 2.6005, 'grad_norm': 4.785708904266357, 'learning_rate': 0.0001249651191085502, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 18.62it/s]                                               {'loss': 2.3088, 'grad_norm': 5.262396335601807, 'learning_rate': 0.00012251482265544135, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 18.62it/s]                                               {'loss': 2.2602, 'grad_norm': 5.263571739196777, 'learning_rate': 0.00012006452620233253, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 18.62it/s] 37%|███▋      | 28/75 [00:01<00:02, 19.31it/s]                                               {'loss': 2.5889, 'grad_norm': 5.394684791564941, 'learning_rate': 0.00011761422974922372, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 19.31it/s]                                               {'loss': 2.267, 'grad_norm': 3.339723825454712, 'learning_rate': 0.00011516393329611489, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 19.31it/s]                                               {'loss': 1.852, 'grad_norm': 12.724706649780273, 'learning_rate': 0.00011271363684300604, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 19.31it/s] 41%|████▏     | 31/75 [00:01<00:02, 20.27it/s]                                               {'loss': 2.2958, 'grad_norm': 5.7014851570129395, 'learning_rate': 0.00011026334038989722, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:02, 20.27it/s]                                               {'loss': 2.45, 'grad_norm': 4.653631687164307, 'learning_rate': 0.0001078130439367884, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:02, 20.27it/s]                                               {'loss': 2.4208, 'grad_norm': 5.102058410644531, 'learning_rate': 0.00010536274748367958, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:02, 20.27it/s] 45%|████▌     | 34/75 [00:01<00:02, 17.77it/s]                                               {'loss': 2.0567, 'grad_norm': 4.647399425506592, 'learning_rate': 0.00010291245103057075, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:02, 17.77it/s]                                               {'loss': 2.1994, 'grad_norm': 3.6044623851776123, 'learning_rate': 0.00010046215457746192, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:02, 17.77it/s]                                               {'loss': 2.2338, 'grad_norm': 4.129918575286865, 'learning_rate': 9.801185812435309e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:02, 17.77it/s] 49%|████▉     | 37/75 [00:01<00:01, 19.21it/s]                                               {'loss': 2.2352, 'grad_norm': 3.893794536590576, 'learning_rate': 9.556156167124427e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 19.21it/s]                                               {'loss': 2.3144, 'grad_norm': 4.77910852432251, 'learning_rate': 9.311126521813544e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:02<00:01, 19.21it/s] 52%|█████▏    | 39/75 [00:02<00:01, 18.91it/s]                                               {'loss': 2.2015, 'grad_norm': 4.638554096221924, 'learning_rate': 9.066096876502662e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:02<00:01, 18.91it/s]                                               {'loss': 2.2235, 'grad_norm': 5.192842960357666, 'learning_rate': 8.821067231191778e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:02<00:01, 18.91it/s] 55%|█████▍    | 41/75 [00:02<00:01, 19.16it/s]                                               {'loss': 2.1998, 'grad_norm': 4.123779296875, 'learning_rate': 8.576037585880895e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:02<00:01, 19.16it/s]                                               {'loss': 2.4215, 'grad_norm': 4.754636287689209, 'learning_rate': 8.331007940570013e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 19.16it/s]                                               {'loss': 2.1797, 'grad_norm': 3.8633368015289307, 'learning_rate': 8.08597829525913e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 19.16it/s] 59%|█████▊    | 44/75 [00:02<00:01, 19.97it/s]                                               {'loss': 2.5629, 'grad_norm': 3.865206241607666, 'learning_rate': 7.840948649948248e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 19.97it/s]                                               {'loss': 2.5697, 'grad_norm': 18.053070068359375, 'learning_rate': 7.595919004637364e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 19.97it/s]                                               {'loss': 2.2502, 'grad_norm': 3.9460983276367188, 'learning_rate': 7.350889359326483e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 19.97it/s] 63%|██████▎   | 47/75 [00:02<00:01, 19.82it/s]                                               {'loss': 2.0589, 'grad_norm': 3.3161323070526123, 'learning_rate': 7.105859714015599e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 19.82it/s]                                               {'loss': 2.2483, 'grad_norm': 4.375794410705566, 'learning_rate': 6.860830068704717e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 19.82it/s]                                               {'loss': 2.4223, 'grad_norm': 6.440756320953369, 'learning_rate': 6.615800423393833e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 19.82it/s] 67%|██████▋   | 50/75 [00:02<00:01, 20.05it/s]                                               {'loss': 2.3751, 'grad_norm': 4.3457112312316895, 'learning_rate': 6.370770778082952e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 20.05it/s]                                               {'loss': 2.303, 'grad_norm': 4.861730098724365, 'learning_rate': 6.125741132772068e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 20.05it/s]                                               {'loss': 2.2121, 'grad_norm': 5.1119384765625, 'learning_rate': 5.880711487461186e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 20.05it/s] 71%|███████   | 53/75 [00:02<00:01, 20.40it/s]                                               {'loss': 2.2132, 'grad_norm': 3.929511547088623, 'learning_rate': 5.635681842150302e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:01, 20.40it/s]                                               {'loss': 2.2907, 'grad_norm': 4.783332347869873, 'learning_rate': 5.39065219683942e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:01, 20.40it/s]                                               {'loss': 2.3236, 'grad_norm': 4.130618095397949, 'learning_rate': 5.145622551528538e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 20.40it/s] 75%|███████▍  | 56/75 [00:02<00:00, 19.31it/s]                                               {'loss': 2.1803, 'grad_norm': 4.105151176452637, 'learning_rate': 4.9005929062176545e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 19.31it/s]                                               {'loss': 2.268, 'grad_norm': 4.452687740325928, 'learning_rate': 4.655563260906772e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:03<00:00, 19.31it/s] 77%|███████▋  | 58/75 [00:03<00:00, 19.15it/s]                                               {'loss': 2.3046, 'grad_norm': 3.9266834259033203, 'learning_rate': 4.410533615595889e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:03<00:00, 19.15it/s]                                               {'loss': 2.2675, 'grad_norm': 5.004158973693848, 'learning_rate': 4.1655039702850064e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:03<00:00, 19.15it/s]                                               {'loss': 1.8981, 'grad_norm': 19.48515510559082, 'learning_rate': 3.920474324974124e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:03<00:00, 19.15it/s] 81%|████████▏ | 61/75 [00:03<00:00, 20.51it/s]                                               {'loss': 2.3876, 'grad_norm': 4.403111934661865, 'learning_rate': 3.6754446796632414e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:03<00:00, 20.51it/s]                                               {'loss': 2.2044, 'grad_norm': 4.684285640716553, 'learning_rate': 3.430415034352358e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:03<00:00, 20.51it/s]                                               {'loss': 2.0862, 'grad_norm': 3.7558488845825195, 'learning_rate': 3.185385389041476e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 20.51it/s] 85%|████████▌ | 64/75 [00:03<00:00, 20.40it/s]                                               {'loss': 2.1954, 'grad_norm': 4.265077114105225, 'learning_rate': 2.940355743730593e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 20.40it/s]                                               {'loss': 2.3912, 'grad_norm': 4.593996047973633, 'learning_rate': 2.69532609841971e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 20.40it/s]                                               {'loss': 2.0668, 'grad_norm': 5.146739959716797, 'learning_rate': 2.4502964531088273e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 20.40it/s] 89%|████████▉ | 67/75 [00:03<00:00, 18.37it/s]                                               {'loss': 2.0195, 'grad_norm': 3.8450353145599365, 'learning_rate': 2.2052668077979444e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 18.37it/s]                                               {'loss': 2.1849, 'grad_norm': 3.4542243480682373, 'learning_rate': 1.960237162487062e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 18.37it/s] 92%|█████████▏| 69/75 [00:03<00:00, 17.38it/s]                                               {'loss': 2.2334, 'grad_norm': 5.795340061187744, 'learning_rate': 1.715207517176179e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 17.38it/s]                                               {'loss': 2.3093, 'grad_norm': 3.540501832962036, 'learning_rate': 1.4701778718652965e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 17.38it/s]                                               {'loss': 2.28, 'grad_norm': 5.007895469665527, 'learning_rate': 1.2251482265544136e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 17.38it/s] 96%|█████████▌| 72/75 [00:03<00:00, 18.06it/s]                                               {'loss': 2.2218, 'grad_norm': 3.487643241882324, 'learning_rate': 9.80118581243531e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 18.06it/s]                                               {'loss': 2.4227, 'grad_norm': 5.02187967300415, 'learning_rate': 7.350889359326482e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 18.06it/s] 99%|█████████▊| 74/75 [00:03<00:00, 18.27it/s]                                               {'loss': 2.3355, 'grad_norm': 5.06866979598999, 'learning_rate': 4.900592906217655e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 18.27it/s]                                               {'loss': 2.3964, 'grad_norm': 12.317550659179688, 'learning_rate': 2.4502964531088274e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 18.27it/s]                                               {'train_runtime': 4.1772, 'train_samples_per_second': 270.517, 'train_steps_per_second': 17.955, 'train_loss': 2.3131764904658, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 18.27it/s]100%|██████████| 75/75 [00:04<00:00, 17.96it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1427, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1136, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1123, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1274, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1410, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1236, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1429, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1237, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1334, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1269, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1486, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1191, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 61.16it/s]  3%|▎         | 14/471 [00:00<00:10, 43.22it/s]  4%|▍         | 19/471 [00:00<00:11, 37.92it/s]  5%|▍         | 23/471 [00:00<00:12, 36.64it/s]  6%|▌         | 27/471 [00:00<00:11, 37.25it/s]  7%|▋         | 31/471 [00:00<00:12, 36.63it/s]  7%|▋         | 35/471 [00:00<00:12, 34.69it/s]  8%|▊         | 40/471 [00:01<00:12, 35.56it/s]  9%|▉         | 44/471 [00:01<00:12, 34.54it/s] 10%|█         | 48/471 [00:01<00:12, 32.74it/s] 11%|█         | 52/471 [00:01<00:12, 32.31it/s] 12%|█▏        | 56/471 [00:01<00:13, 30.80it/s] 13%|█▎        | 60/471 [00:01<00:13, 31.44it/s] 14%|█▎        | 64/471 [00:01<00:13, 31.17it/s] 14%|█▍        | 68/471 [00:01<00:13, 30.14it/s] 15%|█▌        | 73/471 [00:02<00:11, 33.35it/s] 17%|█▋        | 78/471 [00:02<00:10, 36.67it/s] 18%|█▊        | 83/471 [00:02<00:10, 37.87it/s] 18%|█▊        | 87/471 [00:02<00:10, 36.14it/s] 19%|█▉        | 91/471 [00:02<00:10, 34.63it/s] 20%|██        | 95/471 [00:02<00:11, 32.55it/s] 21%|██        | 99/471 [00:02<00:11, 32.31it/s] 22%|██▏       | 103/471 [00:02<00:11, 33.07it/s] 23%|██▎       | 108/471 [00:03<00:10, 35.84it/s] 24%|██▍       | 113/471 [00:03<00:09, 39.12it/s] 25%|██▌       | 118/471 [00:03<00:08, 41.57it/s] 26%|██▌       | 123/471 [00:03<00:08, 43.05it/s] 27%|██▋       | 128/471 [00:03<00:07, 44.35it/s] 28%|██▊       | 133/471 [00:03<00:07, 44.08it/s] 29%|██▉       | 138/471 [00:03<00:07, 43.42it/s] 30%|███       | 143/471 [00:03<00:07, 44.48it/s] 31%|███▏      | 148/471 [00:03<00:07, 44.01it/s] 33%|███▎      | 154/471 [00:04<00:06, 46.60it/s] 34%|███▍      | 160/471 [00:04<00:06, 48.27it/s] 35%|███▌      | 165/471 [00:04<00:06, 48.00it/s] 36%|███▌      | 170/471 [00:04<00:06, 47.37it/s] 37%|███▋      | 175/471 [00:04<00:06, 46.63it/s] 38%|███▊      | 181/471 [00:04<00:05, 48.64it/s] 40%|███▉      | 187/471 [00:04<00:05, 49.88it/s] 41%|████      | 192/471 [00:04<00:05, 48.33it/s] 42%|████▏     | 198/471 [00:04<00:05, 50.04it/s] 43%|████▎     | 204/471 [00:05<00:05, 50.42it/s] 45%|████▍     | 210/471 [00:05<00:05, 49.73it/s] 46%|████▌     | 215/471 [00:05<00:05, 47.74it/s] 47%|████▋     | 220/471 [00:05<00:05, 47.65it/s] 48%|████▊     | 225/471 [00:05<00:05, 47.94it/s] 49%|████▉     | 231/471 [00:05<00:04, 48.47it/s] 50%|█████     | 236/471 [00:05<00:04, 48.25it/s] 51%|█████▏    | 242/471 [00:05<00:04, 49.22it/s] 52%|█████▏    | 247/471 [00:05<00:04, 49.41it/s] 54%|█████▎    | 253/471 [00:06<00:04, 50.45it/s] 55%|█████▍    | 259/471 [00:06<00:04, 51.12it/s] 56%|█████▋    | 265/471 [00:06<00:03, 51.60it/s] 58%|█████▊    | 271/471 [00:06<00:03, 51.16it/s] 59%|█████▉    | 277/471 [00:06<00:03, 50.87it/s] 60%|██████    | 283/471 [00:06<00:03, 51.57it/s] 61%|██████▏   | 289/471 [00:06<00:03, 51.98it/s] 63%|██████▎   | 295/471 [00:06<00:03, 52.25it/s] 64%|██████▍   | 301/471 [00:07<00:03, 52.34it/s] 65%|██████▌   | 307/471 [00:07<00:03, 52.36it/s] 66%|██████▋   | 313/471 [00:07<00:03, 51.93it/s] 68%|██████▊   | 319/471 [00:07<00:02, 52.31it/s] 69%|██████▉   | 325/471 [00:07<00:02, 50.58it/s] 70%|███████   | 331/471 [00:07<00:02, 50.28it/s] 72%|███████▏  | 337/471 [00:07<00:02, 50.47it/s] 73%|███████▎  | 343/471 [00:07<00:02, 49.83it/s] 74%|███████▍  | 349/471 [00:07<00:02, 50.85it/s] 75%|███████▌  | 355/471 [00:08<00:02, 50.44it/s] 77%|███████▋  | 361/471 [00:08<00:02, 48.78it/s] 78%|███████▊  | 367/471 [00:08<00:02, 50.35it/s] 79%|███████▉  | 373/471 [00:08<00:01, 49.87it/s] 80%|████████  | 379/471 [00:08<00:01, 50.79it/s] 82%|████████▏ | 385/471 [00:08<00:01, 50.66it/s] 83%|████████▎ | 391/471 [00:08<00:01, 43.37it/s] 84%|████████▍ | 396/471 [00:09<00:01, 38.71it/s] 85%|████████▌ | 401/471 [00:09<00:01, 35.02it/s] 86%|████████▌ | 405/471 [00:09<00:02, 32.91it/s] 87%|████████▋ | 409/471 [00:09<00:01, 33.72it/s] 88%|████████▊ | 413/471 [00:09<00:01, 34.69it/s] 89%|████████▊ | 417/471 [00:09<00:01, 34.22it/s] 90%|████████▉ | 423/471 [00:09<00:01, 39.90it/s] 91%|█████████ | 428/471 [00:09<00:01, 42.08it/s] 92%|█████████▏| 433/471 [00:10<00:00, 43.04it/s] 93%|█████████▎| 439/471 [00:10<00:00, 46.19it/s] 94%|█████████▍| 445/471 [00:10<00:00, 48.28it/s] 96%|█████████▌| 451/471 [00:10<00:00, 49.39it/s] 97%|█████████▋| 457/471 [00:10<00:00, 49.84it/s] 98%|█████████▊| 463/471 [00:10<00:00, 48.74it/s] 99%|█████████▉| 468/471 [00:10<00:00, 46.47it/s]100%|██████████| 471/471 [00:10<00:00, 43.68it/s]
{'eval_loss': 2.4319958686828613, 'eval_model_preparation_time': 0.01, 'eval_acc': 0.34453000531067446, 'eval_runtime': 10.8115, 'eval_samples_per_second': 696.669, 'eval_steps_per_second': 43.565}
ROUND:5
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.5189, 'grad_norm': 5.583718776702881, 'learning_rate': 0.00017573593128807148, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 19.41it/s]                                              {'loss': 2.4103, 'grad_norm': 5.257728576660156, 'learning_rate': 0.00017339278553756387, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 22.44it/s]  4%|▍         | 3/75 [00:00<00:03, 23.15it/s]                                              {'loss': 2.502, 'grad_norm': 6.014328479766846, 'learning_rate': 0.00017104963978705625, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.15it/s]                                              {'loss': 2.321, 'grad_norm': 5.13458251953125, 'learning_rate': 0.00016870649403654862, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 23.15it/s]                                              {'loss': 2.525, 'grad_norm': 3.5371246337890625, 'learning_rate': 0.000166363348286041, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 23.15it/s]  8%|▊         | 6/75 [00:00<00:03, 22.12it/s]                                              {'loss': 2.319, 'grad_norm': 4.1749162673950195, 'learning_rate': 0.0001640202025355334, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 22.12it/s]                                              {'loss': 2.2021, 'grad_norm': 4.268430233001709, 'learning_rate': 0.00016167705678502576, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 22.12it/s]                                              {'loss': 2.5217, 'grad_norm': 3.925978899002075, 'learning_rate': 0.00015933391103451814, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 22.12it/s] 12%|█▏        | 9/75 [00:00<00:03, 20.07it/s]                                              {'loss': 2.4404, 'grad_norm': 4.081194877624512, 'learning_rate': 0.0001569907652840105, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 20.07it/s]                                              {'loss': 2.2638, 'grad_norm': 4.492068290710449, 'learning_rate': 0.0001546476195335029, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 20.07it/s]                                               {'loss': 2.3661, 'grad_norm': 4.37999963760376, 'learning_rate': 0.00015230447378299528, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 20.07it/s] 16%|█▌        | 12/75 [00:00<00:02, 21.07it/s]                                               {'loss': 2.3998, 'grad_norm': 4.3046345710754395, 'learning_rate': 0.00014996132803248767, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 21.07it/s]                                               {'loss': 2.4062, 'grad_norm': 3.3600962162017822, 'learning_rate': 0.00014761818228198003, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 21.07it/s]                                               {'loss': 2.3229, 'grad_norm': 4.2667646408081055, 'learning_rate': 0.00014527503653147242, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 21.07it/s] 20%|██        | 15/75 [00:00<00:02, 20.96it/s]                                               {'loss': 3.0681, 'grad_norm': 22.695693969726562, 'learning_rate': 0.0001429318907809648, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 20.96it/s]                                               {'loss': 2.6366, 'grad_norm': 5.4439897537231445, 'learning_rate': 0.0001405887450304572, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 20.96it/s]                                               {'loss': 2.2164, 'grad_norm': 4.601527214050293, 'learning_rate': 0.00013824559927994956, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 20.96it/s] 24%|██▍       | 18/75 [00:00<00:02, 20.20it/s]                                               {'loss': 2.2947, 'grad_norm': 3.2222836017608643, 'learning_rate': 0.00013590245352944195, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 20.20it/s]                                               {'loss': 2.3377, 'grad_norm': 5.113653182983398, 'learning_rate': 0.00013355930777893434, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 20.20it/s]                                               {'loss': 2.5186, 'grad_norm': 5.22444486618042, 'learning_rate': 0.0001312161620284267, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 20.20it/s] 28%|██▊       | 21/75 [00:00<00:02, 21.87it/s]                                               {'loss': 2.4175, 'grad_norm': 4.32861852645874, 'learning_rate': 0.0001288730162779191, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 21.87it/s]                                               {'loss': 2.2933, 'grad_norm': 4.615850925445557, 'learning_rate': 0.00012652987052741145, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 21.87it/s]                                               {'loss': 2.1934, 'grad_norm': 3.755389928817749, 'learning_rate': 0.00012418672477690384, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 21.87it/s] 32%|███▏      | 24/75 [00:01<00:02, 22.34it/s]                                               {'loss': 2.1694, 'grad_norm': 3.750274419784546, 'learning_rate': 0.00012184357902639623, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 22.34it/s]                                               {'loss': 2.2064, 'grad_norm': 4.561243057250977, 'learning_rate': 0.00011950043327588861, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 22.34it/s]                                               {'loss': 2.295, 'grad_norm': 4.056856155395508, 'learning_rate': 0.00011715728752538098, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 22.34it/s] 36%|███▌      | 27/75 [00:01<00:02, 22.81it/s]                                               {'loss': 2.2496, 'grad_norm': 3.9611566066741943, 'learning_rate': 0.00011481414177487336, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 22.81it/s]                                               {'loss': 2.2724, 'grad_norm': 5.44387149810791, 'learning_rate': 0.00011247099602436575, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 22.81it/s]                                               {'loss': 2.1831, 'grad_norm': 3.9626214504241943, 'learning_rate': 0.00011012785027385813, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 22.81it/s] 40%|████      | 30/75 [00:01<00:01, 23.43it/s]                                               {'loss': 2.5813, 'grad_norm': 8.983516693115234, 'learning_rate': 0.0001077847045233505, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.43it/s]                                               {'loss': 2.3176, 'grad_norm': 4.54931116104126, 'learning_rate': 0.00010544155877284288, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 23.43it/s]                                               {'loss': 2.0614, 'grad_norm': 4.2803730964660645, 'learning_rate': 0.00010309841302233527, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 23.43it/s] 44%|████▍     | 33/75 [00:01<00:02, 19.68it/s]                                               {'loss': 2.159, 'grad_norm': 3.9326534271240234, 'learning_rate': 0.00010075526727182766, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:02, 19.68it/s]                                               {'loss': 2.2554, 'grad_norm': 3.584399700164795, 'learning_rate': 9.841212152132003e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:02, 19.68it/s]                                               {'loss': 2.3143, 'grad_norm': 4.428935527801514, 'learning_rate': 9.60689757708124e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:02, 19.68it/s] 48%|████▊     | 36/75 [00:01<00:02, 18.72it/s]                                               {'loss': 2.0559, 'grad_norm': 4.678847789764404, 'learning_rate': 9.372583002030478e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:02, 18.72it/s]                                               {'loss': 2.4806, 'grad_norm': 5.040136814117432, 'learning_rate': 9.138268426979717e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:02, 18.72it/s] 51%|█████     | 38/75 [00:01<00:01, 18.78it/s]                                               {'loss': 2.4434, 'grad_norm': 4.273793697357178, 'learning_rate': 8.903953851928956e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 18.78it/s]                                               {'loss': 2.2549, 'grad_norm': 3.5818471908569336, 'learning_rate': 8.669639276878193e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 18.78it/s] 53%|█████▎    | 40/75 [00:01<00:01, 18.95it/s]                                               {'loss': 2.2977, 'grad_norm': 4.086162090301514, 'learning_rate': 8.435324701827431e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 18.95it/s]                                               {'loss': 2.124, 'grad_norm': 3.1522395610809326, 'learning_rate': 8.20101012677667e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:02<00:01, 18.95it/s]                                               {'loss': 2.4297, 'grad_norm': 5.04941463470459, 'learning_rate': 7.966695551725907e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 18.95it/s] 57%|█████▋    | 43/75 [00:02<00:01, 19.08it/s]                                               {'loss': 2.3044, 'grad_norm': 3.7353975772857666, 'learning_rate': 7.732380976675145e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 19.08it/s]                                               {'loss': 2.2428, 'grad_norm': 4.015520095825195, 'learning_rate': 7.498066401624384e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 19.08it/s] 60%|██████    | 45/75 [00:02<00:01, 18.94it/s]                                               {'loss': 2.0228, 'grad_norm': 10.07005500793457, 'learning_rate': 7.263751826573621e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 18.94it/s]                                               {'loss': 2.4386, 'grad_norm': 4.054537296295166, 'learning_rate': 7.02943725152286e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 18.94it/s] 63%|██████▎   | 47/75 [00:02<00:01, 17.28it/s]                                               {'loss': 2.2313, 'grad_norm': 3.0470478534698486, 'learning_rate': 6.795122676472097e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 17.28it/s]                                               {'loss': 2.3348, 'grad_norm': 3.8459420204162598, 'learning_rate': 6.560808101421335e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 17.28it/s] 65%|██████▌   | 49/75 [00:02<00:01, 17.43it/s]                                               {'loss': 2.3266, 'grad_norm': 4.622140884399414, 'learning_rate': 6.326493526370572e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 17.43it/s]                                               {'loss': 2.4041, 'grad_norm': 4.023073673248291, 'learning_rate': 6.092178951319811e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 17.43it/s] 68%|██████▊   | 51/75 [00:02<00:01, 16.50it/s]                                               {'loss': 2.1256, 'grad_norm': 4.567967891693115, 'learning_rate': 5.857864376269049e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 16.50it/s]                                               {'loss': 2.1419, 'grad_norm': 4.026094436645508, 'learning_rate': 5.623549801218288e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 16.50it/s] 71%|███████   | 53/75 [00:02<00:01, 16.58it/s]                                               {'loss': 2.1658, 'grad_norm': 3.7024452686309814, 'learning_rate': 5.389235226167525e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:01, 16.58it/s]                                               {'loss': 2.2398, 'grad_norm': 4.217610836029053, 'learning_rate': 5.1549206511167634e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:01, 16.58it/s] 73%|███████▎  | 55/75 [00:02<00:01, 16.74it/s]                                               {'loss': 2.4125, 'grad_norm': 3.6540422439575195, 'learning_rate': 4.9206060760660015e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:01, 16.74it/s]                                               {'loss': 2.2081, 'grad_norm': 4.106077194213867, 'learning_rate': 4.686291501015239e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:01, 16.74it/s] 76%|███████▌  | 57/75 [00:02<00:01, 17.03it/s]                                               {'loss': 2.0891, 'grad_norm': 4.64496374130249, 'learning_rate': 4.451976925964478e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:01, 17.03it/s]                                               {'loss': 2.3148, 'grad_norm': 5.322414875030518, 'learning_rate': 4.2176623509137154e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 17.03it/s]                                               {'loss': 2.0458, 'grad_norm': 3.4622859954833984, 'learning_rate': 3.9833477758629536e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:03<00:00, 17.03it/s] 80%|████████  | 60/75 [00:03<00:00, 18.65it/s]                                               {'loss': 2.3719, 'grad_norm': 20.57293701171875, 'learning_rate': 3.749033200812192e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:03<00:00, 18.65it/s]                                               {'loss': 2.241, 'grad_norm': 4.095888137817383, 'learning_rate': 3.51471862576143e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:03<00:00, 18.65it/s] 83%|████████▎ | 62/75 [00:03<00:00, 18.86it/s]                                               {'loss': 2.1928, 'grad_norm': 3.273498058319092, 'learning_rate': 3.2804040507106675e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:03<00:00, 18.86it/s]                                               {'loss': 2.2035, 'grad_norm': 3.8278462886810303, 'learning_rate': 3.0460894756599056e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 18.86it/s] 85%|████████▌ | 64/75 [00:03<00:00, 15.80it/s]                                               {'loss': 2.2136, 'grad_norm': 3.7728769779205322, 'learning_rate': 2.811774900609144e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 15.80it/s]                                               {'loss': 2.034, 'grad_norm': 3.3245291709899902, 'learning_rate': 2.5774603255583817e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 15.80it/s] 88%|████████▊ | 66/75 [00:03<00:00, 15.59it/s]                                               {'loss': 2.2817, 'grad_norm': 5.278426170349121, 'learning_rate': 2.3431457505076195e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 15.59it/s]                                               {'loss': 2.2155, 'grad_norm': 4.3052287101745605, 'learning_rate': 2.1088311754568577e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 15.59it/s] 91%|█████████ | 68/75 [00:03<00:00, 15.00it/s]                                               {'loss': 2.1998, 'grad_norm': 4.629166126251221, 'learning_rate': 1.874516600406096e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 15.00it/s]                                               {'loss': 2.1709, 'grad_norm': 3.6948459148406982, 'learning_rate': 1.6402020253553337e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 15.00it/s] 93%|█████████▎| 70/75 [00:03<00:00, 15.09it/s]                                               {'loss': 2.3089, 'grad_norm': 4.6663126945495605, 'learning_rate': 1.405887450304572e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 15.09it/s]                                               {'loss': 2.1836, 'grad_norm': 3.951714277267456, 'learning_rate': 1.1715728752538098e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 15.09it/s] 96%|█████████▌| 72/75 [00:03<00:00, 16.13it/s]                                               {'loss': 2.3128, 'grad_norm': 5.148712635040283, 'learning_rate': 9.37258300203048e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 16.13it/s]                                               {'loss': 2.3122, 'grad_norm': 4.827937126159668, 'learning_rate': 7.02943725152286e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 16.13it/s]                                               {'loss': 2.2341, 'grad_norm': 4.341762065887451, 'learning_rate': 4.68629150101524e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 16.13it/s]100%|██████████| 75/75 [00:04<00:00, 18.91it/s]                                               {'loss': 2.7192, 'grad_norm': 9.351295471191406, 'learning_rate': 2.34314575050762e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 18.91it/s]                                               {'train_runtime': 4.3346, 'train_samples_per_second': 260.696, 'train_steps_per_second': 17.303, 'train_loss': 2.30521653175354, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 18.91it/s]100%|██████████| 75/75 [00:04<00:00, 17.30it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:29
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]  1%|▏         | 1/75 [00:00<00:08,  8.52it/s]                                              {'loss': 2.3691, 'grad_norm': 5.649860858917236, 'learning_rate': 0.00017573593128807148, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:08,  8.52it/s]                                              {'loss': 2.4288, 'grad_norm': 5.449662208557129, 'learning_rate': 0.00017339278553756387, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:08,  8.52it/s]  4%|▍         | 3/75 [00:00<00:06, 11.39it/s]                                              {'loss': 2.5182, 'grad_norm': 5.232810020446777, 'learning_rate': 0.00017104963978705625, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:06, 11.39it/s]                                              {'loss': 2.272, 'grad_norm': 5.722581386566162, 'learning_rate': 0.00016870649403654862, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:06, 11.39it/s]  7%|▋         | 5/75 [00:00<00:05, 12.61it/s]                                              {'loss': 2.3484, 'grad_norm': 5.5398054122924805, 'learning_rate': 0.000166363348286041, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:05, 12.61it/s]                                              {'loss': 2.468, 'grad_norm': 5.3869147300720215, 'learning_rate': 0.0001640202025355334, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:05, 12.61it/s]  9%|▉         | 7/75 [00:00<00:05, 12.97it/s]                                              {'loss': 2.2877, 'grad_norm': 4.700771331787109, 'learning_rate': 0.00016167705678502576, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:05, 12.97it/s]                                              {'loss': 2.4057, 'grad_norm': 4.25153923034668, 'learning_rate': 0.00015933391103451814, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:05, 12.97it/s] 12%|█▏        | 9/75 [00:00<00:04, 14.65it/s]                                              {'loss': 2.2533, 'grad_norm': 4.297848701477051, 'learning_rate': 0.0001569907652840105, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:04, 14.65it/s]                                              {'loss': 2.3056, 'grad_norm': 3.7285144329071045, 'learning_rate': 0.0001546476195335029, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:04, 14.65it/s] 15%|█▍        | 11/75 [00:00<00:04, 15.64it/s]                                               {'loss': 2.2345, 'grad_norm': 4.736344814300537, 'learning_rate': 0.00015230447378299528, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:04, 15.64it/s]                                               {'loss': 2.2499, 'grad_norm': 4.631805896759033, 'learning_rate': 0.00014996132803248767, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:04, 15.64it/s] 17%|█▋        | 13/75 [00:00<00:03, 16.31it/s]                                               {'loss': 2.3916, 'grad_norm': 4.596889495849609, 'learning_rate': 0.00014761818228198003, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:03, 16.31it/s]                                               {'loss': 2.4626, 'grad_norm': 4.182525157928467, 'learning_rate': 0.00014527503653147242, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:03, 16.31it/s] 20%|██        | 15/75 [00:00<00:03, 17.14it/s]                                               {'loss': 2.3021, 'grad_norm': 10.77603816986084, 'learning_rate': 0.0001429318907809648, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:03, 17.14it/s]                                               {'loss': 2.3316, 'grad_norm': 6.035624027252197, 'learning_rate': 0.0001405887450304572, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:01<00:03, 17.14it/s]                                               {'loss': 2.2332, 'grad_norm': 4.129693031311035, 'learning_rate': 0.00013824559927994956, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:01<00:03, 17.14it/s] 24%|██▍       | 18/75 [00:01<00:03, 18.48it/s]                                               {'loss': 2.2941, 'grad_norm': 4.661851406097412, 'learning_rate': 0.00013590245352944195, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:01<00:03, 18.48it/s]                                               {'loss': 2.2664, 'grad_norm': 5.42030143737793, 'learning_rate': 0.00013355930777893434, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:01<00:03, 18.48it/s] 27%|██▋       | 20/75 [00:01<00:03, 18.00it/s]                                               {'loss': 2.2667, 'grad_norm': 3.813807487487793, 'learning_rate': 0.0001312161620284267, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:01<00:03, 18.00it/s]                                               {'loss': 2.362, 'grad_norm': 3.4180357456207275, 'learning_rate': 0.0001288730162779191, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:03, 18.00it/s] 29%|██▉       | 22/75 [00:01<00:02, 17.98it/s]                                               {'loss': 2.4699, 'grad_norm': 4.435288429260254, 'learning_rate': 0.00012652987052741145, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 17.98it/s]                                               {'loss': 2.3183, 'grad_norm': 5.0697021484375, 'learning_rate': 0.00012418672477690384, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 17.98it/s] 32%|███▏      | 24/75 [00:01<00:03, 16.53it/s]                                               {'loss': 2.3411, 'grad_norm': 4.667417526245117, 'learning_rate': 0.00012184357902639623, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:03, 16.53it/s]                                               {'loss': 2.3244, 'grad_norm': 4.89404821395874, 'learning_rate': 0.00011950043327588861, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:03, 16.53it/s] 35%|███▍      | 26/75 [00:01<00:03, 15.51it/s]                                               {'loss': 2.2537, 'grad_norm': 3.900092840194702, 'learning_rate': 0.00011715728752538098, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:03, 15.51it/s]                                               {'loss': 2.1492, 'grad_norm': 3.7746407985687256, 'learning_rate': 0.00011481414177487336, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:03, 15.51it/s]                                               {'loss': 2.1332, 'grad_norm': 4.215224742889404, 'learning_rate': 0.00011247099602436575, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:03, 15.51it/s] 39%|███▊      | 29/75 [00:01<00:02, 16.96it/s]                                               {'loss': 2.1319, 'grad_norm': 4.172712802886963, 'learning_rate': 0.00011012785027385813, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 16.96it/s]                                               {'loss': 2.3949, 'grad_norm': 9.529402732849121, 'learning_rate': 0.0001077847045233505, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 16.96it/s] 41%|████▏     | 31/75 [00:01<00:02, 16.13it/s]                                               {'loss': 2.1367, 'grad_norm': 3.3495960235595703, 'learning_rate': 0.00010544155877284288, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:02, 16.13it/s]                                               {'loss': 2.1953, 'grad_norm': 4.452365398406982, 'learning_rate': 0.00010309841302233527, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:02<00:02, 16.13it/s] 44%|████▍     | 33/75 [00:02<00:02, 15.58it/s]                                               {'loss': 2.1901, 'grad_norm': 3.971459150314331, 'learning_rate': 0.00010075526727182766, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:02<00:02, 15.58it/s]                                               {'loss': 2.1562, 'grad_norm': 3.4803335666656494, 'learning_rate': 9.841212152132003e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:02<00:02, 15.58it/s]                                               {'loss': 2.2284, 'grad_norm': 3.4912281036376953, 'learning_rate': 9.60689757708124e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:02<00:02, 15.58it/s] 48%|████▊     | 36/75 [00:02<00:02, 16.88it/s]                                               {'loss': 2.3359, 'grad_norm': 3.89621639251709, 'learning_rate': 9.372583002030478e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:02<00:02, 16.88it/s]                                               {'loss': 2.259, 'grad_norm': 4.813510894775391, 'learning_rate': 9.138268426979717e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:02<00:02, 16.88it/s] 51%|█████     | 38/75 [00:02<00:02, 16.04it/s]                                               {'loss': 2.2462, 'grad_norm': 5.406660079956055, 'learning_rate': 8.903953851928956e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:02<00:02, 16.04it/s]                                               {'loss': 2.5295, 'grad_norm': 5.280816078186035, 'learning_rate': 8.669639276878193e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:02<00:02, 16.04it/s] 53%|█████▎    | 40/75 [00:02<00:02, 16.17it/s]                                               {'loss': 2.2071, 'grad_norm': 4.648050785064697, 'learning_rate': 8.435324701827431e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:02<00:02, 16.17it/s]                                               {'loss': 2.085, 'grad_norm': 2.683328866958618, 'learning_rate': 8.20101012677667e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:02<00:02, 16.17it/s]                                               {'loss': 2.1592, 'grad_norm': 4.82651424407959, 'learning_rate': 7.966695551725907e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:02, 16.17it/s] 57%|█████▋    | 43/75 [00:02<00:01, 17.37it/s]                                               {'loss': 2.3157, 'grad_norm': 4.777652740478516, 'learning_rate': 7.732380976675145e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 17.37it/s]                                               {'loss': 2.3148, 'grad_norm': 3.7132985591888428, 'learning_rate': 7.498066401624384e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 17.37it/s] 60%|██████    | 45/75 [00:02<00:01, 17.79it/s]                                               {'loss': 2.32, 'grad_norm': 11.265666961669922, 'learning_rate': 7.263751826573621e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 17.79it/s]                                               {'loss': 2.0984, 'grad_norm': 3.4588980674743652, 'learning_rate': 7.02943725152286e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 17.79it/s] 63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]                                               {'loss': 2.3518, 'grad_norm': 3.8464338779449463, 'learning_rate': 6.795122676472097e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 17.69it/s]                                               {'loss': 2.3532, 'grad_norm': 4.033783435821533, 'learning_rate': 6.560808101421335e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 17.69it/s] 65%|██████▌   | 49/75 [00:02<00:01, 17.87it/s]                                               {'loss': 2.2385, 'grad_norm': 4.564736843109131, 'learning_rate': 6.326493526370572e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 17.87it/s]                                               {'loss': 2.0593, 'grad_norm': 4.069810390472412, 'learning_rate': 6.092178951319811e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:03<00:01, 17.87it/s] 68%|██████▊   | 51/75 [00:03<00:01, 18.27it/s]                                               {'loss': 2.082, 'grad_norm': 4.630124568939209, 'learning_rate': 5.857864376269049e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:03<00:01, 18.27it/s]                                               {'loss': 2.101, 'grad_norm': 3.7246806621551514, 'learning_rate': 5.623549801218288e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:03<00:01, 18.27it/s] 71%|███████   | 53/75 [00:03<00:01, 16.94it/s]                                               {'loss': 2.0951, 'grad_norm': 3.7057957649230957, 'learning_rate': 5.389235226167525e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:03<00:01, 16.94it/s]                                               {'loss': 2.2633, 'grad_norm': 5.811892986297607, 'learning_rate': 5.1549206511167634e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:03<00:01, 16.94it/s]                                               {'loss': 2.1948, 'grad_norm': 4.283376216888428, 'learning_rate': 4.9206060760660015e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:03<00:01, 16.94it/s] 75%|███████▍  | 56/75 [00:03<00:01, 18.13it/s]                                               {'loss': 2.2336, 'grad_norm': 4.271714210510254, 'learning_rate': 4.686291501015239e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:03<00:01, 18.13it/s]                                               {'loss': 2.1435, 'grad_norm': 3.5209102630615234, 'learning_rate': 4.451976925964478e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:03<00:00, 18.13it/s] 77%|███████▋  | 58/75 [00:03<00:01, 14.96it/s]                                               {'loss': 2.2355, 'grad_norm': 3.694880723953247, 'learning_rate': 4.2176623509137154e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:03<00:01, 14.96it/s]                                               {'loss': 2.3289, 'grad_norm': 5.126367568969727, 'learning_rate': 3.9833477758629536e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:03<00:01, 14.96it/s]                                               {'loss': 2.2098, 'grad_norm': 8.869217872619629, 'learning_rate': 3.749033200812192e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:03<00:01, 14.96it/s] 81%|████████▏ | 61/75 [00:03<00:00, 15.14it/s]                                               {'loss': 2.259, 'grad_norm': 4.49505090713501, 'learning_rate': 3.51471862576143e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:03<00:00, 15.14it/s]                                               {'loss': 2.1135, 'grad_norm': 3.699978828430176, 'learning_rate': 3.2804040507106675e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:03<00:00, 15.14it/s] 84%|████████▍ | 63/75 [00:03<00:00, 14.74it/s]                                               {'loss': 2.1133, 'grad_norm': 3.309575080871582, 'learning_rate': 3.0460894756599056e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 14.74it/s]                                               {'loss': 2.2314, 'grad_norm': 4.467792510986328, 'learning_rate': 2.811774900609144e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:04<00:00, 14.74it/s] 87%|████████▋ | 65/75 [00:04<00:00, 14.37it/s]                                               {'loss': 2.2894, 'grad_norm': 4.221160411834717, 'learning_rate': 2.5774603255583817e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:04<00:00, 14.37it/s]                                               {'loss': 2.2153, 'grad_norm': 4.270523548126221, 'learning_rate': 2.3431457505076195e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:04<00:00, 14.37it/s] 89%|████████▉ | 67/75 [00:04<00:00, 14.78it/s]                                               {'loss': 2.1095, 'grad_norm': 4.021334648132324, 'learning_rate': 2.1088311754568577e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:04<00:00, 14.78it/s]                                               {'loss': 2.2028, 'grad_norm': 4.430375576019287, 'learning_rate': 1.874516600406096e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:04<00:00, 14.78it/s] 92%|█████████▏| 69/75 [00:04<00:00, 15.54it/s]                                               {'loss': 2.2421, 'grad_norm': 4.370143890380859, 'learning_rate': 1.6402020253553337e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:04<00:00, 15.54it/s]                                               {'loss': 2.2836, 'grad_norm': 3.5085818767547607, 'learning_rate': 1.405887450304572e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:04<00:00, 15.54it/s]                                               {'loss': 2.1938, 'grad_norm': 3.995201587677002, 'learning_rate': 1.1715728752538098e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:04<00:00, 15.54it/s] 96%|█████████▌| 72/75 [00:04<00:00, 17.39it/s]                                               {'loss': 2.2838, 'grad_norm': 4.927734375, 'learning_rate': 9.37258300203048e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:04<00:00, 17.39it/s]                                               {'loss': 2.0371, 'grad_norm': 4.259274959564209, 'learning_rate': 7.02943725152286e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:04<00:00, 17.39it/s]                                               {'loss': 2.0996, 'grad_norm': 3.360914945602417, 'learning_rate': 4.68629150101524e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:04<00:00, 17.39it/s]100%|██████████| 75/75 [00:04<00:00, 20.36it/s]                                               {'loss': 2.2353, 'grad_norm': 18.41563606262207, 'learning_rate': 2.34314575050762e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 20.36it/s]                                               {'train_runtime': 4.8069, 'train_samples_per_second': 235.08, 'train_steps_per_second': 15.603, 'train_loss': 2.25527112642924, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 20.36it/s]100%|██████████| 75/75 [00:04<00:00, 15.63it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:49
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3767, 'grad_norm': 4.221105098724365, 'learning_rate': 0.00017573593128807148, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 19.27it/s]                                              {'loss': 2.4257, 'grad_norm': 4.016450881958008, 'learning_rate': 0.00017339278553756387, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 21.98it/s]  4%|▍         | 3/75 [00:00<00:03, 23.15it/s]                                              {'loss': 2.2593, 'grad_norm': 4.730013370513916, 'learning_rate': 0.00017104963978705625, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.15it/s]                                              {'loss': 2.4911, 'grad_norm': 4.770392417907715, 'learning_rate': 0.00016870649403654862, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 23.15it/s]                                              {'loss': 2.3495, 'grad_norm': 4.601094722747803, 'learning_rate': 0.000166363348286041, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 23.15it/s]  8%|▊         | 6/75 [00:00<00:02, 24.70it/s]                                              {'loss': 2.2037, 'grad_norm': 4.8501410484313965, 'learning_rate': 0.0001640202025355334, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.70it/s]                                              {'loss': 2.439, 'grad_norm': 4.707187175750732, 'learning_rate': 0.00016167705678502576, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.70it/s]                                              {'loss': 2.3798, 'grad_norm': 4.447660446166992, 'learning_rate': 0.00015933391103451814, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.70it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.30it/s]                                              {'loss': 2.3942, 'grad_norm': 4.565337181091309, 'learning_rate': 0.0001569907652840105, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.30it/s]                                              {'loss': 2.5603, 'grad_norm': 4.8316874504089355, 'learning_rate': 0.0001546476195335029, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.30it/s]                                               {'loss': 2.3629, 'grad_norm': 4.937710762023926, 'learning_rate': 0.00015230447378299528, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.30it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.43it/s]                                               {'loss': 2.3344, 'grad_norm': 4.553819179534912, 'learning_rate': 0.00014996132803248767, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.43it/s]                                               {'loss': 2.3575, 'grad_norm': 4.856922626495361, 'learning_rate': 0.00014761818228198003, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.43it/s]                                               {'loss': 2.2011, 'grad_norm': 5.083773136138916, 'learning_rate': 0.00014527503653147242, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.43it/s] 20%|██        | 15/75 [00:00<00:02, 22.79it/s]                                               {'loss': 2.449, 'grad_norm': 4.194485187530518, 'learning_rate': 0.0001429318907809648, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 22.79it/s]                                               {'loss': 2.4211, 'grad_norm': 4.599839210510254, 'learning_rate': 0.0001405887450304572, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 22.79it/s]                                               {'loss': 2.179, 'grad_norm': 3.882293701171875, 'learning_rate': 0.00013824559927994956, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 22.79it/s] 24%|██▍       | 18/75 [00:00<00:02, 22.10it/s]                                               {'loss': 2.1701, 'grad_norm': 3.8566036224365234, 'learning_rate': 0.00013590245352944195, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 22.10it/s]                                               {'loss': 2.3947, 'grad_norm': 3.63441801071167, 'learning_rate': 0.00013355930777893434, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 22.10it/s]                                               {'loss': 2.3943, 'grad_norm': 4.227036476135254, 'learning_rate': 0.0001312161620284267, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 22.10it/s] 28%|██▊       | 21/75 [00:00<00:02, 22.74it/s]                                               {'loss': 2.3332, 'grad_norm': 5.334473609924316, 'learning_rate': 0.0001288730162779191, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 22.74it/s]                                               {'loss': 2.2737, 'grad_norm': 4.530976295471191, 'learning_rate': 0.00012652987052741145, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 22.74it/s]                                               {'loss': 2.1591, 'grad_norm': 3.84717059135437, 'learning_rate': 0.00012418672477690384, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 22.74it/s] 32%|███▏      | 24/75 [00:01<00:02, 23.84it/s]                                               {'loss': 2.3172, 'grad_norm': 5.40993070602417, 'learning_rate': 0.00012184357902639623, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 23.84it/s]                                               {'loss': 2.3284, 'grad_norm': 4.282496452331543, 'learning_rate': 0.00011950043327588861, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.84it/s]                                               {'loss': 2.3579, 'grad_norm': 4.82629919052124, 'learning_rate': 0.00011715728752538098, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.84it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.54it/s]                                               {'loss': 2.3022, 'grad_norm': 4.463900089263916, 'learning_rate': 0.00011481414177487336, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.54it/s]                                               {'loss': 2.1024, 'grad_norm': 3.4933695793151855, 'learning_rate': 0.00011247099602436575, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.54it/s]                                               {'loss': 2.1227, 'grad_norm': 4.135550498962402, 'learning_rate': 0.00011012785027385813, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.54it/s] 40%|████      | 30/75 [00:01<00:01, 25.11it/s]                                               {'loss': 2.3667, 'grad_norm': 3.714402198791504, 'learning_rate': 0.0001077847045233505, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.11it/s]                                               {'loss': 2.2676, 'grad_norm': 4.733484745025635, 'learning_rate': 0.00010544155877284288, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.11it/s]                                               {'loss': 2.459, 'grad_norm': 4.174203872680664, 'learning_rate': 0.00010309841302233527, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.11it/s] 44%|████▍     | 33/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.0716, 'grad_norm': 3.8602302074432373, 'learning_rate': 0.00010075526727182766, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.1067, 'grad_norm': 4.502225399017334, 'learning_rate': 9.841212152132003e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.0593, 'grad_norm': 3.348747491836548, 'learning_rate': 9.60689757708124e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.30it/s] 48%|████▊     | 36/75 [00:01<00:01, 25.72it/s]                                               {'loss': 2.2652, 'grad_norm': 3.732555389404297, 'learning_rate': 9.372583002030478e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.72it/s]                                               {'loss': 2.1674, 'grad_norm': 3.064617395401001, 'learning_rate': 9.138268426979717e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.72it/s]                                               {'loss': 2.2466, 'grad_norm': 4.2675604820251465, 'learning_rate': 8.903953851928956e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.72it/s] 52%|█████▏    | 39/75 [00:01<00:01, 25.54it/s]                                               {'loss': 2.2928, 'grad_norm': 4.537411212921143, 'learning_rate': 8.669639276878193e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.54it/s]                                               {'loss': 2.2274, 'grad_norm': 3.8403704166412354, 'learning_rate': 8.435324701827431e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.54it/s]                                               {'loss': 2.1613, 'grad_norm': 4.613636493682861, 'learning_rate': 8.20101012677667e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.54it/s] 56%|█████▌    | 42/75 [00:01<00:01, 25.52it/s]                                               {'loss': 2.175, 'grad_norm': 5.364838600158691, 'learning_rate': 7.966695551725907e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.52it/s]                                               {'loss': 2.2396, 'grad_norm': 3.9966816902160645, 'learning_rate': 7.732380976675145e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.52it/s]                                               {'loss': 2.3882, 'grad_norm': 4.37404727935791, 'learning_rate': 7.498066401624384e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.52it/s] 60%|██████    | 45/75 [00:01<00:01, 22.84it/s]                                               {'loss': 2.2643, 'grad_norm': 4.548654556274414, 'learning_rate': 7.263751826573621e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 22.84it/s]                                               {'loss': 2.1636, 'grad_norm': 4.027214527130127, 'learning_rate': 7.02943725152286e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 22.84it/s]                                               {'loss': 2.35, 'grad_norm': 5.026100158691406, 'learning_rate': 6.795122676472097e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 22.84it/s] 64%|██████▍   | 48/75 [00:02<00:01, 20.58it/s]                                               {'loss': 2.2547, 'grad_norm': 4.214812755584717, 'learning_rate': 6.560808101421335e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 20.58it/s]                                               {'loss': 2.1128, 'grad_norm': 4.093231678009033, 'learning_rate': 6.326493526370572e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 20.58it/s]                                               {'loss': 2.1996, 'grad_norm': 4.250889778137207, 'learning_rate': 6.092178951319811e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 20.58it/s] 68%|██████▊   | 51/75 [00:02<00:01, 20.85it/s]                                               {'loss': 2.3624, 'grad_norm': 4.069204330444336, 'learning_rate': 5.857864376269049e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 20.85it/s]                                               {'loss': 2.0649, 'grad_norm': 4.424278736114502, 'learning_rate': 5.623549801218288e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 20.85it/s]                                               {'loss': 2.121, 'grad_norm': 4.8968892097473145, 'learning_rate': 5.389235226167525e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:01, 20.85it/s] 72%|███████▏  | 54/75 [00:02<00:00, 21.84it/s]                                               {'loss': 2.0963, 'grad_norm': 4.305452346801758, 'learning_rate': 5.1549206511167634e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 21.84it/s]                                               {'loss': 2.3558, 'grad_norm': 5.329553127288818, 'learning_rate': 4.9206060760660015e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 21.84it/s]                                               {'loss': 2.3553, 'grad_norm': 4.684211730957031, 'learning_rate': 4.686291501015239e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 21.84it/s] 76%|███████▌  | 57/75 [00:02<00:00, 22.42it/s]                                               {'loss': 2.1544, 'grad_norm': 5.158674240112305, 'learning_rate': 4.451976925964478e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 22.42it/s]                                               {'loss': 2.3568, 'grad_norm': 5.541929244995117, 'learning_rate': 4.2176623509137154e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 22.42it/s]                                               {'loss': 2.1225, 'grad_norm': 4.319736957550049, 'learning_rate': 3.9833477758629536e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 22.42it/s] 80%|████████  | 60/75 [00:02<00:00, 23.24it/s]                                               {'loss': 2.2782, 'grad_norm': 4.200464725494385, 'learning_rate': 3.749033200812192e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 23.24it/s]                                               {'loss': 2.2164, 'grad_norm': 4.006227493286133, 'learning_rate': 3.51471862576143e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 23.24it/s]                                               {'loss': 2.2007, 'grad_norm': 4.874804973602295, 'learning_rate': 3.2804040507106675e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 23.24it/s] 84%|████████▍ | 63/75 [00:02<00:00, 23.67it/s]                                               {'loss': 2.2112, 'grad_norm': 3.901750326156616, 'learning_rate': 3.0460894756599056e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 23.67it/s]                                               {'loss': 2.1982, 'grad_norm': 3.988271713256836, 'learning_rate': 2.811774900609144e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 23.67it/s]                                               {'loss': 2.092, 'grad_norm': 4.167881011962891, 'learning_rate': 2.5774603255583817e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 23.67it/s] 88%|████████▊ | 66/75 [00:02<00:00, 23.95it/s]                                               {'loss': 2.1635, 'grad_norm': 3.964911460876465, 'learning_rate': 2.3431457505076195e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 23.95it/s]                                               {'loss': 2.0589, 'grad_norm': 4.248898983001709, 'learning_rate': 2.1088311754568577e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 23.95it/s]                                               {'loss': 2.2904, 'grad_norm': 3.952179431915283, 'learning_rate': 1.874516600406096e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 23.95it/s] 92%|█████████▏| 69/75 [00:02<00:00, 23.09it/s]                                               {'loss': 2.2156, 'grad_norm': 5.283658504486084, 'learning_rate': 1.6402020253553337e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 23.09it/s]                                               {'loss': 2.2224, 'grad_norm': 4.632871150970459, 'learning_rate': 1.405887450304572e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 23.09it/s]                                               {'loss': 2.0868, 'grad_norm': 3.620985507965088, 'learning_rate': 1.1715728752538098e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 23.09it/s] 96%|█████████▌| 72/75 [00:03<00:00, 22.98it/s]                                               {'loss': 2.0742, 'grad_norm': 4.357752323150635, 'learning_rate': 9.37258300203048e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 22.98it/s]                                               {'loss': 2.1032, 'grad_norm': 3.3064560890197754, 'learning_rate': 7.02943725152286e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 22.98it/s]                                               {'loss': 2.3258, 'grad_norm': 6.138990879058838, 'learning_rate': 4.68629150101524e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 22.98it/s]100%|██████████| 75/75 [00:03<00:00, 23.23it/s]                                               {'loss': 2.3447, 'grad_norm': 4.670066833496094, 'learning_rate': 2.34314575050762e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 23.23it/s]                                               {'train_runtime': 3.311, 'train_samples_per_second': 362.434, 'train_steps_per_second': 22.652, 'train_loss': 2.258011229832967, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 23.23it/s]100%|██████████| 75/75 [00:03<00:00, 22.66it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:38
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3414, 'grad_norm': 5.098545551300049, 'learning_rate': 0.00017573593128807148, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:05, 13.20it/s]  3%|▎         | 2/75 [00:00<00:04, 17.51it/s]                                              {'loss': 2.2833, 'grad_norm': 5.132546424865723, 'learning_rate': 0.00017339278553756387, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:04, 17.51it/s]                                              {'loss': 2.5703, 'grad_norm': 5.431902885437012, 'learning_rate': 0.00017104963978705625, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 17.51it/s]                                              {'loss': 2.4661, 'grad_norm': 4.166596412658691, 'learning_rate': 0.00016870649403654862, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:04, 17.51it/s]  7%|▋         | 5/75 [00:00<00:03, 22.42it/s]                                              {'loss': 2.2857, 'grad_norm': 4.884973526000977, 'learning_rate': 0.000166363348286041, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.42it/s]                                              {'loss': 2.474, 'grad_norm': 4.889908313751221, 'learning_rate': 0.0001640202025355334, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 22.42it/s]                                              {'loss': 2.2644, 'grad_norm': 5.478453159332275, 'learning_rate': 0.00016167705678502576, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 22.42it/s] 11%|█         | 8/75 [00:00<00:02, 23.70it/s]                                              {'loss': 2.3847, 'grad_norm': 4.522796630859375, 'learning_rate': 0.00015933391103451814, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.70it/s]                                              {'loss': 2.2616, 'grad_norm': 4.031411647796631, 'learning_rate': 0.0001569907652840105, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.70it/s]                                              {'loss': 2.2049, 'grad_norm': 3.666374683380127, 'learning_rate': 0.0001546476195335029, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.70it/s] 15%|█▍        | 11/75 [00:00<00:02, 24.59it/s]                                               {'loss': 2.5169, 'grad_norm': 3.9661452770233154, 'learning_rate': 0.00015230447378299528, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.59it/s]                                               {'loss': 2.3023, 'grad_norm': 7.736520290374756, 'learning_rate': 0.00014996132803248767, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.59it/s]                                               {'loss': 2.5045, 'grad_norm': 8.389954566955566, 'learning_rate': 0.00014761818228198003, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.59it/s] 19%|█▊        | 14/75 [00:00<00:02, 24.92it/s]                                               {'loss': 2.7531, 'grad_norm': 5.971545696258545, 'learning_rate': 0.00014527503653147242, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.92it/s]                                               {'loss': 2.4235, 'grad_norm': 14.447196960449219, 'learning_rate': 0.0001429318907809648, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.92it/s]                                               {'loss': 2.4938, 'grad_norm': 3.7646071910858154, 'learning_rate': 0.0001405887450304572, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 24.92it/s] 23%|██▎       | 17/75 [00:00<00:02, 23.83it/s]                                               {'loss': 2.2642, 'grad_norm': 4.73460054397583, 'learning_rate': 0.00013824559927994956, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 23.83it/s]                                               {'loss': 2.2499, 'grad_norm': 4.890474796295166, 'learning_rate': 0.00013590245352944195, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 23.83it/s]                                               {'loss': 2.3639, 'grad_norm': 4.841700553894043, 'learning_rate': 0.00013355930777893434, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 23.83it/s] 27%|██▋       | 20/75 [00:00<00:02, 24.46it/s]                                               {'loss': 2.3196, 'grad_norm': 5.454099655151367, 'learning_rate': 0.0001312161620284267, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.46it/s]                                               {'loss': 2.4955, 'grad_norm': 4.419744491577148, 'learning_rate': 0.0001288730162779191, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.46it/s]                                               {'loss': 2.3302, 'grad_norm': 4.757546424865723, 'learning_rate': 0.00012652987052741145, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.46it/s] 31%|███       | 23/75 [00:00<00:02, 22.26it/s]                                               {'loss': 2.2526, 'grad_norm': 2.9004342555999756, 'learning_rate': 0.00012418672477690384, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 22.26it/s]                                               {'loss': 2.214, 'grad_norm': 4.158425807952881, 'learning_rate': 0.00012184357902639623, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 22.26it/s]                                               {'loss': 2.2534, 'grad_norm': 4.493272304534912, 'learning_rate': 0.00011950043327588861, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 22.26it/s] 35%|███▍      | 26/75 [00:01<00:02, 21.90it/s]                                               {'loss': 2.4315, 'grad_norm': 4.671171188354492, 'learning_rate': 0.00011715728752538098, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 21.90it/s]                                               {'loss': 2.25, 'grad_norm': 5.144325256347656, 'learning_rate': 0.00011481414177487336, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 21.90it/s]                                               {'loss': 2.1433, 'grad_norm': 3.792433261871338, 'learning_rate': 0.00011247099602436575, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 21.90it/s] 39%|███▊      | 29/75 [00:01<00:02, 22.50it/s]                                               {'loss': 2.3338, 'grad_norm': 5.275611877441406, 'learning_rate': 0.00011012785027385813, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 22.50it/s]                                               {'loss': 2.6197, 'grad_norm': 13.36673355102539, 'learning_rate': 0.0001077847045233505, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 22.50it/s]                                               {'loss': 2.3548, 'grad_norm': 4.445640563964844, 'learning_rate': 0.00010544155877284288, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 22.50it/s] 43%|████▎     | 32/75 [00:01<00:01, 23.27it/s]                                               {'loss': 2.1555, 'grad_norm': 3.6401009559631348, 'learning_rate': 0.00010309841302233527, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 23.27it/s]                                               {'loss': 2.1661, 'grad_norm': 4.141894340515137, 'learning_rate': 0.00010075526727182766, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 23.27it/s]                                               {'loss': 2.2613, 'grad_norm': 3.6297130584716797, 'learning_rate': 9.841212152132003e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 23.27it/s] 47%|████▋     | 35/75 [00:01<00:01, 23.28it/s]                                               {'loss': 2.4397, 'grad_norm': 5.1794657707214355, 'learning_rate': 9.60689757708124e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 23.28it/s]                                               {'loss': 2.2363, 'grad_norm': 5.413145542144775, 'learning_rate': 9.372583002030478e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 23.28it/s]                                               {'loss': 2.2931, 'grad_norm': 4.5253071784973145, 'learning_rate': 9.138268426979717e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 23.28it/s] 51%|█████     | 38/75 [00:01<00:01, 23.85it/s]                                               {'loss': 2.2874, 'grad_norm': 3.629498243331909, 'learning_rate': 8.903953851928956e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 23.85it/s]                                               {'loss': 2.2206, 'grad_norm': 3.7596075534820557, 'learning_rate': 8.669639276878193e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.85it/s]                                               {'loss': 2.2021, 'grad_norm': 4.750545501708984, 'learning_rate': 8.435324701827431e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.85it/s] 55%|█████▍    | 41/75 [00:01<00:01, 22.22it/s]                                               {'loss': 2.1712, 'grad_norm': 3.6661057472229004, 'learning_rate': 8.20101012677667e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 22.22it/s]                                               {'loss': 2.2549, 'grad_norm': 4.407331466674805, 'learning_rate': 7.966695551725907e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 22.22it/s]                                               {'loss': 2.1524, 'grad_norm': 4.631086349487305, 'learning_rate': 7.732380976675145e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 22.22it/s] 59%|█████▊    | 44/75 [00:01<00:01, 22.45it/s]                                               {'loss': 2.2202, 'grad_norm': 4.1280412673950195, 'learning_rate': 7.498066401624384e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 22.45it/s]                                               {'loss': 2.702, 'grad_norm': 12.282279968261719, 'learning_rate': 7.263751826573621e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 22.45it/s]                                               {'loss': 2.3162, 'grad_norm': 4.233596324920654, 'learning_rate': 7.02943725152286e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 22.45it/s] 63%|██████▎   | 47/75 [00:02<00:01, 22.16it/s]                                               {'loss': 2.1636, 'grad_norm': 4.238259792327881, 'learning_rate': 6.795122676472097e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 22.16it/s]                                               {'loss': 2.2584, 'grad_norm': 3.746345281600952, 'learning_rate': 6.560808101421335e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 22.16it/s]                                               {'loss': 2.1056, 'grad_norm': 3.9858739376068115, 'learning_rate': 6.326493526370572e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 22.16it/s] 67%|██████▋   | 50/75 [00:02<00:01, 19.40it/s]                                               {'loss': 2.1289, 'grad_norm': 4.56972074508667, 'learning_rate': 6.092178951319811e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 19.40it/s]                                               {'loss': 2.188, 'grad_norm': 4.287264347076416, 'learning_rate': 5.857864376269049e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 19.40it/s]                                               {'loss': 2.4778, 'grad_norm': 4.006861209869385, 'learning_rate': 5.623549801218288e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 19.40it/s] 71%|███████   | 53/75 [00:02<00:01, 18.41it/s]                                               {'loss': 2.3307, 'grad_norm': 4.698824405670166, 'learning_rate': 5.389235226167525e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:01, 18.41it/s]                                               {'loss': 2.1829, 'grad_norm': 4.028918266296387, 'learning_rate': 5.1549206511167634e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:01, 18.41it/s] 73%|███████▎  | 55/75 [00:02<00:01, 18.36it/s]                                               {'loss': 2.1785, 'grad_norm': 3.492220401763916, 'learning_rate': 4.9206060760660015e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:01, 18.36it/s]                                               {'loss': 2.2397, 'grad_norm': 3.893852710723877, 'learning_rate': 4.686291501015239e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:01, 18.36it/s] 76%|███████▌  | 57/75 [00:02<00:01, 17.95it/s]                                               {'loss': 2.1613, 'grad_norm': 3.872769355773926, 'learning_rate': 4.451976925964478e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:01, 17.95it/s]                                               {'loss': 2.24, 'grad_norm': 3.536592483520508, 'learning_rate': 4.2176623509137154e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 17.95it/s] 79%|███████▊  | 59/75 [00:02<00:00, 18.13it/s]                                               {'loss': 2.1533, 'grad_norm': 4.4629597663879395, 'learning_rate': 3.9833477758629536e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 18.13it/s]                                               {'loss': 2.2666, 'grad_norm': 11.791954040527344, 'learning_rate': 3.749033200812192e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 18.13it/s] 81%|████████▏ | 61/75 [00:02<00:00, 17.10it/s]                                               {'loss': 2.2441, 'grad_norm': 3.746267080307007, 'learning_rate': 3.51471862576143e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 17.10it/s]                                               {'loss': 2.2669, 'grad_norm': 5.879358291625977, 'learning_rate': 3.2804040507106675e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 17.10it/s] 84%|████████▍ | 63/75 [00:03<00:00, 17.41it/s]                                               {'loss': 2.2004, 'grad_norm': 3.821870803833008, 'learning_rate': 3.0460894756599056e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 17.41it/s]                                               {'loss': 2.2971, 'grad_norm': 3.89363956451416, 'learning_rate': 2.811774900609144e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 17.41it/s]                                               {'loss': 2.1204, 'grad_norm': 4.0417070388793945, 'learning_rate': 2.5774603255583817e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 17.41it/s] 88%|████████▊ | 66/75 [00:03<00:00, 19.40it/s]                                               {'loss': 2.1529, 'grad_norm': 3.984884262084961, 'learning_rate': 2.3431457505076195e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 19.40it/s]                                               {'loss': 2.3128, 'grad_norm': 4.453445911407471, 'learning_rate': 2.1088311754568577e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 19.40it/s]                                               {'loss': 2.0972, 'grad_norm': 4.245508670806885, 'learning_rate': 1.874516600406096e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 19.40it/s] 92%|█████████▏| 69/75 [00:03<00:00, 20.03it/s]                                               {'loss': 2.1366, 'grad_norm': 5.4417901039123535, 'learning_rate': 1.6402020253553337e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 20.03it/s]                                               {'loss': 2.1211, 'grad_norm': 3.3341166973114014, 'learning_rate': 1.405887450304572e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 20.03it/s]                                               {'loss': 2.2243, 'grad_norm': 4.953012943267822, 'learning_rate': 1.1715728752538098e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 20.03it/s] 96%|█████████▌| 72/75 [00:03<00:00, 17.58it/s]                                               {'loss': 2.19, 'grad_norm': 4.060519218444824, 'learning_rate': 9.37258300203048e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 17.58it/s]                                               {'loss': 2.4481, 'grad_norm': 3.9965932369232178, 'learning_rate': 7.02943725152286e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 17.58it/s] 99%|█████████▊| 74/75 [00:03<00:00, 17.49it/s]                                               {'loss': 2.1113, 'grad_norm': 4.0764288902282715, 'learning_rate': 4.68629150101524e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 17.49it/s]                                               {'loss': 2.2886, 'grad_norm': 13.569485664367676, 'learning_rate': 2.34314575050762e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 17.49it/s]                                               {'train_runtime': 3.8495, 'train_samples_per_second': 293.543, 'train_steps_per_second': 19.483, 'train_loss': 2.290309419631958, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 17.49it/s]100%|██████████| 75/75 [00:03<00:00, 19.48it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:45
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4199, 'grad_norm': 4.385015964508057, 'learning_rate': 0.00017573593128807148, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:05, 13.94it/s]  3%|▎         | 2/75 [00:00<00:04, 16.40it/s]                                              {'loss': 2.4094, 'grad_norm': 5.49358606338501, 'learning_rate': 0.00017339278553756387, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:04, 16.40it/s]                                              {'loss': 2.5128, 'grad_norm': 4.9795122146606445, 'learning_rate': 0.00017104963978705625, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 16.40it/s]  5%|▌         | 4/75 [00:00<00:04, 15.63it/s]                                              {'loss': 2.247, 'grad_norm': 5.183372974395752, 'learning_rate': 0.00016870649403654862, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:04, 15.63it/s]                                              {'loss': 2.3462, 'grad_norm': 3.829547882080078, 'learning_rate': 0.000166363348286041, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:04, 15.63it/s]  8%|▊         | 6/75 [00:00<00:05, 13.47it/s]                                              {'loss': 2.4662, 'grad_norm': 4.863636016845703, 'learning_rate': 0.0001640202025355334, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:05, 13.47it/s]                                              {'loss': 2.5672, 'grad_norm': 4.2999372482299805, 'learning_rate': 0.00016167705678502576, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:05, 13.47it/s] 11%|█         | 8/75 [00:00<00:04, 14.91it/s]                                              {'loss': 2.2528, 'grad_norm': 4.908334255218506, 'learning_rate': 0.00015933391103451814, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:04, 14.91it/s]                                              {'loss': 2.5467, 'grad_norm': 5.33739709854126, 'learning_rate': 0.0001569907652840105, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:04, 14.91it/s]                                              {'loss': 2.3947, 'grad_norm': 4.324490070343018, 'learning_rate': 0.0001546476195335029, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:04, 14.91it/s] 15%|█▍        | 11/75 [00:00<00:03, 17.52it/s]                                               {'loss': 2.1402, 'grad_norm': 3.6400392055511475, 'learning_rate': 0.00015230447378299528, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 17.52it/s]                                               {'loss': 2.3027, 'grad_norm': 3.9605600833892822, 'learning_rate': 0.00014996132803248767, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:03, 17.52it/s]                                               {'loss': 2.3553, 'grad_norm': 3.8798129558563232, 'learning_rate': 0.00014761818228198003, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:03, 17.52it/s] 19%|█▊        | 14/75 [00:00<00:03, 18.72it/s]                                               {'loss': 2.3588, 'grad_norm': 5.288295745849609, 'learning_rate': 0.00014527503653147242, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:03, 18.72it/s]                                               {'loss': 2.4556, 'grad_norm': 18.320266723632812, 'learning_rate': 0.0001429318907809648, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:03, 18.72it/s] 21%|██▏       | 16/75 [00:00<00:03, 18.79it/s]                                               {'loss': 2.3737, 'grad_norm': 4.388775825500488, 'learning_rate': 0.0001405887450304572, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:03, 18.79it/s]                                               {'loss': 2.2215, 'grad_norm': 5.397702217102051, 'learning_rate': 0.00013824559927994956, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:03, 18.79it/s]                                               {'loss': 2.224, 'grad_norm': 4.030397415161133, 'learning_rate': 0.00013590245352944195, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:01<00:03, 18.79it/s] 25%|██▌       | 19/75 [00:01<00:02, 19.77it/s]                                               {'loss': 2.1874, 'grad_norm': 3.6921029090881348, 'learning_rate': 0.00013355930777893434, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:01<00:02, 19.77it/s]                                               {'loss': 2.4331, 'grad_norm': 3.896940231323242, 'learning_rate': 0.0001312161620284267, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:01<00:02, 19.77it/s]                                               {'loss': 2.2666, 'grad_norm': 6.024337291717529, 'learning_rate': 0.0001288730162779191, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:02, 19.77it/s] 29%|██▉       | 22/75 [00:01<00:02, 20.05it/s]                                               {'loss': 2.0077, 'grad_norm': 5.1537346839904785, 'learning_rate': 0.00012652987052741145, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 20.05it/s]                                               {'loss': 2.2681, 'grad_norm': 3.9031827449798584, 'learning_rate': 0.00012418672477690384, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 20.05it/s]                                               {'loss': 2.093, 'grad_norm': 3.785512924194336, 'learning_rate': 0.00012184357902639623, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 20.05it/s] 33%|███▎      | 25/75 [00:01<00:03, 16.60it/s]                                               {'loss': 2.2785, 'grad_norm': 3.8117570877075195, 'learning_rate': 0.00011950043327588861, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:03, 16.60it/s]                                               {'loss': 2.0973, 'grad_norm': 4.191919326782227, 'learning_rate': 0.00011715728752538098, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 16.60it/s]                                               {'loss': 2.2344, 'grad_norm': 4.566068649291992, 'learning_rate': 0.00011481414177487336, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 16.60it/s] 37%|███▋      | 28/75 [00:01<00:02, 18.68it/s]                                               {'loss': 2.659, 'grad_norm': 6.194666385650635, 'learning_rate': 0.00011247099602436575, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 18.68it/s]                                               {'loss': 2.3517, 'grad_norm': 3.3977320194244385, 'learning_rate': 0.00011012785027385813, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 18.68it/s]                                               {'loss': 2.4444, 'grad_norm': 16.05080795288086, 'learning_rate': 0.0001077847045233505, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 18.68it/s] 41%|████▏     | 31/75 [00:01<00:02, 20.46it/s]                                               {'loss': 2.295, 'grad_norm': 4.670914649963379, 'learning_rate': 0.00010544155877284288, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:02, 20.46it/s]                                               {'loss': 2.2947, 'grad_norm': 5.538929462432861, 'learning_rate': 0.00010309841302233527, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:02, 20.46it/s]                                               {'loss': 2.3308, 'grad_norm': 4.417627334594727, 'learning_rate': 0.00010075526727182766, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:02, 20.46it/s] 45%|████▌     | 34/75 [00:01<00:02, 19.84it/s]                                               {'loss': 2.2894, 'grad_norm': 4.448992729187012, 'learning_rate': 9.841212152132003e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:02, 19.84it/s]                                               {'loss': 1.9382, 'grad_norm': 4.192841053009033, 'learning_rate': 9.60689757708124e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:02, 19.84it/s]                                               {'loss': 1.9981, 'grad_norm': 5.850676536560059, 'learning_rate': 9.372583002030478e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 19.84it/s] 49%|████▉     | 37/75 [00:02<00:01, 19.73it/s]                                               {'loss': 2.3853, 'grad_norm': 4.787618160247803, 'learning_rate': 9.138268426979717e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:02<00:01, 19.73it/s]                                               {'loss': 2.31, 'grad_norm': 4.5420403480529785, 'learning_rate': 8.903953851928956e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:02<00:01, 19.73it/s]                                               {'loss': 2.2677, 'grad_norm': 4.1490302085876465, 'learning_rate': 8.669639276878193e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:02<00:01, 19.73it/s] 53%|█████▎    | 40/75 [00:02<00:01, 18.89it/s]                                               {'loss': 2.2641, 'grad_norm': 4.694804668426514, 'learning_rate': 8.435324701827431e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:02<00:01, 18.89it/s]                                               {'loss': 2.2262, 'grad_norm': 3.663259506225586, 'learning_rate': 8.20101012677667e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:02<00:01, 18.89it/s] 56%|█████▌    | 42/75 [00:02<00:01, 18.14it/s]                                               {'loss': 2.3225, 'grad_norm': 4.754180431365967, 'learning_rate': 7.966695551725907e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 18.14it/s]                                               {'loss': 2.1666, 'grad_norm': 4.355722904205322, 'learning_rate': 7.732380976675145e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 18.14it/s] 59%|█████▊    | 44/75 [00:02<00:01, 17.86it/s]                                               {'loss': 2.2677, 'grad_norm': 6.369437217712402, 'learning_rate': 7.498066401624384e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 17.86it/s]                                               {'loss': 1.9707, 'grad_norm': 8.819069862365723, 'learning_rate': 7.263751826573621e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 17.86it/s] 61%|██████▏   | 46/75 [00:02<00:01, 16.97it/s]                                               {'loss': 2.2765, 'grad_norm': 4.167717456817627, 'learning_rate': 7.02943725152286e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 16.97it/s]                                               {'loss': 2.1142, 'grad_norm': 4.117781162261963, 'learning_rate': 6.795122676472097e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 16.97it/s] 64%|██████▍   | 48/75 [00:02<00:01, 16.10it/s]                                               {'loss': 2.2428, 'grad_norm': 5.322790622711182, 'learning_rate': 6.560808101421335e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 16.10it/s]                                               {'loss': 2.3165, 'grad_norm': 3.5591583251953125, 'learning_rate': 6.326493526370572e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 16.10it/s] 67%|██████▋   | 50/75 [00:02<00:01, 15.36it/s]                                               {'loss': 1.9588, 'grad_norm': 3.829970121383667, 'learning_rate': 6.092178951319811e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 15.36it/s]                                               {'loss': 2.1367, 'grad_norm': 4.353275775909424, 'learning_rate': 5.857864376269049e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 15.36it/s]                                               {'loss': 2.0083, 'grad_norm': 3.5582985877990723, 'learning_rate': 5.623549801218288e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 15.36it/s] 71%|███████   | 53/75 [00:02<00:01, 17.34it/s]                                               {'loss': 2.2549, 'grad_norm': 3.7465932369232178, 'learning_rate': 5.389235226167525e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:01, 17.34it/s]                                               {'loss': 2.1817, 'grad_norm': 4.581561088562012, 'learning_rate': 5.1549206511167634e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:03<00:01, 17.34it/s] 73%|███████▎  | 55/75 [00:03<00:01, 17.31it/s]                                               {'loss': 2.3727, 'grad_norm': 4.123363018035889, 'learning_rate': 4.9206060760660015e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:03<00:01, 17.31it/s]                                               {'loss': 2.2377, 'grad_norm': 3.890493154525757, 'learning_rate': 4.686291501015239e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:03<00:01, 17.31it/s] 76%|███████▌  | 57/75 [00:03<00:01, 16.67it/s]                                               {'loss': 2.3378, 'grad_norm': 4.195979118347168, 'learning_rate': 4.451976925964478e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:03<00:01, 16.67it/s]                                               {'loss': 2.1588, 'grad_norm': 4.02944803237915, 'learning_rate': 4.2176623509137154e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:03<00:01, 16.67it/s] 79%|███████▊  | 59/75 [00:03<00:00, 17.43it/s]                                               {'loss': 2.2175, 'grad_norm': 5.641228675842285, 'learning_rate': 3.9833477758629536e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:03<00:00, 17.43it/s]                                               {'loss': 2.1127, 'grad_norm': 15.788813591003418, 'learning_rate': 3.749033200812192e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:03<00:00, 17.43it/s] 81%|████████▏ | 61/75 [00:03<00:00, 17.27it/s]                                               {'loss': 2.1203, 'grad_norm': 4.3036651611328125, 'learning_rate': 3.51471862576143e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:03<00:00, 17.27it/s]                                               {'loss': 2.2698, 'grad_norm': 4.076499938964844, 'learning_rate': 3.2804040507106675e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:03<00:00, 17.27it/s] 84%|████████▍ | 63/75 [00:03<00:00, 14.54it/s]                                               {'loss': 2.2059, 'grad_norm': 3.733354091644287, 'learning_rate': 3.0460894756599056e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 14.54it/s]                                               {'loss': 2.2015, 'grad_norm': 3.8742475509643555, 'learning_rate': 2.811774900609144e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 14.54it/s] 87%|████████▋ | 65/75 [00:03<00:00, 14.50it/s]                                               {'loss': 2.2068, 'grad_norm': 3.164240837097168, 'learning_rate': 2.5774603255583817e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 14.50it/s]                                               {'loss': 2.1218, 'grad_norm': 3.540707588195801, 'learning_rate': 2.3431457505076195e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 14.50it/s]                                               {'loss': 2.3438, 'grad_norm': 4.825617790222168, 'learning_rate': 2.1088311754568577e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 14.50it/s] 91%|█████████ | 68/75 [00:03<00:00, 17.10it/s]                                               {'loss': 2.1585, 'grad_norm': 4.5957746505737305, 'learning_rate': 1.874516600406096e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 17.10it/s]                                               {'loss': 2.0514, 'grad_norm': 4.539252281188965, 'learning_rate': 1.6402020253553337e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 17.10it/s]                                               {'loss': 2.1081, 'grad_norm': 3.7986793518066406, 'learning_rate': 1.405887450304572e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 17.10it/s] 95%|█████████▍| 71/75 [00:04<00:00, 18.77it/s]                                               {'loss': 2.1626, 'grad_norm': 5.140669345855713, 'learning_rate': 1.1715728752538098e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:04<00:00, 18.77it/s]                                               {'loss': 2.23, 'grad_norm': 3.7372543811798096, 'learning_rate': 9.37258300203048e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:04<00:00, 18.77it/s]                                               {'loss': 2.1213, 'grad_norm': 3.332690954208374, 'learning_rate': 7.02943725152286e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:04<00:00, 18.77it/s] 99%|█████████▊| 74/75 [00:04<00:00, 20.37it/s]                                               {'loss': 2.3735, 'grad_norm': 3.4704201221466064, 'learning_rate': 4.68629150101524e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:04<00:00, 20.37it/s]                                               {'loss': 1.8543, 'grad_norm': 10.291036605834961, 'learning_rate': 2.34314575050762e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 20.37it/s]                                               {'train_runtime': 4.4635, 'train_samples_per_second': 253.166, 'train_steps_per_second': 16.803, 'train_loss': 2.249332774480184, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 20.37it/s]100%|██████████| 75/75 [00:04<00:00, 16.80it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1338, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1055, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(975, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1209, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1416, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1238, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1362, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1250, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1299, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1218, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1398, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1077, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 6/471 [00:00<00:08, 51.88it/s]  3%|▎         | 12/471 [00:00<00:10, 44.04it/s]  4%|▎         | 17/471 [00:00<00:13, 33.96it/s]  4%|▍         | 21/471 [00:00<00:14, 31.54it/s]  5%|▌         | 25/471 [00:00<00:13, 32.12it/s]  6%|▌         | 29/471 [00:00<00:14, 30.75it/s]  7%|▋         | 33/471 [00:01<00:14, 29.78it/s]  8%|▊         | 37/471 [00:01<00:14, 30.14it/s]  9%|▉         | 42/471 [00:01<00:12, 34.03it/s] 10%|█         | 48/471 [00:01<00:10, 38.96it/s] 11%|█▏        | 53/471 [00:01<00:10, 39.68it/s] 12%|█▏        | 58/471 [00:01<00:10, 39.40it/s] 13%|█▎        | 63/471 [00:01<00:09, 42.11it/s] 14%|█▍        | 68/471 [00:01<00:09, 40.93it/s] 15%|█▌        | 73/471 [00:02<00:11, 35.27it/s] 16%|█▋        | 77/471 [00:02<00:11, 33.48it/s] 17%|█▋        | 81/471 [00:02<00:12, 31.87it/s] 18%|█▊        | 86/471 [00:02<00:11, 33.74it/s] 19%|█▉        | 90/471 [00:02<00:12, 30.83it/s] 20%|█▉        | 94/471 [00:02<00:12, 31.01it/s] 21%|██        | 98/471 [00:02<00:11, 31.57it/s] 22%|██▏       | 102/471 [00:02<00:11, 31.72it/s] 23%|██▎       | 106/471 [00:03<00:11, 32.76it/s] 23%|██▎       | 110/471 [00:03<00:10, 33.17it/s] 24%|██▍       | 114/471 [00:03<00:10, 33.63it/s] 25%|██▌       | 119/471 [00:03<00:09, 36.58it/s] 26%|██▋       | 124/471 [00:03<00:09, 38.55it/s] 28%|██▊       | 130/471 [00:03<00:07, 43.16it/s] 29%|██▊       | 135/471 [00:03<00:07, 42.51it/s] 30%|██▉       | 140/471 [00:03<00:08, 39.83it/s] 31%|███       | 145/471 [00:04<00:08, 38.49it/s] 32%|███▏      | 149/471 [00:04<00:08, 38.67it/s] 32%|███▏      | 153/471 [00:04<00:08, 36.01it/s] 33%|███▎      | 157/471 [00:04<00:08, 35.28it/s] 34%|███▍      | 161/471 [00:04<00:08, 34.45it/s] 35%|███▌      | 166/471 [00:04<00:08, 37.75it/s] 37%|███▋      | 172/471 [00:04<00:07, 42.47it/s] 38%|███▊      | 177/471 [00:04<00:06, 44.39it/s] 39%|███▊      | 182/471 [00:04<00:06, 42.90it/s] 40%|███▉      | 187/471 [00:05<00:06, 40.81it/s] 41%|████      | 192/471 [00:05<00:07, 38.95it/s] 42%|████▏     | 196/471 [00:05<00:07, 38.44it/s] 42%|████▏     | 200/471 [00:05<00:07, 37.60it/s] 44%|████▎     | 205/471 [00:05<00:06, 39.44it/s] 44%|████▍     | 209/471 [00:05<00:06, 38.14it/s] 45%|████▌     | 214/471 [00:05<00:06, 40.18it/s] 47%|████▋     | 220/471 [00:05<00:05, 41.92it/s] 48%|████▊     | 225/471 [00:06<00:05, 43.90it/s] 49%|████▉     | 230/471 [00:06<00:05, 44.44it/s] 50%|████▉     | 235/471 [00:06<00:06, 38.71it/s] 51%|█████     | 240/471 [00:06<00:06, 36.16it/s] 52%|█████▏    | 244/471 [00:06<00:06, 34.61it/s] 53%|█████▎    | 248/471 [00:06<00:06, 33.27it/s] 54%|█████▎    | 252/471 [00:06<00:06, 32.40it/s] 54%|█████▍    | 256/471 [00:06<00:06, 34.21it/s] 55%|█████▌    | 260/471 [00:07<00:06, 34.11it/s] 56%|█████▋    | 266/471 [00:07<00:05, 39.73it/s] 58%|█████▊    | 271/471 [00:07<00:05, 38.90it/s] 58%|█████▊    | 275/471 [00:07<00:05, 37.86it/s] 59%|█████▉    | 279/471 [00:07<00:05, 32.90it/s] 60%|██████    | 283/471 [00:07<00:05, 32.89it/s] 61%|██████    | 288/471 [00:07<00:05, 35.24it/s] 62%|██████▏   | 292/471 [00:08<00:05, 31.41it/s] 63%|██████▎   | 296/471 [00:08<00:06, 29.05it/s] 64%|██████▎   | 300/471 [00:08<00:05, 29.49it/s] 65%|██████▍   | 306/471 [00:08<00:04, 35.44it/s] 66%|██████▌   | 311/471 [00:08<00:04, 36.62it/s] 67%|██████▋   | 315/471 [00:08<00:04, 37.18it/s] 68%|██████▊   | 320/471 [00:08<00:03, 38.82it/s] 69%|██████▉   | 325/471 [00:08<00:03, 40.55it/s] 70%|███████   | 330/471 [00:09<00:03, 40.54it/s] 71%|███████   | 335/471 [00:09<00:03, 35.10it/s] 72%|███████▏  | 339/471 [00:09<00:04, 32.93it/s] 73%|███████▎  | 343/471 [00:09<00:03, 32.22it/s] 74%|███████▎  | 347/471 [00:09<00:03, 32.79it/s] 75%|███████▍  | 351/471 [00:09<00:03, 30.70it/s] 75%|███████▌  | 355/471 [00:09<00:03, 31.11it/s] 76%|███████▌  | 359/471 [00:10<00:03, 31.01it/s] 77%|███████▋  | 363/471 [00:10<00:03, 30.02it/s] 78%|███████▊  | 367/471 [00:10<00:03, 31.02it/s] 79%|███████▉  | 371/471 [00:10<00:03, 30.91it/s] 80%|███████▉  | 375/471 [00:10<00:03, 31.20it/s] 80%|████████  | 379/471 [00:10<00:02, 31.48it/s] 82%|████████▏ | 385/471 [00:10<00:02, 37.90it/s] 83%|████████▎ | 389/471 [00:10<00:02, 37.82it/s] 83%|████████▎ | 393/471 [00:11<00:02, 30.75it/s] 85%|████████▍ | 398/471 [00:11<00:02, 34.29it/s] 85%|████████▌ | 402/471 [00:11<00:01, 35.18it/s] 86%|████████▌ | 406/471 [00:11<00:02, 32.46it/s] 87%|████████▋ | 410/471 [00:11<00:01, 31.62it/s] 88%|████████▊ | 414/471 [00:11<00:01, 32.95it/s] 89%|████████▊ | 418/471 [00:11<00:01, 31.09it/s] 90%|████████▉ | 422/471 [00:11<00:01, 32.49it/s] 90%|█████████ | 426/471 [00:12<00:01, 31.36it/s] 92%|█████████▏| 431/471 [00:12<00:01, 34.50it/s] 93%|█████████▎| 437/471 [00:12<00:00, 39.69it/s] 94%|█████████▍| 442/471 [00:12<00:00, 41.01it/s] 95%|█████████▍| 447/471 [00:12<00:00, 41.97it/s] 96%|█████████▌| 452/471 [00:12<00:00, 43.63it/s] 97%|█████████▋| 457/471 [00:12<00:00, 40.98it/s] 98%|█████████▊| 463/471 [00:12<00:00, 44.59it/s] 99%|█████████▉| 468/471 [00:13<00:00, 40.63it/s]100%|██████████| 471/471 [00:13<00:00, 36.00it/s]
{'eval_loss': 2.395392656326294, 'eval_model_preparation_time': 0.011, 'eval_acc': 0.3593998937865109, 'eval_runtime': 13.1362, 'eval_samples_per_second': 573.376, 'eval_steps_per_second': 35.855}
ROUND:6
CLIENT:7
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4771, 'grad_norm': 4.100342750549316, 'learning_rate': 0.00016905249806888745, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.71it/s]                                              {'loss': 2.4934, 'grad_norm': 5.085531234741211, 'learning_rate': 0.0001667984647613023, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.65it/s]  4%|▍         | 3/75 [00:00<00:02, 24.93it/s]                                              {'loss': 2.2249, 'grad_norm': 6.422308444976807, 'learning_rate': 0.00016454443145371713, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.93it/s]                                              {'loss': 2.373, 'grad_norm': 4.84075927734375, 'learning_rate': 0.00016229039814613194, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.93it/s]                                              {'loss': 2.4013, 'grad_norm': 4.026150226593018, 'learning_rate': 0.00016003636483854678, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.93it/s]  8%|▊         | 6/75 [00:00<00:03, 21.88it/s]                                              {'loss': 2.4677, 'grad_norm': 6.545514106750488, 'learning_rate': 0.00015778233153096162, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 21.88it/s]                                              {'loss': 2.2664, 'grad_norm': 4.453869342803955, 'learning_rate': 0.00015552829822337646, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 21.88it/s]                                              {'loss': 2.1269, 'grad_norm': 4.498602390289307, 'learning_rate': 0.00015327426491579127, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 21.88it/s] 12%|█▏        | 9/75 [00:00<00:02, 22.20it/s]                                              {'loss': 2.4676, 'grad_norm': 4.653148174285889, 'learning_rate': 0.0001510202316082061, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 22.20it/s]                                              {'loss': 2.2932, 'grad_norm': 3.925153970718384, 'learning_rate': 0.00014876619830062095, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 22.20it/s]                                               {'loss': 2.2058, 'grad_norm': 4.0581512451171875, 'learning_rate': 0.0001465121649930358, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 22.20it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.23it/s]                                               {'loss': 2.4117, 'grad_norm': 6.755040168762207, 'learning_rate': 0.00014425813168545063, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.23it/s]                                               {'loss': 2.263, 'grad_norm': 4.2451558113098145, 'learning_rate': 0.00014200409837786544, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.23it/s]                                               {'loss': 2.0452, 'grad_norm': 2.9943103790283203, 'learning_rate': 0.00013975006507028028, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.23it/s]                                               {'loss': 2.5725, 'grad_norm': 12.758235931396484, 'learning_rate': 0.00013749603176269512, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.23it/s] 21%|██▏       | 16/75 [00:00<00:02, 25.59it/s]                                               {'loss': 2.227, 'grad_norm': 3.9856526851654053, 'learning_rate': 0.00013524199845510996, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.59it/s]                                               {'loss': 2.3689, 'grad_norm': 4.9758758544921875, 'learning_rate': 0.0001329879651475248, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.59it/s]                                               {'loss': 2.221, 'grad_norm': 3.659719467163086, 'learning_rate': 0.0001307339318399396, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.59it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.63it/s]                                               {'loss': 2.5756, 'grad_norm': 4.71866512298584, 'learning_rate': 0.00012847989853235445, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.63it/s]                                               {'loss': 2.2005, 'grad_norm': 4.652462959289551, 'learning_rate': 0.0001262258652247693, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.63it/s]                                               {'loss': 2.1105, 'grad_norm': 4.298466205596924, 'learning_rate': 0.00012397183191718413, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.63it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.87it/s]                                               {'loss': 2.0697, 'grad_norm': 3.710005044937134, 'learning_rate': 0.00012171779860959896, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.87it/s]                                               {'loss': 2.1013, 'grad_norm': 4.455718517303467, 'learning_rate': 0.0001194637653020138, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.87it/s]                                               {'loss': 2.2882, 'grad_norm': 5.165350437164307, 'learning_rate': 0.00011720973199442864, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.87it/s] 33%|███▎      | 25/75 [00:01<00:01, 25.38it/s]                                               {'loss': 2.4826, 'grad_norm': 4.518038749694824, 'learning_rate': 0.00011495569868684347, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:01, 25.38it/s]                                               {'loss': 2.319, 'grad_norm': 3.935786008834839, 'learning_rate': 0.0001127016653792583, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.38it/s]                                               {'loss': 2.1952, 'grad_norm': 3.542806625366211, 'learning_rate': 0.00011044763207167313, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.38it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.33it/s]                                               {'loss': 2.0448, 'grad_norm': 3.451892137527466, 'learning_rate': 0.00010819359876408797, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.33it/s]                                               {'loss': 2.0726, 'grad_norm': 4.154752254486084, 'learning_rate': 0.0001059395654565028, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.33it/s]                                               {'loss': 2.5166, 'grad_norm': 8.359686851501465, 'learning_rate': 0.00010368553214891763, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.33it/s]                                               {'loss': 2.1535, 'grad_norm': 3.7114436626434326, 'learning_rate': 0.00010143149884133247, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.33it/s] 43%|████▎     | 32/75 [00:01<00:01, 27.08it/s]                                               {'loss': 2.2896, 'grad_norm': 3.5542664527893066, 'learning_rate': 9.917746553374731e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 27.08it/s]                                               {'loss': 2.1238, 'grad_norm': 4.595011234283447, 'learning_rate': 9.692343222616215e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 27.08it/s]                                               {'loss': 2.1113, 'grad_norm': 4.454182147979736, 'learning_rate': 9.466939891857698e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 27.08it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.43it/s]                                               {'loss': 1.8586, 'grad_norm': 3.484661340713501, 'learning_rate': 9.24153656109918e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.43it/s]                                               {'loss': 2.1172, 'grad_norm': 3.876966714859009, 'learning_rate': 9.016133230340664e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.43it/s]                                               {'loss': 2.3394, 'grad_norm': 3.6588668823242188, 'learning_rate': 8.790729899582148e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.43it/s] 51%|█████     | 38/75 [00:01<00:01, 25.58it/s]                                               {'loss': 2.1898, 'grad_norm': 4.112941741943359, 'learning_rate': 8.565326568823632e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.58it/s]                                               {'loss': 2.2089, 'grad_norm': 4.663941860198975, 'learning_rate': 8.339923238065115e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.58it/s]                                               {'loss': 2.1047, 'grad_norm': 4.450045585632324, 'learning_rate': 8.114519907306597e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.58it/s] 55%|█████▍    | 41/75 [00:01<00:01, 24.90it/s]                                               {'loss': 2.2601, 'grad_norm': 4.288650035858154, 'learning_rate': 7.889116576548081e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.90it/s]                                               {'loss': 2.188, 'grad_norm': 3.5480430126190186, 'learning_rate': 7.663713245789564e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.90it/s]                                               {'loss': 2.2252, 'grad_norm': 3.7980453968048096, 'learning_rate': 7.438309915031048e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.90it/s] 59%|█████▊    | 44/75 [00:01<00:01, 24.33it/s]                                               {'loss': 2.3925, 'grad_norm': 5.352588653564453, 'learning_rate': 7.212906584272532e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.33it/s]                                               {'loss': 2.1319, 'grad_norm': 13.081521987915039, 'learning_rate': 6.987503253514014e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.33it/s]                                               {'loss': 2.2322, 'grad_norm': 4.3087687492370605, 'learning_rate': 6.762099922755498e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.33it/s]                                               {'loss': 2.2847, 'grad_norm': 4.253847599029541, 'learning_rate': 6.53669659199698e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 24.33it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.15it/s]                                               {'loss': 2.2685, 'grad_norm': 3.8207502365112305, 'learning_rate': 6.311293261238465e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.15it/s]                                               {'loss': 2.0431, 'grad_norm': 3.9924182891845703, 'learning_rate': 6.085889930479948e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.15it/s]                                               {'loss': 2.1621, 'grad_norm': 3.0633881092071533, 'learning_rate': 5.860486599721432e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.15it/s] 68%|██████▊   | 51/75 [00:02<00:00, 25.87it/s]                                               {'loss': 2.0543, 'grad_norm': 3.8306028842926025, 'learning_rate': 5.635083268962915e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.87it/s]                                               {'loss': 2.1303, 'grad_norm': 3.557638645172119, 'learning_rate': 5.409679938204398e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.87it/s]                                               {'loss': 2.1718, 'grad_norm': 4.015754222869873, 'learning_rate': 5.1842766074458816e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.87it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.51it/s]                                               {'loss': 2.2816, 'grad_norm': 4.4051432609558105, 'learning_rate': 4.9588732766873655e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.51it/s]                                               {'loss': 2.0933, 'grad_norm': 3.7211010456085205, 'learning_rate': 4.733469945928849e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.51it/s]                                               {'loss': 2.1489, 'grad_norm': 5.636397361755371, 'learning_rate': 4.508066615170332e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.51it/s] 76%|███████▌  | 57/75 [00:02<00:00, 25.00it/s]                                               {'loss': 2.2965, 'grad_norm': 4.0591044425964355, 'learning_rate': 4.282663284411816e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.00it/s]                                               {'loss': 2.0852, 'grad_norm': 2.7770469188690186, 'learning_rate': 4.0572599536532985e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.00it/s]                                               {'loss': 2.1852, 'grad_norm': 4.450368404388428, 'learning_rate': 3.831856622894782e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.00it/s] 80%|████████  | 60/75 [00:02<00:00, 26.11it/s]                                               {'loss': 2.0481, 'grad_norm': 12.64509391784668, 'learning_rate': 3.606453292136266e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 26.11it/s]                                               {'loss': 2.006, 'grad_norm': 3.9646759033203125, 'learning_rate': 3.381049961377749e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.11it/s]                                               {'loss': 2.3805, 'grad_norm': 4.931334972381592, 'learning_rate': 3.155646630619232e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.11it/s] 84%|████████▍ | 63/75 [00:02<00:00, 25.98it/s]                                               {'loss': 2.1796, 'grad_norm': 4.3270158767700195, 'learning_rate': 2.930243299860716e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.98it/s]                                               {'loss': 2.1447, 'grad_norm': 4.43679141998291, 'learning_rate': 2.704839969102199e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.98it/s]                                               {'loss': 2.0738, 'grad_norm': 2.773419141769409, 'learning_rate': 2.4794366383436827e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.98it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.29it/s]                                               {'loss': 2.1076, 'grad_norm': 3.626460313796997, 'learning_rate': 2.254033307585166e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.29it/s]                                               {'loss': 2.1146, 'grad_norm': 3.773134708404541, 'learning_rate': 2.0286299768266493e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.29it/s]                                               {'loss': 2.1261, 'grad_norm': 4.488196849822998, 'learning_rate': 1.803226646068133e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.29it/s] 92%|█████████▏| 69/75 [00:02<00:00, 25.28it/s]                                               {'loss': 1.9937, 'grad_norm': 5.027527332305908, 'learning_rate': 1.577823315309616e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.28it/s]                                               {'loss': 2.3472, 'grad_norm': 5.526711463928223, 'learning_rate': 1.3524199845510996e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.28it/s]                                               {'loss': 2.221, 'grad_norm': 3.799459934234619, 'learning_rate': 1.127016653792583e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.28it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.2241, 'grad_norm': 4.675932884216309, 'learning_rate': 9.016133230340664e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.2934, 'grad_norm': 4.4179205894470215, 'learning_rate': 6.762099922755498e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.0472, 'grad_norm': 4.1265363693237305, 'learning_rate': 4.508066615170332e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.5359, 'grad_norm': 12.845545768737793, 'learning_rate': 2.254033307585166e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.45it/s]                                               {'train_runtime': 3.0472, 'train_samples_per_second': 370.838, 'train_steps_per_second': 24.613, 'train_loss': 2.224785712560018, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.45it/s]100%|██████████| 75/75 [00:03<00:00, 24.62it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:48
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4686, 'grad_norm': 4.384919166564941, 'learning_rate': 0.00016905249806888745, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:05, 13.00it/s]  3%|▎         | 2/75 [00:00<00:04, 16.42it/s]                                              {'loss': 2.3757, 'grad_norm': 4.476662635803223, 'learning_rate': 0.0001667984647613023, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:04, 16.42it/s]                                              {'loss': 2.4877, 'grad_norm': 4.224750995635986, 'learning_rate': 0.00016454443145371713, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 16.42it/s]                                              {'loss': 2.2365, 'grad_norm': 4.503129482269287, 'learning_rate': 0.00016229039814613194, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:04, 16.42it/s]  7%|▋         | 5/75 [00:00<00:03, 21.70it/s]                                              {'loss': 2.3423, 'grad_norm': 3.937965154647827, 'learning_rate': 0.00016003636483854678, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 21.70it/s]                                              {'loss': 2.3323, 'grad_norm': 4.989857196807861, 'learning_rate': 0.00015778233153096162, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 21.70it/s]                                              {'loss': 2.5347, 'grad_norm': 4.689023017883301, 'learning_rate': 0.00015552829822337646, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 21.70it/s] 11%|█         | 8/75 [00:00<00:03, 20.50it/s]                                              {'loss': 2.2825, 'grad_norm': 4.627404689788818, 'learning_rate': 0.00015327426491579127, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 20.50it/s]                                              {'loss': 2.3004, 'grad_norm': 5.146225929260254, 'learning_rate': 0.0001510202316082061, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 20.50it/s]                                              {'loss': 2.3403, 'grad_norm': 4.462539196014404, 'learning_rate': 0.00014876619830062095, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 20.50it/s] 15%|█▍        | 11/75 [00:00<00:03, 20.25it/s]                                               {'loss': 2.4198, 'grad_norm': 3.7376883029937744, 'learning_rate': 0.0001465121649930358, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 20.25it/s]                                               {'loss': 2.3197, 'grad_norm': 4.2335991859436035, 'learning_rate': 0.00014425813168545063, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:03, 20.25it/s]                                               {'loss': 2.7335, 'grad_norm': 6.980671405792236, 'learning_rate': 0.00014200409837786544, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:03, 20.25it/s] 19%|█▊        | 14/75 [00:00<00:03, 19.42it/s]                                               {'loss': 2.1303, 'grad_norm': 3.842834711074829, 'learning_rate': 0.00013975006507028028, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:03, 19.42it/s]                                               {'loss': 1.9054, 'grad_norm': 10.39535903930664, 'learning_rate': 0.00013749603176269512, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:03, 19.42it/s]                                               {'loss': 2.4623, 'grad_norm': 4.116783618927002, 'learning_rate': 0.00013524199845510996, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:03, 19.42it/s] 23%|██▎       | 17/75 [00:00<00:02, 20.08it/s]                                               {'loss': 2.311, 'grad_norm': 3.9269630908966064, 'learning_rate': 0.0001329879651475248, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 20.08it/s]                                               {'loss': 2.683, 'grad_norm': 5.906798839569092, 'learning_rate': 0.0001307339318399396, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 20.08it/s]                                               {'loss': 2.2291, 'grad_norm': 4.300827503204346, 'learning_rate': 0.00012847989853235445, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 20.08it/s] 27%|██▋       | 20/75 [00:00<00:02, 21.89it/s]                                               {'loss': 2.2293, 'grad_norm': 5.139613628387451, 'learning_rate': 0.0001262258652247693, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 21.89it/s]                                               {'loss': 2.2381, 'grad_norm': 4.335690498352051, 'learning_rate': 0.00012397183191718413, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:02, 21.89it/s]                                               {'loss': 2.1088, 'grad_norm': 4.193059921264648, 'learning_rate': 0.00012171779860959896, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 21.89it/s] 31%|███       | 23/75 [00:01<00:02, 22.82it/s]                                               {'loss': 2.138, 'grad_norm': 4.09315299987793, 'learning_rate': 0.0001194637653020138, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 22.82it/s]                                               {'loss': 2.2395, 'grad_norm': 3.606677532196045, 'learning_rate': 0.00011720973199442864, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 22.82it/s]                                               {'loss': 2.193, 'grad_norm': 4.916414260864258, 'learning_rate': 0.00011495569868684347, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 22.82it/s] 35%|███▍      | 26/75 [00:01<00:02, 23.12it/s]                                               {'loss': 2.1882, 'grad_norm': 5.228968143463135, 'learning_rate': 0.0001127016653792583, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.12it/s]                                               {'loss': 2.3339, 'grad_norm': 6.107152938842773, 'learning_rate': 0.00011044763207167313, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 23.12it/s]                                               {'loss': 2.2563, 'grad_norm': 4.589947700500488, 'learning_rate': 0.00010819359876408797, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 23.12it/s] 39%|███▊      | 29/75 [00:01<00:01, 23.63it/s]                                               {'loss': 2.2561, 'grad_norm': 5.18635368347168, 'learning_rate': 0.0001059395654565028, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 23.63it/s]                                               {'loss': 1.9481, 'grad_norm': 8.532648086547852, 'learning_rate': 0.00010368553214891763, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.63it/s]                                               {'loss': 2.2503, 'grad_norm': 4.645798206329346, 'learning_rate': 0.00010143149884133247, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 23.63it/s]                                               {'loss': 2.198, 'grad_norm': 3.9637436866760254, 'learning_rate': 9.917746553374731e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 23.63it/s] 44%|████▍     | 33/75 [00:01<00:01, 25.55it/s]                                               {'loss': 2.361, 'grad_norm': 4.370887756347656, 'learning_rate': 9.692343222616215e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.55it/s]                                               {'loss': 2.2289, 'grad_norm': 4.064785480499268, 'learning_rate': 9.466939891857698e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.55it/s]                                               {'loss': 2.2014, 'grad_norm': 3.779679298400879, 'learning_rate': 9.24153656109918e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.55it/s] 48%|████▊     | 36/75 [00:01<00:01, 25.61it/s]                                               {'loss': 2.2204, 'grad_norm': 4.724961280822754, 'learning_rate': 9.016133230340664e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.61it/s]                                               {'loss': 2.2106, 'grad_norm': 4.158269882202148, 'learning_rate': 8.790729899582148e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.61it/s]                                               {'loss': 2.198, 'grad_norm': 4.732751846313477, 'learning_rate': 8.565326568823632e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.61it/s] 52%|█████▏    | 39/75 [00:01<00:01, 25.75it/s]                                               {'loss': 2.3308, 'grad_norm': 4.773374080657959, 'learning_rate': 8.339923238065115e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.75it/s]                                               {'loss': 2.3628, 'grad_norm': 4.153939723968506, 'learning_rate': 8.114519907306597e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.75it/s]                                               {'loss': 2.117, 'grad_norm': 3.8948497772216797, 'learning_rate': 7.889116576548081e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.75it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.43it/s]                                               {'loss': 2.0611, 'grad_norm': 6.27527379989624, 'learning_rate': 7.663713245789564e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.43it/s]                                               {'loss': 2.3634, 'grad_norm': 5.465179920196533, 'learning_rate': 7.438309915031048e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.43it/s]                                               {'loss': 2.4127, 'grad_norm': 3.6367039680480957, 'learning_rate': 7.212906584272532e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.43it/s] 60%|██████    | 45/75 [00:01<00:01, 24.17it/s]                                               {'loss': 2.4081, 'grad_norm': 12.84171199798584, 'learning_rate': 6.987503253514014e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.17it/s]                                               {'loss': 2.0561, 'grad_norm': 3.5723886489868164, 'learning_rate': 6.762099922755498e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 24.17it/s]                                               {'loss': 2.4154, 'grad_norm': 3.8816332817077637, 'learning_rate': 6.53669659199698e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 24.17it/s] 64%|██████▍   | 48/75 [00:02<00:01, 21.74it/s]                                               {'loss': 2.2148, 'grad_norm': 5.967471122741699, 'learning_rate': 6.311293261238465e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 21.74it/s]                                               {'loss': 2.1157, 'grad_norm': 5.681439399719238, 'learning_rate': 6.085889930479948e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 21.74it/s]                                               {'loss': 2.097, 'grad_norm': 2.6803269386291504, 'learning_rate': 5.860486599721432e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 21.74it/s] 68%|██████▊   | 51/75 [00:02<00:01, 21.04it/s]                                               {'loss': 2.1481, 'grad_norm': 4.255552291870117, 'learning_rate': 5.635083268962915e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 21.04it/s]                                               {'loss': 2.4879, 'grad_norm': 3.9997246265411377, 'learning_rate': 5.409679938204398e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 21.04it/s]                                               {'loss': 2.4488, 'grad_norm': 5.663461685180664, 'learning_rate': 5.1842766074458816e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:01, 21.04it/s] 72%|███████▏  | 54/75 [00:02<00:00, 22.20it/s]                                               {'loss': 2.3482, 'grad_norm': 5.139915943145752, 'learning_rate': 4.9588732766873655e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 22.20it/s]                                               {'loss': 2.2912, 'grad_norm': 5.4921746253967285, 'learning_rate': 4.733469945928849e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 22.20it/s]                                               {'loss': 2.2463, 'grad_norm': 3.187864065170288, 'learning_rate': 4.508066615170332e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 22.20it/s] 76%|███████▌  | 57/75 [00:02<00:00, 23.18it/s]                                               {'loss': 1.957, 'grad_norm': 4.38839864730835, 'learning_rate': 4.282663284411816e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 23.18it/s]                                               {'loss': 2.1331, 'grad_norm': 4.381572723388672, 'learning_rate': 4.0572599536532985e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 23.18it/s]                                               {'loss': 2.2194, 'grad_norm': 4.007364749908447, 'learning_rate': 3.831856622894782e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 23.18it/s]                                               {'loss': 2.3911, 'grad_norm': 7.908128261566162, 'learning_rate': 3.606453292136266e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 23.18it/s] 81%|████████▏ | 61/75 [00:02<00:00, 24.25it/s]                                               {'loss': 2.2071, 'grad_norm': 3.656999111175537, 'learning_rate': 3.381049961377749e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.25it/s]                                               {'loss': 2.3311, 'grad_norm': 4.292663097381592, 'learning_rate': 3.155646630619232e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 24.25it/s]                                               {'loss': 2.2555, 'grad_norm': 4.697638034820557, 'learning_rate': 2.930243299860716e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 24.25it/s] 85%|████████▌ | 64/75 [00:02<00:00, 24.30it/s]                                               {'loss': 1.9737, 'grad_norm': 4.537535190582275, 'learning_rate': 2.704839969102199e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 24.30it/s]                                               {'loss': 2.0381, 'grad_norm': 3.9451704025268555, 'learning_rate': 2.4794366383436827e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.30it/s]                                               {'loss': 2.3147, 'grad_norm': 4.455557823181152, 'learning_rate': 2.254033307585166e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.30it/s] 89%|████████▉ | 67/75 [00:02<00:00, 23.65it/s]                                               {'loss': 2.2168, 'grad_norm': 4.255216121673584, 'learning_rate': 2.0286299768266493e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 23.65it/s]                                               {'loss': 2.0664, 'grad_norm': 5.095088958740234, 'learning_rate': 1.803226646068133e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 23.65it/s]                                               {'loss': 2.2223, 'grad_norm': 4.957783222198486, 'learning_rate': 1.577823315309616e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 23.65it/s] 93%|█████████▎| 70/75 [00:03<00:00, 24.24it/s]                                               {'loss': 2.231, 'grad_norm': 4.419888973236084, 'learning_rate': 1.3524199845510996e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 24.24it/s]                                               {'loss': 2.3189, 'grad_norm': 4.645761489868164, 'learning_rate': 1.127016653792583e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 24.24it/s]                                               {'loss': 2.3347, 'grad_norm': 4.678273677825928, 'learning_rate': 9.016133230340664e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 24.24it/s] 97%|█████████▋| 73/75 [00:03<00:00, 24.48it/s]                                               {'loss': 2.1994, 'grad_norm': 3.561265707015991, 'learning_rate': 6.762099922755498e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 24.48it/s]                                               {'loss': 2.1141, 'grad_norm': 3.347538948059082, 'learning_rate': 4.508066615170332e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.48it/s]                                               {'loss': 2.1566, 'grad_norm': 12.379326820373535, 'learning_rate': 2.254033307585166e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.48it/s]                                               {'train_runtime': 3.4714, 'train_samples_per_second': 325.52, 'train_steps_per_second': 21.605, 'train_loss': 2.25999564965566, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.48it/s]100%|██████████| 75/75 [00:03<00:00, 21.61it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3578, 'grad_norm': 4.615830898284912, 'learning_rate': 0.00016905249806888745, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 21.32it/s]                                              {'loss': 2.5746, 'grad_norm': 4.227554798126221, 'learning_rate': 0.0001667984647613023, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.69it/s]  4%|▍         | 3/75 [00:00<00:03, 21.63it/s]                                              {'loss': 2.2724, 'grad_norm': 4.683129787445068, 'learning_rate': 0.00016454443145371713, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 21.63it/s]                                              {'loss': 2.1772, 'grad_norm': 4.993491172790527, 'learning_rate': 0.00016229039814613194, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 21.63it/s]                                              {'loss': 2.2664, 'grad_norm': 4.0101518630981445, 'learning_rate': 0.00016003636483854678, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 21.63it/s]  8%|▊         | 6/75 [00:00<00:03, 18.06it/s]                                              {'loss': 2.5552, 'grad_norm': 4.36137580871582, 'learning_rate': 0.00015778233153096162, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 18.06it/s]                                              {'loss': 2.4132, 'grad_norm': 4.084733009338379, 'learning_rate': 0.00015552829822337646, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 18.06it/s]                                              {'loss': 2.1836, 'grad_norm': 4.165581703186035, 'learning_rate': 0.00015327426491579127, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 18.06it/s] 12%|█▏        | 9/75 [00:00<00:03, 20.57it/s]                                              {'loss': 2.2763, 'grad_norm': 4.060871601104736, 'learning_rate': 0.0001510202316082061, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 20.57it/s]                                              {'loss': 2.1928, 'grad_norm': 4.694640159606934, 'learning_rate': 0.00014876619830062095, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 20.57it/s]                                               {'loss': 2.3758, 'grad_norm': 4.571774005889893, 'learning_rate': 0.0001465121649930358, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 20.57it/s] 16%|█▌        | 12/75 [00:00<00:02, 21.65it/s]                                               {'loss': 2.3219, 'grad_norm': 4.03678035736084, 'learning_rate': 0.00014425813168545063, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 21.65it/s]                                               {'loss': 2.3879, 'grad_norm': 3.916597843170166, 'learning_rate': 0.00014200409837786544, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 21.65it/s]                                               {'loss': 2.3748, 'grad_norm': 5.104308605194092, 'learning_rate': 0.00013975006507028028, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 21.65it/s] 20%|██        | 15/75 [00:00<00:02, 22.42it/s]                                               {'loss': 2.5734, 'grad_norm': 12.63179874420166, 'learning_rate': 0.00013749603176269512, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 22.42it/s]                                               {'loss': 2.3166, 'grad_norm': 4.113429069519043, 'learning_rate': 0.00013524199845510996, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 22.42it/s]                                               {'loss': 2.3956, 'grad_norm': 4.693833351135254, 'learning_rate': 0.0001329879651475248, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 22.42it/s] 24%|██▍       | 18/75 [00:00<00:02, 22.18it/s]                                               {'loss': 2.3378, 'grad_norm': 4.8054609298706055, 'learning_rate': 0.0001307339318399396, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 22.18it/s]                                               {'loss': 2.2626, 'grad_norm': 3.6523590087890625, 'learning_rate': 0.00012847989853235445, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 22.18it/s]                                               {'loss': 2.3773, 'grad_norm': 3.8615589141845703, 'learning_rate': 0.0001262258652247693, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 22.18it/s] 28%|██▊       | 21/75 [00:00<00:02, 22.06it/s]                                               {'loss': 2.3823, 'grad_norm': 3.826979875564575, 'learning_rate': 0.00012397183191718413, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 22.06it/s]                                               {'loss': 2.2704, 'grad_norm': 4.976617813110352, 'learning_rate': 0.00012171779860959896, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 22.06it/s]                                               {'loss': 2.3264, 'grad_norm': 4.149533748626709, 'learning_rate': 0.0001194637653020138, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 22.06it/s] 32%|███▏      | 24/75 [00:01<00:02, 21.68it/s]                                               {'loss': 2.1866, 'grad_norm': 4.25326681137085, 'learning_rate': 0.00011720973199442864, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 21.68it/s]                                               {'loss': 2.3029, 'grad_norm': 4.49080753326416, 'learning_rate': 0.00011495569868684347, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 21.68it/s]                                               {'loss': 2.1499, 'grad_norm': 4.474099159240723, 'learning_rate': 0.0001127016653792583, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 21.68it/s] 36%|███▌      | 27/75 [00:01<00:02, 20.83it/s]                                               {'loss': 2.144, 'grad_norm': 4.417834758758545, 'learning_rate': 0.00011044763207167313, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 20.83it/s]                                               {'loss': 2.1778, 'grad_norm': 3.240206718444824, 'learning_rate': 0.00010819359876408797, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 20.83it/s]                                               {'loss': 2.0828, 'grad_norm': 4.098537445068359, 'learning_rate': 0.0001059395654565028, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 20.83it/s] 40%|████      | 30/75 [00:01<00:02, 17.49it/s]                                               {'loss': 2.8596, 'grad_norm': 13.328750610351562, 'learning_rate': 0.00010368553214891763, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 17.49it/s]                                               {'loss': 2.1063, 'grad_norm': 3.762044906616211, 'learning_rate': 0.00010143149884133247, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:02, 17.49it/s] 43%|████▎     | 32/75 [00:01<00:02, 15.00it/s]                                               {'loss': 2.3549, 'grad_norm': 4.1660332679748535, 'learning_rate': 9.917746553374731e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:02, 15.00it/s]                                               {'loss': 2.3871, 'grad_norm': 4.016019344329834, 'learning_rate': 9.692343222616215e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:02, 15.00it/s] 45%|████▌     | 34/75 [00:01<00:02, 15.28it/s]                                               {'loss': 2.2864, 'grad_norm': 3.750964879989624, 'learning_rate': 9.466939891857698e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:02, 15.28it/s]                                               {'loss': 2.0062, 'grad_norm': 3.1439664363861084, 'learning_rate': 9.24153656109918e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:02, 15.28it/s]                                               {'loss': 2.1882, 'grad_norm': 4.519443511962891, 'learning_rate': 9.016133230340664e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:02, 15.28it/s] 49%|████▉     | 37/75 [00:01<00:02, 17.20it/s]                                               {'loss': 2.2584, 'grad_norm': 3.462416172027588, 'learning_rate': 8.790729899582148e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:02, 17.20it/s]                                               {'loss': 2.3097, 'grad_norm': 4.05409049987793, 'learning_rate': 8.565326568823632e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:02<00:02, 17.20it/s]                                               {'loss': 2.1935, 'grad_norm': 3.646240472793579, 'learning_rate': 8.339923238065115e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:02<00:02, 17.20it/s] 53%|█████▎    | 40/75 [00:02<00:01, 19.22it/s]                                               {'loss': 2.1764, 'grad_norm': 3.8128058910369873, 'learning_rate': 8.114519907306597e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:02<00:01, 19.22it/s]                                               {'loss': 2.152, 'grad_norm': 3.462043285369873, 'learning_rate': 7.889116576548081e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:02<00:01, 19.22it/s]                                               {'loss': 2.2708, 'grad_norm': 3.565674066543579, 'learning_rate': 7.663713245789564e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 19.22it/s] 57%|█████▋    | 43/75 [00:02<00:01, 20.14it/s]                                               {'loss': 1.9877, 'grad_norm': 4.439443111419678, 'learning_rate': 7.438309915031048e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 20.14it/s]                                               {'loss': 2.2042, 'grad_norm': 3.6406216621398926, 'learning_rate': 7.212906584272532e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 20.14it/s]                                               {'loss': 2.8499, 'grad_norm': 12.782896995544434, 'learning_rate': 6.987503253514014e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 20.14it/s] 61%|██████▏   | 46/75 [00:02<00:01, 21.57it/s]                                               {'loss': 2.2722, 'grad_norm': 3.320420742034912, 'learning_rate': 6.762099922755498e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 21.57it/s]                                               {'loss': 2.4267, 'grad_norm': 4.5984721183776855, 'learning_rate': 6.53669659199698e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 21.57it/s]                                               {'loss': 2.2541, 'grad_norm': 4.8877434730529785, 'learning_rate': 6.311293261238465e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 21.57it/s] 65%|██████▌   | 49/75 [00:02<00:01, 20.13it/s]                                               {'loss': 2.1285, 'grad_norm': 3.3997092247009277, 'learning_rate': 6.085889930479948e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 20.13it/s]                                               {'loss': 2.0382, 'grad_norm': 3.06313419342041, 'learning_rate': 5.860486599721432e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 20.13it/s]                                               {'loss': 2.1555, 'grad_norm': 3.4464194774627686, 'learning_rate': 5.635083268962915e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 20.13it/s] 69%|██████▉   | 52/75 [00:02<00:01, 19.93it/s]                                               {'loss': 2.229, 'grad_norm': 4.133340358734131, 'learning_rate': 5.409679938204398e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 19.93it/s]                                               {'loss': 1.9575, 'grad_norm': 4.339870452880859, 'learning_rate': 5.1842766074458816e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:01, 19.93it/s]                                               {'loss': 2.4924, 'grad_norm': 5.035992622375488, 'learning_rate': 4.9588732766873655e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:01, 19.93it/s] 73%|███████▎  | 55/75 [00:02<00:01, 19.76it/s]                                               {'loss': 2.0476, 'grad_norm': 3.681913375854492, 'learning_rate': 4.733469945928849e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:01, 19.76it/s]                                               {'loss': 2.1846, 'grad_norm': 3.648435354232788, 'learning_rate': 4.508066615170332e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 19.76it/s]                                               {'loss': 2.1103, 'grad_norm': 3.9308276176452637, 'learning_rate': 4.282663284411816e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 19.76it/s] 77%|███████▋  | 58/75 [00:02<00:00, 19.66it/s]                                               {'loss': 2.3316, 'grad_norm': 3.159593343734741, 'learning_rate': 4.0572599536532985e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 19.66it/s]                                               {'loss': 2.1042, 'grad_norm': 3.7390902042388916, 'learning_rate': 3.831856622894782e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:03<00:00, 19.66it/s]                                               {'loss': 2.0685, 'grad_norm': 9.99805736541748, 'learning_rate': 3.606453292136266e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:03<00:00, 19.66it/s] 81%|████████▏ | 61/75 [00:03<00:00, 21.79it/s]                                               {'loss': 1.9943, 'grad_norm': 3.5764853954315186, 'learning_rate': 3.381049961377749e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:03<00:00, 21.79it/s]                                               {'loss': 2.1247, 'grad_norm': 4.321189880371094, 'learning_rate': 3.155646630619232e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:03<00:00, 21.79it/s]                                               {'loss': 2.2817, 'grad_norm': 4.252259254455566, 'learning_rate': 2.930243299860716e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 21.79it/s] 85%|████████▌ | 64/75 [00:03<00:00, 22.07it/s]                                               {'loss': 2.0655, 'grad_norm': 3.1428165435791016, 'learning_rate': 2.704839969102199e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 22.07it/s]                                               {'loss': 2.2142, 'grad_norm': 4.082977294921875, 'learning_rate': 2.4794366383436827e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 22.07it/s]                                               {'loss': 2.1456, 'grad_norm': 3.8019156455993652, 'learning_rate': 2.254033307585166e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 22.07it/s] 89%|████████▉ | 67/75 [00:03<00:00, 22.38it/s]                                               {'loss': 2.2402, 'grad_norm': 4.656774997711182, 'learning_rate': 2.0286299768266493e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 22.38it/s]                                               {'loss': 2.2565, 'grad_norm': 4.034111022949219, 'learning_rate': 1.803226646068133e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 22.38it/s]                                               {'loss': 2.3934, 'grad_norm': 4.362821578979492, 'learning_rate': 1.577823315309616e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 22.38it/s] 93%|█████████▎| 70/75 [00:03<00:00, 22.08it/s]                                               {'loss': 2.0232, 'grad_norm': 2.960423707962036, 'learning_rate': 1.3524199845510996e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 22.08it/s]                                               {'loss': 2.0483, 'grad_norm': 3.3705012798309326, 'learning_rate': 1.127016653792583e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 22.08it/s]                                               {'loss': 2.2676, 'grad_norm': 3.944131851196289, 'learning_rate': 9.016133230340664e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 22.08it/s] 97%|█████████▋| 73/75 [00:03<00:00, 21.78it/s]                                               {'loss': 2.3734, 'grad_norm': 4.344269752502441, 'learning_rate': 6.762099922755498e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 21.78it/s]                                               {'loss': 2.2103, 'grad_norm': 3.7872889041900635, 'learning_rate': 4.508066615170332e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 21.78it/s]                                               {'loss': 1.8403, 'grad_norm': 10.738192558288574, 'learning_rate': 2.254033307585166e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 21.78it/s]                                               {'train_runtime': 3.9179, 'train_samples_per_second': 288.42, 'train_steps_per_second': 19.143, 'train_loss': 2.2518401511510215, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 21.78it/s]100%|██████████| 75/75 [00:03<00:00, 19.14it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:10
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.5147, 'grad_norm': 6.642924785614014, 'learning_rate': 0.00016905249806888745, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:05, 14.44it/s]  3%|▎         | 2/75 [00:00<00:03, 18.36it/s]                                              {'loss': 2.3764, 'grad_norm': 4.665566921234131, 'learning_rate': 0.0001667984647613023, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 18.36it/s]                                              {'loss': 2.2652, 'grad_norm': 4.783797740936279, 'learning_rate': 0.00016454443145371713, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 18.36it/s]                                              {'loss': 2.2472, 'grad_norm': 5.046255111694336, 'learning_rate': 0.00016229039814613194, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 18.36it/s]  7%|▋         | 5/75 [00:00<00:03, 22.79it/s]                                              {'loss': 2.432, 'grad_norm': 4.262214660644531, 'learning_rate': 0.00016003636483854678, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.79it/s]                                              {'loss': 2.4798, 'grad_norm': 4.47382926940918, 'learning_rate': 0.00015778233153096162, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 22.79it/s]                                              {'loss': 2.4903, 'grad_norm': 5.39407205581665, 'learning_rate': 0.00015552829822337646, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 22.79it/s] 11%|█         | 8/75 [00:00<00:02, 24.46it/s]                                              {'loss': 2.6183, 'grad_norm': 3.9821243286132812, 'learning_rate': 0.00015327426491579127, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.46it/s]                                              {'loss': 2.2576, 'grad_norm': 4.571722507476807, 'learning_rate': 0.0001510202316082061, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.46it/s]                                              {'loss': 2.2328, 'grad_norm': 4.572339057922363, 'learning_rate': 0.00014876619830062095, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.46it/s] 15%|█▍        | 11/75 [00:00<00:02, 23.59it/s]                                               {'loss': 2.4102, 'grad_norm': 4.879321098327637, 'learning_rate': 0.0001465121649930358, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.59it/s]                                               {'loss': 2.44, 'grad_norm': 5.874456405639648, 'learning_rate': 0.00014425813168545063, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.59it/s]                                               {'loss': 2.2611, 'grad_norm': 4.333286285400391, 'learning_rate': 0.00014200409837786544, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.59it/s] 19%|█▊        | 14/75 [00:00<00:02, 23.89it/s]                                               {'loss': 2.1789, 'grad_norm': 3.8764758110046387, 'learning_rate': 0.00013975006507028028, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.89it/s]                                               {'loss': 2.0081, 'grad_norm': 10.53853988647461, 'learning_rate': 0.00013749603176269512, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.89it/s]                                               {'loss': 2.2722, 'grad_norm': 3.842597007751465, 'learning_rate': 0.00013524199845510996, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 23.89it/s] 23%|██▎       | 17/75 [00:00<00:02, 24.35it/s]                                               {'loss': 2.3332, 'grad_norm': 5.186136722564697, 'learning_rate': 0.0001329879651475248, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.35it/s]                                               {'loss': 2.3501, 'grad_norm': 3.9740042686462402, 'learning_rate': 0.0001307339318399396, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.35it/s]                                               {'loss': 2.2097, 'grad_norm': 3.832967519760132, 'learning_rate': 0.00012847989853235445, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.35it/s] 27%|██▋       | 20/75 [00:00<00:02, 24.59it/s]                                               {'loss': 2.435, 'grad_norm': 4.240364074707031, 'learning_rate': 0.0001262258652247693, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.59it/s]                                               {'loss': 2.2226, 'grad_norm': 5.461067199707031, 'learning_rate': 0.00012397183191718413, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.59it/s]                                               {'loss': 2.3201, 'grad_norm': 4.197547912597656, 'learning_rate': 0.00012171779860959896, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.59it/s] 31%|███       | 23/75 [00:00<00:02, 24.37it/s]                                               {'loss': 2.2434, 'grad_norm': 5.058077812194824, 'learning_rate': 0.0001194637653020138, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.37it/s]                                               {'loss': 2.0673, 'grad_norm': 3.3941519260406494, 'learning_rate': 0.00011720973199442864, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 24.37it/s]                                               {'loss': 2.256, 'grad_norm': 4.163169860839844, 'learning_rate': 0.00011495569868684347, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.37it/s] 35%|███▍      | 26/75 [00:01<00:02, 24.48it/s]                                               {'loss': 2.2143, 'grad_norm': 3.806605339050293, 'learning_rate': 0.0001127016653792583, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.48it/s]                                               {'loss': 2.5195, 'grad_norm': 4.244928359985352, 'learning_rate': 0.00011044763207167313, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.48it/s]                                               {'loss': 2.2242, 'grad_norm': 3.729750394821167, 'learning_rate': 0.00010819359876408797, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.48it/s] 39%|███▊      | 29/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.294, 'grad_norm': 4.495086193084717, 'learning_rate': 0.0001059395654565028, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.348, 'grad_norm': 11.902395248413086, 'learning_rate': 0.00010368553214891763, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.2249, 'grad_norm': 4.190937519073486, 'learning_rate': 0.00010143149884133247, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.2275, 'grad_norm': 4.042281627655029, 'learning_rate': 9.917746553374731e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.56it/s] 44%|████▍     | 33/75 [00:01<00:01, 26.24it/s]                                               {'loss': 2.4038, 'grad_norm': 5.00311279296875, 'learning_rate': 9.692343222616215e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.24it/s]                                               {'loss': 2.4409, 'grad_norm': 4.617124557495117, 'learning_rate': 9.466939891857698e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.24it/s]                                               {'loss': 2.2718, 'grad_norm': 4.189167022705078, 'learning_rate': 9.24153656109918e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.24it/s] 48%|████▊     | 36/75 [00:01<00:01, 25.99it/s]                                               {'loss': 2.4351, 'grad_norm': 4.249216079711914, 'learning_rate': 9.016133230340664e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.99it/s]                                               {'loss': 2.3622, 'grad_norm': 6.0017619132995605, 'learning_rate': 8.790729899582148e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.99it/s]                                               {'loss': 2.0284, 'grad_norm': 4.443597793579102, 'learning_rate': 8.565326568823632e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.99it/s] 52%|█████▏    | 39/75 [00:01<00:01, 25.76it/s]                                               {'loss': 2.0699, 'grad_norm': 3.5752391815185547, 'learning_rate': 8.339923238065115e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.76it/s]                                               {'loss': 2.3159, 'grad_norm': 4.150018692016602, 'learning_rate': 8.114519907306597e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.76it/s]                                               {'loss': 2.2152, 'grad_norm': 4.977497100830078, 'learning_rate': 7.889116576548081e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.76it/s] 56%|█████▌    | 42/75 [00:01<00:01, 25.10it/s]                                               {'loss': 2.2728, 'grad_norm': 3.939594030380249, 'learning_rate': 7.663713245789564e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.10it/s]                                               {'loss': 2.2006, 'grad_norm': 4.21594762802124, 'learning_rate': 7.438309915031048e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.10it/s]                                               {'loss': 2.032, 'grad_norm': 3.7709240913391113, 'learning_rate': 7.212906584272532e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.10it/s]                                               {'loss': 2.0007, 'grad_norm': 11.631002426147461, 'learning_rate': 6.987503253514014e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.10it/s] 61%|██████▏   | 46/75 [00:01<00:01, 26.23it/s]                                               {'loss': 2.2485, 'grad_norm': 4.558467388153076, 'learning_rate': 6.762099922755498e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.23it/s]                                               {'loss': 2.1045, 'grad_norm': 4.457686424255371, 'learning_rate': 6.53669659199698e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.23it/s]                                               {'loss': 2.1787, 'grad_norm': 4.65854024887085, 'learning_rate': 6.311293261238465e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.23it/s] 65%|██████▌   | 49/75 [00:01<00:00, 26.11it/s]                                               {'loss': 2.2897, 'grad_norm': 4.777988910675049, 'learning_rate': 6.085889930479948e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.11it/s]                                               {'loss': 2.4336, 'grad_norm': 4.80723237991333, 'learning_rate': 5.860486599721432e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.11it/s]                                               {'loss': 2.4375, 'grad_norm': 4.7951226234436035, 'learning_rate': 5.635083268962915e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 26.11it/s] 69%|██████▉   | 52/75 [00:02<00:00, 25.37it/s]                                               {'loss': 2.1016, 'grad_norm': 3.5134520530700684, 'learning_rate': 5.409679938204398e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.37it/s]                                               {'loss': 2.2258, 'grad_norm': 5.1925740242004395, 'learning_rate': 5.1842766074458816e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.37it/s]                                               {'loss': 1.9355, 'grad_norm': 3.746636390686035, 'learning_rate': 4.9588732766873655e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.37it/s] 73%|███████▎  | 55/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.1734, 'grad_norm': 3.93607497215271, 'learning_rate': 4.733469945928849e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.2105, 'grad_norm': 4.366276741027832, 'learning_rate': 4.508066615170332e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.0878, 'grad_norm': 4.399930477142334, 'learning_rate': 4.282663284411816e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.33it/s] 77%|███████▋  | 58/75 [00:02<00:00, 25.59it/s]                                               {'loss': 2.2878, 'grad_norm': 3.5781476497650146, 'learning_rate': 4.0572599536532985e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.59it/s]                                               {'loss': 2.2739, 'grad_norm': 3.561988592147827, 'learning_rate': 3.831856622894782e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.59it/s]                                               {'loss': 2.6174, 'grad_norm': 16.22562599182129, 'learning_rate': 3.606453292136266e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.59it/s] 81%|████████▏ | 61/75 [00:02<00:00, 26.19it/s]                                               {'loss': 2.2383, 'grad_norm': 4.139847278594971, 'learning_rate': 3.381049961377749e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.19it/s]                                               {'loss': 2.1407, 'grad_norm': 4.2835283279418945, 'learning_rate': 3.155646630619232e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.19it/s]                                               {'loss': 2.1794, 'grad_norm': 3.9228713512420654, 'learning_rate': 2.930243299860716e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.19it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.96it/s]                                               {'loss': 2.1107, 'grad_norm': 4.482728004455566, 'learning_rate': 2.704839969102199e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.96it/s]                                               {'loss': 2.0443, 'grad_norm': 3.7334728240966797, 'learning_rate': 2.4794366383436827e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.96it/s]                                               {'loss': 2.28, 'grad_norm': 3.375425338745117, 'learning_rate': 2.254033307585166e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.96it/s] 89%|████████▉ | 67/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.1892, 'grad_norm': 3.762881278991699, 'learning_rate': 2.0286299768266493e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.0837, 'grad_norm': 5.059850215911865, 'learning_rate': 1.803226646068133e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.3243, 'grad_norm': 3.8149685859680176, 'learning_rate': 1.577823315309616e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.23it/s] 93%|█████████▎| 70/75 [00:02<00:00, 25.25it/s]                                               {'loss': 2.3345, 'grad_norm': 3.421718120574951, 'learning_rate': 1.3524199845510996e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.25it/s]                                               {'loss': 2.3544, 'grad_norm': 5.0817999839782715, 'learning_rate': 1.127016653792583e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.25it/s]                                               {'loss': 2.2357, 'grad_norm': 4.335587501525879, 'learning_rate': 9.016133230340664e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.25it/s] 97%|█████████▋| 73/75 [00:02<00:00, 24.94it/s]                                               {'loss': 1.9139, 'grad_norm': 4.052268028259277, 'learning_rate': 6.762099922755498e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.94it/s]                                               {'loss': 2.2743, 'grad_norm': 4.863650321960449, 'learning_rate': 4.508066615170332e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.94it/s]                                               {'loss': 2.5359, 'grad_norm': 9.763226509094238, 'learning_rate': 2.254033307585166e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.94it/s]                                               {'train_runtime': 3.0891, 'train_samples_per_second': 365.804, 'train_steps_per_second': 24.279, 'train_loss': 2.264927242596944, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.94it/s]100%|██████████| 75/75 [00:03<00:00, 24.28it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4136, 'grad_norm': 5.242827892303467, 'learning_rate': 0.00016905249806888745, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:05, 12.43it/s]  3%|▎         | 2/75 [00:00<00:05, 14.55it/s]                                              {'loss': 2.4292, 'grad_norm': 3.9580276012420654, 'learning_rate': 0.0001667984647613023, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:05, 14.55it/s]                                              {'loss': 2.3546, 'grad_norm': 4.461857318878174, 'learning_rate': 0.00016454443145371713, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 14.55it/s]  5%|▌         | 4/75 [00:00<00:05, 14.10it/s]                                              {'loss': 2.5172, 'grad_norm': 4.16713285446167, 'learning_rate': 0.00016229039814613194, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:05, 14.10it/s]                                              {'loss': 2.42, 'grad_norm': 5.496877193450928, 'learning_rate': 0.00016003636483854678, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:04, 14.10it/s]  8%|▊         | 6/75 [00:00<00:04, 16.16it/s]                                              {'loss': 2.4323, 'grad_norm': 4.7028985023498535, 'learning_rate': 0.00015778233153096162, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:04, 16.16it/s]                                              {'loss': 2.3883, 'grad_norm': 4.152723789215088, 'learning_rate': 0.00015552829822337646, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:04, 16.16it/s] 11%|█         | 8/75 [00:00<00:04, 14.67it/s]                                              {'loss': 2.2904, 'grad_norm': 4.331033706665039, 'learning_rate': 0.00015327426491579127, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:04, 14.67it/s]                                              {'loss': 2.3599, 'grad_norm': 4.539546966552734, 'learning_rate': 0.0001510202316082061, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:04, 14.67it/s] 13%|█▎        | 10/75 [00:00<00:04, 15.43it/s]                                               {'loss': 2.3087, 'grad_norm': 4.5001935958862305, 'learning_rate': 0.00014876619830062095, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:04, 15.43it/s]                                               {'loss': 2.3716, 'grad_norm': 4.022619724273682, 'learning_rate': 0.0001465121649930358, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:04, 15.43it/s] 16%|█▌        | 12/75 [00:00<00:04, 14.82it/s]                                               {'loss': 2.3167, 'grad_norm': 3.338136672973633, 'learning_rate': 0.00014425813168545063, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:04, 14.82it/s]                                               {'loss': 2.304, 'grad_norm': 5.14015007019043, 'learning_rate': 0.00014200409837786544, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:04, 14.82it/s] 19%|█▊        | 14/75 [00:00<00:04, 15.15it/s]                                               {'loss': 2.2847, 'grad_norm': 3.610126256942749, 'learning_rate': 0.00013975006507028028, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:04, 15.15it/s]                                               {'loss': 2.3359, 'grad_norm': 9.728323936462402, 'learning_rate': 0.00013749603176269512, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:03, 15.15it/s] 21%|██▏       | 16/75 [00:01<00:03, 14.94it/s]                                               {'loss': 2.4002, 'grad_norm': 4.127914905548096, 'learning_rate': 0.00013524199845510996, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:01<00:03, 14.94it/s]                                               {'loss': 2.2503, 'grad_norm': 4.392063617706299, 'learning_rate': 0.0001329879651475248, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:01<00:03, 14.94it/s] 24%|██▍       | 18/75 [00:01<00:03, 14.58it/s]                                               {'loss': 2.1216, 'grad_norm': 3.6393942832946777, 'learning_rate': 0.0001307339318399396, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:01<00:03, 14.58it/s]                                               {'loss': 2.1851, 'grad_norm': 3.9462480545043945, 'learning_rate': 0.00012847989853235445, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:01<00:03, 14.58it/s] 27%|██▋       | 20/75 [00:01<00:03, 14.44it/s]                                               {'loss': 2.2341, 'grad_norm': 4.521103858947754, 'learning_rate': 0.0001262258652247693, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:01<00:03, 14.44it/s]                                               {'loss': 2.291, 'grad_norm': 4.281616687774658, 'learning_rate': 0.00012397183191718413, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:03, 14.44it/s] 29%|██▉       | 22/75 [00:01<00:03, 14.54it/s]                                               {'loss': 2.2401, 'grad_norm': 3.8392157554626465, 'learning_rate': 0.00012171779860959896, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:03, 14.54it/s]                                               {'loss': 2.176, 'grad_norm': 3.5059542655944824, 'learning_rate': 0.0001194637653020138, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:03, 14.54it/s] 32%|███▏      | 24/75 [00:01<00:03, 13.48it/s]                                               {'loss': 2.2197, 'grad_norm': 4.152594089508057, 'learning_rate': 0.00011720973199442864, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:03, 13.48it/s]                                               {'loss': 2.3062, 'grad_norm': 3.864244222640991, 'learning_rate': 0.00011495569868684347, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:03, 13.48it/s] 35%|███▍      | 26/75 [00:01<00:03, 14.31it/s]                                               {'loss': 2.4025, 'grad_norm': 5.630730628967285, 'learning_rate': 0.0001127016653792583, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:03, 14.31it/s]                                               {'loss': 2.0474, 'grad_norm': 2.9888594150543213, 'learning_rate': 0.00011044763207167313, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:03, 14.31it/s]                                               {'loss': 2.3292, 'grad_norm': 4.987561225891113, 'learning_rate': 0.00010819359876408797, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:03, 14.31it/s] 39%|███▊      | 29/75 [00:01<00:02, 16.69it/s]                                               {'loss': 2.3707, 'grad_norm': 3.6511409282684326, 'learning_rate': 0.0001059395654565028, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 16.69it/s]                                               {'loss': 2.5121, 'grad_norm': 11.734923362731934, 'learning_rate': 0.00010368553214891763, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 16.69it/s]                                               {'loss': 2.1222, 'grad_norm': 4.119515895843506, 'learning_rate': 0.00010143149884133247, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:02<00:02, 16.69it/s] 43%|████▎     | 32/75 [00:02<00:02, 18.40it/s]                                               {'loss': 2.1999, 'grad_norm': 3.6117708683013916, 'learning_rate': 9.917746553374731e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:02<00:02, 18.40it/s]                                               {'loss': 2.2728, 'grad_norm': 3.677442789077759, 'learning_rate': 9.692343222616215e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:02<00:02, 18.40it/s] 45%|████▌     | 34/75 [00:02<00:02, 16.93it/s]                                               {'loss': 2.2182, 'grad_norm': 3.690722703933716, 'learning_rate': 9.466939891857698e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:02<00:02, 16.93it/s]                                               {'loss': 2.1812, 'grad_norm': 3.988908290863037, 'learning_rate': 9.24153656109918e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:02<00:02, 16.93it/s]                                               {'loss': 2.0737, 'grad_norm': 3.688446521759033, 'learning_rate': 9.016133230340664e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:02<00:02, 16.93it/s] 49%|████▉     | 37/75 [00:02<00:02, 18.85it/s]                                               {'loss': 2.0869, 'grad_norm': 4.712449550628662, 'learning_rate': 8.790729899582148e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:02<00:02, 18.85it/s]                                               {'loss': 2.1276, 'grad_norm': 4.3583245277404785, 'learning_rate': 8.565326568823632e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:02<00:01, 18.85it/s]                                               {'loss': 2.2928, 'grad_norm': 4.145506381988525, 'learning_rate': 8.339923238065115e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:02<00:01, 18.85it/s] 53%|█████▎    | 40/75 [00:02<00:01, 20.41it/s]                                               {'loss': 2.3497, 'grad_norm': 4.271117687225342, 'learning_rate': 8.114519907306597e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:02<00:01, 20.41it/s]                                               {'loss': 2.2476, 'grad_norm': 3.4853856563568115, 'learning_rate': 7.889116576548081e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:02<00:01, 20.41it/s]                                               {'loss': 2.3359, 'grad_norm': 4.024505615234375, 'learning_rate': 7.663713245789564e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 20.41it/s] 57%|█████▋    | 43/75 [00:02<00:01, 19.58it/s]                                               {'loss': 2.4578, 'grad_norm': 4.053677082061768, 'learning_rate': 7.438309915031048e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 19.58it/s]                                               {'loss': 2.1304, 'grad_norm': 3.3079519271850586, 'learning_rate': 7.212906584272532e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 19.58it/s] 60%|██████    | 45/75 [00:02<00:01, 18.36it/s]                                               {'loss': 2.3478, 'grad_norm': 11.009698867797852, 'learning_rate': 6.987503253514014e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 18.36it/s]                                               {'loss': 2.1383, 'grad_norm': 3.4897122383117676, 'learning_rate': 6.762099922755498e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 18.36it/s] 63%|██████▎   | 47/75 [00:02<00:01, 18.14it/s]                                               {'loss': 2.0858, 'grad_norm': 3.4620590209960938, 'learning_rate': 6.53669659199698e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 18.14it/s]                                               {'loss': 2.172, 'grad_norm': 3.853426694869995, 'learning_rate': 6.311293261238465e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 18.14it/s]                                               {'loss': 2.1759, 'grad_norm': 3.740905523300171, 'learning_rate': 6.085889930479948e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 18.14it/s] 67%|██████▋   | 50/75 [00:03<00:01, 18.72it/s]                                               {'loss': 2.21, 'grad_norm': 3.607714891433716, 'learning_rate': 5.860486599721432e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:03<00:01, 18.72it/s]                                               {'loss': 2.3111, 'grad_norm': 3.7245407104492188, 'learning_rate': 5.635083268962915e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:03<00:01, 18.72it/s] 69%|██████▉   | 52/75 [00:03<00:01, 18.58it/s]                                               {'loss': 2.3078, 'grad_norm': 4.196770668029785, 'learning_rate': 5.409679938204398e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:03<00:01, 18.58it/s]                                               {'loss': 2.3249, 'grad_norm': 5.137099266052246, 'learning_rate': 5.1842766074458816e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:03<00:01, 18.58it/s] 72%|███████▏  | 54/75 [00:03<00:01, 17.10it/s]                                               {'loss': 2.0889, 'grad_norm': 3.7960896492004395, 'learning_rate': 4.9588732766873655e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:03<00:01, 17.10it/s]                                               {'loss': 2.0638, 'grad_norm': 3.2164804935455322, 'learning_rate': 4.733469945928849e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:03<00:01, 17.10it/s] 75%|███████▍  | 56/75 [00:03<00:01, 16.04it/s]                                               {'loss': 2.2711, 'grad_norm': 4.391599178314209, 'learning_rate': 4.508066615170332e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:03<00:01, 16.04it/s]                                               {'loss': 2.2221, 'grad_norm': 3.5347180366516113, 'learning_rate': 4.282663284411816e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:03<00:01, 16.04it/s] 77%|███████▋  | 58/75 [00:03<00:01, 15.02it/s]                                               {'loss': 2.1909, 'grad_norm': 3.69535493850708, 'learning_rate': 4.0572599536532985e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:03<00:01, 15.02it/s]                                               {'loss': 2.3055, 'grad_norm': 7.452586650848389, 'learning_rate': 3.831856622894782e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:03<00:01, 15.02it/s] 80%|████████  | 60/75 [00:03<00:00, 15.75it/s]                                               {'loss': 2.1978, 'grad_norm': 9.119394302368164, 'learning_rate': 3.606453292136266e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:03<00:00, 15.75it/s]                                               {'loss': 2.2629, 'grad_norm': 4.460140228271484, 'learning_rate': 3.381049961377749e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:03<00:00, 15.75it/s] 83%|████████▎ | 62/75 [00:03<00:00, 15.57it/s]                                               {'loss': 2.2935, 'grad_norm': 3.9727976322174072, 'learning_rate': 3.155646630619232e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:03<00:00, 15.57it/s]                                               {'loss': 2.089, 'grad_norm': 3.449989080429077, 'learning_rate': 2.930243299860716e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 15.57it/s] 85%|████████▌ | 64/75 [00:03<00:00, 16.44it/s]                                               {'loss': 2.1193, 'grad_norm': 4.121743202209473, 'learning_rate': 2.704839969102199e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 16.44it/s]                                               {'loss': 2.282, 'grad_norm': 4.113002777099609, 'learning_rate': 2.4794366383436827e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 16.44it/s] 88%|████████▊ | 66/75 [00:04<00:00, 15.72it/s]                                               {'loss': 2.1822, 'grad_norm': 3.8447272777557373, 'learning_rate': 2.254033307585166e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:04<00:00, 15.72it/s]                                               {'loss': 2.2352, 'grad_norm': 3.736560583114624, 'learning_rate': 2.0286299768266493e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:04<00:00, 15.72it/s] 91%|█████████ | 68/75 [00:04<00:00, 16.62it/s]                                               {'loss': 2.2494, 'grad_norm': 4.123239517211914, 'learning_rate': 1.803226646068133e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:04<00:00, 16.62it/s]                                               {'loss': 2.2255, 'grad_norm': 3.8921360969543457, 'learning_rate': 1.577823315309616e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:04<00:00, 16.62it/s]                                               {'loss': 2.0626, 'grad_norm': 3.8045599460601807, 'learning_rate': 1.3524199845510996e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:04<00:00, 16.62it/s] 95%|█████████▍| 71/75 [00:04<00:00, 18.94it/s]                                               {'loss': 1.9871, 'grad_norm': 3.3170454502105713, 'learning_rate': 1.127016653792583e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:04<00:00, 18.94it/s]                                               {'loss': 2.2884, 'grad_norm': 5.323054790496826, 'learning_rate': 9.016133230340664e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:04<00:00, 18.94it/s]                                               {'loss': 2.0383, 'grad_norm': 3.2782652378082275, 'learning_rate': 6.762099922755498e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:04<00:00, 18.94it/s] 99%|█████████▊| 74/75 [00:04<00:00, 20.42it/s]                                               {'loss': 2.1963, 'grad_norm': 3.0346193313598633, 'learning_rate': 4.508066615170332e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:04<00:00, 20.42it/s]                                               {'loss': 2.5788, 'grad_norm': 16.668128967285156, 'learning_rate': 2.254033307585166e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 20.42it/s]                                               {'train_runtime': 4.5252, 'train_samples_per_second': 249.712, 'train_steps_per_second': 16.574, 'train_loss': 2.2561881748835244, 'epoch': 5.0}
100%|██████████| 75/75 [00:04<00:00, 20.42it/s]100%|██████████| 75/75 [00:04<00:00, 16.58it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1311, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(997, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(943, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1149, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1366, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1178, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1308, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1207, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1292, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1233, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1353, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1022, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 65.76it/s]  3%|▎         | 14/471 [00:00<00:08, 56.95it/s]  4%|▍         | 20/471 [00:00<00:08, 51.79it/s]  6%|▌         | 26/471 [00:00<00:08, 50.55it/s]  7%|▋         | 32/471 [00:00<00:08, 52.14it/s]  8%|▊         | 38/471 [00:00<00:08, 52.47it/s]  9%|▉         | 44/471 [00:00<00:08, 52.08it/s] 11%|█         | 50/471 [00:00<00:08, 52.25it/s] 12%|█▏        | 56/471 [00:01<00:08, 51.11it/s] 13%|█▎        | 62/471 [00:01<00:07, 51.84it/s] 14%|█▍        | 68/471 [00:01<00:07, 50.66it/s] 16%|█▌        | 74/471 [00:01<00:07, 51.15it/s] 17%|█▋        | 80/471 [00:01<00:07, 51.72it/s] 18%|█▊        | 86/471 [00:01<00:07, 51.10it/s] 20%|█▉        | 92/471 [00:01<00:07, 51.59it/s] 21%|██        | 98/471 [00:01<00:07, 50.53it/s] 22%|██▏       | 104/471 [00:02<00:07, 51.49it/s] 23%|██▎       | 110/471 [00:02<00:06, 51.80it/s] 25%|██▍       | 116/471 [00:02<00:06, 52.25it/s] 26%|██▌       | 122/471 [00:02<00:06, 51.72it/s] 27%|██▋       | 128/471 [00:02<00:06, 51.57it/s] 28%|██▊       | 134/471 [00:02<00:06, 51.65it/s] 30%|██▉       | 140/471 [00:02<00:06, 51.99it/s] 31%|███       | 146/471 [00:02<00:06, 52.19it/s] 32%|███▏      | 152/471 [00:02<00:06, 50.63it/s] 34%|███▎      | 158/471 [00:03<00:06, 51.72it/s] 35%|███▍      | 164/471 [00:03<00:05, 51.95it/s] 36%|███▌      | 170/471 [00:03<00:05, 52.12it/s] 37%|███▋      | 176/471 [00:03<00:05, 51.79it/s] 39%|███▊      | 182/471 [00:03<00:05, 51.95it/s] 40%|███▉      | 188/471 [00:03<00:05, 50.39it/s] 41%|████      | 194/471 [00:03<00:05, 51.56it/s] 42%|████▏     | 200/471 [00:03<00:05, 51.90it/s] 44%|████▎     | 206/471 [00:03<00:05, 52.27it/s] 45%|████▌     | 212/471 [00:04<00:04, 52.24it/s] 46%|████▋     | 218/471 [00:04<00:04, 52.26it/s] 48%|████▊     | 224/471 [00:04<00:04, 51.41it/s] 49%|████▉     | 230/471 [00:04<00:04, 51.78it/s] 50%|█████     | 236/471 [00:04<00:04, 51.20it/s] 51%|█████▏    | 242/471 [00:04<00:04, 51.86it/s] 53%|█████▎    | 248/471 [00:04<00:04, 51.79it/s] 54%|█████▍    | 254/471 [00:04<00:04, 50.95it/s] 55%|█████▌    | 260/471 [00:05<00:04, 51.05it/s] 56%|█████▋    | 266/471 [00:05<00:04, 50.47it/s] 58%|█████▊    | 272/471 [00:05<00:03, 49.89it/s] 59%|█████▉    | 277/471 [00:05<00:03, 49.81it/s] 60%|██████    | 283/471 [00:05<00:03, 50.77it/s] 61%|██████▏   | 289/471 [00:05<00:03, 50.70it/s] 63%|██████▎   | 295/471 [00:05<00:03, 50.27it/s] 64%|██████▍   | 301/471 [00:05<00:03, 50.99it/s] 65%|██████▌   | 307/471 [00:05<00:03, 50.04it/s] 66%|██████▋   | 313/471 [00:06<00:03, 51.08it/s] 68%|██████▊   | 319/471 [00:06<00:02, 51.41it/s] 69%|██████▉   | 325/471 [00:06<00:02, 51.69it/s] 70%|███████   | 331/471 [00:06<00:02, 51.85it/s] 72%|███████▏  | 337/471 [00:06<00:02, 51.03it/s] 73%|███████▎  | 343/471 [00:06<00:02, 51.88it/s] 74%|███████▍  | 349/471 [00:06<00:02, 52.07it/s] 75%|███████▌  | 355/471 [00:06<00:02, 51.28it/s] 77%|███████▋  | 361/471 [00:07<00:02, 51.55it/s] 78%|███████▊  | 367/471 [00:07<00:02, 51.79it/s] 79%|███████▉  | 373/471 [00:07<00:01, 51.76it/s] 80%|████████  | 379/471 [00:07<00:01, 50.42it/s] 82%|████████▏ | 385/471 [00:07<00:01, 51.60it/s] 83%|████████▎ | 391/471 [00:07<00:01, 51.32it/s] 84%|████████▍ | 397/471 [00:07<00:01, 51.81it/s] 86%|████████▌ | 403/471 [00:07<00:01, 51.73it/s] 87%|████████▋ | 409/471 [00:07<00:01, 51.93it/s] 88%|████████▊ | 415/471 [00:08<00:01, 52.24it/s] 89%|████████▉ | 421/471 [00:08<00:00, 52.24it/s] 91%|█████████ | 427/471 [00:08<00:00, 52.12it/s] 92%|█████████▏| 433/471 [00:08<00:00, 51.52it/s] 93%|█████████▎| 439/471 [00:08<00:00, 52.02it/s] 94%|█████████▍| 445/471 [00:08<00:00, 52.08it/s] 96%|█████████▌| 451/471 [00:08<00:00, 52.09it/s] 97%|█████████▋| 457/471 [00:08<00:00, 52.19it/s] 98%|█████████▊| 463/471 [00:08<00:00, 52.33it/s]100%|█████████▉| 469/471 [00:09<00:00, 52.30it/s]100%|██████████| 471/471 [00:09<00:00, 51.67it/s]
{'eval_loss': 2.3656792640686035, 'eval_model_preparation_time': 0.003, 'eval_acc': 0.3702867764206054, 'eval_runtime': 9.1383, 'eval_samples_per_second': 824.226, 'eval_steps_per_second': 51.541}
ROUND:7
CLIENT:9
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2683, 'grad_norm': 4.489145278930664, 'learning_rate': 0.00016333997346592444, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 17.30it/s]  3%|▎         | 2/75 [00:00<00:03, 18.43it/s]                                              {'loss': 2.4585, 'grad_norm': 4.41990852355957, 'learning_rate': 0.00016116210715304546, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 18.43it/s]                                              {'loss': 2.5496, 'grad_norm': 5.836651802062988, 'learning_rate': 0.00015898424084016645, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 18.43it/s]                                              {'loss': 2.5004, 'grad_norm': 4.903260707855225, 'learning_rate': 0.00015680637452728744, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 18.43it/s]  7%|▋         | 5/75 [00:00<00:03, 22.92it/s]                                              {'loss': 2.1642, 'grad_norm': 3.91587495803833, 'learning_rate': 0.00015462850821440846, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.92it/s]                                              {'loss': 2.3884, 'grad_norm': 4.712697505950928, 'learning_rate': 0.00015245064190152948, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 22.92it/s]                                              {'loss': 2.5315, 'grad_norm': 4.199859142303467, 'learning_rate': 0.0001502727755886505, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 22.92it/s] 11%|█         | 8/75 [00:00<00:02, 24.26it/s]                                              {'loss': 2.2731, 'grad_norm': 3.7338483333587646, 'learning_rate': 0.0001480949092757715, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.26it/s]                                              {'loss': 2.2922, 'grad_norm': 4.665958404541016, 'learning_rate': 0.00014591704296289249, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.26it/s]                                              {'loss': 2.3396, 'grad_norm': 3.9223906993865967, 'learning_rate': 0.0001437391766500135, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.26it/s] 15%|█▍        | 11/75 [00:00<00:02, 24.90it/s]                                               {'loss': 2.2357, 'grad_norm': 4.115024089813232, 'learning_rate': 0.00014156131033713453, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.90it/s]                                               {'loss': 2.3094, 'grad_norm': 5.9369001388549805, 'learning_rate': 0.00013938344402425552, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.90it/s]                                               {'loss': 2.5025, 'grad_norm': 4.366450309753418, 'learning_rate': 0.0001372055777113765, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.90it/s] 19%|█▊        | 14/75 [00:00<00:02, 24.69it/s]                                               {'loss': 2.556, 'grad_norm': 4.484958648681641, 'learning_rate': 0.00013502771139849753, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.69it/s]                                               {'loss': 2.6964, 'grad_norm': 8.498767852783203, 'learning_rate': 0.00013284984508561855, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.69it/s]                                               {'loss': 2.2509, 'grad_norm': 3.331350564956665, 'learning_rate': 0.00013067197877273954, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 24.69it/s] 23%|██▎       | 17/75 [00:00<00:02, 24.85it/s]                                               {'loss': 2.2135, 'grad_norm': 4.57476806640625, 'learning_rate': 0.00012849411245986054, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.85it/s]                                               {'loss': 2.3854, 'grad_norm': 3.828014850616455, 'learning_rate': 0.00012631624614698156, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.85it/s]                                               {'loss': 2.2679, 'grad_norm': 4.298867225646973, 'learning_rate': 0.00012413837983410258, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.85it/s] 27%|██▋       | 20/75 [00:00<00:02, 23.44it/s]                                               {'loss': 2.2587, 'grad_norm': 5.117941856384277, 'learning_rate': 0.00012196051352122358, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 23.44it/s]                                               {'loss': 2.2307, 'grad_norm': 4.3183441162109375, 'learning_rate': 0.00011978264720834457, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 23.44it/s]                                               {'loss': 2.3431, 'grad_norm': 4.807113170623779, 'learning_rate': 0.0001176047808954656, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 23.44it/s] 31%|███       | 23/75 [00:00<00:02, 23.73it/s]                                               {'loss': 2.3239, 'grad_norm': 4.163153171539307, 'learning_rate': 0.0001154269145825866, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 23.73it/s]                                               {'loss': 2.1571, 'grad_norm': 3.6177306175231934, 'learning_rate': 0.00011324904826970761, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 23.73it/s]                                               {'loss': 2.0914, 'grad_norm': 4.0899248123168945, 'learning_rate': 0.00011107118195682863, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.73it/s] 35%|███▍      | 26/75 [00:01<00:02, 23.78it/s]                                               {'loss': 2.4875, 'grad_norm': 4.181302070617676, 'learning_rate': 0.00010889331564394962, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.78it/s]                                               {'loss': 2.3852, 'grad_norm': 4.277590274810791, 'learning_rate': 0.00010671544933107063, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 23.78it/s]                                               {'loss': 2.2575, 'grad_norm': 4.492032527923584, 'learning_rate': 0.00010453758301819165, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 23.78it/s] 39%|███▊      | 29/75 [00:01<00:01, 23.48it/s]                                               {'loss': 2.3724, 'grad_norm': 5.076807022094727, 'learning_rate': 0.00010235971670531265, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 23.48it/s]                                               {'loss': 2.5681, 'grad_norm': 10.428153991699219, 'learning_rate': 0.00010018185039243364, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.48it/s]                                               {'loss': 2.1386, 'grad_norm': 4.333100318908691, 'learning_rate': 9.800398407955466e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 23.48it/s] 43%|████▎     | 32/75 [00:01<00:01, 23.52it/s]                                               {'loss': 2.3468, 'grad_norm': 4.186592102050781, 'learning_rate': 9.582611776667567e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 23.52it/s]                                               {'loss': 2.5432, 'grad_norm': 4.567474842071533, 'learning_rate': 9.364825145379668e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 23.52it/s]                                               {'loss': 2.2953, 'grad_norm': 4.0391411781311035, 'learning_rate': 9.14703851409177e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 23.52it/s] 47%|████▋     | 35/75 [00:01<00:01, 23.46it/s]                                               {'loss': 2.122, 'grad_norm': 3.4248719215393066, 'learning_rate': 8.929251882803869e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 23.46it/s]                                               {'loss': 2.3855, 'grad_norm': 3.934424638748169, 'learning_rate': 8.71146525151597e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 23.46it/s]                                               {'loss': 2.1763, 'grad_norm': 3.6998472213745117, 'learning_rate': 8.493678620228072e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 23.46it/s] 51%|█████     | 38/75 [00:01<00:01, 23.21it/s]                                               {'loss': 2.3005, 'grad_norm': 5.385347843170166, 'learning_rate': 8.275891988940172e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 23.21it/s]                                               {'loss': 2.2504, 'grad_norm': 4.582864761352539, 'learning_rate': 8.058105357652273e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.21it/s]                                               {'loss': 2.2765, 'grad_norm': 3.8366193771362305, 'learning_rate': 7.840318726364372e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.21it/s] 55%|█████▍    | 41/75 [00:01<00:01, 23.35it/s]                                               {'loss': 2.3526, 'grad_norm': 6.448755264282227, 'learning_rate': 7.622532095076474e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 23.35it/s]                                               {'loss': 2.0886, 'grad_norm': 3.147228479385376, 'learning_rate': 7.404745463788575e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 23.35it/s]                                               {'loss': 2.2976, 'grad_norm': 4.789244651794434, 'learning_rate': 7.186958832500675e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 23.35it/s] 59%|█████▊    | 44/75 [00:01<00:01, 23.64it/s]                                               {'loss': 2.1407, 'grad_norm': 3.5707497596740723, 'learning_rate': 6.969172201212776e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 23.64it/s]                                               {'loss': 2.3808, 'grad_norm': 11.467141151428223, 'learning_rate': 6.751385569924877e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 23.64it/s]                                               {'loss': 2.3437, 'grad_norm': 4.228257656097412, 'learning_rate': 6.533598938636977e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 23.64it/s] 63%|██████▎   | 47/75 [00:02<00:01, 21.53it/s]                                               {'loss': 2.5709, 'grad_norm': 4.064803600311279, 'learning_rate': 6.315812307349078e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 21.53it/s]                                               {'loss': 2.2133, 'grad_norm': 4.489935398101807, 'learning_rate': 6.098025676061179e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 21.53it/s]                                               {'loss': 2.1036, 'grad_norm': 4.099468231201172, 'learning_rate': 5.88023904477328e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 21.53it/s] 67%|██████▋   | 50/75 [00:02<00:01, 22.61it/s]                                               {'loss': 2.2683, 'grad_norm': 4.911224842071533, 'learning_rate': 5.6624524134853803e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 22.61it/s]                                               {'loss': 2.4075, 'grad_norm': 4.860750198364258, 'learning_rate': 5.444665782197481e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 22.61it/s]                                               {'loss': 2.2402, 'grad_norm': 4.599404811859131, 'learning_rate': 5.226879150909582e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 22.61it/s] 71%|███████   | 53/75 [00:02<00:00, 23.60it/s]                                               {'loss': 2.0922, 'grad_norm': 3.4443399906158447, 'learning_rate': 5.009092519621682e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 23.60it/s]                                               {'loss': 2.2228, 'grad_norm': 3.1518239974975586, 'learning_rate': 4.7913058883337835e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 23.60it/s]                                               {'loss': 2.0391, 'grad_norm': 3.8648462295532227, 'learning_rate': 4.573519257045885e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 23.60it/s] 75%|███████▍  | 56/75 [00:02<00:00, 22.64it/s]                                               {'loss': 2.0576, 'grad_norm': 3.3958663940429688, 'learning_rate': 4.355732625757985e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 22.64it/s]                                               {'loss': 2.1167, 'grad_norm': 3.6162545680999756, 'learning_rate': 4.137945994470086e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 22.64it/s]                                               {'loss': 2.2871, 'grad_norm': 3.2544729709625244, 'learning_rate': 3.920159363182186e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 22.64it/s] 79%|███████▊  | 59/75 [00:02<00:00, 22.16it/s]                                               {'loss': 2.2071, 'grad_norm': 3.7863361835479736, 'learning_rate': 3.702372731894287e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 22.16it/s]                                               {'loss': 2.9135, 'grad_norm': 17.537492752075195, 'learning_rate': 3.484586100606388e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 22.16it/s]                                               {'loss': 2.1715, 'grad_norm': 2.924426794052124, 'learning_rate': 3.2667994693184886e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 22.16it/s] 83%|████████▎ | 62/75 [00:02<00:00, 22.06it/s]                                               {'loss': 2.2491, 'grad_norm': 3.6348888874053955, 'learning_rate': 3.0490128380305895e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 22.06it/s]                                               {'loss': 2.516, 'grad_norm': 5.725877285003662, 'learning_rate': 2.8312262067426902e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 22.06it/s]                                               {'loss': 2.2257, 'grad_norm': 3.676417350769043, 'learning_rate': 2.613439575454791e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 22.06it/s] 87%|████████▋ | 65/75 [00:02<00:00, 20.59it/s]                                               {'loss': 2.1488, 'grad_norm': 3.89778733253479, 'learning_rate': 2.3956529441668918e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 20.59it/s]                                               {'loss': 2.2037, 'grad_norm': 6.525781631469727, 'learning_rate': 2.1778663128789924e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 20.59it/s]                                               {'loss': 2.3304, 'grad_norm': 4.046026229858398, 'learning_rate': 1.960079681591093e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 20.59it/s] 91%|█████████ | 68/75 [00:02<00:00, 21.01it/s]                                               {'loss': 2.2506, 'grad_norm': 5.372694969177246, 'learning_rate': 1.742293050303194e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 21.01it/s]                                               {'loss': 2.28, 'grad_norm': 4.315561771392822, 'learning_rate': 1.5245064190152948e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 21.01it/s]                                               {'loss': 2.1579, 'grad_norm': 3.8244547843933105, 'learning_rate': 1.3067197877273956e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 21.01it/s] 95%|█████████▍| 71/75 [00:03<00:00, 22.03it/s]                                               {'loss': 1.9969, 'grad_norm': 3.5729966163635254, 'learning_rate': 1.0889331564394962e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 22.03it/s]                                               {'loss': 2.1579, 'grad_norm': 3.5965843200683594, 'learning_rate': 8.71146525151597e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 22.03it/s]                                               {'loss': 2.2847, 'grad_norm': 4.030395030975342, 'learning_rate': 6.533598938636978e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 22.03it/s] 99%|█████████▊| 74/75 [00:03<00:00, 22.08it/s]                                               {'loss': 2.202, 'grad_norm': 4.920568466186523, 'learning_rate': 4.355732625757985e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 22.08it/s]                                               {'loss': 1.5343, 'grad_norm': 7.9690117835998535, 'learning_rate': 2.1778663128789925e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 22.08it/s]                                               {'train_runtime': 3.4411, 'train_samples_per_second': 328.379, 'train_steps_per_second': 21.795, 'train_loss': 2.2872999811172487, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 22.08it/s]100%|██████████| 75/75 [00:03<00:00, 21.80it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:14
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.1976, 'grad_norm': 5.16254997253418, 'learning_rate': 0.00016333997346592444, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 15.88it/s]  3%|▎         | 2/75 [00:00<00:03, 19.30it/s]                                              {'loss': 2.3053, 'grad_norm': 4.082242965698242, 'learning_rate': 0.00016116210715304546, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 19.30it/s]                                              {'loss': 2.2633, 'grad_norm': 4.274147033691406, 'learning_rate': 0.00015898424084016645, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 19.30it/s]  5%|▌         | 4/75 [00:00<00:03, 18.97it/s]                                              {'loss': 2.2723, 'grad_norm': 4.2702178955078125, 'learning_rate': 0.00015680637452728744, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 18.97it/s]                                              {'loss': 2.2186, 'grad_norm': 4.512077331542969, 'learning_rate': 0.00015462850821440846, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 18.97it/s]                                              {'loss': 2.2021, 'grad_norm': 4.61089563369751, 'learning_rate': 0.00015245064190152948, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 18.97it/s]  9%|▉         | 7/75 [00:00<00:03, 21.79it/s]                                              {'loss': 2.2738, 'grad_norm': 4.618523120880127, 'learning_rate': 0.0001502727755886505, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 21.79it/s]                                              {'loss': 2.2321, 'grad_norm': 4.674845218658447, 'learning_rate': 0.0001480949092757715, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 21.79it/s]                                              {'loss': 2.2597, 'grad_norm': 3.8633811473846436, 'learning_rate': 0.00014591704296289249, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 21.79it/s] 13%|█▎        | 10/75 [00:00<00:02, 22.47it/s]                                               {'loss': 2.2737, 'grad_norm': 4.126017093658447, 'learning_rate': 0.0001437391766500135, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 22.47it/s]                                               {'loss': 2.2327, 'grad_norm': 5.818975448608398, 'learning_rate': 0.00014156131033713453, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 22.47it/s]                                               {'loss': 2.2532, 'grad_norm': 4.522470951080322, 'learning_rate': 0.00013938344402425552, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 22.47it/s] 17%|█▋        | 13/75 [00:00<00:02, 22.78it/s]                                               {'loss': 2.3685, 'grad_norm': 4.420593738555908, 'learning_rate': 0.0001372055777113765, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 22.78it/s]                                               {'loss': 2.3293, 'grad_norm': 5.570334434509277, 'learning_rate': 0.00013502771139849753, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 22.78it/s]                                               {'loss': 2.357, 'grad_norm': 20.209087371826172, 'learning_rate': 0.00013284984508561855, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 22.78it/s] 21%|██▏       | 16/75 [00:00<00:02, 22.30it/s]                                               {'loss': 2.2468, 'grad_norm': 5.730605125427246, 'learning_rate': 0.00013067197877273954, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 22.30it/s]                                               {'loss': 2.2497, 'grad_norm': 4.190061569213867, 'learning_rate': 0.00012849411245986054, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 22.30it/s]                                               {'loss': 2.1006, 'grad_norm': 4.360253810882568, 'learning_rate': 0.00012631624614698156, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 22.30it/s] 25%|██▌       | 19/75 [00:00<00:02, 22.57it/s]                                               {'loss': 2.2777, 'grad_norm': 4.886809825897217, 'learning_rate': 0.00012413837983410258, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 22.57it/s]                                               {'loss': 2.2039, 'grad_norm': 4.806599140167236, 'learning_rate': 0.00012196051352122358, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 22.57it/s]                                               {'loss': 2.0584, 'grad_norm': 3.611570119857788, 'learning_rate': 0.00011978264720834457, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 22.57it/s] 29%|██▉       | 22/75 [00:00<00:02, 23.17it/s]                                               {'loss': 2.3604, 'grad_norm': 3.7795250415802, 'learning_rate': 0.0001176047808954656, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 23.17it/s]                                               {'loss': 2.3698, 'grad_norm': 4.581374168395996, 'learning_rate': 0.0001154269145825866, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 23.17it/s]                                               {'loss': 2.2908, 'grad_norm': 4.487548351287842, 'learning_rate': 0.00011324904826970761, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 23.17it/s] 33%|███▎      | 25/75 [00:01<00:02, 23.96it/s]                                               {'loss': 1.9384, 'grad_norm': 4.6456828117370605, 'learning_rate': 0.00011107118195682863, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.96it/s]                                               {'loss': 2.0348, 'grad_norm': 3.612046003341675, 'learning_rate': 0.00010889331564394962, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.96it/s]                                               {'loss': 2.0035, 'grad_norm': 4.165607452392578, 'learning_rate': 0.00010671544933107063, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 23.96it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.39it/s]                                               {'loss': 2.2866, 'grad_norm': 4.541491508483887, 'learning_rate': 0.00010453758301819165, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.39it/s]                                               {'loss': 2.2351, 'grad_norm': 4.245243549346924, 'learning_rate': 0.00010235971670531265, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.39it/s]                                               {'loss': 1.9916, 'grad_norm': 13.93770980834961, 'learning_rate': 0.00010018185039243364, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.39it/s] 41%|████▏     | 31/75 [00:01<00:01, 25.27it/s]                                               {'loss': 2.3159, 'grad_norm': 3.7790114879608154, 'learning_rate': 9.800398407955466e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.27it/s]                                               {'loss': 2.0374, 'grad_norm': 3.5228500366210938, 'learning_rate': 9.582611776667567e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.27it/s]                                               {'loss': 2.1141, 'grad_norm': 3.3815953731536865, 'learning_rate': 9.364825145379668e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.27it/s] 45%|████▌     | 34/75 [00:01<00:01, 24.35it/s]                                               {'loss': 2.0996, 'grad_norm': 3.7453012466430664, 'learning_rate': 9.14703851409177e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.35it/s]                                               {'loss': 2.2351, 'grad_norm': 5.8346991539001465, 'learning_rate': 8.929251882803869e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.35it/s]                                               {'loss': 1.9737, 'grad_norm': 3.7211906909942627, 'learning_rate': 8.71146525151597e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.35it/s] 49%|████▉     | 37/75 [00:01<00:01, 24.54it/s]                                               {'loss': 2.1274, 'grad_norm': 4.447266101837158, 'learning_rate': 8.493678620228072e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.54it/s]                                               {'loss': 2.3348, 'grad_norm': 3.8500735759735107, 'learning_rate': 8.275891988940172e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.54it/s]                                               {'loss': 2.2287, 'grad_norm': 3.825173854827881, 'learning_rate': 8.058105357652273e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.54it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.03it/s]                                               {'loss': 2.2302, 'grad_norm': 4.026507377624512, 'learning_rate': 7.840318726364372e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.03it/s]                                               {'loss': 2.1272, 'grad_norm': 5.1503214836120605, 'learning_rate': 7.622532095076474e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.03it/s]                                               {'loss': 2.0071, 'grad_norm': 3.7724742889404297, 'learning_rate': 7.404745463788575e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.03it/s] 57%|█████▋    | 43/75 [00:01<00:01, 24.97it/s]                                               {'loss': 2.1889, 'grad_norm': 3.7319674491882324, 'learning_rate': 7.186958832500675e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.97it/s]                                               {'loss': 2.1562, 'grad_norm': 3.926879644393921, 'learning_rate': 6.969172201212776e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.97it/s]                                               {'loss': 2.3961, 'grad_norm': 18.480876922607422, 'learning_rate': 6.751385569924877e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.97it/s] 61%|██████▏   | 46/75 [00:01<00:01, 26.23it/s]                                               {'loss': 2.12, 'grad_norm': 3.953280448913574, 'learning_rate': 6.533598938636977e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.23it/s]                                               {'loss': 2.1225, 'grad_norm': 4.202720642089844, 'learning_rate': 6.315812307349078e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.23it/s]                                               {'loss': 2.0474, 'grad_norm': 3.8436312675476074, 'learning_rate': 6.098025676061179e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.23it/s] 65%|██████▌   | 49/75 [00:02<00:01, 25.09it/s]                                               {'loss': 2.0982, 'grad_norm': 5.133280277252197, 'learning_rate': 5.88023904477328e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 25.09it/s]                                               {'loss': 2.1731, 'grad_norm': 4.046538352966309, 'learning_rate': 5.6624524134853803e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.09it/s]                                               {'loss': 2.2549, 'grad_norm': 4.224848747253418, 'learning_rate': 5.444665782197481e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.09it/s] 69%|██████▉   | 52/75 [00:02<00:00, 23.62it/s]                                               {'loss': 2.1287, 'grad_norm': 4.824594974517822, 'learning_rate': 5.226879150909582e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 23.62it/s]                                               {'loss': 2.2253, 'grad_norm': 5.210070610046387, 'learning_rate': 5.009092519621682e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 23.62it/s]                                               {'loss': 2.0076, 'grad_norm': 4.1700286865234375, 'learning_rate': 4.7913058883337835e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 23.62it/s] 73%|███████▎  | 55/75 [00:02<00:00, 22.75it/s]                                               {'loss': 2.2008, 'grad_norm': 3.664318323135376, 'learning_rate': 4.573519257045885e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 22.75it/s]                                               {'loss': 2.2062, 'grad_norm': 3.682495355606079, 'learning_rate': 4.355732625757985e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 22.75it/s]                                               {'loss': 2.2396, 'grad_norm': 3.8647685050964355, 'learning_rate': 4.137945994470086e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 22.75it/s] 77%|███████▋  | 58/75 [00:02<00:00, 23.34it/s]                                               {'loss': 1.9598, 'grad_norm': 3.905435562133789, 'learning_rate': 3.920159363182186e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 23.34it/s]                                               {'loss': 2.1751, 'grad_norm': 3.243154287338257, 'learning_rate': 3.702372731894287e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 23.34it/s]                                               {'loss': 1.8938, 'grad_norm': 8.824555397033691, 'learning_rate': 3.484586100606388e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 23.34it/s] 81%|████████▏ | 61/75 [00:02<00:00, 23.60it/s]                                               {'loss': 2.1757, 'grad_norm': 3.644472122192383, 'learning_rate': 3.2667994693184886e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 23.60it/s]                                               {'loss': 2.3112, 'grad_norm': 5.007821083068848, 'learning_rate': 3.0490128380305895e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 23.60it/s]                                               {'loss': 2.0025, 'grad_norm': 4.1078572273254395, 'learning_rate': 2.8312262067426902e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 23.60it/s] 85%|████████▌ | 64/75 [00:02<00:00, 23.85it/s]                                               {'loss': 2.3814, 'grad_norm': 6.411593437194824, 'learning_rate': 2.613439575454791e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 23.85it/s]                                               {'loss': 2.0535, 'grad_norm': 3.857168197631836, 'learning_rate': 2.3956529441668918e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 23.85it/s]                                               {'loss': 1.9123, 'grad_norm': 2.94522762298584, 'learning_rate': 2.1778663128789924e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 23.85it/s] 89%|████████▉ | 67/75 [00:02<00:00, 23.56it/s]                                               {'loss': 2.0584, 'grad_norm': 3.639557361602783, 'learning_rate': 1.960079681591093e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 23.56it/s]                                               {'loss': 1.9881, 'grad_norm': 4.446680545806885, 'learning_rate': 1.742293050303194e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 23.56it/s]                                               {'loss': 2.1872, 'grad_norm': 3.629038095474243, 'learning_rate': 1.5245064190152948e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 23.56it/s] 93%|█████████▎| 70/75 [00:02<00:00, 23.96it/s]                                               {'loss': 2.3021, 'grad_norm': 4.20633602142334, 'learning_rate': 1.3067197877273956e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 23.96it/s]                                               {'loss': 2.0755, 'grad_norm': 4.543099403381348, 'learning_rate': 1.0889331564394962e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 23.96it/s]                                               {'loss': 2.0129, 'grad_norm': 3.365406036376953, 'learning_rate': 8.71146525151597e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 23.96it/s] 97%|█████████▋| 73/75 [00:03<00:00, 22.87it/s]                                               {'loss': 1.9368, 'grad_norm': 4.3478102684021, 'learning_rate': 6.533598938636978e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 22.87it/s]                                               {'loss': 2.1066, 'grad_norm': 3.463494062423706, 'learning_rate': 4.355732625757985e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 22.87it/s]                                               {'loss': 2.1349, 'grad_norm': 7.9667816162109375, 'learning_rate': 2.1778663128789925e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 22.87it/s]                                               {'train_runtime': 3.3008, 'train_samples_per_second': 342.338, 'train_steps_per_second': 22.722, 'train_loss': 2.1700198634465537, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 22.87it/s]100%|██████████| 75/75 [00:03<00:00, 22.72it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4734, 'grad_norm': 4.7188310623168945, 'learning_rate': 0.00016333997346592444, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 22.35it/s]                                              {'loss': 2.4041, 'grad_norm': 5.398139953613281, 'learning_rate': 0.00016116210715304546, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 24.14it/s]  4%|▍         | 3/75 [00:00<00:03, 23.90it/s]                                              {'loss': 2.3642, 'grad_norm': 4.500054359436035, 'learning_rate': 0.00015898424084016645, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.90it/s]                                              {'loss': 2.2827, 'grad_norm': 4.780538082122803, 'learning_rate': 0.00015680637452728744, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 23.90it/s]                                              {'loss': 2.5229, 'grad_norm': 5.226827144622803, 'learning_rate': 0.00015462850821440846, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.90it/s]  8%|▊         | 6/75 [00:00<00:03, 21.12it/s]                                              {'loss': 2.2218, 'grad_norm': 3.7052979469299316, 'learning_rate': 0.00015245064190152948, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 21.12it/s]                                              {'loss': 2.284, 'grad_norm': 4.5401458740234375, 'learning_rate': 0.0001502727755886505, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 21.12it/s]                                              {'loss': 2.1714, 'grad_norm': 4.183292388916016, 'learning_rate': 0.0001480949092757715, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 21.12it/s] 12%|█▏        | 9/75 [00:00<00:03, 21.86it/s]                                              {'loss': 2.2697, 'grad_norm': 3.8742587566375732, 'learning_rate': 0.00014591704296289249, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 21.86it/s]                                              {'loss': 2.1102, 'grad_norm': 4.149787902832031, 'learning_rate': 0.0001437391766500135, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 21.86it/s]                                               {'loss': 2.4782, 'grad_norm': 4.789682388305664, 'learning_rate': 0.00014156131033713453, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 21.86it/s] 16%|█▌        | 12/75 [00:00<00:03, 20.63it/s]                                               {'loss': 2.304, 'grad_norm': 4.063230037689209, 'learning_rate': 0.00013938344402425552, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:03, 20.63it/s]                                               {'loss': 2.2968, 'grad_norm': 4.239557266235352, 'learning_rate': 0.0001372055777113765, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:03, 20.63it/s]                                               {'loss': 2.2755, 'grad_norm': 3.478304862976074, 'learning_rate': 0.00013502771139849753, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 20.63it/s] 20%|██        | 15/75 [00:00<00:02, 22.23it/s]                                               {'loss': 2.2112, 'grad_norm': 6.798191070556641, 'learning_rate': 0.00013284984508561855, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 22.23it/s]                                               {'loss': 2.2054, 'grad_norm': 3.5006866455078125, 'learning_rate': 0.00013067197877273954, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 22.23it/s]                                               {'loss': 2.392, 'grad_norm': 5.181242942810059, 'learning_rate': 0.00012849411245986054, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 22.23it/s] 24%|██▍       | 18/75 [00:00<00:02, 22.04it/s]                                               {'loss': 2.385, 'grad_norm': 3.8880772590637207, 'learning_rate': 0.00012631624614698156, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 22.04it/s]                                               {'loss': 2.2151, 'grad_norm': 3.4067575931549072, 'learning_rate': 0.00012413837983410258, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 22.04it/s]                                               {'loss': 2.0168, 'grad_norm': 3.1587741374969482, 'learning_rate': 0.00012196051352122358, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 22.04it/s] 28%|██▊       | 21/75 [00:00<00:02, 22.89it/s]                                               {'loss': 2.2654, 'grad_norm': 3.4009077548980713, 'learning_rate': 0.00011978264720834457, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 22.89it/s]                                               {'loss': 2.1836, 'grad_norm': 4.952090263366699, 'learning_rate': 0.0001176047808954656, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 22.89it/s]                                               {'loss': 1.9976, 'grad_norm': 2.945681095123291, 'learning_rate': 0.0001154269145825866, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 22.89it/s] 32%|███▏      | 24/75 [00:01<00:02, 23.08it/s]                                               {'loss': 2.2571, 'grad_norm': 4.385583400726318, 'learning_rate': 0.00011324904826970761, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 23.08it/s]                                               {'loss': 2.159, 'grad_norm': 4.050528526306152, 'learning_rate': 0.00011107118195682863, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.08it/s]                                               {'loss': 2.3071, 'grad_norm': 3.0016353130340576, 'learning_rate': 0.00010889331564394962, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.08it/s] 36%|███▌      | 27/75 [00:01<00:02, 23.33it/s]                                               {'loss': 2.3791, 'grad_norm': 3.9234533309936523, 'learning_rate': 0.00010671544933107063, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 23.33it/s]                                               {'loss': 2.2639, 'grad_norm': 3.7210347652435303, 'learning_rate': 0.00010453758301819165, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 23.33it/s]                                               {'loss': 2.2639, 'grad_norm': 4.116303443908691, 'learning_rate': 0.00010235971670531265, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 23.33it/s]                                               {'loss': 2.9439, 'grad_norm': 9.666945457458496, 'learning_rate': 0.00010018185039243364, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.33it/s] 41%|████▏     | 31/75 [00:01<00:01, 22.46it/s]                                               {'loss': 2.347, 'grad_norm': 4.775972366333008, 'learning_rate': 9.800398407955466e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 22.46it/s]                                               {'loss': 2.028, 'grad_norm': 3.7570767402648926, 'learning_rate': 9.582611776667567e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 22.46it/s]                                               {'loss': 2.0467, 'grad_norm': 3.932159662246704, 'learning_rate': 9.364825145379668e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 22.46it/s] 45%|████▌     | 34/75 [00:01<00:02, 18.22it/s]                                               {'loss': 2.2211, 'grad_norm': 4.330032825469971, 'learning_rate': 9.14703851409177e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:02, 18.22it/s]                                               {'loss': 2.2727, 'grad_norm': 3.3735575675964355, 'learning_rate': 8.929251882803869e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:02, 18.22it/s]                                               {'loss': 2.3053, 'grad_norm': 4.048489093780518, 'learning_rate': 8.71146525151597e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:02, 18.22it/s] 49%|████▉     | 37/75 [00:01<00:02, 18.72it/s]                                               {'loss': 2.0642, 'grad_norm': 4.156296730041504, 'learning_rate': 8.493678620228072e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:02, 18.72it/s]                                               {'loss': 2.2706, 'grad_norm': 3.7878525257110596, 'learning_rate': 8.275891988940172e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 18.72it/s] 52%|█████▏    | 39/75 [00:01<00:02, 16.85it/s]                                               {'loss': 2.1192, 'grad_norm': 3.2299182415008545, 'learning_rate': 8.058105357652273e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:02, 16.85it/s]                                               {'loss': 2.0425, 'grad_norm': 3.426732063293457, 'learning_rate': 7.840318726364372e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:02, 16.85it/s]                                               {'loss': 2.4292, 'grad_norm': 4.854276180267334, 'learning_rate': 7.622532095076474e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:02<00:02, 16.85it/s] 56%|█████▌    | 42/75 [00:02<00:01, 17.54it/s]                                               {'loss': 2.4251, 'grad_norm': 4.155829429626465, 'learning_rate': 7.404745463788575e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 17.54it/s]                                               {'loss': 2.2323, 'grad_norm': 3.4689981937408447, 'learning_rate': 7.186958832500675e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 17.54it/s]                                               {'loss': 2.0337, 'grad_norm': 2.8634331226348877, 'learning_rate': 6.969172201212776e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 17.54it/s] 60%|██████    | 45/75 [00:02<00:01, 18.69it/s]                                               {'loss': 2.1878, 'grad_norm': 12.12774658203125, 'learning_rate': 6.751385569924877e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 18.69it/s]                                               {'loss': 2.2208, 'grad_norm': 3.4795262813568115, 'learning_rate': 6.533598938636977e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 18.69it/s] 63%|██████▎   | 47/75 [00:02<00:01, 18.97it/s]                                               {'loss': 2.1632, 'grad_norm': 2.8152551651000977, 'learning_rate': 6.315812307349078e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 18.97it/s]                                               {'loss': 2.0936, 'grad_norm': 3.483794689178467, 'learning_rate': 6.098025676061179e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 18.97it/s]                                               {'loss': 2.3452, 'grad_norm': 4.61622953414917, 'learning_rate': 5.88023904477328e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 18.97it/s] 67%|██████▋   | 50/75 [00:02<00:01, 20.15it/s]                                               {'loss': 2.126, 'grad_norm': 4.520750522613525, 'learning_rate': 5.6624524134853803e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 20.15it/s]                                               {'loss': 1.986, 'grad_norm': 3.6493003368377686, 'learning_rate': 5.444665782197481e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 20.15it/s]                                               {'loss': 2.2538, 'grad_norm': 3.881833553314209, 'learning_rate': 5.226879150909582e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 20.15it/s] 71%|███████   | 53/75 [00:02<00:01, 19.47it/s]                                               {'loss': 2.3282, 'grad_norm': 4.16978120803833, 'learning_rate': 5.009092519621682e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:01, 19.47it/s]                                               {'loss': 2.2384, 'grad_norm': 4.667428970336914, 'learning_rate': 4.7913058883337835e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:01, 19.47it/s] 73%|███████▎  | 55/75 [00:02<00:01, 18.40it/s]                                               {'loss': 2.1211, 'grad_norm': 3.5175621509552, 'learning_rate': 4.573519257045885e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:01, 18.40it/s]                                               {'loss': 2.1203, 'grad_norm': 3.610846519470215, 'learning_rate': 4.355732625757985e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:01, 18.40it/s] 76%|███████▌  | 57/75 [00:02<00:01, 17.41it/s]                                               {'loss': 2.1528, 'grad_norm': 4.176815509796143, 'learning_rate': 4.137945994470086e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:01, 17.41it/s]                                               {'loss': 2.2227, 'grad_norm': 4.816359043121338, 'learning_rate': 3.920159363182186e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 17.41it/s] 79%|███████▊  | 59/75 [00:02<00:00, 17.90it/s]                                               {'loss': 2.1068, 'grad_norm': 4.511914253234863, 'learning_rate': 3.702372731894287e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 17.90it/s]                                               {'loss': 1.6668, 'grad_norm': 6.764275074005127, 'learning_rate': 3.484586100606388e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:03<00:00, 17.90it/s]                                               {'loss': 2.3091, 'grad_norm': 4.115045070648193, 'learning_rate': 3.2667994693184886e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:03<00:00, 17.90it/s] 83%|████████▎ | 62/75 [00:03<00:00, 20.08it/s]                                               {'loss': 2.3463, 'grad_norm': 4.040740013122559, 'learning_rate': 3.0490128380305895e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:03<00:00, 20.08it/s]                                               {'loss': 2.0784, 'grad_norm': 3.451338529586792, 'learning_rate': 2.8312262067426902e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:03<00:00, 20.08it/s]                                               {'loss': 2.005, 'grad_norm': 3.25321102142334, 'learning_rate': 2.613439575454791e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:03<00:00, 20.08it/s] 87%|████████▋ | 65/75 [00:03<00:00, 21.06it/s]                                               {'loss': 2.1792, 'grad_norm': 3.4378950595855713, 'learning_rate': 2.3956529441668918e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:03<00:00, 21.06it/s]                                               {'loss': 2.1479, 'grad_norm': 4.2660346031188965, 'learning_rate': 2.1778663128789924e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:03<00:00, 21.06it/s]                                               {'loss': 2.1237, 'grad_norm': 3.1120283603668213, 'learning_rate': 1.960079681591093e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 21.06it/s] 91%|█████████ | 68/75 [00:03<00:00, 21.41it/s]                                               {'loss': 2.1784, 'grad_norm': 3.6386806964874268, 'learning_rate': 1.742293050303194e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 21.41it/s]                                               {'loss': 2.2865, 'grad_norm': 4.292200088500977, 'learning_rate': 1.5245064190152948e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 21.41it/s]                                               {'loss': 2.2518, 'grad_norm': 4.023146152496338, 'learning_rate': 1.3067197877273956e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 21.41it/s] 95%|█████████▍| 71/75 [00:03<00:00, 22.35it/s]                                               {'loss': 2.0452, 'grad_norm': 3.9767861366271973, 'learning_rate': 1.0889331564394962e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 22.35it/s]                                               {'loss': 2.0872, 'grad_norm': 4.01492166519165, 'learning_rate': 8.71146525151597e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 22.35it/s]                                               {'loss': 2.1086, 'grad_norm': 3.7508606910705566, 'learning_rate': 6.533598938636978e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 22.35it/s] 99%|█████████▊| 74/75 [00:03<00:00, 22.91it/s]                                               {'loss': 2.0983, 'grad_norm': 3.3237295150756836, 'learning_rate': 4.355732625757985e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 22.91it/s]                                               {'loss': 1.8126, 'grad_norm': 11.177955627441406, 'learning_rate': 2.1778663128789925e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 22.91it/s]                                               {'train_runtime': 3.7866, 'train_samples_per_second': 298.423, 'train_steps_per_second': 19.807, 'train_loss': 2.21512491385142, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 22.91it/s]100%|██████████| 75/75 [00:03<00:00, 19.80it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:35
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2519, 'grad_norm': 4.357647895812988, 'learning_rate': 0.00016333997346592444, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:05, 12.35it/s]  3%|▎         | 2/75 [00:00<00:04, 17.09it/s]                                              {'loss': 2.3165, 'grad_norm': 4.079289436340332, 'learning_rate': 0.00016116210715304546, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:04, 17.09it/s]                                              {'loss': 2.1448, 'grad_norm': 4.717850208282471, 'learning_rate': 0.00015898424084016645, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 17.09it/s]                                              {'loss': 2.2667, 'grad_norm': 4.199798107147217, 'learning_rate': 0.00015680637452728744, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:04, 17.09it/s]  7%|▋         | 5/75 [00:00<00:03, 22.05it/s]                                              {'loss': 2.4318, 'grad_norm': 5.2031145095825195, 'learning_rate': 0.00015462850821440846, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.05it/s]                                              {'loss': 2.2001, 'grad_norm': 3.697056770324707, 'learning_rate': 0.00015245064190152948, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 22.05it/s]                                              {'loss': 2.4105, 'grad_norm': 4.48057222366333, 'learning_rate': 0.0001502727755886505, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 22.05it/s] 11%|█         | 8/75 [00:00<00:02, 23.41it/s]                                              {'loss': 2.4555, 'grad_norm': 5.020995616912842, 'learning_rate': 0.0001480949092757715, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.41it/s]                                              {'loss': 2.4668, 'grad_norm': 5.1696367263793945, 'learning_rate': 0.00014591704296289249, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.41it/s]                                              {'loss': 2.1996, 'grad_norm': 5.526601791381836, 'learning_rate': 0.0001437391766500135, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.41it/s] 15%|█▍        | 11/75 [00:00<00:02, 24.01it/s]                                               {'loss': 2.2511, 'grad_norm': 3.7027227878570557, 'learning_rate': 0.00014156131033713453, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.01it/s]                                               {'loss': 2.1294, 'grad_norm': 4.06401252746582, 'learning_rate': 0.00013938344402425552, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.01it/s]                                               {'loss': 2.3846, 'grad_norm': 4.989360809326172, 'learning_rate': 0.0001372055777113765, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.01it/s] 19%|█▊        | 14/75 [00:00<00:02, 24.28it/s]                                               {'loss': 2.3924, 'grad_norm': 4.585373401641846, 'learning_rate': 0.00013502771139849753, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.28it/s]                                               {'loss': 2.0102, 'grad_norm': 8.07314395904541, 'learning_rate': 0.00013284984508561855, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.28it/s]                                               {'loss': 2.3785, 'grad_norm': 4.55596399307251, 'learning_rate': 0.00013067197877273954, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 24.28it/s]                                               {'loss': 2.2096, 'grad_norm': 4.402161598205566, 'learning_rate': 0.00012849411245986054, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.28it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.86it/s]                                               {'loss': 2.0989, 'grad_norm': 4.214921474456787, 'learning_rate': 0.00012631624614698156, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.86it/s]                                               {'loss': 2.0738, 'grad_norm': 4.31077241897583, 'learning_rate': 0.00012413837983410258, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.86it/s]                                               {'loss': 2.2669, 'grad_norm': 3.737398624420166, 'learning_rate': 0.00012196051352122358, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.86it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.32it/s]                                               {'loss': 2.1614, 'grad_norm': 4.271796703338623, 'learning_rate': 0.00011978264720834457, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.32it/s]                                               {'loss': 2.246, 'grad_norm': 3.7423250675201416, 'learning_rate': 0.0001176047808954656, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.32it/s]                                               {'loss': 2.2987, 'grad_norm': 4.057379245758057, 'learning_rate': 0.0001154269145825866, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.32it/s] 32%|███▏      | 24/75 [00:01<00:02, 24.00it/s]                                               {'loss': 2.1969, 'grad_norm': 3.9819984436035156, 'learning_rate': 0.00011324904826970761, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 24.00it/s]                                               {'loss': 2.1563, 'grad_norm': 4.041198253631592, 'learning_rate': 0.00011107118195682863, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.00it/s]                                               {'loss': 2.4109, 'grad_norm': 4.933352947235107, 'learning_rate': 0.00010889331564394962, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.00it/s] 36%|███▌      | 27/75 [00:01<00:02, 22.69it/s]                                               {'loss': 2.2084, 'grad_norm': 3.9266843795776367, 'learning_rate': 0.00010671544933107063, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 22.69it/s]                                               {'loss': 2.1343, 'grad_norm': 3.7071869373321533, 'learning_rate': 0.00010453758301819165, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 22.69it/s]                                               {'loss': 2.3218, 'grad_norm': 3.7717480659484863, 'learning_rate': 0.00010235971670531265, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 22.69it/s] 40%|████      | 30/75 [00:01<00:01, 24.07it/s]                                               {'loss': 2.1579, 'grad_norm': 12.7621488571167, 'learning_rate': 0.00010018185039243364, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.07it/s]                                               {'loss': 2.2996, 'grad_norm': 3.821575164794922, 'learning_rate': 9.800398407955466e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.07it/s]                                               {'loss': 2.0165, 'grad_norm': 3.4625296592712402, 'learning_rate': 9.582611776667567e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.07it/s] 44%|████▍     | 33/75 [00:01<00:01, 23.93it/s]                                               {'loss': 2.1819, 'grad_norm': 3.8751819133758545, 'learning_rate': 9.364825145379668e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 23.93it/s]                                               {'loss': 2.1673, 'grad_norm': 3.305065155029297, 'learning_rate': 9.14703851409177e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 23.93it/s]                                               {'loss': 2.152, 'grad_norm': 4.233980178833008, 'learning_rate': 8.929251882803869e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 23.93it/s] 48%|████▊     | 36/75 [00:01<00:01, 24.62it/s]                                               {'loss': 2.2102, 'grad_norm': 3.8384640216827393, 'learning_rate': 8.71146525151597e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.62it/s]                                               {'loss': 2.3161, 'grad_norm': 3.5630767345428467, 'learning_rate': 8.493678620228072e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.62it/s]                                               {'loss': 2.1157, 'grad_norm': 4.17500114440918, 'learning_rate': 8.275891988940172e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.62it/s] 52%|█████▏    | 39/75 [00:01<00:01, 24.52it/s]                                               {'loss': 2.1557, 'grad_norm': 3.626404285430908, 'learning_rate': 8.058105357652273e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.52it/s]                                               {'loss': 2.2441, 'grad_norm': 3.277752161026001, 'learning_rate': 7.840318726364372e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.52it/s]                                               {'loss': 2.0654, 'grad_norm': 3.7862136363983154, 'learning_rate': 7.622532095076474e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.52it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.1835, 'grad_norm': 3.9309494495391846, 'learning_rate': 7.404745463788575e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.3866, 'grad_norm': 4.9953742027282715, 'learning_rate': 7.186958832500675e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.1126, 'grad_norm': 3.5195560455322266, 'learning_rate': 6.969172201212776e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.1768, 'grad_norm': 12.510801315307617, 'learning_rate': 6.751385569924877e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.57it/s] 61%|██████▏   | 46/75 [00:01<00:01, 26.35it/s]                                               {'loss': 2.3788, 'grad_norm': 4.479491710662842, 'learning_rate': 6.533598938636977e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.35it/s]                                               {'loss': 2.0175, 'grad_norm': 3.0491249561309814, 'learning_rate': 6.315812307349078e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.35it/s]                                               {'loss': 2.1479, 'grad_norm': 4.418367862701416, 'learning_rate': 6.098025676061179e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.35it/s] 65%|██████▌   | 49/75 [00:01<00:01, 25.85it/s]                                               {'loss': 2.1932, 'grad_norm': 4.410333633422852, 'learning_rate': 5.88023904477328e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.85it/s]                                               {'loss': 2.2855, 'grad_norm': 4.168162822723389, 'learning_rate': 5.6624524134853803e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.85it/s]                                               {'loss': 1.9593, 'grad_norm': 4.018220901489258, 'learning_rate': 5.444665782197481e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.85it/s] 69%|██████▉   | 52/75 [00:02<00:00, 23.47it/s]                                               {'loss': 2.0068, 'grad_norm': 4.445307731628418, 'learning_rate': 5.226879150909582e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 23.47it/s]                                               {'loss': 2.1775, 'grad_norm': 4.398221492767334, 'learning_rate': 5.009092519621682e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 23.47it/s]                                               {'loss': 2.0642, 'grad_norm': 4.224506855010986, 'learning_rate': 4.7913058883337835e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 23.47it/s] 73%|███████▎  | 55/75 [00:02<00:00, 22.93it/s]                                               {'loss': 2.4216, 'grad_norm': 3.386579990386963, 'learning_rate': 4.573519257045885e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 22.93it/s]                                               {'loss': 2.2222, 'grad_norm': 4.499337673187256, 'learning_rate': 4.355732625757985e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 22.93it/s]                                               {'loss': 2.0254, 'grad_norm': 3.3284499645233154, 'learning_rate': 4.137945994470086e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 22.93it/s] 77%|███████▋  | 58/75 [00:02<00:00, 23.39it/s]                                               {'loss': 1.9328, 'grad_norm': 3.260660171508789, 'learning_rate': 3.920159363182186e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 23.39it/s]                                               {'loss': 2.3512, 'grad_norm': 4.049692630767822, 'learning_rate': 3.702372731894287e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 23.39it/s]                                               {'loss': 1.9056, 'grad_norm': 7.263978958129883, 'learning_rate': 3.484586100606388e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 23.39it/s]                                               {'loss': 2.0896, 'grad_norm': 3.1564266681671143, 'learning_rate': 3.2667994693184886e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 23.39it/s] 83%|████████▎ | 62/75 [00:02<00:00, 25.25it/s]                                               {'loss': 2.0215, 'grad_norm': 4.469934463500977, 'learning_rate': 3.0490128380305895e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.25it/s]                                               {'loss': 2.0824, 'grad_norm': 3.5208487510681152, 'learning_rate': 2.8312262067426902e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.25it/s]                                               {'loss': 2.1681, 'grad_norm': 3.5289952754974365, 'learning_rate': 2.613439575454791e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.25it/s] 87%|████████▋ | 65/75 [00:02<00:00, 25.28it/s]                                               {'loss': 2.1691, 'grad_norm': 5.0189361572265625, 'learning_rate': 2.3956529441668918e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.28it/s]                                               {'loss': 2.1887, 'grad_norm': 5.282013416290283, 'learning_rate': 2.1778663128789924e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.28it/s]                                               {'loss': 2.0336, 'grad_norm': 3.5429749488830566, 'learning_rate': 1.960079681591093e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.28it/s] 91%|█████████ | 68/75 [00:02<00:00, 25.13it/s]                                               {'loss': 1.8764, 'grad_norm': 3.5121469497680664, 'learning_rate': 1.742293050303194e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.13it/s]                                               {'loss': 2.0561, 'grad_norm': 3.3190057277679443, 'learning_rate': 1.5245064190152948e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.13it/s]                                               {'loss': 2.2912, 'grad_norm': 4.6229472160339355, 'learning_rate': 1.3067197877273956e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.13it/s] 95%|█████████▍| 71/75 [00:02<00:00, 25.04it/s]                                               {'loss': 2.319, 'grad_norm': 4.120250701904297, 'learning_rate': 1.0889331564394962e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.04it/s]                                               {'loss': 2.3239, 'grad_norm': 3.491680383682251, 'learning_rate': 8.71146525151597e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.04it/s]                                               {'loss': 2.216, 'grad_norm': 4.354290008544922, 'learning_rate': 6.533598938636978e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.04it/s] 99%|█████████▊| 74/75 [00:03<00:00, 24.96it/s]                                               {'loss': 2.1232, 'grad_norm': 3.923781156539917, 'learning_rate': 4.355732625757985e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.96it/s]                                               {'loss': 2.5415, 'grad_norm': 13.01620101928711, 'learning_rate': 2.1778663128789925e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.96it/s]                                               {'train_runtime': 3.1681, 'train_samples_per_second': 356.684, 'train_steps_per_second': 23.674, 'train_loss': 2.2000815025965372, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.96it/s]100%|██████████| 75/75 [00:03<00:00, 23.70it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:10
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4777, 'grad_norm': 6.255306243896484, 'learning_rate': 0.00016333997346592444, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:05, 12.41it/s]  3%|▎         | 2/75 [00:00<00:04, 16.44it/s]                                              {'loss': 2.35, 'grad_norm': 4.588693141937256, 'learning_rate': 0.00016116210715304546, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:04, 16.44it/s]                                              {'loss': 2.2367, 'grad_norm': 4.536222457885742, 'learning_rate': 0.00015898424084016645, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 16.44it/s]  5%|▌         | 4/75 [00:00<00:04, 17.06it/s]                                              {'loss': 2.2181, 'grad_norm': 4.778460502624512, 'learning_rate': 0.00015680637452728744, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:04, 17.06it/s]                                              {'loss': 2.4091, 'grad_norm': 4.168778419494629, 'learning_rate': 0.00015462850821440846, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:04, 17.06it/s]  8%|▊         | 6/75 [00:00<00:03, 17.94it/s]                                              {'loss': 2.4668, 'grad_norm': 4.425508975982666, 'learning_rate': 0.00015245064190152948, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 17.94it/s]                                              {'loss': 2.4832, 'grad_norm': 5.46494197845459, 'learning_rate': 0.0001502727755886505, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 17.94it/s]                                              {'loss': 2.6051, 'grad_norm': 3.9797167778015137, 'learning_rate': 0.0001480949092757715, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 17.94it/s] 12%|█▏        | 9/75 [00:00<00:03, 19.49it/s]                                              {'loss': 2.2469, 'grad_norm': 4.549152374267578, 'learning_rate': 0.00014591704296289249, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 19.49it/s]                                              {'loss': 2.2193, 'grad_norm': 4.452152729034424, 'learning_rate': 0.0001437391766500135, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 19.49it/s] 15%|█▍        | 11/75 [00:00<00:03, 18.75it/s]                                               {'loss': 2.4025, 'grad_norm': 4.793446063995361, 'learning_rate': 0.00014156131033713453, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 18.75it/s]                                               {'loss': 2.4399, 'grad_norm': 5.924397945404053, 'learning_rate': 0.00013938344402425552, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:03, 18.75it/s]                                               {'loss': 2.2541, 'grad_norm': 4.428529739379883, 'learning_rate': 0.0001372055777113765, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:03, 18.75it/s] 19%|█▊        | 14/75 [00:00<00:03, 19.96it/s]                                               {'loss': 2.1726, 'grad_norm': 3.8956921100616455, 'learning_rate': 0.00013502771139849753, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:03, 19.96it/s]                                               {'loss': 1.9888, 'grad_norm': 10.672927856445312, 'learning_rate': 0.00013284984508561855, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:03, 19.96it/s]                                               {'loss': 2.2715, 'grad_norm': 3.814504861831665, 'learning_rate': 0.00013067197877273954, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 19.96it/s] 23%|██▎       | 17/75 [00:00<00:02, 21.50it/s]                                               {'loss': 2.3235, 'grad_norm': 5.056858062744141, 'learning_rate': 0.00012849411245986054, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 21.50it/s]                                               {'loss': 2.344, 'grad_norm': 4.036022663116455, 'learning_rate': 0.00012631624614698156, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 21.50it/s]                                               {'loss': 2.2075, 'grad_norm': 3.769867181777954, 'learning_rate': 0.00012413837983410258, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 21.50it/s] 27%|██▋       | 20/75 [00:00<00:02, 22.52it/s]                                               {'loss': 2.425, 'grad_norm': 4.059246063232422, 'learning_rate': 0.00012196051352122358, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 22.52it/s]                                               {'loss': 2.2129, 'grad_norm': 5.0682291984558105, 'learning_rate': 0.00011978264720834457, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:02, 22.52it/s]                                               {'loss': 2.3159, 'grad_norm': 4.122833251953125, 'learning_rate': 0.0001176047808954656, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 22.52it/s] 31%|███       | 23/75 [00:01<00:02, 23.06it/s]                                               {'loss': 2.236, 'grad_norm': 5.0169677734375, 'learning_rate': 0.0001154269145825866, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 23.06it/s]                                               {'loss': 2.0667, 'grad_norm': 3.4682931900024414, 'learning_rate': 0.00011324904826970761, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 23.06it/s]                                               {'loss': 2.2517, 'grad_norm': 4.186462879180908, 'learning_rate': 0.00011107118195682863, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.06it/s] 35%|███▍      | 26/75 [00:01<00:02, 22.29it/s]                                               {'loss': 2.2147, 'grad_norm': 3.7410271167755127, 'learning_rate': 0.00010889331564394962, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 22.29it/s]                                               {'loss': 2.5144, 'grad_norm': 4.3947014808654785, 'learning_rate': 0.00010671544933107063, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 22.29it/s]                                               {'loss': 2.2217, 'grad_norm': 3.688462495803833, 'learning_rate': 0.00010453758301819165, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 22.29it/s] 39%|███▊      | 29/75 [00:01<00:02, 22.59it/s]                                               {'loss': 2.2852, 'grad_norm': 4.400471210479736, 'learning_rate': 0.00010235971670531265, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 22.59it/s]                                               {'loss': 2.3468, 'grad_norm': 11.742583274841309, 'learning_rate': 0.00010018185039243364, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 22.59it/s]                                               {'loss': 2.2248, 'grad_norm': 4.155550956726074, 'learning_rate': 9.800398407955466e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 22.59it/s] 43%|████▎     | 32/75 [00:01<00:01, 24.39it/s]                                               {'loss': 2.2218, 'grad_norm': 3.911571502685547, 'learning_rate': 9.582611776667567e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.39it/s]                                               {'loss': 2.3995, 'grad_norm': 4.960021018981934, 'learning_rate': 9.364825145379668e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.39it/s]                                               {'loss': 2.4433, 'grad_norm': 4.620136260986328, 'learning_rate': 9.14703851409177e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.39it/s] 47%|████▋     | 35/75 [00:01<00:01, 24.90it/s]                                               {'loss': 2.2696, 'grad_norm': 4.210598945617676, 'learning_rate': 8.929251882803869e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.90it/s]                                               {'loss': 2.4273, 'grad_norm': 4.339582443237305, 'learning_rate': 8.71146525151597e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.90it/s]                                               {'loss': 2.3635, 'grad_norm': 6.042085647583008, 'learning_rate': 8.493678620228072e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.90it/s] 51%|█████     | 38/75 [00:01<00:01, 25.19it/s]                                               {'loss': 2.0261, 'grad_norm': 4.4234747886657715, 'learning_rate': 8.275891988940172e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.19it/s]                                               {'loss': 2.0703, 'grad_norm': 3.5798275470733643, 'learning_rate': 8.058105357652273e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.19it/s]                                               {'loss': 2.3154, 'grad_norm': 4.175578594207764, 'learning_rate': 7.840318726364372e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.19it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.06it/s]                                               {'loss': 2.2114, 'grad_norm': 5.1741766929626465, 'learning_rate': 7.622532095076474e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.06it/s]                                               {'loss': 2.2694, 'grad_norm': 3.9451558589935303, 'learning_rate': 7.404745463788575e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.06it/s]                                               {'loss': 2.1935, 'grad_norm': 4.253440856933594, 'learning_rate': 7.186958832500675e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.06it/s] 59%|█████▊    | 44/75 [00:01<00:01, 24.88it/s]                                               {'loss': 2.0291, 'grad_norm': 3.7131168842315674, 'learning_rate': 6.969172201212776e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.88it/s]                                               {'loss': 2.0019, 'grad_norm': 11.673251152038574, 'learning_rate': 6.751385569924877e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.88it/s]                                               {'loss': 2.2496, 'grad_norm': 4.567978382110596, 'learning_rate': 6.533598938636977e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 24.88it/s]                                               {'loss': 2.1047, 'grad_norm': 4.427396297454834, 'learning_rate': 6.315812307349078e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 24.88it/s] 64%|██████▍   | 48/75 [00:02<00:01, 26.51it/s]                                               {'loss': 2.1751, 'grad_norm': 4.761260986328125, 'learning_rate': 6.098025676061179e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 26.51it/s]                                               {'loss': 2.2883, 'grad_norm': 4.70253324508667, 'learning_rate': 5.88023904477328e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:00, 26.51it/s]                                               {'loss': 2.4358, 'grad_norm': 4.785821437835693, 'learning_rate': 5.6624524134853803e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 26.51it/s] 68%|██████▊   | 51/75 [00:02<00:00, 26.11it/s]                                               {'loss': 2.4349, 'grad_norm': 4.743153095245361, 'learning_rate': 5.444665782197481e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 26.11it/s]                                               {'loss': 2.1024, 'grad_norm': 3.516329050064087, 'learning_rate': 5.226879150909582e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 26.11it/s]                                               {'loss': 2.2261, 'grad_norm': 5.155338287353516, 'learning_rate': 5.009092519621682e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 26.11it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.89it/s]                                               {'loss': 1.933, 'grad_norm': 3.7559170722961426, 'learning_rate': 4.7913058883337835e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.89it/s]                                               {'loss': 2.1709, 'grad_norm': 3.9033172130584717, 'learning_rate': 4.573519257045885e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.89it/s]                                               {'loss': 2.2114, 'grad_norm': 4.340670585632324, 'learning_rate': 4.355732625757985e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.89it/s] 76%|███████▌  | 57/75 [00:02<00:00, 25.59it/s]                                               {'loss': 2.0868, 'grad_norm': 4.308652877807617, 'learning_rate': 4.137945994470086e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.59it/s]                                               {'loss': 2.2832, 'grad_norm': 3.446084499359131, 'learning_rate': 3.920159363182186e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.59it/s]                                               {'loss': 2.2745, 'grad_norm': 3.4347052574157715, 'learning_rate': 3.702372731894287e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.59it/s]                                               {'loss': 2.5981, 'grad_norm': 16.103418350219727, 'learning_rate': 3.484586100606388e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.59it/s] 81%|████████▏ | 61/75 [00:02<00:00, 27.06it/s]                                               {'loss': 2.2291, 'grad_norm': 4.070079326629639, 'learning_rate': 3.2667994693184886e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 27.06it/s]                                               {'loss': 2.1399, 'grad_norm': 4.21656608581543, 'learning_rate': 3.0490128380305895e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 27.06it/s]                                               {'loss': 2.1801, 'grad_norm': 3.94132137298584, 'learning_rate': 2.8312262067426902e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 27.06it/s] 85%|████████▌ | 64/75 [00:02<00:00, 26.67it/s]                                               {'loss': 2.1112, 'grad_norm': 4.4362592697143555, 'learning_rate': 2.613439575454791e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.67it/s]                                               {'loss': 2.0415, 'grad_norm': 3.6887056827545166, 'learning_rate': 2.3956529441668918e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.67it/s]                                               {'loss': 2.2797, 'grad_norm': 3.4130773544311523, 'learning_rate': 2.1778663128789924e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.67it/s] 89%|████████▉ | 67/75 [00:02<00:00, 26.28it/s]                                               {'loss': 2.1858, 'grad_norm': 3.6981940269470215, 'learning_rate': 1.960079681591093e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.28it/s]                                               {'loss': 2.0891, 'grad_norm': 5.222358703613281, 'learning_rate': 1.742293050303194e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 26.28it/s]                                               {'loss': 2.3213, 'grad_norm': 3.768305540084839, 'learning_rate': 1.5245064190152948e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 26.28it/s] 93%|█████████▎| 70/75 [00:02<00:00, 26.12it/s]                                               {'loss': 2.3365, 'grad_norm': 3.4249520301818848, 'learning_rate': 1.3067197877273956e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 26.12it/s]                                               {'loss': 2.3553, 'grad_norm': 5.104737758636475, 'learning_rate': 1.0889331564394962e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 26.12it/s]                                               {'loss': 2.2355, 'grad_norm': 4.296083450317383, 'learning_rate': 8.71146525151597e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 26.12it/s] 97%|█████████▋| 73/75 [00:03<00:00, 25.76it/s]                                               {'loss': 1.9183, 'grad_norm': 4.033527374267578, 'learning_rate': 6.533598938636978e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 25.76it/s]                                               {'loss': 2.2742, 'grad_norm': 4.799783229827881, 'learning_rate': 4.355732625757985e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 25.76it/s]                                               {'loss': 2.5314, 'grad_norm': 9.767239570617676, 'learning_rate': 2.1778663128789925e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.76it/s]                                               {'train_runtime': 3.2348, 'train_samples_per_second': 349.331, 'train_steps_per_second': 23.186, 'train_loss': 2.259715010325114, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.76it/s]100%|██████████| 75/75 [00:03<00:00, 23.19it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1286, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(986, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(898, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1104, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1309, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1128, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1288, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1189, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1252, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1152, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1332, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(965, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 61.71it/s]  3%|▎         | 14/471 [00:00<00:08, 51.05it/s]  4%|▍         | 20/471 [00:00<00:08, 51.11it/s]  6%|▌         | 26/471 [00:00<00:08, 51.48it/s]  7%|▋         | 32/471 [00:00<00:08, 51.71it/s]  8%|▊         | 38/471 [00:00<00:08, 52.20it/s]  9%|▉         | 44/471 [00:00<00:08, 52.53it/s] 11%|█         | 50/471 [00:00<00:08, 52.08it/s] 12%|█▏        | 56/471 [00:01<00:07, 52.10it/s] 13%|█▎        | 62/471 [00:01<00:08, 50.58it/s] 14%|█▍        | 68/471 [00:01<00:07, 51.53it/s] 16%|█▌        | 74/471 [00:01<00:07, 51.14it/s] 17%|█▋        | 80/471 [00:01<00:07, 51.76it/s] 18%|█▊        | 86/471 [00:01<00:07, 51.87it/s] 20%|█▉        | 92/471 [00:01<00:07, 51.33it/s] 21%|██        | 98/471 [00:01<00:07, 50.48it/s] 22%|██▏       | 104/471 [00:02<00:07, 50.86it/s] 23%|██▎       | 110/471 [00:02<00:07, 51.28it/s] 25%|██▍       | 116/471 [00:02<00:06, 50.77it/s] 26%|██▌       | 122/471 [00:02<00:07, 48.54it/s] 27%|██▋       | 128/471 [00:02<00:06, 49.90it/s] 28%|██▊       | 134/471 [00:02<00:06, 50.49it/s] 30%|██▉       | 140/471 [00:02<00:06, 51.24it/s] 31%|███       | 146/471 [00:02<00:06, 50.28it/s] 32%|███▏      | 152/471 [00:02<00:06, 51.59it/s] 34%|███▎      | 158/471 [00:03<00:06, 51.73it/s] 35%|███▍      | 164/471 [00:03<00:05, 52.04it/s] 36%|███▌      | 170/471 [00:03<00:05, 51.84it/s] 37%|███▋      | 176/471 [00:03<00:05, 51.43it/s] 39%|███▊      | 182/471 [00:03<00:05, 52.10it/s] 40%|███▉      | 188/471 [00:03<00:05, 51.89it/s] 41%|████      | 194/471 [00:03<00:05, 51.26it/s] 42%|████▏     | 200/471 [00:03<00:05, 50.89it/s] 44%|████▎     | 206/471 [00:04<00:05, 51.11it/s] 45%|████▌     | 212/471 [00:04<00:05, 51.21it/s] 46%|████▋     | 218/471 [00:04<00:05, 49.19it/s] 48%|████▊     | 224/471 [00:04<00:04, 50.57it/s] 49%|████▉     | 230/471 [00:04<00:04, 51.13it/s] 50%|█████     | 236/471 [00:04<00:04, 51.08it/s] 51%|█████▏    | 242/471 [00:04<00:04, 51.70it/s] 53%|█████▎    | 248/471 [00:04<00:04, 51.65it/s] 54%|█████▍    | 254/471 [00:04<00:04, 50.62it/s] 55%|█████▌    | 260/471 [00:05<00:04, 51.15it/s] 56%|█████▋    | 266/471 [00:05<00:04, 51.23it/s] 58%|█████▊    | 272/471 [00:05<00:03, 51.05it/s] 59%|█████▉    | 278/471 [00:05<00:03, 51.53it/s] 60%|██████    | 284/471 [00:05<00:03, 50.17it/s] 62%|██████▏   | 290/471 [00:05<00:03, 49.89it/s] 63%|██████▎   | 296/471 [00:05<00:03, 51.17it/s] 64%|██████▍   | 302/471 [00:05<00:03, 51.30it/s] 65%|██████▌   | 308/471 [00:06<00:03, 51.65it/s] 67%|██████▋   | 314/471 [00:06<00:03, 51.78it/s] 68%|██████▊   | 320/471 [00:06<00:02, 52.14it/s] 69%|██████▉   | 326/471 [00:06<00:02, 51.13it/s] 70%|███████   | 332/471 [00:06<00:02, 51.68it/s] 72%|███████▏  | 338/471 [00:06<00:02, 50.99it/s] 73%|███████▎  | 344/471 [00:06<00:02, 49.66it/s] 74%|███████▍  | 349/471 [00:06<00:02, 49.65it/s] 75%|███████▌  | 355/471 [00:06<00:02, 50.93it/s] 77%|███████▋  | 361/471 [00:07<00:02, 49.73it/s] 78%|███████▊  | 367/471 [00:07<00:02, 50.04it/s] 79%|███████▉  | 373/471 [00:07<00:01, 50.20it/s] 80%|████████  | 379/471 [00:07<00:01, 50.54it/s] 82%|████████▏ | 385/471 [00:07<00:01, 47.37it/s] 83%|████████▎ | 391/471 [00:07<00:01, 47.84it/s] 84%|████████▍ | 397/471 [00:07<00:01, 49.14it/s] 85%|████████▌ | 402/471 [00:07<00:01, 49.24it/s] 87%|████████▋ | 408/471 [00:08<00:01, 49.66it/s] 88%|████████▊ | 414/471 [00:08<00:01, 50.46it/s] 89%|████████▉ | 420/471 [00:08<00:01, 50.38it/s] 90%|█████████ | 426/471 [00:08<00:00, 50.16it/s] 92%|█████████▏| 432/471 [00:08<00:00, 48.90it/s] 93%|█████████▎| 438/471 [00:08<00:00, 49.78it/s] 94%|█████████▍| 443/471 [00:08<00:00, 49.72it/s] 95%|█████████▌| 448/471 [00:08<00:00, 48.01it/s] 96%|█████████▋| 454/471 [00:08<00:00, 48.80it/s] 97%|█████████▋| 459/471 [00:09<00:00, 49.03it/s] 99%|█████████▊| 465/471 [00:09<00:00, 49.51it/s]100%|█████████▉| 470/471 [00:09<00:00, 49.59it/s]100%|██████████| 471/471 [00:09<00:00, 50.72it/s]
{'eval_loss': 2.346078872680664, 'eval_model_preparation_time': 0.003, 'eval_acc': 0.37533191715347847, 'eval_runtime': 9.3093, 'eval_samples_per_second': 809.083, 'eval_steps_per_second': 50.595}
ROUND:8
CLIENT:32
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2494, 'grad_norm': 4.688345432281494, 'learning_rate': 0.00015835921350012617, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.75it/s]                                              {'loss': 2.5051, 'grad_norm': 3.758080244064331, 'learning_rate': 0.00015624775732012448, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 25.08it/s]  4%|▍         | 3/75 [00:00<00:02, 25.66it/s]                                              {'loss': 2.3289, 'grad_norm': 4.424746990203857, 'learning_rate': 0.00015413630114012282, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.66it/s]                                              {'loss': 2.3557, 'grad_norm': 4.62145471572876, 'learning_rate': 0.0001520248449601211, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.66it/s]                                              {'loss': 2.3888, 'grad_norm': 3.793165683746338, 'learning_rate': 0.00014991338878011943, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.66it/s]  8%|▊         | 6/75 [00:00<00:02, 25.86it/s]                                              {'loss': 2.1851, 'grad_norm': 4.096089839935303, 'learning_rate': 0.00014780193260011777, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.86it/s]                                              {'loss': 2.373, 'grad_norm': 4.9529924392700195, 'learning_rate': 0.00014569047642011608, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.86it/s]                                              {'loss': 2.4597, 'grad_norm': 4.207663536071777, 'learning_rate': 0.0001435790202401144, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.86it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.32it/s]                                              {'loss': 2.2807, 'grad_norm': 3.0563106536865234, 'learning_rate': 0.0001414675640601127, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.32it/s]                                              {'loss': 2.2754, 'grad_norm': 3.8714425563812256, 'learning_rate': 0.00013935610788011103, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.32it/s]                                               {'loss': 2.2981, 'grad_norm': 3.76924467086792, 'learning_rate': 0.00013724465170010934, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.32it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.11it/s]                                               {'loss': 2.4245, 'grad_norm': 3.959962844848633, 'learning_rate': 0.00013513319552010768, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.11it/s]                                               {'loss': 2.2879, 'grad_norm': 4.789979457855225, 'learning_rate': 0.00013302173934010597, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.11it/s]                                               {'loss': 2.2552, 'grad_norm': 4.6601972579956055, 'learning_rate': 0.0001309102831601043, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.11it/s]                                               {'loss': 1.7928, 'grad_norm': 8.28811264038086, 'learning_rate': 0.00012879882698010262, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.11it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.61it/s]                                               {'loss': 2.2565, 'grad_norm': 3.5883538722991943, 'learning_rate': 0.00012668737080010094, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.61it/s]                                               {'loss': 2.2193, 'grad_norm': 3.7024364471435547, 'learning_rate': 0.00012457591462009925, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.61it/s]                                               {'loss': 2.1665, 'grad_norm': 3.7599706649780273, 'learning_rate': 0.00012246445844009757, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.61it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.09it/s]                                               {'loss': 2.2615, 'grad_norm': 5.155585765838623, 'learning_rate': 0.00012035300226009588, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.09it/s]                                               {'loss': 2.3695, 'grad_norm': 4.159276008605957, 'learning_rate': 0.00011824154608009421, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.09it/s]                                               {'loss': 2.0674, 'grad_norm': 3.5731799602508545, 'learning_rate': 0.00011613008990009251, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.09it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.46it/s]                                               {'loss': 2.0851, 'grad_norm': 3.581590414047241, 'learning_rate': 0.00011401863372009084, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.46it/s]                                               {'loss': 2.2484, 'grad_norm': 4.267848491668701, 'learning_rate': 0.00011190717754008915, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.46it/s]                                               {'loss': 2.3651, 'grad_norm': 4.294515132904053, 'learning_rate': 0.00010979572136008748, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 25.46it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.48it/s]                                               {'loss': 2.3489, 'grad_norm': 4.2763352394104, 'learning_rate': 0.0001076842651800858, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.48it/s]                                               {'loss': 2.3517, 'grad_norm': 3.7749650478363037, 'learning_rate': 0.00010557280900008411, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.48it/s]                                               {'loss': 2.1642, 'grad_norm': 4.5680131912231445, 'learning_rate': 0.00010346135282008243, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.48it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.36it/s]                                               {'loss': 2.2026, 'grad_norm': 5.808368682861328, 'learning_rate': 0.00010134989664008075, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.36it/s]                                               {'loss': 2.2238, 'grad_norm': 3.859753131866455, 'learning_rate': 9.923844046007907e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.36it/s]                                               {'loss': 1.7631, 'grad_norm': 9.0480375289917, 'learning_rate': 9.712698428007737e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.36it/s]                                               {'loss': 2.2827, 'grad_norm': 3.7922306060791016, 'learning_rate': 9.50155281000757e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.36it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.66it/s]                                               {'loss': 2.0621, 'grad_norm': 3.433645725250244, 'learning_rate': 9.290407192007401e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.66it/s]                                               {'loss': 2.4152, 'grad_norm': 4.19169807434082, 'learning_rate': 9.079261574007234e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.66it/s]                                               {'loss': 2.1832, 'grad_norm': 4.031910419464111, 'learning_rate': 8.868115956007067e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.66it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.17it/s]                                               {'loss': 2.0868, 'grad_norm': 3.5464704036712646, 'learning_rate': 8.656970338006897e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.17it/s]                                               {'loss': 2.197, 'grad_norm': 4.731474876403809, 'learning_rate': 8.445824720006728e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.17it/s]                                               {'loss': 2.0255, 'grad_norm': 4.060207843780518, 'learning_rate': 8.234679102006561e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.17it/s] 51%|█████     | 38/75 [00:01<00:01, 26.06it/s]                                               {'loss': 2.2853, 'grad_norm': 4.210155963897705, 'learning_rate': 8.023533484006393e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.06it/s]                                               {'loss': 2.2815, 'grad_norm': 5.928143501281738, 'learning_rate': 7.812387866006224e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.06it/s]                                               {'loss': 2.3274, 'grad_norm': 3.9189319610595703, 'learning_rate': 7.601242248006056e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 26.06it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.0837, 'grad_norm': 3.4503960609436035, 'learning_rate': 7.390096630005888e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.0715, 'grad_norm': 5.187880992889404, 'learning_rate': 7.17895101200572e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.1493, 'grad_norm': 4.388453960418701, 'learning_rate': 6.967805394005551e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.39it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.25it/s]                                               {'loss': 2.1657, 'grad_norm': 2.9226787090301514, 'learning_rate': 6.756659776005384e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.25it/s]                                               {'loss': 2.1377, 'grad_norm': 11.293708801269531, 'learning_rate': 6.545514158005216e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.25it/s]                                               {'loss': 2.0762, 'grad_norm': 3.745699405670166, 'learning_rate': 6.334368540005047e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.25it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.20it/s]                                               {'loss': 2.1338, 'grad_norm': 4.011258602142334, 'learning_rate': 6.123222922004878e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.20it/s]                                               {'loss': 2.1882, 'grad_norm': 3.9544105529785156, 'learning_rate': 5.9120773040047105e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.20it/s]                                               {'loss': 2.2202, 'grad_norm': 3.8411331176757812, 'learning_rate': 5.700931686004542e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.20it/s] 67%|██████▋   | 50/75 [00:01<00:01, 24.83it/s]                                               {'loss': 2.1651, 'grad_norm': 3.688356637954712, 'learning_rate': 5.489786068004374e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:01, 24.83it/s]                                               {'loss': 2.0889, 'grad_norm': 4.857814788818359, 'learning_rate': 5.2786404500042056e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 24.83it/s]                                               {'loss': 2.0422, 'grad_norm': 4.560000896453857, 'learning_rate': 5.067494832004038e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.83it/s] 71%|███████   | 53/75 [00:02<00:00, 24.40it/s]                                               {'loss': 2.1067, 'grad_norm': 3.8893561363220215, 'learning_rate': 4.8563492140038685e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.40it/s]                                               {'loss': 2.4318, 'grad_norm': 4.457427501678467, 'learning_rate': 4.6452035960037006e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.40it/s]                                               {'loss': 2.139, 'grad_norm': 3.9294660091400146, 'learning_rate': 4.4340579780035334e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.40it/s] 75%|███████▍  | 56/75 [00:02<00:00, 24.18it/s]                                               {'loss': 2.4365, 'grad_norm': 4.006421089172363, 'learning_rate': 4.222912360003364e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.18it/s]                                               {'loss': 2.0171, 'grad_norm': 4.975558280944824, 'learning_rate': 4.011766742003196e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.18it/s]                                               {'loss': 2.1438, 'grad_norm': 3.8641679286956787, 'learning_rate': 3.800621124003028e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.18it/s] 79%|███████▊  | 59/75 [00:02<00:00, 24.51it/s]                                               {'loss': 2.1737, 'grad_norm': 3.744210958480835, 'learning_rate': 3.58947550600286e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.51it/s]                                               {'loss': 2.3334, 'grad_norm': 15.10814094543457, 'learning_rate': 3.378329888002692e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.51it/s]                                               {'loss': 2.3301, 'grad_norm': 5.239861011505127, 'learning_rate': 3.1671842700025235e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.51it/s] 83%|████████▎ | 62/75 [00:02<00:00, 23.30it/s]                                               {'loss': 2.0358, 'grad_norm': 3.6471245288848877, 'learning_rate': 2.9560386520023553e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 23.30it/s]                                               {'loss': 2.1186, 'grad_norm': 3.792144536972046, 'learning_rate': 2.744893034002187e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 23.30it/s]                                               {'loss': 2.063, 'grad_norm': 4.661687850952148, 'learning_rate': 2.533747416002019e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 23.30it/s] 87%|████████▋ | 65/75 [00:02<00:00, 23.20it/s]                                               {'loss': 2.5097, 'grad_norm': 5.240769863128662, 'learning_rate': 2.3226017980018503e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 23.20it/s]                                               {'loss': 2.2126, 'grad_norm': 4.360620021820068, 'learning_rate': 2.111456180001682e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 23.20it/s]                                               {'loss': 2.2248, 'grad_norm': 3.855146646499634, 'learning_rate': 1.900310562001514e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 23.20it/s] 91%|█████████ | 68/75 [00:02<00:00, 23.37it/s]                                               {'loss': 2.0175, 'grad_norm': 3.4378185272216797, 'learning_rate': 1.689164944001346e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 23.37it/s]                                               {'loss': 2.2205, 'grad_norm': 3.3825125694274902, 'learning_rate': 1.4780193260011776e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 23.37it/s]                                               {'loss': 2.1453, 'grad_norm': 3.743511199951172, 'learning_rate': 1.2668737080010094e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 23.37it/s] 95%|█████████▍| 71/75 [00:02<00:00, 23.67it/s]                                               {'loss': 1.99, 'grad_norm': 3.856595993041992, 'learning_rate': 1.055728090000841e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 23.67it/s]                                               {'loss': 2.1829, 'grad_norm': 3.2108964920043945, 'learning_rate': 8.44582472000673e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 23.67it/s]                                               {'loss': 2.0266, 'grad_norm': 3.287580966949463, 'learning_rate': 6.334368540005047e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 23.67it/s] 99%|█████████▊| 74/75 [00:02<00:00, 24.17it/s]                                               {'loss': 2.1127, 'grad_norm': 4.784886360168457, 'learning_rate': 4.222912360003365e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.17it/s]                                               {'loss': 1.9173, 'grad_norm': 8.465147018432617, 'learning_rate': 2.1114561800016825e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.17it/s]                                               {'train_runtime': 3.1167, 'train_samples_per_second': 362.561, 'train_steps_per_second': 24.064, 'train_loss': 2.2014965915679934, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.17it/s]100%|██████████| 75/75 [00:03<00:00, 24.07it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.5899, 'grad_norm': 4.97465705871582, 'learning_rate': 0.00015835921350012617, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 20.90it/s]                                              {'loss': 2.5077, 'grad_norm': 5.223952293395996, 'learning_rate': 0.00015624775732012448, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 22.46it/s]  4%|▍         | 3/75 [00:00<00:03, 23.29it/s]                                              {'loss': 2.4551, 'grad_norm': 4.6207380294799805, 'learning_rate': 0.00015413630114012282, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.29it/s]                                              {'loss': 2.3427, 'grad_norm': 4.9941725730896, 'learning_rate': 0.0001520248449601211, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 23.29it/s]                                              {'loss': 2.4553, 'grad_norm': 4.079967021942139, 'learning_rate': 0.00014991338878011943, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 23.29it/s]  8%|▊         | 6/75 [00:00<00:02, 23.74it/s]                                              {'loss': 2.1551, 'grad_norm': 4.008955478668213, 'learning_rate': 0.00014780193260011777, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.74it/s]                                              {'loss': 2.4348, 'grad_norm': 3.6927366256713867, 'learning_rate': 0.00014569047642011608, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.74it/s]                                              {'loss': 2.2351, 'grad_norm': 3.4652504920959473, 'learning_rate': 0.0001435790202401144, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.74it/s] 12%|█▏        | 9/75 [00:00<00:02, 22.19it/s]                                              {'loss': 2.6156, 'grad_norm': 5.302285194396973, 'learning_rate': 0.0001414675640601127, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 22.19it/s]                                              {'loss': 2.2839, 'grad_norm': 5.044027805328369, 'learning_rate': 0.00013935610788011103, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 22.19it/s]                                               {'loss': 2.3594, 'grad_norm': 3.750263214111328, 'learning_rate': 0.00013724465170010934, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 22.19it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.19it/s]                                               {'loss': 2.112, 'grad_norm': 4.058900833129883, 'learning_rate': 0.00013513319552010768, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.19it/s]                                               {'loss': 2.3777, 'grad_norm': 5.179887294769287, 'learning_rate': 0.00013302173934010597, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.19it/s]                                               {'loss': 2.3409, 'grad_norm': 4.662224292755127, 'learning_rate': 0.0001309102831601043, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.19it/s] 20%|██        | 15/75 [00:00<00:02, 23.89it/s]                                               {'loss': 2.5355, 'grad_norm': 9.294316291809082, 'learning_rate': 0.00012879882698010262, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.89it/s]                                               {'loss': 2.4801, 'grad_norm': 4.456697463989258, 'learning_rate': 0.00012668737080010094, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 23.89it/s]                                               {'loss': 2.2933, 'grad_norm': 5.126972675323486, 'learning_rate': 0.00012457591462009925, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 23.89it/s] 24%|██▍       | 18/75 [00:00<00:02, 24.01it/s]                                               {'loss': 2.242, 'grad_norm': 4.8419389724731445, 'learning_rate': 0.00012246445844009757, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.01it/s]                                               {'loss': 2.5849, 'grad_norm': 4.817992210388184, 'learning_rate': 0.00012035300226009588, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.01it/s]                                               {'loss': 2.3681, 'grad_norm': 3.2443182468414307, 'learning_rate': 0.00011824154608009421, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.01it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.63it/s]                                               {'loss': 2.4331, 'grad_norm': 5.394421100616455, 'learning_rate': 0.00011613008990009251, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.63it/s]                                               {'loss': 2.3098, 'grad_norm': 4.165095329284668, 'learning_rate': 0.00011401863372009084, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.63it/s]                                               {'loss': 2.3072, 'grad_norm': 3.584113121032715, 'learning_rate': 0.00011190717754008915, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.63it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.73it/s]                                               {'loss': 2.1216, 'grad_norm': 4.32472038269043, 'learning_rate': 0.00010979572136008748, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.73it/s]                                               {'loss': 2.2839, 'grad_norm': 4.47392463684082, 'learning_rate': 0.0001076842651800858, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.73it/s]                                               {'loss': 2.3807, 'grad_norm': 4.391677379608154, 'learning_rate': 0.00010557280900008411, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.73it/s] 36%|███▌      | 27/75 [00:01<00:02, 23.13it/s]                                               {'loss': 2.2117, 'grad_norm': 4.54506778717041, 'learning_rate': 0.00010346135282008243, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 23.13it/s]                                               {'loss': 2.1212, 'grad_norm': 4.114720344543457, 'learning_rate': 0.00010134989664008075, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 23.13it/s]                                               {'loss': 2.2971, 'grad_norm': 5.283895492553711, 'learning_rate': 9.923844046007907e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 23.13it/s]                                               {'loss': 2.1854, 'grad_norm': 7.6602277755737305, 'learning_rate': 9.712698428007737e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.13it/s] 41%|████▏     | 31/75 [00:01<00:01, 24.76it/s]                                               {'loss': 2.2602, 'grad_norm': 5.17841100692749, 'learning_rate': 9.50155281000757e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.76it/s]                                               {'loss': 2.4266, 'grad_norm': 4.465770721435547, 'learning_rate': 9.290407192007401e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.76it/s]                                               {'loss': 2.34, 'grad_norm': 4.328889846801758, 'learning_rate': 9.079261574007234e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.76it/s] 45%|████▌     | 34/75 [00:01<00:01, 23.63it/s]                                               {'loss': 2.3977, 'grad_norm': 4.800994873046875, 'learning_rate': 8.868115956007067e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 23.63it/s]                                               {'loss': 2.2935, 'grad_norm': 4.1261982917785645, 'learning_rate': 8.656970338006897e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 23.63it/s]                                               {'loss': 2.053, 'grad_norm': 3.8081657886505127, 'learning_rate': 8.445824720006728e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 23.63it/s] 49%|████▉     | 37/75 [00:01<00:01, 23.78it/s]                                               {'loss': 2.2784, 'grad_norm': 3.6814892292022705, 'learning_rate': 8.234679102006561e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 23.78it/s]                                               {'loss': 2.204, 'grad_norm': 3.9001355171203613, 'learning_rate': 8.023533484006393e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 23.78it/s]                                               {'loss': 2.2525, 'grad_norm': 3.6928815841674805, 'learning_rate': 7.812387866006224e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.78it/s] 53%|█████▎    | 40/75 [00:01<00:01, 24.22it/s]                                               {'loss': 2.2163, 'grad_norm': 4.282623291015625, 'learning_rate': 7.601242248006056e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.22it/s]                                               {'loss': 2.3523, 'grad_norm': 6.588790416717529, 'learning_rate': 7.390096630005888e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.22it/s]                                               {'loss': 2.1645, 'grad_norm': 3.5939064025878906, 'learning_rate': 7.17895101200572e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.22it/s] 57%|█████▋    | 43/75 [00:01<00:01, 22.23it/s]                                               {'loss': 2.2467, 'grad_norm': 4.102111339569092, 'learning_rate': 6.967805394005551e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 22.23it/s]                                               {'loss': 2.4659, 'grad_norm': 5.004209518432617, 'learning_rate': 6.756659776005384e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 22.23it/s]                                               {'loss': 2.1845, 'grad_norm': 9.516923904418945, 'learning_rate': 6.545514158005216e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 22.23it/s]                                               {'loss': 2.2838, 'grad_norm': 3.8280155658721924, 'learning_rate': 6.334368540005047e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 22.23it/s] 63%|██████▎   | 47/75 [00:01<00:01, 24.87it/s]                                               {'loss': 2.3626, 'grad_norm': 4.765772342681885, 'learning_rate': 6.123222922004878e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 24.87it/s]                                               {'loss': 2.182, 'grad_norm': 4.172707557678223, 'learning_rate': 5.9120773040047105e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 24.87it/s]                                               {'loss': 2.263, 'grad_norm': 3.7673277854919434, 'learning_rate': 5.700931686004542e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 24.87it/s] 67%|██████▋   | 50/75 [00:02<00:00, 25.09it/s]                                               {'loss': 1.9692, 'grad_norm': 3.346337080001831, 'learning_rate': 5.489786068004374e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.09it/s]                                               {'loss': 2.1317, 'grad_norm': 4.6683125495910645, 'learning_rate': 5.2786404500042056e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.09it/s]                                               {'loss': 2.3139, 'grad_norm': 3.8651375770568848, 'learning_rate': 5.067494832004038e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.09it/s] 71%|███████   | 53/75 [00:02<00:00, 24.20it/s]                                               {'loss': 2.5006, 'grad_norm': 4.399820804595947, 'learning_rate': 4.8563492140038685e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.20it/s]                                               {'loss': 2.3509, 'grad_norm': 4.884677886962891, 'learning_rate': 4.6452035960037006e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.20it/s]                                               {'loss': 2.2773, 'grad_norm': 4.092336654663086, 'learning_rate': 4.4340579780035334e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.20it/s] 75%|███████▍  | 56/75 [00:02<00:00, 23.68it/s]                                               {'loss': 2.1775, 'grad_norm': 4.215599060058594, 'learning_rate': 4.222912360003364e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 23.68it/s]                                               {'loss': 2.364, 'grad_norm': 4.618218898773193, 'learning_rate': 4.011766742003196e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 23.68it/s]                                               {'loss': 2.343, 'grad_norm': 5.768156051635742, 'learning_rate': 3.800621124003028e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 23.68it/s] 79%|███████▊  | 59/75 [00:02<00:00, 24.12it/s]                                               {'loss': 2.098, 'grad_norm': 5.543659687042236, 'learning_rate': 3.58947550600286e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.12it/s]                                               {'loss': 2.2561, 'grad_norm': 16.370708465576172, 'learning_rate': 3.378329888002692e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.12it/s]                                               {'loss': 2.1545, 'grad_norm': 3.599012613296509, 'learning_rate': 3.1671842700025235e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.12it/s]                                               {'loss': 2.3395, 'grad_norm': 5.294536113739014, 'learning_rate': 2.9560386520023553e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 24.12it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.02it/s]                                               {'loss': 2.1914, 'grad_norm': 4.271160125732422, 'learning_rate': 2.744893034002187e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.02it/s]                                               {'loss': 1.9877, 'grad_norm': 3.3079583644866943, 'learning_rate': 2.533747416002019e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.02it/s]                                               {'loss': 2.4049, 'grad_norm': 4.218274116516113, 'learning_rate': 2.3226017980018503e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.02it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.80it/s]                                               {'loss': 2.283, 'grad_norm': 3.769024610519409, 'learning_rate': 2.111456180001682e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.80it/s]                                               {'loss': 2.1876, 'grad_norm': 3.998723030090332, 'learning_rate': 1.900310562001514e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.80it/s]                                               {'loss': 2.2847, 'grad_norm': 4.493223190307617, 'learning_rate': 1.689164944001346e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.80it/s] 92%|█████████▏| 69/75 [00:02<00:00, 24.79it/s]                                               {'loss': 2.16, 'grad_norm': 4.239348888397217, 'learning_rate': 1.4780193260011776e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.79it/s]                                               {'loss': 2.1754, 'grad_norm': 4.58501672744751, 'learning_rate': 1.2668737080010094e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.79it/s]                                               {'loss': 2.3322, 'grad_norm': 5.613507270812988, 'learning_rate': 1.055728090000841e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.79it/s] 96%|█████████▌| 72/75 [00:02<00:00, 24.27it/s]                                               {'loss': 2.2929, 'grad_norm': 4.091203212738037, 'learning_rate': 8.44582472000673e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.27it/s]                                               {'loss': 2.3595, 'grad_norm': 4.272421836853027, 'learning_rate': 6.334368540005047e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 24.27it/s]                                               {'loss': 2.0527, 'grad_norm': 3.86710786819458, 'learning_rate': 4.222912360003365e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.27it/s]                                               {'loss': 2.3249, 'grad_norm': 18.928680419921875, 'learning_rate': 2.1114561800016825e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.27it/s]                                               {'train_runtime': 3.1972, 'train_samples_per_second': 353.433, 'train_steps_per_second': 23.458, 'train_loss': 2.292819973627726, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.27it/s]100%|██████████| 75/75 [00:03<00:00, 23.46it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3057, 'grad_norm': 3.9299302101135254, 'learning_rate': 0.00015835921350012617, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.69it/s]                                              {'loss': 2.5429, 'grad_norm': 4.229705810546875, 'learning_rate': 0.00015624775732012448, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 25.10it/s]  4%|▍         | 3/75 [00:00<00:02, 25.50it/s]                                              {'loss': 2.2362, 'grad_norm': 4.356943607330322, 'learning_rate': 0.00015413630114012282, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.50it/s]                                              {'loss': 2.1306, 'grad_norm': 4.7251482009887695, 'learning_rate': 0.0001520248449601211, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.50it/s]                                              {'loss': 2.2478, 'grad_norm': 3.858704090118408, 'learning_rate': 0.00014991338878011943, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.50it/s]  8%|▊         | 6/75 [00:00<00:02, 25.96it/s]                                              {'loss': 2.5269, 'grad_norm': 4.286521911621094, 'learning_rate': 0.00014780193260011777, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.96it/s]                                              {'loss': 2.3861, 'grad_norm': 3.881486177444458, 'learning_rate': 0.00014569047642011608, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.96it/s]                                              {'loss': 2.1646, 'grad_norm': 4.024041652679443, 'learning_rate': 0.0001435790202401144, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.96it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.01it/s]                                              {'loss': 2.2559, 'grad_norm': 3.8678202629089355, 'learning_rate': 0.0001414675640601127, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.01it/s]                                              {'loss': 2.1821, 'grad_norm': 4.6416015625, 'learning_rate': 0.00013935610788011103, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.01it/s]                                               {'loss': 2.3615, 'grad_norm': 4.4488749504089355, 'learning_rate': 0.00013724465170010934, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.01it/s] 16%|█▌        | 12/75 [00:00<00:03, 20.59it/s]                                               {'loss': 2.3073, 'grad_norm': 3.9771530628204346, 'learning_rate': 0.00013513319552010768, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:03, 20.59it/s]                                               {'loss': 2.3777, 'grad_norm': 3.8039798736572266, 'learning_rate': 0.00013302173934010597, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:03, 20.59it/s]                                               {'loss': 2.3554, 'grad_norm': 4.986537933349609, 'learning_rate': 0.0001309102831601043, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 20.59it/s] 20%|██        | 15/75 [00:00<00:02, 23.10it/s]                                               {'loss': 2.5932, 'grad_norm': 12.316191673278809, 'learning_rate': 0.00012879882698010262, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.10it/s]                                               {'loss': 2.3071, 'grad_norm': 4.128686904907227, 'learning_rate': 0.00012668737080010094, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 23.10it/s]                                               {'loss': 2.3896, 'grad_norm': 4.676593780517578, 'learning_rate': 0.00012457591462009925, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 23.10it/s] 24%|██▍       | 18/75 [00:00<00:02, 23.86it/s]                                               {'loss': 2.3332, 'grad_norm': 4.830021858215332, 'learning_rate': 0.00012246445844009757, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 23.86it/s]                                               {'loss': 2.2503, 'grad_norm': 3.4759163856506348, 'learning_rate': 0.00012035300226009588, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 23.86it/s]                                               {'loss': 2.3762, 'grad_norm': 3.9119138717651367, 'learning_rate': 0.00011824154608009421, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 23.86it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.61it/s]                                               {'loss': 2.3725, 'grad_norm': 3.8534982204437256, 'learning_rate': 0.00011613008990009251, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.61it/s]                                               {'loss': 2.2621, 'grad_norm': 4.763912677764893, 'learning_rate': 0.00011401863372009084, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.61it/s]                                               {'loss': 2.3118, 'grad_norm': 4.103668212890625, 'learning_rate': 0.00011190717754008915, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.61it/s] 32%|███▏      | 24/75 [00:01<00:02, 24.11it/s]                                               {'loss': 2.1751, 'grad_norm': 4.213119983673096, 'learning_rate': 0.00010979572136008748, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 24.11it/s]                                               {'loss': 2.2906, 'grad_norm': 4.19879150390625, 'learning_rate': 0.0001076842651800858, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.11it/s]                                               {'loss': 2.1344, 'grad_norm': 4.339205265045166, 'learning_rate': 0.00010557280900008411, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.11it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.00it/s]                                               {'loss': 2.1354, 'grad_norm': 4.251603126525879, 'learning_rate': 0.00010346135282008243, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.00it/s]                                               {'loss': 2.1693, 'grad_norm': 3.2754223346710205, 'learning_rate': 0.00010134989664008075, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.00it/s]                                               {'loss': 2.0712, 'grad_norm': 3.955338478088379, 'learning_rate': 9.923844046007907e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.00it/s]                                               {'loss': 2.8519, 'grad_norm': 13.325498580932617, 'learning_rate': 9.712698428007737e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.00it/s] 41%|████▏     | 31/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.0936, 'grad_norm': 3.7558393478393555, 'learning_rate': 9.50155281000757e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.358, 'grad_norm': 4.054775714874268, 'learning_rate': 9.290407192007401e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.3837, 'grad_norm': 3.9328033924102783, 'learning_rate': 9.079261574007234e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.31it/s] 45%|████▌     | 34/75 [00:01<00:01, 25.27it/s]                                               {'loss': 2.2826, 'grad_norm': 3.772498846054077, 'learning_rate': 8.868115956007067e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.27it/s]                                               {'loss': 2.006, 'grad_norm': 3.0561084747314453, 'learning_rate': 8.656970338006897e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.27it/s]                                               {'loss': 2.1799, 'grad_norm': 4.427511215209961, 'learning_rate': 8.445824720006728e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.27it/s] 49%|████▉     | 37/75 [00:01<00:01, 24.89it/s]                                               {'loss': 2.2584, 'grad_norm': 3.4530279636383057, 'learning_rate': 8.234679102006561e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.89it/s]                                               {'loss': 2.312, 'grad_norm': 4.141373634338379, 'learning_rate': 8.023533484006393e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.89it/s]                                               {'loss': 2.1875, 'grad_norm': 3.635068416595459, 'learning_rate': 7.812387866006224e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.89it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.05it/s]                                               {'loss': 2.1717, 'grad_norm': 3.6973788738250732, 'learning_rate': 7.601242248006056e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.05it/s]                                               {'loss': 2.1497, 'grad_norm': 3.3434457778930664, 'learning_rate': 7.390096630005888e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.05it/s]                                               {'loss': 2.2685, 'grad_norm': 3.548585891723633, 'learning_rate': 7.17895101200572e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.05it/s] 57%|█████▋    | 43/75 [00:01<00:01, 25.24it/s]                                               {'loss': 1.9819, 'grad_norm': 4.433736801147461, 'learning_rate': 6.967805394005551e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.1993, 'grad_norm': 3.525949001312256, 'learning_rate': 6.756659776005384e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.833, 'grad_norm': 13.207629203796387, 'learning_rate': 6.545514158005216e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.24it/s] 61%|██████▏   | 46/75 [00:01<00:01, 26.32it/s]                                               {'loss': 2.2695, 'grad_norm': 3.3019847869873047, 'learning_rate': 6.334368540005047e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.32it/s]                                               {'loss': 2.4229, 'grad_norm': 4.585656642913818, 'learning_rate': 6.123222922004878e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.32it/s]                                               {'loss': 2.2584, 'grad_norm': 4.950525760650635, 'learning_rate': 5.9120773040047105e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.32it/s] 65%|██████▌   | 49/75 [00:01<00:01, 25.85it/s]                                               {'loss': 2.1303, 'grad_norm': 3.4371280670166016, 'learning_rate': 5.700931686004542e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.85it/s]                                               {'loss': 2.0359, 'grad_norm': 3.0807721614837646, 'learning_rate': 5.489786068004374e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.85it/s]                                               {'loss': 2.156, 'grad_norm': 3.4175589084625244, 'learning_rate': 5.2786404500042056e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.85it/s] 69%|██████▉   | 52/75 [00:02<00:00, 25.56it/s]                                               {'loss': 2.2245, 'grad_norm': 4.090044975280762, 'learning_rate': 5.067494832004038e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.56it/s]                                               {'loss': 1.9552, 'grad_norm': 4.299299240112305, 'learning_rate': 4.8563492140038685e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.56it/s]                                               {'loss': 2.4954, 'grad_norm': 5.10858154296875, 'learning_rate': 4.6452035960037006e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.56it/s] 73%|███████▎  | 55/75 [00:02<00:00, 25.04it/s]                                               {'loss': 2.048, 'grad_norm': 3.651599645614624, 'learning_rate': 4.4340579780035334e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.04it/s]                                               {'loss': 2.1796, 'grad_norm': 3.6022045612335205, 'learning_rate': 4.222912360003364e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.04it/s]                                               {'loss': 2.1088, 'grad_norm': 3.9294660091400146, 'learning_rate': 4.011766742003196e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.04it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.12it/s]                                               {'loss': 2.327, 'grad_norm': 3.017336845397949, 'learning_rate': 3.800621124003028e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.12it/s]                                               {'loss': 2.1038, 'grad_norm': 3.6271369457244873, 'learning_rate': 3.58947550600286e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.12it/s]                                               {'loss': 2.0779, 'grad_norm': 9.752182006835938, 'learning_rate': 3.378329888002692e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.12it/s] 81%|████████▏ | 61/75 [00:02<00:00, 25.28it/s]                                               {'loss': 1.9953, 'grad_norm': 3.5846641063690186, 'learning_rate': 3.1671842700025235e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.28it/s]                                               {'loss': 2.1285, 'grad_norm': 4.189591884613037, 'learning_rate': 2.9560386520023553e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.28it/s]                                               {'loss': 2.2822, 'grad_norm': 4.22894811630249, 'learning_rate': 2.744893034002187e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.28it/s] 85%|████████▌ | 64/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.0635, 'grad_norm': 3.059849500656128, 'learning_rate': 2.533747416002019e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.2175, 'grad_norm': 4.093484401702881, 'learning_rate': 2.3226017980018503e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.1484, 'grad_norm': 3.7661283016204834, 'learning_rate': 2.111456180001682e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.99it/s] 89%|████████▉ | 67/75 [00:02<00:00, 24.97it/s]                                               {'loss': 2.243, 'grad_norm': 4.7010908126831055, 'learning_rate': 1.900310562001514e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.97it/s]                                               {'loss': 2.2603, 'grad_norm': 4.043979167938232, 'learning_rate': 1.689164944001346e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.97it/s]                                               {'loss': 2.3914, 'grad_norm': 4.342722415924072, 'learning_rate': 1.4780193260011776e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.97it/s] 93%|█████████▎| 70/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.0236, 'grad_norm': 2.94134521484375, 'learning_rate': 1.2668737080010094e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.0423, 'grad_norm': 3.3396222591400146, 'learning_rate': 1.055728090000841e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.2631, 'grad_norm': 3.8602371215820312, 'learning_rate': 8.44582472000673e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.63it/s] 97%|█████████▋| 73/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.3743, 'grad_norm': 4.221707820892334, 'learning_rate': 6.334368540005047e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.2132, 'grad_norm': 3.7496724128723145, 'learning_rate': 4.222912360003365e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.63it/s]                                               {'loss': 1.835, 'grad_norm': 10.644261360168457, 'learning_rate': 2.1114561800016825e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.63it/s]                                               {'train_runtime': 3.1386, 'train_samples_per_second': 360.038, 'train_steps_per_second': 23.896, 'train_loss': 2.2446015055974327, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.63it/s]100%|██████████| 75/75 [00:03<00:00, 23.90it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:47
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3139, 'grad_norm': 4.721354961395264, 'learning_rate': 0.00015835921350012617, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 19.19it/s]  3%|▎         | 2/75 [00:00<00:03, 18.83it/s]                                              {'loss': 2.4529, 'grad_norm': 3.8367111682891846, 'learning_rate': 0.00015624775732012448, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 18.83it/s]                                              {'loss': 2.5263, 'grad_norm': 4.834456443786621, 'learning_rate': 0.00015413630114012282, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 18.83it/s]                                              {'loss': 2.3767, 'grad_norm': 4.436887741088867, 'learning_rate': 0.0001520248449601211, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 18.83it/s]  7%|▋         | 5/75 [00:00<00:03, 21.62it/s]                                              {'loss': 2.501, 'grad_norm': 3.9114458560943604, 'learning_rate': 0.00014991338878011943, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 21.62it/s]                                              {'loss': 2.3204, 'grad_norm': 4.83024263381958, 'learning_rate': 0.00014780193260011777, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 21.62it/s]                                              {'loss': 2.3962, 'grad_norm': 4.07543420791626, 'learning_rate': 0.00014569047642011608, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 21.62it/s] 11%|█         | 8/75 [00:00<00:02, 23.13it/s]                                              {'loss': 2.2884, 'grad_norm': 4.4502716064453125, 'learning_rate': 0.0001435790202401144, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.13it/s]                                              {'loss': 2.3953, 'grad_norm': 4.6269612312316895, 'learning_rate': 0.0001414675640601127, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.13it/s]                                              {'loss': 2.4543, 'grad_norm': 3.8214168548583984, 'learning_rate': 0.00013935610788011103, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.13it/s] 15%|█▍        | 11/75 [00:00<00:02, 23.94it/s]                                               {'loss': 2.5687, 'grad_norm': 4.146746635437012, 'learning_rate': 0.00013724465170010934, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.94it/s]                                               {'loss': 2.2161, 'grad_norm': 4.597027778625488, 'learning_rate': 0.00013513319552010768, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.94it/s]                                               {'loss': 2.4952, 'grad_norm': 4.966442108154297, 'learning_rate': 0.00013302173934010597, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.94it/s] 19%|█▊        | 14/75 [00:00<00:02, 23.84it/s]                                               {'loss': 2.3202, 'grad_norm': 3.991580009460449, 'learning_rate': 0.0001309102831601043, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.84it/s]                                               {'loss': 2.6845, 'grad_norm': 10.850236892700195, 'learning_rate': 0.00012879882698010262, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.84it/s]                                               {'loss': 2.3508, 'grad_norm': 4.270898818969727, 'learning_rate': 0.00012668737080010094, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 23.84it/s] 23%|██▎       | 17/75 [00:00<00:02, 24.25it/s]                                               {'loss': 2.397, 'grad_norm': 4.799683570861816, 'learning_rate': 0.00012457591462009925, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.25it/s]                                               {'loss': 2.3039, 'grad_norm': 4.037668228149414, 'learning_rate': 0.00012246445844009757, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.25it/s]                                               {'loss': 2.1721, 'grad_norm': 3.8426783084869385, 'learning_rate': 0.00012035300226009588, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.25it/s] 27%|██▋       | 20/75 [00:00<00:02, 24.21it/s]                                               {'loss': 2.5313, 'grad_norm': 5.349149227142334, 'learning_rate': 0.00011824154608009421, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.21it/s]                                               {'loss': 2.2349, 'grad_norm': 3.8754823207855225, 'learning_rate': 0.00011613008990009251, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.21it/s]                                               {'loss': 2.2531, 'grad_norm': 4.630373001098633, 'learning_rate': 0.00011401863372009084, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.21it/s] 31%|███       | 23/75 [00:00<00:02, 23.60it/s]                                               {'loss': 2.4113, 'grad_norm': 3.666543483734131, 'learning_rate': 0.00011190717754008915, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 23.60it/s]                                               {'loss': 2.2549, 'grad_norm': 3.9231796264648438, 'learning_rate': 0.00010979572136008748, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 23.60it/s]                                               {'loss': 2.4715, 'grad_norm': 4.551848411560059, 'learning_rate': 0.0001076842651800858, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.60it/s] 35%|███▍      | 26/75 [00:01<00:02, 24.01it/s]                                               {'loss': 2.1302, 'grad_norm': 3.581573247909546, 'learning_rate': 0.00010557280900008411, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.01it/s]                                               {'loss': 2.2743, 'grad_norm': 4.725656509399414, 'learning_rate': 0.00010346135282008243, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.01it/s]                                               {'loss': 2.2093, 'grad_norm': 4.956742286682129, 'learning_rate': 0.00010134989664008075, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.01it/s] 39%|███▊      | 29/75 [00:01<00:01, 24.01it/s]                                               {'loss': 2.4101, 'grad_norm': 4.068155288696289, 'learning_rate': 9.923844046007907e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.01it/s]                                               {'loss': 2.5349, 'grad_norm': 11.461971282958984, 'learning_rate': 9.712698428007737e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.01it/s]                                               {'loss': 2.0894, 'grad_norm': 3.887303113937378, 'learning_rate': 9.50155281000757e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.01it/s] 43%|████▎     | 32/75 [00:01<00:01, 25.26it/s]                                               {'loss': 2.2197, 'grad_norm': 4.4007158279418945, 'learning_rate': 9.290407192007401e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.26it/s]                                               {'loss': 2.4056, 'grad_norm': 3.6445565223693848, 'learning_rate': 9.079261574007234e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.26it/s]                                               {'loss': 2.2577, 'grad_norm': 6.588807106018066, 'learning_rate': 8.868115956007067e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.26it/s] 47%|████▋     | 35/75 [00:01<00:01, 24.95it/s]                                               {'loss': 2.3666, 'grad_norm': 4.694154262542725, 'learning_rate': 8.656970338006897e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.95it/s]                                               {'loss': 2.5323, 'grad_norm': 4.911888599395752, 'learning_rate': 8.445824720006728e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.95it/s]                                               {'loss': 2.1502, 'grad_norm': 3.568570613861084, 'learning_rate': 8.234679102006561e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.95it/s] 51%|█████     | 38/75 [00:01<00:01, 25.06it/s]                                               {'loss': 2.1288, 'grad_norm': 3.0415139198303223, 'learning_rate': 8.023533484006393e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.06it/s]                                               {'loss': 2.1838, 'grad_norm': 3.981782913208008, 'learning_rate': 7.812387866006224e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.06it/s]                                               {'loss': 2.2507, 'grad_norm': 4.210465431213379, 'learning_rate': 7.601242248006056e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.06it/s] 55%|█████▍    | 41/75 [00:01<00:01, 24.97it/s]                                               {'loss': 2.3054, 'grad_norm': 3.9841108322143555, 'learning_rate': 7.390096630005888e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.97it/s]                                               {'loss': 2.4602, 'grad_norm': 3.7378041744232178, 'learning_rate': 7.17895101200572e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.97it/s]                                               {'loss': 2.1072, 'grad_norm': 4.409579753875732, 'learning_rate': 6.967805394005551e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.97it/s] 59%|█████▊    | 44/75 [00:01<00:01, 24.01it/s]                                               {'loss': 2.4205, 'grad_norm': 5.798543453216553, 'learning_rate': 6.756659776005384e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.01it/s]                                               {'loss': 2.3658, 'grad_norm': 16.057052612304688, 'learning_rate': 6.545514158005216e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.01it/s]                                               {'loss': 2.2633, 'grad_norm': 3.5128490924835205, 'learning_rate': 6.334368540005047e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.01it/s] 63%|██████▎   | 47/75 [00:01<00:01, 24.31it/s]                                               {'loss': 2.2627, 'grad_norm': 4.025907039642334, 'learning_rate': 6.123222922004878e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 24.31it/s]                                               {'loss': 2.1452, 'grad_norm': 3.867032766342163, 'learning_rate': 5.9120773040047105e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 24.31it/s]                                               {'loss': 2.3059, 'grad_norm': 4.961822032928467, 'learning_rate': 5.700931686004542e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 24.31it/s] 67%|██████▋   | 50/75 [00:02<00:01, 23.49it/s]                                               {'loss': 2.3617, 'grad_norm': 5.353472709655762, 'learning_rate': 5.489786068004374e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 23.49it/s]                                               {'loss': 2.1899, 'grad_norm': 4.5392279624938965, 'learning_rate': 5.2786404500042056e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 23.49it/s]                                               {'loss': 2.4904, 'grad_norm': 4.622278213500977, 'learning_rate': 5.067494832004038e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 23.49it/s] 71%|███████   | 53/75 [00:02<00:00, 23.35it/s]                                               {'loss': 2.1213, 'grad_norm': 3.3746211528778076, 'learning_rate': 4.8563492140038685e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 23.35it/s]                                               {'loss': 2.2735, 'grad_norm': 4.670089244842529, 'learning_rate': 4.6452035960037006e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 23.35it/s]                                               {'loss': 2.5046, 'grad_norm': 4.070931434631348, 'learning_rate': 4.4340579780035334e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 23.35it/s] 75%|███████▍  | 56/75 [00:02<00:00, 23.68it/s]                                               {'loss': 2.2698, 'grad_norm': 4.547240734100342, 'learning_rate': 4.222912360003364e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 23.68it/s]                                               {'loss': 2.1637, 'grad_norm': 4.277693271636963, 'learning_rate': 4.011766742003196e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 23.68it/s]                                               {'loss': 2.1523, 'grad_norm': 3.7386138439178467, 'learning_rate': 3.800621124003028e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 23.68it/s] 79%|███████▊  | 59/75 [00:02<00:00, 23.53it/s]                                               {'loss': 2.3152, 'grad_norm': 4.938338756561279, 'learning_rate': 3.58947550600286e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 23.53it/s]                                               {'loss': 1.7806, 'grad_norm': 12.233524322509766, 'learning_rate': 3.378329888002692e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 23.53it/s]                                               {'loss': 2.3884, 'grad_norm': 5.431135654449463, 'learning_rate': 3.1671842700025235e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 23.53it/s] 83%|████████▎ | 62/75 [00:02<00:00, 24.90it/s]                                               {'loss': 2.2234, 'grad_norm': 3.8718647956848145, 'learning_rate': 2.9560386520023553e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 24.90it/s]                                               {'loss': 2.1584, 'grad_norm': 4.968283653259277, 'learning_rate': 2.744893034002187e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 24.90it/s]                                               {'loss': 2.1808, 'grad_norm': 2.6874430179595947, 'learning_rate': 2.533747416002019e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 24.90it/s] 87%|████████▋ | 65/75 [00:02<00:00, 24.58it/s]                                               {'loss': 2.18, 'grad_norm': 3.7122113704681396, 'learning_rate': 2.3226017980018503e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.58it/s]                                               {'loss': 2.0928, 'grad_norm': 3.1073408126831055, 'learning_rate': 2.111456180001682e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.58it/s]                                               {'loss': 2.3065, 'grad_norm': 4.278642654418945, 'learning_rate': 1.900310562001514e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.58it/s] 91%|█████████ | 68/75 [00:02<00:00, 24.59it/s]                                               {'loss': 2.1049, 'grad_norm': 5.897214412689209, 'learning_rate': 1.689164944001346e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.59it/s]                                               {'loss': 2.2321, 'grad_norm': 3.831019878387451, 'learning_rate': 1.4780193260011776e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.59it/s]                                               {'loss': 2.1247, 'grad_norm': 4.3863935470581055, 'learning_rate': 1.2668737080010094e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.59it/s] 95%|█████████▍| 71/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.3093, 'grad_norm': 4.225165843963623, 'learning_rate': 1.055728090000841e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.451, 'grad_norm': 4.087833404541016, 'learning_rate': 8.44582472000673e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.2547, 'grad_norm': 4.316469669342041, 'learning_rate': 6.334368540005047e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 24.83it/s] 99%|█████████▊| 74/75 [00:03<00:00, 24.82it/s]                                               {'loss': 2.2291, 'grad_norm': 3.9600155353546143, 'learning_rate': 4.222912360003365e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.82it/s]                                               {'loss': 2.346, 'grad_norm': 10.447222709655762, 'learning_rate': 2.1114561800016825e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.82it/s]                                               {'train_runtime': 3.1943, 'train_samples_per_second': 353.75, 'train_steps_per_second': 23.479, 'train_loss': 2.302342079480489, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.82it/s]100%|██████████| 75/75 [00:03<00:00, 23.48it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4513, 'grad_norm': 5.629315376281738, 'learning_rate': 0.00015835921350012617, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 19.45it/s]                                              {'loss': 2.3315, 'grad_norm': 4.547567367553711, 'learning_rate': 0.00015624775732012448, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 21.89it/s]  4%|▍         | 3/75 [00:00<00:03, 22.52it/s]                                              {'loss': 2.4578, 'grad_norm': 5.481115341186523, 'learning_rate': 0.00015413630114012282, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 22.52it/s]                                              {'loss': 2.263, 'grad_norm': 4.347387313842773, 'learning_rate': 0.0001520248449601211, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 22.52it/s]                                              {'loss': 2.4875, 'grad_norm': 3.6893434524536133, 'learning_rate': 0.00014991338878011943, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.52it/s]  8%|▊         | 6/75 [00:00<00:02, 24.10it/s]                                              {'loss': 2.2788, 'grad_norm': 3.9914064407348633, 'learning_rate': 0.00014780193260011777, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.10it/s]                                              {'loss': 2.1666, 'grad_norm': 4.021662712097168, 'learning_rate': 0.00014569047642011608, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.10it/s]                                              {'loss': 2.4973, 'grad_norm': 3.9769511222839355, 'learning_rate': 0.0001435790202401144, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.10it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.21it/s]                                              {'loss': 2.4257, 'grad_norm': 4.022660255432129, 'learning_rate': 0.0001414675640601127, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.21it/s]                                              {'loss': 2.2369, 'grad_norm': 4.245161533355713, 'learning_rate': 0.00013935610788011103, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.21it/s]                                               {'loss': 2.3367, 'grad_norm': 4.105282306671143, 'learning_rate': 0.00013724465170010934, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.21it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.57it/s]                                               {'loss': 2.3844, 'grad_norm': 3.969306468963623, 'learning_rate': 0.00013513319552010768, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.57it/s]                                               {'loss': 2.3903, 'grad_norm': 3.3764283657073975, 'learning_rate': 0.00013302173934010597, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.57it/s]                                               {'loss': 2.2927, 'grad_norm': 4.18081521987915, 'learning_rate': 0.0001309102831601043, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.57it/s] 20%|██        | 15/75 [00:00<00:02, 25.50it/s]                                               {'loss': 3.0591, 'grad_norm': 21.692668914794922, 'learning_rate': 0.00012879882698010262, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.50it/s]                                               {'loss': 2.6308, 'grad_norm': 5.490662574768066, 'learning_rate': 0.00012668737080010094, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.50it/s]                                               {'loss': 2.2175, 'grad_norm': 4.811194896697998, 'learning_rate': 0.00012457591462009925, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.50it/s] 24%|██▍       | 18/75 [00:00<00:02, 24.26it/s]                                               {'loss': 2.2829, 'grad_norm': 3.2381927967071533, 'learning_rate': 0.00012246445844009757, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.26it/s]                                               {'loss': 2.3365, 'grad_norm': 5.337603569030762, 'learning_rate': 0.00012035300226009588, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.26it/s]                                               {'loss': 2.5103, 'grad_norm': 5.258611679077148, 'learning_rate': 0.00011824154608009421, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.26it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.75it/s]                                               {'loss': 2.4029, 'grad_norm': 4.14382266998291, 'learning_rate': 0.00011613008990009251, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.75it/s]                                               {'loss': 2.2839, 'grad_norm': 4.4824700355529785, 'learning_rate': 0.00011401863372009084, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.75it/s]                                               {'loss': 2.1892, 'grad_norm': 3.664673089981079, 'learning_rate': 0.00011190717754008915, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.75it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.53it/s]                                               {'loss': 2.1531, 'grad_norm': 3.657639741897583, 'learning_rate': 0.00010979572136008748, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.53it/s]                                               {'loss': 2.1809, 'grad_norm': 4.1889729499816895, 'learning_rate': 0.0001076842651800858, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.53it/s]                                               {'loss': 2.2882, 'grad_norm': 4.046877384185791, 'learning_rate': 0.00010557280900008411, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.53it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.58it/s]                                               {'loss': 2.2359, 'grad_norm': 3.913383722305298, 'learning_rate': 0.00010346135282008243, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.58it/s]                                               {'loss': 2.2708, 'grad_norm': 4.883044242858887, 'learning_rate': 0.00010134989664008075, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.58it/s]                                               {'loss': 2.1834, 'grad_norm': 4.2304558753967285, 'learning_rate': 9.923844046007907e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.58it/s] 40%|████      | 30/75 [00:01<00:01, 25.25it/s]                                               {'loss': 2.5957, 'grad_norm': 9.127394676208496, 'learning_rate': 9.712698428007737e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.25it/s]                                               {'loss': 2.3143, 'grad_norm': 4.473909378051758, 'learning_rate': 9.50155281000757e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.25it/s]                                               {'loss': 2.0514, 'grad_norm': 4.132100582122803, 'learning_rate': 9.290407192007401e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.25it/s] 44%|████▍     | 33/75 [00:01<00:01, 24.67it/s]                                               {'loss': 2.152, 'grad_norm': 3.8521339893341064, 'learning_rate': 9.079261574007234e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.67it/s]                                               {'loss': 2.2507, 'grad_norm': 3.5351507663726807, 'learning_rate': 8.868115956007067e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.67it/s]                                               {'loss': 2.3313, 'grad_norm': 4.353593826293945, 'learning_rate': 8.656970338006897e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.67it/s] 48%|████▊     | 36/75 [00:01<00:01, 23.63it/s]                                               {'loss': 2.0535, 'grad_norm': 4.617190361022949, 'learning_rate': 8.445824720006728e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 23.63it/s]                                               {'loss': 2.4734, 'grad_norm': 4.994940280914307, 'learning_rate': 8.234679102006561e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 23.63it/s]                                               {'loss': 2.4352, 'grad_norm': 4.056333065032959, 'learning_rate': 8.023533484006393e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 23.63it/s] 52%|█████▏    | 39/75 [00:01<00:01, 23.37it/s]                                               {'loss': 2.2466, 'grad_norm': 3.7411305904388428, 'learning_rate': 7.812387866006224e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.37it/s]                                               {'loss': 2.2868, 'grad_norm': 4.059088706970215, 'learning_rate': 7.601242248006056e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.37it/s]                                               {'loss': 2.1209, 'grad_norm': 3.133373737335205, 'learning_rate': 7.390096630005888e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 23.37it/s] 56%|█████▌    | 42/75 [00:01<00:01, 23.65it/s]                                               {'loss': 2.436, 'grad_norm': 4.906241416931152, 'learning_rate': 7.17895101200572e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 23.65it/s]                                               {'loss': 2.3091, 'grad_norm': 3.749244213104248, 'learning_rate': 6.967805394005551e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 23.65it/s]                                               {'loss': 2.2469, 'grad_norm': 4.136529922485352, 'learning_rate': 6.756659776005384e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 23.65it/s]                                               {'loss': 2.0558, 'grad_norm': 10.089895248413086, 'learning_rate': 6.545514158005216e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 23.65it/s] 61%|██████▏   | 46/75 [00:01<00:01, 25.73it/s]                                               {'loss': 2.4396, 'grad_norm': 4.001504421234131, 'learning_rate': 6.334368540005047e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.73it/s]                                               {'loss': 2.2362, 'grad_norm': 3.052957773208618, 'learning_rate': 6.123222922004878e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.73it/s]                                               {'loss': 2.3359, 'grad_norm': 3.870542049407959, 'learning_rate': 5.9120773040047105e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.73it/s] 65%|██████▌   | 49/75 [00:01<00:01, 25.50it/s]                                               {'loss': 2.319, 'grad_norm': 4.560125350952148, 'learning_rate': 5.700931686004542e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.50it/s]                                               {'loss': 2.409, 'grad_norm': 4.023847579956055, 'learning_rate': 5.489786068004374e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.50it/s]                                               {'loss': 2.1286, 'grad_norm': 4.536895275115967, 'learning_rate': 5.2786404500042056e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.50it/s] 69%|██████▉   | 52/75 [00:02<00:00, 24.65it/s]                                               {'loss': 2.142, 'grad_norm': 4.270915985107422, 'learning_rate': 5.067494832004038e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.65it/s]                                               {'loss': 2.1719, 'grad_norm': 3.7756149768829346, 'learning_rate': 4.8563492140038685e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.65it/s]                                               {'loss': 2.2377, 'grad_norm': 4.320345401763916, 'learning_rate': 4.6452035960037006e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.65it/s] 73%|███████▎  | 55/75 [00:02<00:00, 24.62it/s]                                               {'loss': 2.4233, 'grad_norm': 3.6912832260131836, 'learning_rate': 4.4340579780035334e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.62it/s]                                               {'loss': 2.217, 'grad_norm': 4.03088903427124, 'learning_rate': 4.222912360003364e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.62it/s]                                               {'loss': 2.0934, 'grad_norm': 4.565769672393799, 'learning_rate': 4.011766742003196e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.62it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.3209, 'grad_norm': 5.095409870147705, 'learning_rate': 3.800621124003028e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.0528, 'grad_norm': 3.4478142261505127, 'learning_rate': 3.58947550600286e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.3822, 'grad_norm': 21.334318161010742, 'learning_rate': 3.378329888002692e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.2598, 'grad_norm': 4.047048091888428, 'learning_rate': 3.1671842700025235e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.53it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.31it/s]                                               {'loss': 2.1985, 'grad_norm': 3.183751106262207, 'learning_rate': 2.9560386520023553e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.31it/s]                                               {'loss': 2.2046, 'grad_norm': 3.8489432334899902, 'learning_rate': 2.744893034002187e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.31it/s]                                               {'loss': 2.2178, 'grad_norm': 3.8404266834259033, 'learning_rate': 2.533747416002019e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.31it/s] 87%|████████▋ | 65/75 [00:02<00:00, 26.21it/s]                                               {'loss': 2.0439, 'grad_norm': 3.644613265991211, 'learning_rate': 2.3226017980018503e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.21it/s]                                               {'loss': 2.285, 'grad_norm': 5.189907550811768, 'learning_rate': 2.111456180001682e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.21it/s]                                               {'loss': 2.211, 'grad_norm': 4.259641647338867, 'learning_rate': 1.900310562001514e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.21it/s] 91%|█████████ | 68/75 [00:02<00:00, 25.73it/s]                                               {'loss': 2.2006, 'grad_norm': 4.691352367401123, 'learning_rate': 1.689164944001346e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.73it/s]                                               {'loss': 2.1705, 'grad_norm': 3.575981378555298, 'learning_rate': 1.4780193260011776e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.73it/s]                                               {'loss': 2.3201, 'grad_norm': 4.7839250564575195, 'learning_rate': 1.2668737080010094e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.73it/s] 95%|█████████▍| 71/75 [00:02<00:00, 25.89it/s]                                               {'loss': 2.1952, 'grad_norm': 4.006448745727539, 'learning_rate': 1.055728090000841e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.89it/s]                                               {'loss': 2.3185, 'grad_norm': 5.272769451141357, 'learning_rate': 8.44582472000673e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.89it/s]                                               {'loss': 2.3172, 'grad_norm': 4.884551048278809, 'learning_rate': 6.334368540005047e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.89it/s] 99%|█████████▊| 74/75 [00:02<00:00, 25.80it/s]                                               {'loss': 2.2412, 'grad_norm': 4.367987632751465, 'learning_rate': 4.222912360003365e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.80it/s]                                               {'loss': 2.7254, 'grad_norm': 9.347498893737793, 'learning_rate': 2.1114561800016825e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.80it/s]                                               {'train_runtime': 3.1023, 'train_samples_per_second': 364.243, 'train_steps_per_second': 24.175, 'train_loss': 2.29872475306193, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.80it/s]100%|██████████| 75/75 [00:03<00:00, 24.18it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1203, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(981, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(875, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1051, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1290, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1100, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1233, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1223, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1159, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1079, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1347, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(913, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 65.36it/s]  3%|▎         | 14/471 [00:00<00:08, 56.52it/s]  4%|▍         | 20/471 [00:00<00:08, 51.53it/s]  6%|▌         | 26/471 [00:00<00:08, 53.55it/s]  7%|▋         | 32/471 [00:00<00:08, 53.50it/s]  8%|▊         | 38/471 [00:00<00:08, 53.08it/s]  9%|▉         | 44/471 [00:00<00:08, 52.33it/s] 11%|█         | 50/471 [00:00<00:08, 52.52it/s] 12%|█▏        | 56/471 [00:01<00:08, 50.90it/s] 13%|█▎        | 62/471 [00:01<00:07, 51.16it/s] 14%|█▍        | 68/471 [00:01<00:07, 51.95it/s] 16%|█▌        | 74/471 [00:01<00:07, 52.10it/s] 17%|█▋        | 80/471 [00:01<00:07, 50.47it/s] 18%|█▊        | 86/471 [00:01<00:07, 48.18it/s] 19%|█▉        | 91/471 [00:01<00:07, 48.30it/s] 21%|██        | 97/471 [00:01<00:07, 49.37it/s] 22%|██▏       | 103/471 [00:02<00:07, 50.19it/s] 23%|██▎       | 109/471 [00:02<00:07, 48.47it/s] 24%|██▍       | 115/471 [00:02<00:07, 49.45it/s] 26%|██▌       | 121/471 [00:02<00:06, 50.45it/s] 27%|██▋       | 127/471 [00:02<00:06, 49.88it/s] 28%|██▊       | 133/471 [00:02<00:06, 50.62it/s] 30%|██▉       | 139/471 [00:02<00:06, 49.61it/s] 31%|███       | 145/471 [00:02<00:06, 49.41it/s] 32%|███▏      | 151/471 [00:02<00:06, 50.73it/s] 33%|███▎      | 157/471 [00:03<00:06, 48.87it/s] 35%|███▍      | 163/471 [00:03<00:06, 49.12it/s] 36%|███▌      | 168/471 [00:03<00:06, 48.77it/s] 37%|███▋      | 174/471 [00:03<00:05, 49.92it/s] 38%|███▊      | 180/471 [00:03<00:05, 50.57it/s] 39%|███▉      | 186/471 [00:03<00:05, 50.77it/s] 41%|████      | 192/471 [00:03<00:05, 50.93it/s] 42%|████▏     | 198/471 [00:03<00:05, 49.94it/s] 43%|████▎     | 204/471 [00:04<00:05, 48.70it/s] 45%|████▍     | 210/471 [00:04<00:05, 49.43it/s] 46%|████▌     | 216/471 [00:04<00:05, 50.47it/s] 47%|████▋     | 222/471 [00:04<00:04, 50.69it/s] 48%|████▊     | 228/471 [00:04<00:04, 51.04it/s] 50%|████▉     | 234/471 [00:04<00:04, 51.42it/s] 51%|█████     | 240/471 [00:04<00:04, 51.34it/s] 52%|█████▏    | 246/471 [00:04<00:04, 51.87it/s] 54%|█████▎    | 252/471 [00:04<00:04, 51.97it/s] 55%|█████▍    | 258/471 [00:05<00:04, 51.34it/s] 56%|█████▌    | 264/471 [00:05<00:04, 51.43it/s] 57%|█████▋    | 270/471 [00:05<00:03, 51.88it/s] 59%|█████▊    | 276/471 [00:05<00:03, 51.03it/s] 60%|█████▉    | 282/471 [00:05<00:03, 51.66it/s] 61%|██████    | 288/471 [00:05<00:03, 51.77it/s] 62%|██████▏   | 294/471 [00:05<00:03, 51.88it/s] 64%|██████▎   | 300/471 [00:05<00:03, 52.16it/s] 65%|██████▍   | 306/471 [00:06<00:03, 51.32it/s] 66%|██████▌   | 312/471 [00:06<00:03, 49.66it/s] 67%|██████▋   | 317/471 [00:06<00:03, 49.12it/s] 69%|██████▊   | 323/471 [00:06<00:02, 50.13it/s] 70%|██████▉   | 329/471 [00:06<00:02, 49.60it/s] 71%|███████   | 335/471 [00:06<00:02, 50.11it/s] 72%|███████▏  | 341/471 [00:06<00:02, 50.76it/s] 74%|███████▎  | 347/471 [00:06<00:02, 51.47it/s] 75%|███████▍  | 353/471 [00:06<00:02, 51.08it/s] 76%|███████▌  | 359/471 [00:07<00:02, 51.39it/s] 77%|███████▋  | 365/471 [00:07<00:02, 51.54it/s] 79%|███████▉  | 371/471 [00:07<00:01, 51.81it/s] 80%|████████  | 377/471 [00:07<00:01, 51.99it/s] 81%|████████▏ | 383/471 [00:07<00:01, 51.70it/s] 83%|████████▎ | 389/471 [00:07<00:01, 50.17it/s] 84%|████████▍ | 395/471 [00:07<00:01, 51.17it/s] 85%|████████▌ | 401/471 [00:07<00:01, 51.43it/s] 86%|████████▋ | 407/471 [00:08<00:01, 50.72it/s] 88%|████████▊ | 413/471 [00:08<00:01, 50.92it/s] 89%|████████▉ | 419/471 [00:08<00:01, 50.19it/s] 90%|█████████ | 425/471 [00:08<00:00, 48.94it/s] 91%|█████████▏| 430/471 [00:08<00:00, 48.27it/s] 93%|█████████▎| 436/471 [00:08<00:00, 49.59it/s] 94%|█████████▍| 442/471 [00:08<00:00, 50.54it/s] 95%|█████████▌| 448/471 [00:08<00:00, 47.93it/s] 96%|█████████▌| 453/471 [00:08<00:00, 48.34it/s] 97%|█████████▋| 459/471 [00:09<00:00, 49.49it/s] 99%|█████████▊| 464/471 [00:09<00:00, 49.61it/s]100%|█████████▉| 469/471 [00:09<00:00, 49.37it/s]100%|██████████| 471/471 [00:09<00:00, 50.63it/s]
{'eval_loss': 2.331456184387207, 'eval_model_preparation_time': 0.003, 'eval_acc': 0.38090812533191715, 'eval_runtime': 9.3295, 'eval_samples_per_second': 807.332, 'eval_steps_per_second': 50.485}
ROUND:9
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.5101, 'grad_norm': 4.584715366363525, 'learning_rate': 0.00015395010584845858, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 22.76it/s]                                              {'loss': 2.0968, 'grad_norm': 4.02367639541626, 'learning_rate': 0.00015189743777047914, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 22.65it/s]  4%|▍         | 3/75 [00:00<00:03, 23.34it/s]                                              {'loss': 2.2872, 'grad_norm': 4.655230522155762, 'learning_rate': 0.0001498447696924997, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.34it/s]                                              {'loss': 2.5047, 'grad_norm': 5.613506317138672, 'learning_rate': 0.00014779210161452023, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 23.34it/s]                                              {'loss': 2.5372, 'grad_norm': 4.934408664703369, 'learning_rate': 0.0001457394335365408, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.34it/s]  8%|▊         | 6/75 [00:00<00:02, 23.43it/s]                                              {'loss': 2.3056, 'grad_norm': 3.622389793395996, 'learning_rate': 0.00014368676545856136, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.43it/s]                                              {'loss': 2.5126, 'grad_norm': 5.032593250274658, 'learning_rate': 0.0001416340973805819, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.43it/s]                                              {'loss': 2.1461, 'grad_norm': 5.353115558624268, 'learning_rate': 0.00013958142930260245, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.43it/s] 12%|█▏        | 9/75 [00:00<00:02, 23.70it/s]                                              {'loss': 2.0578, 'grad_norm': 4.222418785095215, 'learning_rate': 0.00013752876122462298, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.70it/s]                                              {'loss': 2.2627, 'grad_norm': 4.138805389404297, 'learning_rate': 0.00013547609314664354, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.70it/s]                                               {'loss': 2.3188, 'grad_norm': 4.89475679397583, 'learning_rate': 0.0001334234250686641, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.70it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.70it/s]                                               {'loss': 2.401, 'grad_norm': 5.048122406005859, 'learning_rate': 0.00013137075699068467, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.70it/s]                                               {'loss': 2.4457, 'grad_norm': 5.525400161743164, 'learning_rate': 0.0001293180889127052, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.70it/s]                                               {'loss': 2.3238, 'grad_norm': 5.388434886932373, 'learning_rate': 0.00012726542083472576, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.70it/s] 20%|██        | 15/75 [00:00<00:02, 25.04it/s]                                               {'loss': 2.0864, 'grad_norm': 7.5372314453125, 'learning_rate': 0.00012521275275674632, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.04it/s]                                               {'loss': 2.0264, 'grad_norm': 3.451909303665161, 'learning_rate': 0.00012316008467876688, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.04it/s]                                               {'loss': 2.2163, 'grad_norm': 5.417869567871094, 'learning_rate': 0.00012110741660078741, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.04it/s] 24%|██▍       | 18/75 [00:00<00:02, 23.18it/s]                                               {'loss': 2.5137, 'grad_norm': 4.33225679397583, 'learning_rate': 0.00011905474852280796, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 23.18it/s]                                               {'loss': 2.1374, 'grad_norm': 4.144909381866455, 'learning_rate': 0.00011700208044482852, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 23.18it/s]                                               {'loss': 2.3665, 'grad_norm': 4.142258167266846, 'learning_rate': 0.00011494941236684908, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 23.18it/s] 28%|██▊       | 21/75 [00:00<00:02, 23.21it/s]                                               {'loss': 2.2148, 'grad_norm': 3.4111762046813965, 'learning_rate': 0.00011289674428886962, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 23.21it/s]                                               {'loss': 2.187, 'grad_norm': 3.7474498748779297, 'learning_rate': 0.00011084407621089018, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 23.21it/s]                                               {'loss': 2.1124, 'grad_norm': 4.513655185699463, 'learning_rate': 0.00010879140813291072, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 23.21it/s] 32%|███▏      | 24/75 [00:01<00:02, 23.68it/s]                                               {'loss': 2.275, 'grad_norm': 5.036658763885498, 'learning_rate': 0.00010673874005493128, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 23.68it/s]                                               {'loss': 2.4632, 'grad_norm': 4.116300106048584, 'learning_rate': 0.00010468607197695184, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.68it/s]                                               {'loss': 2.3925, 'grad_norm': 4.019946098327637, 'learning_rate': 0.00010263340389897238, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.68it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.20it/s]                                               {'loss': 2.2411, 'grad_norm': 6.1149373054504395, 'learning_rate': 0.00010058073582099294, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.20it/s]                                               {'loss': 2.194, 'grad_norm': 4.670490264892578, 'learning_rate': 9.85280677430135e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.20it/s]                                               {'loss': 2.3408, 'grad_norm': 3.742109537124634, 'learning_rate': 9.647539966503405e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.20it/s]                                               {'loss': 1.6102, 'grad_norm': 8.664740562438965, 'learning_rate': 9.442273158705459e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.20it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.10it/s]                                               {'loss': 2.3873, 'grad_norm': 4.316613674163818, 'learning_rate': 9.237006350907514e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.10it/s]                                               {'loss': 2.3266, 'grad_norm': 3.650756359100342, 'learning_rate': 9.03173954310957e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.10it/s]                                               {'loss': 2.121, 'grad_norm': 4.773228645324707, 'learning_rate': 8.826472735311626e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.10it/s] 45%|████▌     | 34/75 [00:01<00:01, 25.87it/s]                                               {'loss': 2.1596, 'grad_norm': 3.5990493297576904, 'learning_rate': 8.621205927513681e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.87it/s]                                               {'loss': 2.2058, 'grad_norm': 4.390504360198975, 'learning_rate': 8.415939119715736e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.87it/s]                                               {'loss': 2.2345, 'grad_norm': 4.103087902069092, 'learning_rate': 8.21067231191779e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.87it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.61it/s]                                               {'loss': 2.19, 'grad_norm': 3.8863823413848877, 'learning_rate': 8.005405504119846e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.61it/s]                                               {'loss': 2.3428, 'grad_norm': 3.9248783588409424, 'learning_rate': 7.800138696321902e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.61it/s]                                               {'loss': 2.2347, 'grad_norm': 3.9539756774902344, 'learning_rate': 7.594871888523957e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.61it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.23it/s]                                               {'loss': 2.179, 'grad_norm': 4.053136825561523, 'learning_rate': 7.389605080726012e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.23it/s]                                               {'loss': 2.3786, 'grad_norm': 5.623774528503418, 'learning_rate': 7.184338272928068e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.23it/s]                                               {'loss': 2.2133, 'grad_norm': 4.4099836349487305, 'learning_rate': 6.979071465130123e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.23it/s] 57%|█████▋    | 43/75 [00:01<00:01, 25.19it/s]                                               {'loss': 2.0724, 'grad_norm': 4.211825370788574, 'learning_rate': 6.773804657332177e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.19it/s]                                               {'loss': 2.1769, 'grad_norm': 3.983462333679199, 'learning_rate': 6.568537849534233e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.19it/s]                                               {'loss': 1.441, 'grad_norm': 10.49892520904541, 'learning_rate': 6.363271041736288e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.19it/s]                                               {'loss': 2.3379, 'grad_norm': 3.411328077316284, 'learning_rate': 6.158004233938344e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.19it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.89it/s]                                               {'loss': 2.3227, 'grad_norm': 3.854836940765381, 'learning_rate': 5.952737426140398e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.89it/s]                                               {'loss': 1.9969, 'grad_norm': 3.3571527004241943, 'learning_rate': 5.747470618342454e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.89it/s]                                               {'loss': 2.2152, 'grad_norm': 4.964370250701904, 'learning_rate': 5.542203810544509e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.89it/s] 67%|██████▋   | 50/75 [00:02<00:00, 25.65it/s]                                               {'loss': 2.0283, 'grad_norm': 5.484403133392334, 'learning_rate': 5.336937002746564e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.65it/s]                                               {'loss': 2.1132, 'grad_norm': 4.088770866394043, 'learning_rate': 5.131670194948619e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.65it/s]                                               {'loss': 2.4212, 'grad_norm': 4.953672885894775, 'learning_rate': 4.926403387150675e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.65it/s] 71%|███████   | 53/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.283, 'grad_norm': 3.319767951965332, 'learning_rate': 4.7211365793527297e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.1505, 'grad_norm': 4.97715950012207, 'learning_rate': 4.515869771554785e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.1888, 'grad_norm': 3.678866386413574, 'learning_rate': 4.3106029637568404e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.63it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.52it/s]                                               {'loss': 2.3728, 'grad_norm': 5.376829147338867, 'learning_rate': 4.105336155958895e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.52it/s]                                               {'loss': 2.0924, 'grad_norm': 3.6631810665130615, 'learning_rate': 3.900069348160951e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.52it/s]                                               {'loss': 2.3499, 'grad_norm': 3.8640151023864746, 'learning_rate': 3.694802540363006e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.52it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.39it/s]                                               {'loss': 2.1052, 'grad_norm': 4.110871315002441, 'learning_rate': 3.489535732565061e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.39it/s]                                               {'loss': 2.0487, 'grad_norm': 9.736838340759277, 'learning_rate': 3.2842689247671166e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.39it/s]                                               {'loss': 2.1484, 'grad_norm': 4.136231899261475, 'learning_rate': 3.079002116969172e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.39it/s]                                               {'loss': 2.2981, 'grad_norm': 4.187053203582764, 'learning_rate': 2.873735309171227e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.39it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.58it/s]                                               {'loss': 2.3115, 'grad_norm': 4.049566268920898, 'learning_rate': 2.668468501373282e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.58it/s]                                               {'loss': 2.3018, 'grad_norm': 6.877694129943848, 'learning_rate': 2.4632016935753375e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.58it/s]                                               {'loss': 2.2034, 'grad_norm': 4.041561603546143, 'learning_rate': 2.2579348857773925e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.58it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.25it/s]                                               {'loss': 2.2152, 'grad_norm': 4.392336845397949, 'learning_rate': 2.0526680779794476e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.25it/s]                                               {'loss': 2.2116, 'grad_norm': 3.722066879272461, 'learning_rate': 1.847401270181503e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.25it/s]                                               {'loss': 2.3249, 'grad_norm': 5.3469085693359375, 'learning_rate': 1.6421344623835583e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.25it/s] 92%|█████████▏| 69/75 [00:02<00:00, 24.22it/s]                                               {'loss': 2.1305, 'grad_norm': 3.853374719619751, 'learning_rate': 1.4368676545856135e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.22it/s]                                               {'loss': 2.1309, 'grad_norm': 3.8059916496276855, 'learning_rate': 1.2316008467876687e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.22it/s]                                               {'loss': 2.1015, 'grad_norm': 3.4315805435180664, 'learning_rate': 1.0263340389897238e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.22it/s] 96%|█████████▌| 72/75 [00:02<00:00, 23.14it/s]                                               {'loss': 1.9246, 'grad_norm': 3.253704071044922, 'learning_rate': 8.210672311917792e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 23.14it/s]                                               {'loss': 2.3128, 'grad_norm': 4.559799671173096, 'learning_rate': 6.158004233938344e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 23.14it/s]                                               {'loss': 2.1161, 'grad_norm': 3.8651976585388184, 'learning_rate': 4.105336155958896e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 23.14it/s]                                               {'loss': 1.6211, 'grad_norm': 9.137709617614746, 'learning_rate': 2.052668077979448e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 23.14it/s]                                               {'train_runtime': 3.1382, 'train_samples_per_second': 360.082, 'train_steps_per_second': 23.899, 'train_loss': 2.21507261912028, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 23.14it/s]100%|██████████| 75/75 [00:03<00:00, 23.90it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:47
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3028, 'grad_norm': 4.721202373504639, 'learning_rate': 0.00015395010584845858, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:06, 11.46it/s]  3%|▎         | 2/75 [00:00<00:04, 16.24it/s]                                              {'loss': 2.4413, 'grad_norm': 3.7860376834869385, 'learning_rate': 0.00015189743777047914, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:04, 16.24it/s]                                              {'loss': 2.5118, 'grad_norm': 4.7827677726745605, 'learning_rate': 0.0001498447696924997, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 16.24it/s]                                              {'loss': 2.3671, 'grad_norm': 4.401461124420166, 'learning_rate': 0.00014779210161452023, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:04, 16.24it/s]  7%|▋         | 5/75 [00:00<00:03, 21.44it/s]                                              {'loss': 2.4895, 'grad_norm': 3.9083621501922607, 'learning_rate': 0.0001457394335365408, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 21.44it/s]                                              {'loss': 2.305, 'grad_norm': 4.761832237243652, 'learning_rate': 0.00014368676545856136, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 21.44it/s]                                              {'loss': 2.3892, 'grad_norm': 4.097640037536621, 'learning_rate': 0.0001416340973805819, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 21.44it/s] 11%|█         | 8/75 [00:00<00:02, 22.76it/s]                                              {'loss': 2.2773, 'grad_norm': 4.367807388305664, 'learning_rate': 0.00013958142930260245, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 22.76it/s]                                              {'loss': 2.3873, 'grad_norm': 4.624079704284668, 'learning_rate': 0.00013752876122462298, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 22.76it/s]                                              {'loss': 2.4491, 'grad_norm': 3.738957166671753, 'learning_rate': 0.00013547609314664354, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 22.76it/s] 15%|█▍        | 11/75 [00:00<00:02, 23.77it/s]                                               {'loss': 2.5629, 'grad_norm': 4.133059501647949, 'learning_rate': 0.0001334234250686641, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.77it/s]                                               {'loss': 2.2048, 'grad_norm': 4.523941516876221, 'learning_rate': 0.00013137075699068467, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.77it/s]                                               {'loss': 2.4874, 'grad_norm': 4.937283992767334, 'learning_rate': 0.0001293180889127052, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.77it/s] 19%|█▊        | 14/75 [00:00<00:02, 23.63it/s]                                               {'loss': 2.3146, 'grad_norm': 3.9754083156585693, 'learning_rate': 0.00012726542083472576, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.63it/s]                                               {'loss': 2.6706, 'grad_norm': 10.886908531188965, 'learning_rate': 0.00012521275275674632, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.63it/s]                                               {'loss': 2.3437, 'grad_norm': 4.26081657409668, 'learning_rate': 0.00012316008467876688, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 23.63it/s] 23%|██▎       | 17/75 [00:00<00:02, 23.39it/s]                                               {'loss': 2.3963, 'grad_norm': 4.807461261749268, 'learning_rate': 0.00012110741660078741, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 23.39it/s]                                               {'loss': 2.3005, 'grad_norm': 4.058651447296143, 'learning_rate': 0.00011905474852280796, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 23.39it/s]                                               {'loss': 2.1674, 'grad_norm': 3.840236186981201, 'learning_rate': 0.00011700208044482852, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 23.39it/s] 27%|██▋       | 20/75 [00:00<00:02, 23.56it/s]                                               {'loss': 2.5234, 'grad_norm': 5.36940860748291, 'learning_rate': 0.00011494941236684908, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 23.56it/s]                                               {'loss': 2.2333, 'grad_norm': 3.9320290088653564, 'learning_rate': 0.00011289674428886962, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 23.56it/s]                                               {'loss': 2.2493, 'grad_norm': 4.612362861633301, 'learning_rate': 0.00011084407621089018, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 23.56it/s] 31%|███       | 23/75 [00:01<00:02, 22.53it/s]                                               {'loss': 2.4069, 'grad_norm': 3.687032699584961, 'learning_rate': 0.00010879140813291072, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 22.53it/s]                                               {'loss': 2.2506, 'grad_norm': 3.9225523471832275, 'learning_rate': 0.00010673874005493128, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 22.53it/s]                                               {'loss': 2.4689, 'grad_norm': 4.494405269622803, 'learning_rate': 0.00010468607197695184, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 22.53it/s] 35%|███▍      | 26/75 [00:01<00:02, 22.76it/s]                                               {'loss': 2.128, 'grad_norm': 3.483793020248413, 'learning_rate': 0.00010263340389897238, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 22.76it/s]                                               {'loss': 2.2737, 'grad_norm': 4.739657402038574, 'learning_rate': 0.00010058073582099294, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 22.76it/s]                                               {'loss': 2.2061, 'grad_norm': 4.963161945343018, 'learning_rate': 9.85280677430135e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 22.76it/s] 39%|███▊      | 29/75 [00:01<00:02, 22.39it/s]                                               {'loss': 2.4041, 'grad_norm': 4.0427565574646, 'learning_rate': 9.647539966503405e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 22.39it/s]                                               {'loss': 2.5238, 'grad_norm': 11.438401222229004, 'learning_rate': 9.442273158705459e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 22.39it/s]                                               {'loss': 2.0871, 'grad_norm': 3.9743387699127197, 'learning_rate': 9.237006350907514e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 22.39it/s]                                               {'loss': 2.2143, 'grad_norm': 4.357065200805664, 'learning_rate': 9.03173954310957e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 22.39it/s] 44%|████▍     | 33/75 [00:01<00:01, 24.41it/s]                                               {'loss': 2.4084, 'grad_norm': 3.6104981899261475, 'learning_rate': 8.826472735311626e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.41it/s]                                               {'loss': 2.2513, 'grad_norm': 6.589901447296143, 'learning_rate': 8.621205927513681e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.41it/s]                                               {'loss': 2.3669, 'grad_norm': 4.650457859039307, 'learning_rate': 8.415939119715736e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.41it/s] 48%|████▊     | 36/75 [00:01<00:01, 24.64it/s]                                               {'loss': 2.5323, 'grad_norm': 4.929351806640625, 'learning_rate': 8.21067231191779e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.64it/s]                                               {'loss': 2.1487, 'grad_norm': 3.5466437339782715, 'learning_rate': 8.005405504119846e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.64it/s]                                               {'loss': 2.1279, 'grad_norm': 3.053185224533081, 'learning_rate': 7.800138696321902e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.64it/s] 52%|█████▏    | 39/75 [00:01<00:01, 24.46it/s]                                               {'loss': 2.1802, 'grad_norm': 3.979918956756592, 'learning_rate': 7.594871888523957e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.46it/s]                                               {'loss': 2.2478, 'grad_norm': 4.1932806968688965, 'learning_rate': 7.389605080726012e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.46it/s]                                               {'loss': 2.3013, 'grad_norm': 3.952200412750244, 'learning_rate': 7.184338272928068e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.46it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.62it/s]                                               {'loss': 2.458, 'grad_norm': 3.5897769927978516, 'learning_rate': 6.979071465130123e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.62it/s]                                               {'loss': 2.1058, 'grad_norm': 4.384867191314697, 'learning_rate': 6.773804657332177e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.62it/s]                                               {'loss': 2.4214, 'grad_norm': 5.720506191253662, 'learning_rate': 6.568537849534233e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.62it/s] 60%|██████    | 45/75 [00:01<00:01, 25.51it/s]                                               {'loss': 2.3566, 'grad_norm': 16.420080184936523, 'learning_rate': 6.363271041736288e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.51it/s]                                               {'loss': 2.265, 'grad_norm': 3.522853374481201, 'learning_rate': 6.158004233938344e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.51it/s]                                               {'loss': 2.2644, 'grad_norm': 4.016256809234619, 'learning_rate': 5.952737426140398e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.51it/s] 64%|██████▍   | 48/75 [00:02<00:01, 24.19it/s]                                               {'loss': 2.145, 'grad_norm': 3.812549591064453, 'learning_rate': 5.747470618342454e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 24.19it/s]                                               {'loss': 2.306, 'grad_norm': 4.936976432800293, 'learning_rate': 5.542203810544509e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 24.19it/s]                                               {'loss': 2.364, 'grad_norm': 5.38290548324585, 'learning_rate': 5.336937002746564e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 24.19it/s] 68%|██████▊   | 51/75 [00:02<00:00, 24.18it/s]                                               {'loss': 2.1889, 'grad_norm': 4.546829700469971, 'learning_rate': 5.131670194948619e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 24.18it/s]                                               {'loss': 2.4831, 'grad_norm': 4.462655544281006, 'learning_rate': 4.926403387150675e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.18it/s]                                               {'loss': 2.1197, 'grad_norm': 3.363600730895996, 'learning_rate': 4.7211365793527297e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.18it/s] 72%|███████▏  | 54/75 [00:02<00:00, 24.67it/s]                                               {'loss': 2.2735, 'grad_norm': 4.70338249206543, 'learning_rate': 4.515869771554785e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.67it/s]                                               {'loss': 2.5022, 'grad_norm': 4.144404888153076, 'learning_rate': 4.3106029637568404e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.67it/s]                                               {'loss': 2.2693, 'grad_norm': 4.509333610534668, 'learning_rate': 4.105336155958895e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.67it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.48it/s]                                               {'loss': 2.163, 'grad_norm': 4.226687908172607, 'learning_rate': 3.900069348160951e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.48it/s]                                               {'loss': 2.1512, 'grad_norm': 3.7228636741638184, 'learning_rate': 3.694802540363006e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.48it/s]                                               {'loss': 2.313, 'grad_norm': 4.896728515625, 'learning_rate': 3.489535732565061e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.48it/s]                                               {'loss': 1.7867, 'grad_norm': 12.814277648925781, 'learning_rate': 3.2842689247671166e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.48it/s] 81%|████████▏ | 61/75 [00:02<00:00, 25.69it/s]                                               {'loss': 2.3879, 'grad_norm': 5.433842182159424, 'learning_rate': 3.079002116969172e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.69it/s]                                               {'loss': 2.2224, 'grad_norm': 3.842369794845581, 'learning_rate': 2.873735309171227e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.69it/s]                                               {'loss': 2.1597, 'grad_norm': 4.949109077453613, 'learning_rate': 2.668468501373282e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.69it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.1799, 'grad_norm': 2.6795520782470703, 'learning_rate': 2.4632016935753375e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.1776, 'grad_norm': 3.7074620723724365, 'learning_rate': 2.2579348857773925e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.0952, 'grad_norm': 3.129758358001709, 'learning_rate': 2.0526680779794476e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.16it/s] 89%|████████▉ | 67/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.3077, 'grad_norm': 4.240359783172607, 'learning_rate': 1.847401270181503e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.1055, 'grad_norm': 5.8238325119018555, 'learning_rate': 1.6421344623835583e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.2316, 'grad_norm': 3.777759552001953, 'learning_rate': 1.4368676545856135e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.53it/s] 93%|█████████▎| 70/75 [00:02<00:00, 24.35it/s]                                               {'loss': 2.1211, 'grad_norm': 4.421205043792725, 'learning_rate': 1.2316008467876687e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.35it/s]                                               {'loss': 2.306, 'grad_norm': 4.219722270965576, 'learning_rate': 1.0263340389897238e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.35it/s]                                               {'loss': 2.4515, 'grad_norm': 4.047811985015869, 'learning_rate': 8.210672311917792e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.35it/s] 97%|█████████▋| 73/75 [00:03<00:00, 24.74it/s]                                               {'loss': 2.258, 'grad_norm': 4.262207984924316, 'learning_rate': 6.158004233938344e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 24.74it/s]                                               {'loss': 2.2317, 'grad_norm': 3.976494789123535, 'learning_rate': 4.105336155958896e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.74it/s]                                               {'loss': 2.3423, 'grad_norm': 10.388662338256836, 'learning_rate': 2.052668077979448e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.74it/s]                                               {'train_runtime': 3.1989, 'train_samples_per_second': 353.244, 'train_steps_per_second': 23.445, 'train_loss': 2.2989077027638753, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.74it/s]100%|██████████| 75/75 [00:03<00:00, 23.49it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2872, 'grad_norm': 3.760746479034424, 'learning_rate': 0.00015395010584845858, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 20.55it/s]                                              {'loss': 2.5298, 'grad_norm': 4.088409900665283, 'learning_rate': 0.00015189743777047914, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.43it/s]  4%|▍         | 3/75 [00:00<00:02, 24.31it/s]                                              {'loss': 2.2249, 'grad_norm': 4.283568382263184, 'learning_rate': 0.0001498447696924997, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.31it/s]                                              {'loss': 2.1189, 'grad_norm': 4.683341979980469, 'learning_rate': 0.00014779210161452023, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.31it/s]                                              {'loss': 2.2355, 'grad_norm': 3.7606613636016846, 'learning_rate': 0.0001457394335365408, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.31it/s]  8%|▊         | 6/75 [00:00<00:02, 25.18it/s]                                              {'loss': 2.5175, 'grad_norm': 4.257176399230957, 'learning_rate': 0.00014368676545856136, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.18it/s]                                              {'loss': 2.3801, 'grad_norm': 3.811022996902466, 'learning_rate': 0.0001416340973805819, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.18it/s]                                              {'loss': 2.1578, 'grad_norm': 4.013733386993408, 'learning_rate': 0.00013958142930260245, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.18it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.05it/s]                                              {'loss': 2.2494, 'grad_norm': 3.8508708477020264, 'learning_rate': 0.00013752876122462298, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.05it/s]                                              {'loss': 2.1798, 'grad_norm': 4.663344860076904, 'learning_rate': 0.00013547609314664354, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.05it/s]                                               {'loss': 2.3573, 'grad_norm': 4.4145827293396, 'learning_rate': 0.0001334234250686641, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.05it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.07it/s]                                               {'loss': 2.2989, 'grad_norm': 3.9575278759002686, 'learning_rate': 0.00013137075699068467, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.07it/s]                                               {'loss': 2.3717, 'grad_norm': 3.8443241119384766, 'learning_rate': 0.0001293180889127052, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.07it/s]                                               {'loss': 2.3426, 'grad_norm': 4.933233261108398, 'learning_rate': 0.00012726542083472576, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.07it/s]                                               {'loss': 2.5794, 'grad_norm': 12.006275177001953, 'learning_rate': 0.00012521275275674632, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.07it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.55it/s]                                               {'loss': 2.2994, 'grad_norm': 4.119351863861084, 'learning_rate': 0.00012316008467876688, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.55it/s]                                               {'loss': 2.3867, 'grad_norm': 4.6988067626953125, 'learning_rate': 0.00012110741660078741, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.55it/s]                                               {'loss': 2.334, 'grad_norm': 4.779131889343262, 'learning_rate': 0.00011905474852280796, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.55it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.85it/s]                                               {'loss': 2.2454, 'grad_norm': 3.4126808643341064, 'learning_rate': 0.00011700208044482852, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.85it/s]                                               {'loss': 2.3742, 'grad_norm': 3.8745415210723877, 'learning_rate': 0.00011494941236684908, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.85it/s]                                               {'loss': 2.3695, 'grad_norm': 3.8590824604034424, 'learning_rate': 0.00011289674428886962, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.85it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.01it/s]                                               {'loss': 2.2532, 'grad_norm': 4.489772319793701, 'learning_rate': 0.00011084407621089018, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.01it/s]                                               {'loss': 2.3058, 'grad_norm': 4.074033260345459, 'learning_rate': 0.00010879140813291072, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.01it/s]                                               {'loss': 2.1711, 'grad_norm': 4.227345943450928, 'learning_rate': 0.00010673874005493128, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 25.01it/s] 33%|███▎      | 25/75 [00:00<00:02, 24.95it/s]                                               {'loss': 2.2837, 'grad_norm': 4.182336807250977, 'learning_rate': 0.00010468607197695184, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:02, 24.95it/s]                                               {'loss': 2.1283, 'grad_norm': 4.257957458496094, 'learning_rate': 0.00010263340389897238, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.95it/s]                                               {'loss': 2.1336, 'grad_norm': 4.315412998199463, 'learning_rate': 0.00010058073582099294, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.95it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.1652, 'grad_norm': 3.2023401260375977, 'learning_rate': 9.85280677430135e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.0643, 'grad_norm': 3.899310827255249, 'learning_rate': 9.647539966503405e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.8413, 'grad_norm': 13.306890487670898, 'learning_rate': 9.442273158705459e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.0882, 'grad_norm': 3.6875243186950684, 'learning_rate': 9.237006350907514e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.13it/s] 43%|████▎     | 32/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.3581, 'grad_norm': 4.031819820404053, 'learning_rate': 9.03173954310957e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.3811, 'grad_norm': 3.928157329559326, 'learning_rate': 8.826472735311626e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.2773, 'grad_norm': 3.731729745864868, 'learning_rate': 8.621205927513681e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.37it/s] 47%|████▋     | 35/75 [00:01<00:01, 25.02it/s]                                               {'loss': 2.0027, 'grad_norm': 3.003667116165161, 'learning_rate': 8.415939119715736e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.02it/s]                                               {'loss': 2.1782, 'grad_norm': 4.429327964782715, 'learning_rate': 8.21067231191779e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.02it/s]                                               {'loss': 2.2559, 'grad_norm': 3.4280195236206055, 'learning_rate': 8.005405504119846e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.02it/s] 51%|█████     | 38/75 [00:01<00:01, 25.09it/s]                                               {'loss': 2.3127, 'grad_norm': 4.086363315582275, 'learning_rate': 7.800138696321902e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.09it/s]                                               {'loss': 2.1841, 'grad_norm': 3.6816041469573975, 'learning_rate': 7.594871888523957e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.09it/s]                                               {'loss': 2.1703, 'grad_norm': 3.719346284866333, 'learning_rate': 7.389605080726012e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.09it/s] 55%|█████▍    | 41/75 [00:01<00:01, 23.71it/s]                                               {'loss': 2.1472, 'grad_norm': 3.304133653640747, 'learning_rate': 7.184338272928068e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 23.71it/s]                                               {'loss': 2.2664, 'grad_norm': 3.5259623527526855, 'learning_rate': 6.979071465130123e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 23.71it/s]                                               {'loss': 1.9764, 'grad_norm': 4.391544818878174, 'learning_rate': 6.773804657332177e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 23.71it/s] 59%|█████▊    | 44/75 [00:01<00:01, 23.72it/s]                                               {'loss': 2.1976, 'grad_norm': 3.476057767868042, 'learning_rate': 6.568537849534233e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 23.72it/s]                                               {'loss': 2.8341, 'grad_norm': 13.108572006225586, 'learning_rate': 6.363271041736288e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 23.72it/s]                                               {'loss': 2.268, 'grad_norm': 3.26006817817688, 'learning_rate': 6.158004233938344e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 23.72it/s] 63%|██████▎   | 47/75 [00:01<00:01, 23.96it/s]                                               {'loss': 2.421, 'grad_norm': 4.477810859680176, 'learning_rate': 5.952737426140398e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 23.96it/s]                                               {'loss': 2.2547, 'grad_norm': 4.955624103546143, 'learning_rate': 5.747470618342454e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 23.96it/s]                                               {'loss': 2.1292, 'grad_norm': 3.45878005027771, 'learning_rate': 5.542203810544509e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 23.96it/s] 67%|██████▋   | 50/75 [00:02<00:01, 23.32it/s]                                               {'loss': 2.0333, 'grad_norm': 3.0796353816986084, 'learning_rate': 5.336937002746564e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 23.32it/s]                                               {'loss': 2.1523, 'grad_norm': 3.3809821605682373, 'learning_rate': 5.131670194948619e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 23.32it/s]                                               {'loss': 2.2238, 'grad_norm': 4.1052069664001465, 'learning_rate': 4.926403387150675e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 23.32it/s] 71%|███████   | 53/75 [00:02<00:00, 22.94it/s]                                               {'loss': 1.9524, 'grad_norm': 4.235841751098633, 'learning_rate': 4.7211365793527297e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 22.94it/s]                                               {'loss': 2.4964, 'grad_norm': 5.11958122253418, 'learning_rate': 4.515869771554785e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 22.94it/s]                                               {'loss': 2.0426, 'grad_norm': 3.6450414657592773, 'learning_rate': 4.3106029637568404e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 22.94it/s] 75%|███████▍  | 56/75 [00:02<00:00, 23.33it/s]                                               {'loss': 2.1798, 'grad_norm': 3.6079962253570557, 'learning_rate': 4.105336155958895e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 23.33it/s]                                               {'loss': 2.1106, 'grad_norm': 3.947592258453369, 'learning_rate': 3.900069348160951e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 23.33it/s]                                               {'loss': 2.3261, 'grad_norm': 2.9760563373565674, 'learning_rate': 3.694802540363006e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 23.33it/s] 79%|███████▊  | 59/75 [00:02<00:00, 23.24it/s]                                               {'loss': 2.1046, 'grad_norm': 3.629159688949585, 'learning_rate': 3.489535732565061e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 23.24it/s]                                               {'loss': 2.0776, 'grad_norm': 9.828557968139648, 'learning_rate': 3.2842689247671166e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 23.24it/s]                                               {'loss': 1.9922, 'grad_norm': 3.5209059715270996, 'learning_rate': 3.079002116969172e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 23.24it/s] 83%|████████▎ | 62/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.126, 'grad_norm': 4.220330238342285, 'learning_rate': 2.873735309171227e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.2834, 'grad_norm': 4.137876510620117, 'learning_rate': 2.668468501373282e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.0605, 'grad_norm': 3.0386102199554443, 'learning_rate': 2.4632016935753375e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 24.63it/s] 87%|████████▋ | 65/75 [00:02<00:00, 24.14it/s]                                               {'loss': 2.2163, 'grad_norm': 3.9693408012390137, 'learning_rate': 2.2579348857773925e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.14it/s]                                               {'loss': 2.1463, 'grad_norm': 3.7783050537109375, 'learning_rate': 2.0526680779794476e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.14it/s]                                               {'loss': 2.2444, 'grad_norm': 4.72161340713501, 'learning_rate': 1.847401270181503e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.14it/s] 91%|█████████ | 68/75 [00:02<00:00, 23.68it/s]                                               {'loss': 2.2609, 'grad_norm': 4.010088920593262, 'learning_rate': 1.6421344623835583e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 23.68it/s]                                               {'loss': 2.3908, 'grad_norm': 4.340417861938477, 'learning_rate': 1.4368676545856135e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 23.68it/s]                                               {'loss': 2.0206, 'grad_norm': 2.946638822555542, 'learning_rate': 1.2316008467876687e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 23.68it/s] 95%|█████████▍| 71/75 [00:02<00:00, 23.33it/s]                                               {'loss': 2.0411, 'grad_norm': 3.357210636138916, 'learning_rate': 1.0263340389897238e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 23.33it/s]                                               {'loss': 2.2621, 'grad_norm': 3.8279342651367188, 'learning_rate': 8.210672311917792e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 23.33it/s]                                               {'loss': 2.3709, 'grad_norm': 4.203670024871826, 'learning_rate': 6.158004233938344e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 23.33it/s] 99%|█████████▊| 74/75 [00:03<00:00, 23.69it/s]                                               {'loss': 2.2125, 'grad_norm': 3.7410688400268555, 'learning_rate': 4.105336155958896e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 23.69it/s]                                               {'loss': 1.8367, 'grad_norm': 10.803248405456543, 'learning_rate': 2.052668077979448e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 23.69it/s]                                               {'train_runtime': 3.1709, 'train_samples_per_second': 356.367, 'train_steps_per_second': 23.653, 'train_loss': 2.2407137393951415, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 23.69it/s]100%|██████████| 75/75 [00:03<00:00, 23.65it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:5
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3175, 'grad_norm': 5.615462779998779, 'learning_rate': 0.00015395010584845858, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 21.43it/s]                                              {'loss': 2.2818, 'grad_norm': 4.460766315460205, 'learning_rate': 0.00015189743777047914, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 22.20it/s]  4%|▍         | 3/75 [00:00<00:03, 23.60it/s]                                              {'loss': 2.3652, 'grad_norm': 3.8084540367126465, 'learning_rate': 0.0001498447696924997, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.60it/s]                                              {'loss': 2.5091, 'grad_norm': 4.247457027435303, 'learning_rate': 0.00014779210161452023, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 23.60it/s]                                              {'loss': 2.0306, 'grad_norm': 3.0078179836273193, 'learning_rate': 0.0001457394335365408, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.60it/s]  8%|▊         | 6/75 [00:00<00:02, 23.54it/s]                                              {'loss': 2.3096, 'grad_norm': 5.2145185470581055, 'learning_rate': 0.00014368676545856136, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.54it/s]                                              {'loss': 2.2114, 'grad_norm': 5.8652753829956055, 'learning_rate': 0.0001416340973805819, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.54it/s]                                              {'loss': 2.2215, 'grad_norm': 3.2894532680511475, 'learning_rate': 0.00013958142930260245, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.54it/s] 12%|█▏        | 9/75 [00:00<00:02, 22.27it/s]                                              {'loss': 2.2022, 'grad_norm': 4.231832981109619, 'learning_rate': 0.00013752876122462298, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 22.27it/s]                                              {'loss': 2.3207, 'grad_norm': 3.598710060119629, 'learning_rate': 0.00013547609314664354, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 22.27it/s]                                               {'loss': 2.1183, 'grad_norm': 6.173716068267822, 'learning_rate': 0.0001334234250686641, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 22.27it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.40it/s]                                               {'loss': 2.2169, 'grad_norm': 4.930384635925293, 'learning_rate': 0.00013137075699068467, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.40it/s]                                               {'loss': 2.3027, 'grad_norm': 4.20387601852417, 'learning_rate': 0.0001293180889127052, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.40it/s]                                               {'loss': 2.3432, 'grad_norm': 4.796156883239746, 'learning_rate': 0.00012726542083472576, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.40it/s] 20%|██        | 15/75 [00:00<00:02, 25.36it/s]                                               {'loss': 2.9987, 'grad_norm': 13.655074119567871, 'learning_rate': 0.00012521275275674632, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.36it/s]                                               {'loss': 2.2957, 'grad_norm': 3.9552958011627197, 'learning_rate': 0.00012316008467876688, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.36it/s]                                               {'loss': 2.2617, 'grad_norm': 4.196502685546875, 'learning_rate': 0.00012110741660078741, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.36it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.36it/s]                                               {'loss': 2.2088, 'grad_norm': 3.1162590980529785, 'learning_rate': 0.00011905474852280796, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.36it/s]                                               {'loss': 2.19, 'grad_norm': 3.7340617179870605, 'learning_rate': 0.00011700208044482852, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.36it/s]                                               {'loss': 2.2032, 'grad_norm': 3.834651470184326, 'learning_rate': 0.00011494941236684908, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.36it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.29it/s]                                               {'loss': 2.1857, 'grad_norm': 4.451110363006592, 'learning_rate': 0.00011289674428886962, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.29it/s]                                               {'loss': 2.2193, 'grad_norm': 4.436130523681641, 'learning_rate': 0.00011084407621089018, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.29it/s]                                               {'loss': 2.2058, 'grad_norm': 3.8549108505249023, 'learning_rate': 0.00010879140813291072, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.29it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.82it/s]                                               {'loss': 2.1974, 'grad_norm': 3.841770648956299, 'learning_rate': 0.00010673874005493128, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.82it/s]                                               {'loss': 2.2316, 'grad_norm': 5.4372148513793945, 'learning_rate': 0.00010468607197695184, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.82it/s]                                               {'loss': 2.2067, 'grad_norm': 4.605638027191162, 'learning_rate': 0.00010263340389897238, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.82it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.69it/s]                                               {'loss': 2.1718, 'grad_norm': 3.872758150100708, 'learning_rate': 0.00010058073582099294, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.69it/s]                                               {'loss': 2.1334, 'grad_norm': 3.324289321899414, 'learning_rate': 9.85280677430135e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.69it/s]                                               {'loss': 1.9923, 'grad_norm': 3.1926984786987305, 'learning_rate': 9.647539966503405e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.69it/s] 40%|████      | 30/75 [00:01<00:01, 25.49it/s]                                               {'loss': 2.5596, 'grad_norm': 13.789127349853516, 'learning_rate': 9.442273158705459e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.49it/s]                                               {'loss': 1.9882, 'grad_norm': 4.15313720703125, 'learning_rate': 9.237006350907514e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.49it/s]                                               {'loss': 2.2653, 'grad_norm': 3.5806925296783447, 'learning_rate': 9.03173954310957e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.49it/s] 44%|████▍     | 33/75 [00:01<00:01, 24.28it/s]                                               {'loss': 2.1714, 'grad_norm': 3.8264529705047607, 'learning_rate': 8.826472735311626e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.28it/s]                                               {'loss': 2.3014, 'grad_norm': 4.080121994018555, 'learning_rate': 8.621205927513681e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.28it/s]                                               {'loss': 1.9053, 'grad_norm': 4.6295294761657715, 'learning_rate': 8.415939119715736e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.28it/s] 48%|████▊     | 36/75 [00:01<00:01, 24.14it/s]                                               {'loss': 2.3729, 'grad_norm': 5.193654537200928, 'learning_rate': 8.21067231191779e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.14it/s]                                               {'loss': 2.0501, 'grad_norm': 3.335702419281006, 'learning_rate': 8.005405504119846e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.14it/s]                                               {'loss': 2.1597, 'grad_norm': 5.492757797241211, 'learning_rate': 7.800138696321902e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.14it/s] 52%|█████▏    | 39/75 [00:01<00:01, 24.47it/s]                                               {'loss': 2.2715, 'grad_norm': 3.86251163482666, 'learning_rate': 7.594871888523957e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.47it/s]                                               {'loss': 2.1108, 'grad_norm': 3.7383270263671875, 'learning_rate': 7.389605080726012e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.47it/s]                                               {'loss': 2.221, 'grad_norm': 5.3871169090271, 'learning_rate': 7.184338272928068e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.47it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.07it/s]                                               {'loss': 2.2847, 'grad_norm': 4.310550212860107, 'learning_rate': 6.979071465130123e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.07it/s]                                               {'loss': 2.2598, 'grad_norm': 3.817807674407959, 'learning_rate': 6.773804657332177e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.07it/s]                                               {'loss': 2.0012, 'grad_norm': 3.654327630996704, 'learning_rate': 6.568537849534233e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.07it/s]                                               {'loss': 2.3335, 'grad_norm': 8.77353572845459, 'learning_rate': 6.363271041736288e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.07it/s] 61%|██████▏   | 46/75 [00:01<00:01, 25.94it/s]                                               {'loss': 2.0034, 'grad_norm': 3.5494439601898193, 'learning_rate': 6.158004233938344e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.94it/s]                                               {'loss': 2.2815, 'grad_norm': 3.533247232437134, 'learning_rate': 5.952737426140398e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.94it/s]                                               {'loss': 2.1449, 'grad_norm': 2.935913324356079, 'learning_rate': 5.747470618342454e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.94it/s] 65%|██████▌   | 49/75 [00:02<00:01, 24.01it/s]                                               {'loss': 2.0751, 'grad_norm': 3.1317734718322754, 'learning_rate': 5.542203810544509e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 24.01it/s]                                               {'loss': 2.1209, 'grad_norm': 3.809084177017212, 'learning_rate': 5.336937002746564e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 24.01it/s]                                               {'loss': 2.2801, 'grad_norm': 4.890101909637451, 'learning_rate': 5.131670194948619e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 24.01it/s] 69%|██████▉   | 52/75 [00:02<00:00, 24.07it/s]                                               {'loss': 2.0509, 'grad_norm': 2.820020914077759, 'learning_rate': 4.926403387150675e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.07it/s]                                               {'loss': 2.5442, 'grad_norm': 4.431668281555176, 'learning_rate': 4.7211365793527297e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.07it/s]                                               {'loss': 2.1451, 'grad_norm': 3.666163444519043, 'learning_rate': 4.515869771554785e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.07it/s] 73%|███████▎  | 55/75 [00:02<00:00, 24.34it/s]                                               {'loss': 2.2111, 'grad_norm': 4.863759994506836, 'learning_rate': 4.3106029637568404e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.34it/s]                                               {'loss': 2.0677, 'grad_norm': 3.178941488265991, 'learning_rate': 4.105336155958895e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.34it/s]                                               {'loss': 2.0996, 'grad_norm': 3.624232769012451, 'learning_rate': 3.900069348160951e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.34it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.92it/s]                                               {'loss': 1.9107, 'grad_norm': 5.93219518661499, 'learning_rate': 3.694802540363006e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.92it/s]                                               {'loss': 2.293, 'grad_norm': 2.953021287918091, 'learning_rate': 3.489535732565061e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.92it/s]                                               {'loss': 1.8575, 'grad_norm': 10.45165729522705, 'learning_rate': 3.2842689247671166e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.92it/s] 81%|████████▏ | 61/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.1522, 'grad_norm': 3.496373176574707, 'learning_rate': 3.079002116969172e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.3132, 'grad_norm': 3.918260097503662, 'learning_rate': 2.873735309171227e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.2222, 'grad_norm': 4.388809680938721, 'learning_rate': 2.668468501373282e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.33it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.43it/s]                                               {'loss': 2.3514, 'grad_norm': 3.195821523666382, 'learning_rate': 2.4632016935753375e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.43it/s]                                               {'loss': 2.0326, 'grad_norm': 4.016045093536377, 'learning_rate': 2.2579348857773925e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.43it/s]                                               {'loss': 2.0741, 'grad_norm': 3.6961801052093506, 'learning_rate': 2.0526680779794476e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.43it/s] 89%|████████▉ | 67/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.0253, 'grad_norm': 4.597519397735596, 'learning_rate': 1.847401270181503e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.171, 'grad_norm': 5.485409736633301, 'learning_rate': 1.6421344623835583e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.1146, 'grad_norm': 3.605872392654419, 'learning_rate': 1.4368676545856135e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.96it/s] 93%|█████████▎| 70/75 [00:02<00:00, 24.41it/s]                                               {'loss': 2.0561, 'grad_norm': 3.2763853073120117, 'learning_rate': 1.2316008467876687e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.41it/s]                                               {'loss': 2.0369, 'grad_norm': 4.071755886077881, 'learning_rate': 1.0263340389897238e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.41it/s]                                               {'loss': 2.3221, 'grad_norm': 3.419975519180298, 'learning_rate': 8.210672311917792e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.41it/s] 97%|█████████▋| 73/75 [00:02<00:00, 24.21it/s]                                               {'loss': 2.0503, 'grad_norm': 3.3645753860473633, 'learning_rate': 6.158004233938344e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.21it/s]                                               {'loss': 2.1921, 'grad_norm': 3.8670787811279297, 'learning_rate': 4.105336155958896e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.21it/s]                                               {'loss': 2.5513, 'grad_norm': 8.970646858215332, 'learning_rate': 2.052668077979448e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.21it/s]                                               {'train_runtime': 3.1641, 'train_samples_per_second': 357.134, 'train_steps_per_second': 23.704, 'train_loss': 2.206131935119629, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.21it/s]100%|██████████| 75/75 [00:03<00:00, 23.71it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:1
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2017, 'grad_norm': 4.091012477874756, 'learning_rate': 0.00015395010584845858, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 14.99it/s]  3%|▎         | 2/75 [00:00<00:03, 18.82it/s]                                              {'loss': 2.3322, 'grad_norm': 4.396890640258789, 'learning_rate': 0.00015189743777047914, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 18.82it/s]                                              {'loss': 2.4294, 'grad_norm': 4.555092811584473, 'learning_rate': 0.0001498447696924997, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 18.82it/s]                                              {'loss': 2.5517, 'grad_norm': 5.29573392868042, 'learning_rate': 0.00014779210161452023, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 18.82it/s]  7%|▋         | 5/75 [00:00<00:03, 20.89it/s]                                              {'loss': 2.3646, 'grad_norm': 4.4677324295043945, 'learning_rate': 0.0001457394335365408, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 20.89it/s]                                              {'loss': 2.2292, 'grad_norm': 4.219328880310059, 'learning_rate': 0.00014368676545856136, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 20.89it/s]                                              {'loss': 2.4077, 'grad_norm': 5.511266231536865, 'learning_rate': 0.0001416340973805819, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 20.89it/s] 11%|█         | 8/75 [00:00<00:03, 21.28it/s]                                              {'loss': 2.2706, 'grad_norm': 4.326283931732178, 'learning_rate': 0.00013958142930260245, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 21.28it/s]                                              {'loss': 2.5343, 'grad_norm': 4.550136089324951, 'learning_rate': 0.00013752876122462298, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 21.28it/s]                                              {'loss': 2.571, 'grad_norm': 4.464115619659424, 'learning_rate': 0.00013547609314664354, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 21.28it/s] 15%|█▍        | 11/75 [00:00<00:03, 20.95it/s]                                               {'loss': 2.1787, 'grad_norm': 4.6729512214660645, 'learning_rate': 0.0001334234250686641, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 20.95it/s]                                               {'loss': 2.2579, 'grad_norm': 4.228636741638184, 'learning_rate': 0.00013137075699068467, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:03, 20.95it/s]                                               {'loss': 2.4322, 'grad_norm': 4.463553428649902, 'learning_rate': 0.0001293180889127052, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 20.95it/s] 19%|█▊        | 14/75 [00:00<00:02, 22.24it/s]                                               {'loss': 2.417, 'grad_norm': 5.636630058288574, 'learning_rate': 0.00012726542083472576, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 22.24it/s]                                               {'loss': 2.3452, 'grad_norm': 10.609285354614258, 'learning_rate': 0.00012521275275674632, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 22.24it/s]                                               {'loss': 2.3027, 'grad_norm': 3.324476957321167, 'learning_rate': 0.00012316008467876688, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 22.24it/s]                                               {'loss': 2.405, 'grad_norm': 4.745887756347656, 'learning_rate': 0.00012110741660078741, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 22.24it/s] 24%|██▍       | 18/75 [00:00<00:02, 24.72it/s]                                               {'loss': 2.2684, 'grad_norm': 4.532596588134766, 'learning_rate': 0.00011905474852280796, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.72it/s]                                               {'loss': 2.2299, 'grad_norm': 4.725655555725098, 'learning_rate': 0.00011700208044482852, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.72it/s]                                               {'loss': 2.073, 'grad_norm': 4.408639907836914, 'learning_rate': 0.00011494941236684908, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.72it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.41it/s]                                               {'loss': 2.305, 'grad_norm': 4.16187047958374, 'learning_rate': 0.00011289674428886962, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.41it/s]                                               {'loss': 2.3211, 'grad_norm': 4.0100908279418945, 'learning_rate': 0.00011084407621089018, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.41it/s]                                               {'loss': 2.5182, 'grad_norm': 5.49062967300415, 'learning_rate': 0.00010879140813291072, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.41it/s] 32%|███▏      | 24/75 [00:01<00:02, 24.05it/s]                                               {'loss': 2.1856, 'grad_norm': 4.3520827293396, 'learning_rate': 0.00010673874005493128, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 24.05it/s]                                               {'loss': 2.1698, 'grad_norm': 3.934100389480591, 'learning_rate': 0.00010468607197695184, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.05it/s]                                               {'loss': 2.2924, 'grad_norm': 4.21530294418335, 'learning_rate': 0.00010263340389897238, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.05it/s] 36%|███▌      | 27/75 [00:01<00:02, 23.08it/s]                                               {'loss': 2.223, 'grad_norm': 4.044454574584961, 'learning_rate': 0.00010058073582099294, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 23.08it/s]                                               {'loss': 2.3037, 'grad_norm': 3.8757846355438232, 'learning_rate': 9.85280677430135e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 23.08it/s]                                               {'loss': 2.2254, 'grad_norm': 5.0237603187561035, 'learning_rate': 9.647539966503405e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 23.08it/s]                                               {'loss': 2.3597, 'grad_norm': 9.054747581481934, 'learning_rate': 9.442273158705459e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.08it/s] 41%|████▏     | 31/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.1974, 'grad_norm': 3.4790923595428467, 'learning_rate': 9.237006350907514e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.2974, 'grad_norm': 4.8250732421875, 'learning_rate': 9.03173954310957e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.1639, 'grad_norm': 4.478788375854492, 'learning_rate': 8.826472735311626e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.00it/s] 45%|████▌     | 34/75 [00:01<00:01, 23.76it/s]                                               {'loss': 2.2713, 'grad_norm': 4.965331554412842, 'learning_rate': 8.621205927513681e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 23.76it/s]                                               {'loss': 2.1492, 'grad_norm': 4.3935041427612305, 'learning_rate': 8.415939119715736e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 23.76it/s]                                               {'loss': 2.2542, 'grad_norm': 4.745696067810059, 'learning_rate': 8.21067231191779e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 23.76it/s] 49%|████▉     | 37/75 [00:01<00:01, 23.54it/s]                                               {'loss': 2.2653, 'grad_norm': 5.109418869018555, 'learning_rate': 8.005405504119846e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 23.54it/s]                                               {'loss': 2.2864, 'grad_norm': 3.749537944793701, 'learning_rate': 7.800138696321902e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 23.54it/s]                                               {'loss': 2.1851, 'grad_norm': 5.275826930999756, 'learning_rate': 7.594871888523957e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.54it/s] 53%|█████▎    | 40/75 [00:01<00:01, 23.20it/s]                                               {'loss': 2.176, 'grad_norm': 3.444620132446289, 'learning_rate': 7.389605080726012e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.20it/s]                                               {'loss': 2.2348, 'grad_norm': 4.004769325256348, 'learning_rate': 7.184338272928068e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 23.20it/s]                                               {'loss': 2.2765, 'grad_norm': 3.981978416442871, 'learning_rate': 6.979071465130123e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 23.20it/s] 57%|█████▋    | 43/75 [00:01<00:01, 23.70it/s]                                               {'loss': 2.3352, 'grad_norm': 4.250670433044434, 'learning_rate': 6.773804657332177e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 23.70it/s]                                               {'loss': 2.2461, 'grad_norm': 3.5289206504821777, 'learning_rate': 6.568537849534233e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 23.70it/s]                                               {'loss': 2.1698, 'grad_norm': 8.23728084564209, 'learning_rate': 6.363271041736288e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 23.70it/s]                                               {'loss': 2.3274, 'grad_norm': 3.8188390731811523, 'learning_rate': 6.158004233938344e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 23.70it/s] 63%|██████▎   | 47/75 [00:01<00:01, 25.57it/s]                                               {'loss': 2.4017, 'grad_norm': 4.303342342376709, 'learning_rate': 5.952737426140398e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.57it/s]                                               {'loss': 2.371, 'grad_norm': 4.852607250213623, 'learning_rate': 5.747470618342454e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 25.57it/s]                                               {'loss': 2.4015, 'grad_norm': 4.418096542358398, 'learning_rate': 5.542203810544509e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 25.57it/s] 67%|██████▋   | 50/75 [00:02<00:01, 24.82it/s]                                               {'loss': 2.2353, 'grad_norm': 4.1527485847473145, 'learning_rate': 5.336937002746564e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 24.82it/s]                                               {'loss': 2.0792, 'grad_norm': 3.9314193725585938, 'learning_rate': 5.131670194948619e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 24.82it/s]                                               {'loss': 2.1631, 'grad_norm': 4.635509014129639, 'learning_rate': 4.926403387150675e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.82it/s] 71%|███████   | 53/75 [00:02<00:00, 24.74it/s]                                               {'loss': 2.0996, 'grad_norm': 3.834153413772583, 'learning_rate': 4.7211365793527297e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.74it/s]                                               {'loss': 2.2945, 'grad_norm': 4.094478130340576, 'learning_rate': 4.515869771554785e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.74it/s]                                               {'loss': 1.9699, 'grad_norm': 3.2597057819366455, 'learning_rate': 4.3106029637568404e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.74it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.0262, 'grad_norm': 4.105899333953857, 'learning_rate': 4.105336155958895e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.23it/s]                                               {'loss': 1.9687, 'grad_norm': 4.289809226989746, 'learning_rate': 3.900069348160951e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.2929, 'grad_norm': 3.9376065731048584, 'learning_rate': 3.694802540363006e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.23it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.10it/s]                                               {'loss': 2.3143, 'grad_norm': 4.697829246520996, 'learning_rate': 3.489535732565061e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.10it/s]                                               {'loss': 2.0587, 'grad_norm': 12.754384994506836, 'learning_rate': 3.2842689247671166e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.10it/s]                                               {'loss': 2.3355, 'grad_norm': 4.242899417877197, 'learning_rate': 3.079002116969172e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.10it/s] 83%|████████▎ | 62/75 [00:02<00:00, 25.68it/s]                                               {'loss': 2.2589, 'grad_norm': 4.036407947540283, 'learning_rate': 2.873735309171227e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.68it/s]                                               {'loss': 2.2553, 'grad_norm': 3.8134403228759766, 'learning_rate': 2.668468501373282e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.68it/s]                                               {'loss': 2.258, 'grad_norm': 3.4372611045837402, 'learning_rate': 2.4632016935753375e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.68it/s] 87%|████████▋ | 65/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.1822, 'grad_norm': 3.1856861114501953, 'learning_rate': 2.2579348857773925e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.1149, 'grad_norm': 4.357986927032471, 'learning_rate': 2.0526680779794476e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.2159, 'grad_norm': 4.020253658294678, 'learning_rate': 1.847401270181503e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.46it/s] 91%|█████████ | 68/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.1141, 'grad_norm': 3.654195785522461, 'learning_rate': 1.6421344623835583e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.83it/s]                                               {'loss': 1.9252, 'grad_norm': 3.1017673015594482, 'learning_rate': 1.4368676545856135e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.0667, 'grad_norm': 4.451015472412109, 'learning_rate': 1.2316008467876687e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.83it/s] 95%|█████████▍| 71/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.1958, 'grad_norm': 3.8338398933410645, 'learning_rate': 1.0263340389897238e-05, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.2705, 'grad_norm': 3.9522228240966797, 'learning_rate': 8.210672311917792e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.3124, 'grad_norm': 4.782649993896484, 'learning_rate': 6.158004233938344e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 24.83it/s] 99%|█████████▊| 74/75 [00:03<00:00, 24.92it/s]                                               {'loss': 2.0761, 'grad_norm': 4.027254581451416, 'learning_rate': 4.105336155958896e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.92it/s]                                               {'loss': 2.6119, 'grad_norm': 14.095220565795898, 'learning_rate': 2.052668077979448e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.92it/s]                                               {'train_runtime': 3.1929, 'train_samples_per_second': 353.913, 'train_steps_per_second': 23.49, 'train_loss': 2.2588269329071045, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.92it/s]100%|██████████| 75/75 [00:03<00:00, 23.49it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1163, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(935, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(827, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1010, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1237, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1022, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1143, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1181, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1094, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(986, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1311, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(853, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 62.71it/s]  3%|▎         | 14/471 [00:00<00:08, 56.59it/s]  4%|▍         | 20/471 [00:00<00:08, 54.94it/s]  6%|▌         | 26/471 [00:00<00:08, 54.34it/s]  7%|▋         | 32/471 [00:00<00:08, 53.64it/s]  8%|▊         | 38/471 [00:00<00:08, 52.01it/s]  9%|▉         | 44/471 [00:00<00:08, 52.66it/s] 11%|█         | 50/471 [00:00<00:07, 52.91it/s] 12%|█▏        | 56/471 [00:01<00:07, 52.38it/s] 13%|█▎        | 62/471 [00:01<00:07, 51.76it/s] 14%|█▍        | 68/471 [00:01<00:07, 51.73it/s] 16%|█▌        | 74/471 [00:01<00:07, 51.79it/s] 17%|█▋        | 80/471 [00:01<00:07, 50.92it/s] 18%|█▊        | 86/471 [00:01<00:07, 51.69it/s] 20%|█▉        | 92/471 [00:01<00:07, 50.74it/s] 21%|██        | 98/471 [00:01<00:07, 50.30it/s] 22%|██▏       | 104/471 [00:01<00:07, 50.92it/s] 23%|██▎       | 110/471 [00:02<00:07, 50.77it/s] 25%|██▍       | 116/471 [00:02<00:07, 50.29it/s] 26%|██▌       | 122/471 [00:02<00:06, 50.33it/s] 27%|██▋       | 128/471 [00:02<00:06, 50.30it/s] 28%|██▊       | 134/471 [00:02<00:06, 50.90it/s] 30%|██▉       | 140/471 [00:02<00:06, 50.33it/s] 31%|███       | 146/471 [00:02<00:06, 50.85it/s] 32%|███▏      | 152/471 [00:02<00:06, 50.42it/s] 34%|███▎      | 158/471 [00:03<00:06, 50.78it/s] 35%|███▍      | 164/471 [00:03<00:06, 50.30it/s] 36%|███▌      | 170/471 [00:03<00:06, 48.87it/s] 37%|███▋      | 175/471 [00:03<00:06, 49.07it/s] 38%|███▊      | 181/471 [00:03<00:05, 49.45it/s] 40%|███▉      | 187/471 [00:03<00:05, 50.29it/s] 41%|████      | 193/471 [00:03<00:05, 50.61it/s] 42%|████▏     | 199/471 [00:03<00:05, 50.44it/s] 44%|████▎     | 205/471 [00:03<00:05, 51.11it/s] 45%|████▍     | 211/471 [00:04<00:05, 51.23it/s] 46%|████▌     | 217/471 [00:04<00:04, 51.18it/s] 47%|████▋     | 223/471 [00:04<00:04, 49.96it/s] 49%|████▊     | 229/471 [00:04<00:04, 50.71it/s] 50%|████▉     | 235/471 [00:04<00:04, 50.93it/s] 51%|█████     | 241/471 [00:04<00:04, 50.26it/s] 52%|█████▏    | 247/471 [00:04<00:04, 49.95it/s] 54%|█████▎    | 253/471 [00:04<00:04, 50.72it/s] 55%|█████▍    | 259/471 [00:05<00:04, 49.42it/s] 56%|█████▌    | 264/471 [00:05<00:04, 48.67it/s] 57%|█████▋    | 269/471 [00:05<00:04, 48.11it/s] 58%|█████▊    | 274/471 [00:05<00:04, 48.17it/s] 59%|█████▉    | 279/471 [00:05<00:04, 47.61it/s] 61%|██████    | 285/471 [00:05<00:03, 48.71it/s] 62%|██████▏   | 290/471 [00:05<00:03, 48.66it/s] 63%|██████▎   | 296/471 [00:05<00:03, 50.11it/s] 64%|██████▍   | 302/471 [00:05<00:03, 50.94it/s] 65%|██████▌   | 308/471 [00:06<00:03, 51.33it/s] 67%|██████▋   | 314/471 [00:06<00:03, 51.03it/s] 68%|██████▊   | 320/471 [00:06<00:02, 50.53it/s] 69%|██████▉   | 326/471 [00:06<00:02, 50.61it/s] 70%|███████   | 332/471 [00:06<00:02, 50.30it/s] 72%|███████▏  | 338/471 [00:06<00:02, 50.86it/s] 73%|███████▎  | 344/471 [00:06<00:02, 50.77it/s] 74%|███████▍  | 350/471 [00:06<00:02, 49.58it/s] 75%|███████▌  | 355/471 [00:07<00:02, 49.58it/s] 77%|███████▋  | 361/471 [00:07<00:02, 50.02it/s] 78%|███████▊  | 367/471 [00:07<00:02, 49.08it/s] 79%|███████▉  | 373/471 [00:07<00:01, 50.19it/s] 80%|████████  | 379/471 [00:07<00:01, 50.39it/s] 82%|████████▏ | 385/471 [00:07<00:01, 49.37it/s] 83%|████████▎ | 391/471 [00:07<00:01, 50.14it/s] 84%|████████▍ | 397/471 [00:07<00:01, 49.94it/s] 86%|████████▌ | 403/471 [00:07<00:01, 50.82it/s] 87%|████████▋ | 409/471 [00:08<00:01, 51.40it/s] 88%|████████▊ | 415/471 [00:08<00:01, 51.72it/s] 89%|████████▉ | 421/471 [00:08<00:00, 51.83it/s] 91%|█████████ | 427/471 [00:08<00:00, 51.04it/s] 92%|█████████▏| 433/471 [00:08<00:00, 50.51it/s] 93%|█████████▎| 439/471 [00:08<00:00, 50.61it/s] 94%|█████████▍| 445/471 [00:08<00:00, 50.62it/s] 96%|█████████▌| 451/471 [00:08<00:00, 51.24it/s] 97%|█████████▋| 457/471 [00:09<00:00, 51.63it/s] 98%|█████████▊| 463/471 [00:09<00:00, 51.79it/s]100%|█████████▉| 469/471 [00:09<00:00, 51.38it/s]100%|██████████| 471/471 [00:09<00:00, 50.77it/s]
{'eval_loss': 2.3190808296203613, 'eval_model_preparation_time': 0.003, 'eval_acc': 0.3842272968667021, 'eval_runtime': 9.2981, 'eval_samples_per_second': 810.061, 'eval_steps_per_second': 50.656}
ROUND:10
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3106, 'grad_norm': 4.382294654846191, 'learning_rate': 0.00015, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 20.32it/s]                                              {'loss': 2.3799, 'grad_norm': 3.6171324253082275, 'learning_rate': 0.000148, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 21.80it/s]  4%|▍         | 3/75 [00:00<00:03, 22.41it/s]                                              {'loss': 2.3022, 'grad_norm': 4.21314001083374, 'learning_rate': 0.000146, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 22.41it/s]                                              {'loss': 2.4924, 'grad_norm': 4.240781784057617, 'learning_rate': 0.00014399999999999998, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 22.41it/s]                                              {'loss': 2.3741, 'grad_norm': 5.221978664398193, 'learning_rate': 0.00014199999999999998, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.41it/s]  8%|▊         | 6/75 [00:00<00:02, 24.37it/s]                                              {'loss': 2.3834, 'grad_norm': 4.5169477462768555, 'learning_rate': 0.00014, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.37it/s]                                              {'loss': 2.3602, 'grad_norm': 3.94513201713562, 'learning_rate': 0.000138, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.37it/s]                                              {'loss': 2.2438, 'grad_norm': 4.052590370178223, 'learning_rate': 0.00013599999999999997, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.37it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.06it/s]                                              {'loss': 2.3306, 'grad_norm': 4.458874225616455, 'learning_rate': 0.00013399999999999998, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.06it/s]                                              {'loss': 2.2895, 'grad_norm': 4.45964241027832, 'learning_rate': 0.00013199999999999998, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.06it/s]                                               {'loss': 2.3344, 'grad_norm': 3.838197708129883, 'learning_rate': 0.00013, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.06it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.74it/s]                                               {'loss': 2.3166, 'grad_norm': 3.2519900798797607, 'learning_rate': 0.000128, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.74it/s]                                               {'loss': 2.2713, 'grad_norm': 5.0997395515441895, 'learning_rate': 0.00012599999999999997, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.74it/s]                                               {'loss': 2.273, 'grad_norm': 3.553175449371338, 'learning_rate': 0.00012399999999999998, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.74it/s] 20%|██        | 15/75 [00:00<00:02, 25.04it/s]                                               {'loss': 2.3661, 'grad_norm': 9.639874458312988, 'learning_rate': 0.000122, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.04it/s]                                               {'loss': 2.4006, 'grad_norm': 4.066163539886475, 'learning_rate': 0.00011999999999999999, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.04it/s]                                               {'loss': 2.2277, 'grad_norm': 4.106114864349365, 'learning_rate': 0.00011799999999999998, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.04it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.27it/s]                                               {'loss': 2.106, 'grad_norm': 3.6354739665985107, 'learning_rate': 0.00011599999999999999, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.27it/s]                                               {'loss': 2.1544, 'grad_norm': 3.742978811264038, 'learning_rate': 0.00011399999999999999, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.27it/s]                                               {'loss': 2.2178, 'grad_norm': 4.255153179168701, 'learning_rate': 0.000112, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.27it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.74it/s]                                               {'loss': 2.2709, 'grad_norm': 4.270434379577637, 'learning_rate': 0.00010999999999999998, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.74it/s]                                               {'loss': 2.2379, 'grad_norm': 3.7261571884155273, 'learning_rate': 0.00010799999999999998, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.74it/s]                                               {'loss': 2.1589, 'grad_norm': 3.2975213527679443, 'learning_rate': 0.00010599999999999999, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.74it/s] 32%|███▏      | 24/75 [00:00<00:02, 25.03it/s]                                               {'loss': 2.1965, 'grad_norm': 3.893453598022461, 'learning_rate': 0.000104, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 25.03it/s]                                               {'loss': 2.3099, 'grad_norm': 3.8908305168151855, 'learning_rate': 0.000102, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:01, 25.03it/s]                                               {'loss': 2.3947, 'grad_norm': 5.508078575134277, 'learning_rate': 9.999999999999999e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.03it/s] 36%|███▌      | 27/75 [00:01<00:01, 25.09it/s]                                               {'loss': 2.0374, 'grad_norm': 3.0397751331329346, 'learning_rate': 9.799999999999998e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.09it/s]                                               {'loss': 2.3285, 'grad_norm': 4.8226423263549805, 'learning_rate': 9.599999999999999e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.09it/s]                                               {'loss': 2.3578, 'grad_norm': 3.6594865322113037, 'learning_rate': 9.4e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.09it/s]                                               {'loss': 2.545, 'grad_norm': 11.282306671142578, 'learning_rate': 9.199999999999999e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.09it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.61it/s]                                               {'loss': 2.1045, 'grad_norm': 4.047786235809326, 'learning_rate': 8.999999999999999e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.61it/s]                                               {'loss': 2.1998, 'grad_norm': 3.4631710052490234, 'learning_rate': 8.8e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.61it/s]                                               {'loss': 2.2543, 'grad_norm': 3.536102771759033, 'learning_rate': 8.6e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.61it/s] 45%|████▌     | 34/75 [00:01<00:01, 26.10it/s]                                               {'loss': 2.2136, 'grad_norm': 3.5016326904296875, 'learning_rate': 8.4e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.10it/s]                                               {'loss': 2.1767, 'grad_norm': 4.035408020019531, 'learning_rate': 8.199999999999999e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.10it/s]                                               {'loss': 2.07, 'grad_norm': 3.7131946086883545, 'learning_rate': 7.999999999999999e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.10it/s] 49%|████▉     | 37/75 [00:01<00:01, 24.39it/s]                                               {'loss': 2.1115, 'grad_norm': 4.9436354637146, 'learning_rate': 7.8e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.39it/s]                                               {'loss': 2.1189, 'grad_norm': 4.335338115692139, 'learning_rate': 7.6e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.39it/s]                                               {'loss': 2.2981, 'grad_norm': 4.212154388427734, 'learning_rate': 7.4e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.39it/s] 53%|█████▎    | 40/75 [00:01<00:01, 24.51it/s]                                               {'loss': 2.334, 'grad_norm': 4.124710559844971, 'learning_rate': 7.199999999999999e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.51it/s]                                               {'loss': 2.2339, 'grad_norm': 3.401794910430908, 'learning_rate': 7e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.51it/s]                                               {'loss': 2.3436, 'grad_norm': 3.9320638179779053, 'learning_rate': 6.799999999999999e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.51it/s] 57%|█████▋    | 43/75 [00:01<00:01, 24.13it/s]                                               {'loss': 2.4481, 'grad_norm': 3.9084603786468506, 'learning_rate': 6.599999999999999e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.13it/s]                                               {'loss': 2.134, 'grad_norm': 3.265159845352173, 'learning_rate': 6.4e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.13it/s]                                               {'loss': 2.3628, 'grad_norm': 11.214179992675781, 'learning_rate': 6.199999999999999e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.13it/s]                                               {'loss': 2.138, 'grad_norm': 3.3822529315948486, 'learning_rate': 5.9999999999999995e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.13it/s] 63%|██████▎   | 47/75 [00:01<00:01, 25.63it/s]                                               {'loss': 2.0944, 'grad_norm': 3.419970989227295, 'learning_rate': 5.7999999999999994e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.63it/s]                                               {'loss': 2.1744, 'grad_norm': 3.7048704624176025, 'learning_rate': 5.6e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.63it/s]                                               {'loss': 2.1748, 'grad_norm': 3.6590209007263184, 'learning_rate': 5.399999999999999e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.63it/s] 67%|██████▋   | 50/75 [00:02<00:00, 25.15it/s]                                               {'loss': 2.1969, 'grad_norm': 3.4782071113586426, 'learning_rate': 5.2e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.15it/s]                                               {'loss': 2.3039, 'grad_norm': 3.824235677719116, 'learning_rate': 4.9999999999999996e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.15it/s]                                               {'loss': 2.311, 'grad_norm': 4.095242023468018, 'learning_rate': 4.7999999999999994e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.15it/s] 71%|███████   | 53/75 [00:02<00:00, 25.06it/s]                                               {'loss': 2.3362, 'grad_norm': 5.050607204437256, 'learning_rate': 4.599999999999999e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.06it/s]                                               {'loss': 2.0886, 'grad_norm': 3.64658522605896, 'learning_rate': 4.4e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.06it/s]                                               {'loss': 2.0769, 'grad_norm': 3.3150532245635986, 'learning_rate': 4.2e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.06it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.2807, 'grad_norm': 4.273777484893799, 'learning_rate': 3.9999999999999996e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.218, 'grad_norm': 3.330946683883667, 'learning_rate': 3.8e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.1932, 'grad_norm': 3.657644271850586, 'learning_rate': 3.5999999999999994e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.14it/s] 79%|███████▊  | 59/75 [00:02<00:00, 24.95it/s]                                               {'loss': 2.306, 'grad_norm': 7.074296951293945, 'learning_rate': 3.399999999999999e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.95it/s]                                               {'loss': 2.2082, 'grad_norm': 9.104862213134766, 'learning_rate': 3.2e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.95it/s]                                               {'loss': 2.2668, 'grad_norm': 4.484603404998779, 'learning_rate': 2.9999999999999997e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.95it/s]                                               {'loss': 2.3019, 'grad_norm': 3.7542808055877686, 'learning_rate': 2.8e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 24.95it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.41it/s]                                               {'loss': 2.0882, 'grad_norm': 3.475337505340576, 'learning_rate': 2.6e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.41it/s]                                               {'loss': 2.1212, 'grad_norm': 4.124328136444092, 'learning_rate': 2.3999999999999997e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.41it/s]                                               {'loss': 2.2828, 'grad_norm': 4.0367560386657715, 'learning_rate': 2.2e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.41it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.13it/s]                                               {'loss': 2.1742, 'grad_norm': 3.808128595352173, 'learning_rate': 1.9999999999999998e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.13it/s]                                               {'loss': 2.2217, 'grad_norm': 3.514918327331543, 'learning_rate': 1.7999999999999997e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.13it/s]                                               {'loss': 2.2536, 'grad_norm': 4.184177875518799, 'learning_rate': 1.6e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 26.13it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.02it/s]                                               {'loss': 2.2229, 'grad_norm': 3.8514351844787598, 'learning_rate': 1.4e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 26.02it/s]                                               {'loss': 2.071, 'grad_norm': 3.7592010498046875, 'learning_rate': 1.1999999999999999e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 26.02it/s]                                               {'loss': 1.9948, 'grad_norm': 3.272301197052002, 'learning_rate': 9.999999999999999e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 26.02it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.72it/s]                                               {'loss': 2.288, 'grad_norm': 5.066278457641602, 'learning_rate': 8e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.72it/s]                                               {'loss': 2.0423, 'grad_norm': 3.244880437850952, 'learning_rate': 5.999999999999999e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.72it/s]                                               {'loss': 2.2058, 'grad_norm': 3.035078287124634, 'learning_rate': 4e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.72it/s]                                               {'loss': 2.6073, 'grad_norm': 16.861257553100586, 'learning_rate': 2e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.72it/s]                                               {'train_runtime': 3.0538, 'train_samples_per_second': 370.033, 'train_steps_per_second': 24.56, 'train_loss': 2.2482644589742025, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.72it/s]100%|██████████| 75/75 [00:03<00:00, 24.56it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:5
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2879, 'grad_norm': 5.549983501434326, 'learning_rate': 0.00015, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 21.45it/s]                                              {'loss': 2.2716, 'grad_norm': 4.382431983947754, 'learning_rate': 0.000148, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 22.46it/s]  4%|▍         | 3/75 [00:00<00:03, 23.12it/s]                                              {'loss': 2.3544, 'grad_norm': 3.704713821411133, 'learning_rate': 0.000146, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.12it/s]                                              {'loss': 2.501, 'grad_norm': 4.226107120513916, 'learning_rate': 0.00014399999999999998, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 23.12it/s]                                              {'loss': 2.0207, 'grad_norm': 2.8761990070343018, 'learning_rate': 0.00014199999999999998, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 23.12it/s]  8%|▊         | 6/75 [00:00<00:02, 23.96it/s]                                              {'loss': 2.3057, 'grad_norm': 5.2002081871032715, 'learning_rate': 0.00014, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.96it/s]                                              {'loss': 2.2029, 'grad_norm': 5.849206447601318, 'learning_rate': 0.000138, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.96it/s]                                              {'loss': 2.2145, 'grad_norm': 3.2091991901397705, 'learning_rate': 0.00013599999999999997, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.96it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.34it/s]                                              {'loss': 2.1974, 'grad_norm': 4.15956974029541, 'learning_rate': 0.00013399999999999998, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.34it/s]                                              {'loss': 2.3163, 'grad_norm': 3.4785444736480713, 'learning_rate': 0.00013199999999999998, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.34it/s]                                               {'loss': 2.1069, 'grad_norm': 6.126549243927002, 'learning_rate': 0.00013, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.34it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.00it/s]                                               {'loss': 2.2119, 'grad_norm': 4.862453460693359, 'learning_rate': 0.000128, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.00it/s]                                               {'loss': 2.295, 'grad_norm': 4.105574607849121, 'learning_rate': 0.00012599999999999997, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.00it/s]                                               {'loss': 2.3388, 'grad_norm': 4.692881107330322, 'learning_rate': 0.00012399999999999998, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.00it/s] 20%|██        | 15/75 [00:00<00:02, 25.66it/s]                                               {'loss': 2.9932, 'grad_norm': 13.738324165344238, 'learning_rate': 0.000122, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.66it/s]                                               {'loss': 2.291, 'grad_norm': 3.913179397583008, 'learning_rate': 0.00011999999999999999, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.66it/s]                                               {'loss': 2.2618, 'grad_norm': 4.160021781921387, 'learning_rate': 0.00011799999999999998, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.66it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.97it/s]                                               {'loss': 2.2044, 'grad_norm': 3.133256673812866, 'learning_rate': 0.00011599999999999999, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.97it/s]                                               {'loss': 2.1844, 'grad_norm': 3.6618597507476807, 'learning_rate': 0.00011399999999999999, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.97it/s]                                               {'loss': 2.1969, 'grad_norm': 3.7683401107788086, 'learning_rate': 0.000112, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.97it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.98it/s]                                               {'loss': 2.1777, 'grad_norm': 4.404592037200928, 'learning_rate': 0.00010999999999999998, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.98it/s]                                               {'loss': 2.2131, 'grad_norm': 4.3324055671691895, 'learning_rate': 0.00010799999999999998, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.98it/s]                                               {'loss': 2.197, 'grad_norm': 3.8186957836151123, 'learning_rate': 0.00010599999999999999, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.98it/s] 32%|███▏      | 24/75 [00:00<00:02, 25.12it/s]                                               {'loss': 2.1918, 'grad_norm': 3.811836004257202, 'learning_rate': 0.000104, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 25.12it/s]                                               {'loss': 2.2304, 'grad_norm': 5.294879913330078, 'learning_rate': 0.000102, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:01, 25.12it/s]                                               {'loss': 2.2062, 'grad_norm': 4.579820156097412, 'learning_rate': 9.999999999999999e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.12it/s] 36%|███▌      | 27/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.1629, 'grad_norm': 3.802769899368286, 'learning_rate': 9.799999999999998e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.1299, 'grad_norm': 3.3028876781463623, 'learning_rate': 9.599999999999999e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.00it/s]                                               {'loss': 1.9882, 'grad_norm': 3.155026912689209, 'learning_rate': 9.4e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.00it/s] 40%|████      | 30/75 [00:01<00:01, 25.99it/s]                                               {'loss': 2.5496, 'grad_norm': 13.559102058410645, 'learning_rate': 9.199999999999999e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.99it/s]                                               {'loss': 1.9851, 'grad_norm': 4.0897626876831055, 'learning_rate': 8.999999999999999e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.99it/s]                                               {'loss': 2.2595, 'grad_norm': 3.5011253356933594, 'learning_rate': 8.8e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.99it/s] 44%|████▍     | 33/75 [00:01<00:01, 24.83it/s]                                               {'loss': 2.1693, 'grad_norm': 3.786447286605835, 'learning_rate': 8.6e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.83it/s]                                               {'loss': 2.2969, 'grad_norm': 4.027384281158447, 'learning_rate': 8.4e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.83it/s]                                               {'loss': 1.9, 'grad_norm': 4.583399295806885, 'learning_rate': 8.199999999999999e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.83it/s] 48%|████▊     | 36/75 [00:01<00:01, 24.04it/s]                                               {'loss': 2.3678, 'grad_norm': 5.175987720489502, 'learning_rate': 7.999999999999999e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.04it/s]                                               {'loss': 2.0475, 'grad_norm': 3.3196942806243896, 'learning_rate': 7.8e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.04it/s]                                               {'loss': 2.1571, 'grad_norm': 5.449546813964844, 'learning_rate': 7.6e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.04it/s] 52%|█████▏    | 39/75 [00:01<00:01, 24.06it/s]                                               {'loss': 2.2735, 'grad_norm': 3.8609390258789062, 'learning_rate': 7.4e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.06it/s]                                               {'loss': 2.1074, 'grad_norm': 3.70505952835083, 'learning_rate': 7.199999999999999e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.06it/s]                                               {'loss': 2.2199, 'grad_norm': 5.437419414520264, 'learning_rate': 7e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.06it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.05it/s]                                               {'loss': 2.2837, 'grad_norm': 4.321491718292236, 'learning_rate': 6.799999999999999e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.05it/s]                                               {'loss': 2.2538, 'grad_norm': 3.814302682876587, 'learning_rate': 6.599999999999999e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.05it/s]                                               {'loss': 1.9976, 'grad_norm': 3.5968289375305176, 'learning_rate': 6.4e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.05it/s]                                               {'loss': 2.3309, 'grad_norm': 8.809673309326172, 'learning_rate': 6.199999999999999e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.05it/s] 61%|██████▏   | 46/75 [00:01<00:01, 25.84it/s]                                               {'loss': 2.0022, 'grad_norm': 3.4846324920654297, 'learning_rate': 5.9999999999999995e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.84it/s]                                               {'loss': 2.2773, 'grad_norm': 3.5084292888641357, 'learning_rate': 5.7999999999999994e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.84it/s]                                               {'loss': 2.142, 'grad_norm': 2.9251198768615723, 'learning_rate': 5.6e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.84it/s] 65%|██████▌   | 49/75 [00:01<00:01, 25.70it/s]                                               {'loss': 2.0738, 'grad_norm': 3.1194639205932617, 'learning_rate': 5.399999999999999e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.70it/s]                                               {'loss': 2.1166, 'grad_norm': 3.8054747581481934, 'learning_rate': 5.2e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.70it/s]                                               {'loss': 2.2808, 'grad_norm': 4.80210018157959, 'learning_rate': 4.9999999999999996e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.70it/s] 69%|██████▉   | 52/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.0499, 'grad_norm': 2.7950007915496826, 'learning_rate': 4.7999999999999994e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.5413, 'grad_norm': 4.384718418121338, 'learning_rate': 4.599999999999999e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.1435, 'grad_norm': 3.6569812297821045, 'learning_rate': 4.4e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.49it/s] 73%|███████▎  | 55/75 [00:02<00:00, 24.76it/s]                                               {'loss': 2.2073, 'grad_norm': 4.818863391876221, 'learning_rate': 4.2e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.76it/s]                                               {'loss': 2.067, 'grad_norm': 3.1273810863494873, 'learning_rate': 3.9999999999999996e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.76it/s]                                               {'loss': 2.096, 'grad_norm': 3.570040464401245, 'learning_rate': 3.8e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.76it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.95it/s]                                               {'loss': 1.9047, 'grad_norm': 5.909237384796143, 'learning_rate': 3.5999999999999994e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.95it/s]                                               {'loss': 2.2931, 'grad_norm': 2.9185309410095215, 'learning_rate': 3.399999999999999e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.95it/s]                                               {'loss': 1.8592, 'grad_norm': 10.464634895324707, 'learning_rate': 3.2e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.95it/s] 81%|████████▏ | 61/75 [00:02<00:00, 26.22it/s]                                               {'loss': 2.1474, 'grad_norm': 3.4841113090515137, 'learning_rate': 2.9999999999999997e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.22it/s]                                               {'loss': 2.3119, 'grad_norm': 3.918886661529541, 'learning_rate': 2.8e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.22it/s]                                               {'loss': 2.2193, 'grad_norm': 4.327240943908691, 'learning_rate': 2.6e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.22it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.20it/s]                                               {'loss': 2.3507, 'grad_norm': 3.21014404296875, 'learning_rate': 2.3999999999999997e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.20it/s]                                               {'loss': 2.033, 'grad_norm': 4.010159969329834, 'learning_rate': 2.2e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.20it/s]                                               {'loss': 2.0716, 'grad_norm': 3.654789447784424, 'learning_rate': 1.9999999999999998e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.20it/s] 89%|████████▉ | 67/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.0226, 'grad_norm': 4.563364505767822, 'learning_rate': 1.7999999999999997e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.1698, 'grad_norm': 5.437823295593262, 'learning_rate': 1.6e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.1111, 'grad_norm': 3.5560202598571777, 'learning_rate': 1.4e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.23it/s] 93%|█████████▎| 70/75 [00:02<00:00, 24.95it/s]                                               {'loss': 2.0536, 'grad_norm': 3.2330403327941895, 'learning_rate': 1.1999999999999999e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.95it/s]                                               {'loss': 2.0352, 'grad_norm': 4.053311347961426, 'learning_rate': 9.999999999999999e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.95it/s]                                               {'loss': 2.3243, 'grad_norm': 3.4011945724487305, 'learning_rate': 8e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.95it/s] 97%|█████████▋| 73/75 [00:02<00:00, 24.72it/s]                                               {'loss': 2.0474, 'grad_norm': 3.329181671142578, 'learning_rate': 5.999999999999999e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.72it/s]                                               {'loss': 2.1932, 'grad_norm': 3.8319995403289795, 'learning_rate': 4e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.72it/s]                                               {'loss': 2.55, 'grad_norm': 9.044084548950195, 'learning_rate': 2e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.72it/s]                                               {'train_runtime': 3.0918, 'train_samples_per_second': 365.481, 'train_steps_per_second': 24.258, 'train_loss': 2.201996301015218, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.72it/s]100%|██████████| 75/75 [00:03<00:00, 24.26it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:32
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2142, 'grad_norm': 4.474363327026367, 'learning_rate': 0.00015, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 17.94it/s]                                              {'loss': 2.491, 'grad_norm': 3.7218704223632812, 'learning_rate': 0.000148, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 21.35it/s]  4%|▍         | 3/75 [00:00<00:03, 22.52it/s]                                              {'loss': 2.3064, 'grad_norm': 4.29605770111084, 'learning_rate': 0.000146, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 22.52it/s]                                              {'loss': 2.346, 'grad_norm': 4.616189956665039, 'learning_rate': 0.00014399999999999998, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 22.52it/s]                                              {'loss': 2.3745, 'grad_norm': 3.725391149520874, 'learning_rate': 0.00014199999999999998, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.52it/s]  8%|▊         | 6/75 [00:00<00:03, 22.83it/s]                                              {'loss': 2.1677, 'grad_norm': 3.897714138031006, 'learning_rate': 0.00014, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 22.83it/s]                                              {'loss': 2.3655, 'grad_norm': 4.907357692718506, 'learning_rate': 0.000138, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 22.83it/s]                                              {'loss': 2.4547, 'grad_norm': 4.170423984527588, 'learning_rate': 0.00013599999999999997, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 22.83it/s] 12%|█▏        | 9/75 [00:00<00:02, 23.66it/s]                                              {'loss': 2.2683, 'grad_norm': 2.8895931243896484, 'learning_rate': 0.00013399999999999998, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.66it/s]                                              {'loss': 2.2652, 'grad_norm': 3.755699396133423, 'learning_rate': 0.00013199999999999998, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.66it/s]                                               {'loss': 2.2859, 'grad_norm': 3.734361171722412, 'learning_rate': 0.00013, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.66it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.61it/s]                                               {'loss': 2.416, 'grad_norm': 3.9342803955078125, 'learning_rate': 0.000128, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.61it/s]                                               {'loss': 2.2803, 'grad_norm': 4.691178321838379, 'learning_rate': 0.00012599999999999997, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.61it/s]                                               {'loss': 2.2528, 'grad_norm': 4.661704063415527, 'learning_rate': 0.00012399999999999998, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.61it/s] 20%|██        | 15/75 [00:00<00:02, 24.57it/s]                                               {'loss': 1.7919, 'grad_norm': 7.987016677856445, 'learning_rate': 0.000122, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.57it/s]                                               {'loss': 2.2549, 'grad_norm': 3.603111505508423, 'learning_rate': 0.00011999999999999999, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 24.57it/s]                                               {'loss': 2.2158, 'grad_norm': 3.598351240158081, 'learning_rate': 0.00011799999999999998, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.57it/s] 24%|██▍       | 18/75 [00:00<00:02, 24.23it/s]                                               {'loss': 2.1609, 'grad_norm': 3.721428394317627, 'learning_rate': 0.00011599999999999999, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.23it/s]                                               {'loss': 2.2495, 'grad_norm': 5.130579948425293, 'learning_rate': 0.00011399999999999999, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.23it/s]                                               {'loss': 2.3699, 'grad_norm': 4.153083801269531, 'learning_rate': 0.000112, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.23it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.81it/s]                                               {'loss': 2.064, 'grad_norm': 3.4872918128967285, 'learning_rate': 0.00010999999999999998, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.81it/s]                                               {'loss': 2.0799, 'grad_norm': 3.4962430000305176, 'learning_rate': 0.00010799999999999998, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.81it/s]                                               {'loss': 2.2515, 'grad_norm': 4.157898426055908, 'learning_rate': 0.00010599999999999999, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.81it/s] 32%|███▏      | 24/75 [00:00<00:02, 25.15it/s]                                               {'loss': 2.3626, 'grad_norm': 4.301078796386719, 'learning_rate': 0.000104, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 25.15it/s]                                               {'loss': 2.3504, 'grad_norm': 4.27714729309082, 'learning_rate': 0.000102, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:01, 25.15it/s]                                               {'loss': 2.349, 'grad_norm': 3.81893253326416, 'learning_rate': 9.999999999999999e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.15it/s] 36%|███▌      | 27/75 [00:01<00:01, 25.22it/s]                                               {'loss': 2.1625, 'grad_norm': 4.59785270690918, 'learning_rate': 9.799999999999998e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.22it/s]                                               {'loss': 2.1915, 'grad_norm': 5.594880104064941, 'learning_rate': 9.599999999999999e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.22it/s]                                               {'loss': 2.2153, 'grad_norm': 3.761296033859253, 'learning_rate': 9.4e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.22it/s]                                               {'loss': 1.7648, 'grad_norm': 8.814350128173828, 'learning_rate': 9.199999999999999e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.22it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.43it/s]                                               {'loss': 2.2721, 'grad_norm': 3.771636486053467, 'learning_rate': 8.999999999999999e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.43it/s]                                               {'loss': 2.0587, 'grad_norm': 3.4398701190948486, 'learning_rate': 8.8e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.43it/s]                                               {'loss': 2.4182, 'grad_norm': 4.231369972229004, 'learning_rate': 8.6e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.43it/s] 45%|████▌     | 34/75 [00:01<00:01, 25.88it/s]                                               {'loss': 2.1795, 'grad_norm': 3.9837844371795654, 'learning_rate': 8.4e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.88it/s]                                               {'loss': 2.088, 'grad_norm': 3.426948308944702, 'learning_rate': 8.199999999999999e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.88it/s]                                               {'loss': 2.2027, 'grad_norm': 4.727696895599365, 'learning_rate': 7.999999999999999e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.88it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.02it/s]                                               {'loss': 2.0198, 'grad_norm': 4.017587184906006, 'learning_rate': 7.8e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.02it/s]                                               {'loss': 2.2795, 'grad_norm': 4.136865615844727, 'learning_rate': 7.6e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.02it/s]                                               {'loss': 2.2841, 'grad_norm': 5.758593559265137, 'learning_rate': 7.4e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.02it/s] 53%|█████▎    | 40/75 [00:01<00:01, 24.77it/s]                                               {'loss': 2.3278, 'grad_norm': 3.849990129470825, 'learning_rate': 7.199999999999999e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.77it/s]                                               {'loss': 2.0821, 'grad_norm': 3.3602399826049805, 'learning_rate': 7e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.77it/s]                                               {'loss': 2.0663, 'grad_norm': 5.098223686218262, 'learning_rate': 6.799999999999999e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.77it/s] 57%|█████▋    | 43/75 [00:01<00:01, 24.76it/s]                                               {'loss': 2.1448, 'grad_norm': 4.219207286834717, 'learning_rate': 6.599999999999999e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.76it/s]                                               {'loss': 2.1642, 'grad_norm': 2.8694686889648438, 'learning_rate': 6.4e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.76it/s]                                               {'loss': 2.1403, 'grad_norm': 11.034098625183105, 'learning_rate': 6.199999999999999e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.76it/s]                                               {'loss': 2.0797, 'grad_norm': 3.7220065593719482, 'learning_rate': 5.9999999999999995e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.76it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.46it/s]                                               {'loss': 2.1317, 'grad_norm': 3.9173495769500732, 'learning_rate': 5.7999999999999994e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.46it/s]                                               {'loss': 2.1869, 'grad_norm': 3.876051664352417, 'learning_rate': 5.6e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.46it/s]                                               {'loss': 2.2173, 'grad_norm': 3.7822682857513428, 'learning_rate': 5.399999999999999e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.46it/s] 67%|██████▋   | 50/75 [00:01<00:00, 25.62it/s]                                               {'loss': 2.1687, 'grad_norm': 3.6937198638916016, 'learning_rate': 5.2e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.62it/s]                                               {'loss': 2.0854, 'grad_norm': 4.6498847007751465, 'learning_rate': 4.9999999999999996e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.62it/s]                                               {'loss': 2.0411, 'grad_norm': 4.4969587326049805, 'learning_rate': 4.7999999999999994e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.62it/s] 71%|███████   | 53/75 [00:02<00:00, 24.64it/s]                                               {'loss': 2.1105, 'grad_norm': 3.8443448543548584, 'learning_rate': 4.599999999999999e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.64it/s]                                               {'loss': 2.4308, 'grad_norm': 4.3380818367004395, 'learning_rate': 4.4e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.64it/s]                                               {'loss': 2.1321, 'grad_norm': 3.79834246635437, 'learning_rate': 4.2e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.64it/s] 75%|███████▍  | 56/75 [00:02<00:00, 24.40it/s]                                               {'loss': 2.4382, 'grad_norm': 3.978574752807617, 'learning_rate': 3.9999999999999996e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.40it/s]                                               {'loss': 2.0201, 'grad_norm': 5.019500255584717, 'learning_rate': 3.8e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.40it/s]                                               {'loss': 2.1416, 'grad_norm': 3.8140406608581543, 'learning_rate': 3.5999999999999994e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.40it/s] 79%|███████▊  | 59/75 [00:02<00:00, 24.07it/s]                                               {'loss': 2.1714, 'grad_norm': 3.647369623184204, 'learning_rate': 3.399999999999999e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.07it/s]                                               {'loss': 2.3232, 'grad_norm': 14.73868465423584, 'learning_rate': 3.2e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.07it/s]                                               {'loss': 2.3252, 'grad_norm': 5.166964054107666, 'learning_rate': 2.9999999999999997e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.07it/s] 83%|████████▎ | 62/75 [00:02<00:00, 25.38it/s]                                               {'loss': 2.0344, 'grad_norm': 3.604966878890991, 'learning_rate': 2.8e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.38it/s]                                               {'loss': 2.1215, 'grad_norm': 3.6989009380340576, 'learning_rate': 2.6e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.38it/s]                                               {'loss': 2.0669, 'grad_norm': 4.630131244659424, 'learning_rate': 2.3999999999999997e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.38it/s] 87%|████████▋ | 65/75 [00:02<00:00, 25.68it/s]                                               {'loss': 2.5129, 'grad_norm': 5.161983013153076, 'learning_rate': 2.2e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.68it/s]                                               {'loss': 2.2168, 'grad_norm': 4.315152645111084, 'learning_rate': 1.9999999999999998e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.68it/s]                                               {'loss': 2.2265, 'grad_norm': 3.8289668560028076, 'learning_rate': 1.7999999999999997e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.68it/s] 91%|█████████ | 68/75 [00:02<00:00, 25.67it/s]                                               {'loss': 2.0204, 'grad_norm': 3.4182958602905273, 'learning_rate': 1.6e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.67it/s]                                               {'loss': 2.223, 'grad_norm': 3.3942294120788574, 'learning_rate': 1.4e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.67it/s]                                               {'loss': 2.146, 'grad_norm': 3.6815404891967773, 'learning_rate': 1.1999999999999999e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.67it/s] 95%|█████████▍| 71/75 [00:02<00:00, 25.23it/s]                                               {'loss': 1.9942, 'grad_norm': 3.7915995121002197, 'learning_rate': 9.999999999999999e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.1834, 'grad_norm': 3.1705784797668457, 'learning_rate': 8e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.028, 'grad_norm': 3.2259697914123535, 'learning_rate': 5.999999999999999e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.23it/s] 99%|█████████▊| 74/75 [00:02<00:00, 25.04it/s]                                               {'loss': 2.1146, 'grad_norm': 4.700605869293213, 'learning_rate': 4e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.04it/s]                                               {'loss': 1.9083, 'grad_norm': 8.333843231201172, 'learning_rate': 2e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.04it/s]                                               {'train_runtime': 3.0871, 'train_samples_per_second': 366.038, 'train_steps_per_second': 24.295, 'train_loss': 2.1980232508977253, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.04it/s]100%|██████████| 75/75 [00:03<00:00, 24.30it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:26
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.1282, 'grad_norm': 5.6127238273620605, 'learning_rate': 0.00015, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.22it/s]                                              {'loss': 2.4304, 'grad_norm': 3.815359592437744, 'learning_rate': 0.000148, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 24.33it/s]  4%|▍         | 3/75 [00:00<00:02, 24.54it/s]                                              {'loss': 2.2283, 'grad_norm': 4.161794185638428, 'learning_rate': 0.000146, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.54it/s]                                              {'loss': 2.2362, 'grad_norm': 4.196016788482666, 'learning_rate': 0.00014399999999999998, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.54it/s]                                              {'loss': 2.4041, 'grad_norm': 5.108608722686768, 'learning_rate': 0.00014199999999999998, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.54it/s]  8%|▊         | 6/75 [00:00<00:02, 24.94it/s]                                              {'loss': 2.3888, 'grad_norm': 3.6260783672332764, 'learning_rate': 0.00014, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.94it/s]                                              {'loss': 2.2259, 'grad_norm': 3.606563091278076, 'learning_rate': 0.000138, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.94it/s]                                              {'loss': 2.3783, 'grad_norm': 5.352457046508789, 'learning_rate': 0.00013599999999999997, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.94it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.37it/s]                                              {'loss': 2.3846, 'grad_norm': 4.1155500411987305, 'learning_rate': 0.00013399999999999998, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.37it/s]                                              {'loss': 2.1195, 'grad_norm': 4.2048234939575195, 'learning_rate': 0.00013199999999999998, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.37it/s]                                               {'loss': 2.3643, 'grad_norm': 4.049307823181152, 'learning_rate': 0.00013, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.37it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.52it/s]                                               {'loss': 2.2271, 'grad_norm': 3.4652700424194336, 'learning_rate': 0.000128, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.52it/s]                                               {'loss': 2.2366, 'grad_norm': 4.79622745513916, 'learning_rate': 0.00012599999999999997, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.52it/s]                                               {'loss': 2.0905, 'grad_norm': 4.137829780578613, 'learning_rate': 0.00012399999999999998, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.52it/s] 20%|██        | 15/75 [00:00<00:02, 25.97it/s]                                               {'loss': 2.2343, 'grad_norm': 9.91463851928711, 'learning_rate': 0.000122, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.97it/s]                                               {'loss': 2.1944, 'grad_norm': 3.435943603515625, 'learning_rate': 0.00011999999999999999, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.97it/s]                                               {'loss': 2.097, 'grad_norm': 3.122464418411255, 'learning_rate': 0.00011799999999999998, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.97it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.61it/s]                                               {'loss': 2.2729, 'grad_norm': 4.555795669555664, 'learning_rate': 0.00011599999999999999, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.61it/s]                                               {'loss': 2.2895, 'grad_norm': 3.651350259780884, 'learning_rate': 0.00011399999999999999, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.61it/s]                                               {'loss': 2.0524, 'grad_norm': 2.82427978515625, 'learning_rate': 0.000112, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.61it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.47it/s]                                               {'loss': 2.1504, 'grad_norm': 3.677530288696289, 'learning_rate': 0.00010999999999999998, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.47it/s]                                               {'loss': 2.0328, 'grad_norm': 4.852611064910889, 'learning_rate': 0.00010799999999999998, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.47it/s]                                               {'loss': 2.3055, 'grad_norm': 3.246807336807251, 'learning_rate': 0.00010599999999999999, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.47it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.67it/s]                                               {'loss': 2.0956, 'grad_norm': 3.1287949085235596, 'learning_rate': 0.000104, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.67it/s]                                               {'loss': 2.3272, 'grad_norm': 3.8945043087005615, 'learning_rate': 0.000102, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.67it/s]                                               {'loss': 2.564, 'grad_norm': 5.486237049102783, 'learning_rate': 9.999999999999999e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.67it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.50it/s]                                               {'loss': 2.1978, 'grad_norm': 3.129195213317871, 'learning_rate': 9.799999999999998e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.50it/s]                                               {'loss': 2.268, 'grad_norm': 4.804765224456787, 'learning_rate': 9.599999999999999e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.50it/s]                                               {'loss': 2.185, 'grad_norm': 3.3133535385131836, 'learning_rate': 9.4e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.50it/s]                                               {'loss': 2.6455, 'grad_norm': 9.585875511169434, 'learning_rate': 9.199999999999999e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.50it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.15it/s]                                               {'loss': 2.1654, 'grad_norm': 3.552150249481201, 'learning_rate': 8.999999999999999e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.15it/s]                                               {'loss': 2.2753, 'grad_norm': 3.558224678039551, 'learning_rate': 8.8e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.15it/s]                                               {'loss': 2.1432, 'grad_norm': 3.566537618637085, 'learning_rate': 8.6e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.15it/s] 45%|████▌     | 34/75 [00:01<00:01, 25.90it/s]                                               {'loss': 2.3727, 'grad_norm': 4.6338348388671875, 'learning_rate': 8.4e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.90it/s]                                               {'loss': 2.3885, 'grad_norm': 3.854553461074829, 'learning_rate': 8.199999999999999e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.90it/s]                                               {'loss': 2.3866, 'grad_norm': 3.7058379650115967, 'learning_rate': 7.999999999999999e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.90it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.78it/s]                                               {'loss': 2.132, 'grad_norm': 3.964299201965332, 'learning_rate': 7.8e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.78it/s]                                               {'loss': 2.1142, 'grad_norm': 4.0353498458862305, 'learning_rate': 7.6e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.78it/s]                                               {'loss': 2.0959, 'grad_norm': 3.6728644371032715, 'learning_rate': 7.4e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.78it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.64it/s]                                               {'loss': 2.0937, 'grad_norm': 3.647487163543701, 'learning_rate': 7.199999999999999e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.64it/s]                                               {'loss': 2.1303, 'grad_norm': 3.316660165786743, 'learning_rate': 7e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.64it/s]                                               {'loss': 2.3864, 'grad_norm': 3.990344285964966, 'learning_rate': 6.799999999999999e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.64it/s] 57%|█████▋    | 43/75 [00:01<00:01, 25.20it/s]                                               {'loss': 2.0829, 'grad_norm': 3.558523178100586, 'learning_rate': 6.599999999999999e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.20it/s]                                               {'loss': 1.9988, 'grad_norm': 4.151820182800293, 'learning_rate': 6.4e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.20it/s]                                               {'loss': 2.3672, 'grad_norm': 17.358375549316406, 'learning_rate': 6.199999999999999e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.20it/s] 61%|██████▏   | 46/75 [00:01<00:01, 26.01it/s]                                               {'loss': 2.2608, 'grad_norm': 3.8620193004608154, 'learning_rate': 5.9999999999999995e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.01it/s]                                               {'loss': 2.2526, 'grad_norm': 5.321869373321533, 'learning_rate': 5.7999999999999994e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.01it/s]                                               {'loss': 2.3624, 'grad_norm': 4.642447471618652, 'learning_rate': 5.6e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.01it/s] 65%|██████▌   | 49/75 [00:01<00:01, 25.69it/s]                                               {'loss': 2.4401, 'grad_norm': 4.090267181396484, 'learning_rate': 5.399999999999999e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.69it/s]                                               {'loss': 1.9629, 'grad_norm': 3.618802547454834, 'learning_rate': 5.2e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.69it/s]                                               {'loss': 2.1593, 'grad_norm': 3.6259334087371826, 'learning_rate': 4.9999999999999996e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.69it/s] 69%|██████▉   | 52/75 [00:02<00:00, 24.74it/s]                                               {'loss': 2.1727, 'grad_norm': 3.9274587631225586, 'learning_rate': 4.7999999999999994e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.74it/s]                                               {'loss': 2.0862, 'grad_norm': 4.483997821807861, 'learning_rate': 4.599999999999999e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.74it/s]                                               {'loss': 2.0829, 'grad_norm': 4.070774555206299, 'learning_rate': 4.4e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.74it/s] 73%|███████▎  | 55/75 [00:02<00:00, 24.16it/s]                                               {'loss': 2.0285, 'grad_norm': 3.4138779640197754, 'learning_rate': 4.2e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.16it/s]                                               {'loss': 2.027, 'grad_norm': 2.7910265922546387, 'learning_rate': 3.9999999999999996e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.16it/s]                                               {'loss': 2.2167, 'grad_norm': 3.8270108699798584, 'learning_rate': 3.8e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.16it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.02it/s]                                               {'loss': 2.2034, 'grad_norm': 3.548306703567505, 'learning_rate': 3.5999999999999994e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.02it/s]                                               {'loss': 2.1556, 'grad_norm': 3.5083868503570557, 'learning_rate': 3.399999999999999e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.02it/s]                                               {'loss': 2.3777, 'grad_norm': 8.890583992004395, 'learning_rate': 3.2e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.02it/s]                                               {'loss': 2.2302, 'grad_norm': 3.721250295639038, 'learning_rate': 2.9999999999999997e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.02it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.00it/s]                                               {'loss': 2.0088, 'grad_norm': 3.2259883880615234, 'learning_rate': 2.8e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.00it/s]                                               {'loss': 1.9919, 'grad_norm': 3.0886621475219727, 'learning_rate': 2.6e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.00it/s]                                               {'loss': 2.0621, 'grad_norm': 3.8180172443389893, 'learning_rate': 2.3999999999999997e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.00it/s] 87%|████████▋ | 65/75 [00:02<00:00, 25.75it/s]                                               {'loss': 2.1872, 'grad_norm': 3.8898627758026123, 'learning_rate': 2.2e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.75it/s]                                               {'loss': 2.1033, 'grad_norm': 4.03164005279541, 'learning_rate': 1.9999999999999998e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.75it/s]                                               {'loss': 2.2211, 'grad_norm': 2.9386825561523438, 'learning_rate': 1.7999999999999997e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.75it/s] 91%|█████████ | 68/75 [00:02<00:00, 25.40it/s]                                               {'loss': 2.1369, 'grad_norm': 3.303250551223755, 'learning_rate': 1.6e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.40it/s]                                               {'loss': 2.344, 'grad_norm': 4.412045001983643, 'learning_rate': 1.4e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.40it/s]                                               {'loss': 2.0378, 'grad_norm': 3.6572227478027344, 'learning_rate': 1.1999999999999999e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.40it/s] 95%|█████████▍| 71/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.2379, 'grad_norm': 4.266814708709717, 'learning_rate': 9.999999999999999e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.2484, 'grad_norm': 3.670189380645752, 'learning_rate': 8e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.2003, 'grad_norm': 4.021988391876221, 'learning_rate': 5.999999999999999e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.02it/s] 99%|█████████▊| 74/75 [00:02<00:00, 25.50it/s]                                               {'loss': 2.1787, 'grad_norm': 3.4430558681488037, 'learning_rate': 4e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.50it/s]                                               {'loss': 2.1806, 'grad_norm': 14.709064483642578, 'learning_rate': 2e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.50it/s]                                               {'train_runtime': 3.0636, 'train_samples_per_second': 368.844, 'train_steps_per_second': 24.481, 'train_loss': 2.213951978683472, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.50it/s]100%|██████████| 75/75 [00:03<00:00, 24.48it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:19
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4123, 'grad_norm': 3.8552777767181396, 'learning_rate': 0.00015, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.74it/s]                                              {'loss': 2.375, 'grad_norm': 3.192777633666992, 'learning_rate': 0.000148, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 25.26it/s]  4%|▍         | 3/75 [00:00<00:02, 25.57it/s]                                              {'loss': 2.3406, 'grad_norm': 5.830805778503418, 'learning_rate': 0.000146, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.57it/s]                                              {'loss': 2.2522, 'grad_norm': 4.384954452514648, 'learning_rate': 0.00014399999999999998, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.57it/s]                                              {'loss': 2.6827, 'grad_norm': 5.370059013366699, 'learning_rate': 0.00014199999999999998, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.57it/s]  8%|▊         | 6/75 [00:00<00:02, 25.24it/s]                                              {'loss': 2.4845, 'grad_norm': 4.607028007507324, 'learning_rate': 0.00014, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.24it/s]                                              {'loss': 2.6072, 'grad_norm': 5.497878074645996, 'learning_rate': 0.000138, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.24it/s]                                              {'loss': 2.3572, 'grad_norm': 3.804608106613159, 'learning_rate': 0.00013599999999999997, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.24it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.08it/s]                                              {'loss': 2.4196, 'grad_norm': 4.4004621505737305, 'learning_rate': 0.00013399999999999998, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.08it/s]                                              {'loss': 2.1494, 'grad_norm': 4.772107124328613, 'learning_rate': 0.00013199999999999998, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.08it/s]                                               {'loss': 2.5993, 'grad_norm': 4.671926021575928, 'learning_rate': 0.00013, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.08it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.63it/s]                                               {'loss': 2.4088, 'grad_norm': 3.818007230758667, 'learning_rate': 0.000128, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.63it/s]                                               {'loss': 2.2681, 'grad_norm': 7.072181701660156, 'learning_rate': 0.00012599999999999997, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.63it/s]                                               {'loss': 2.2341, 'grad_norm': 3.7218942642211914, 'learning_rate': 0.00012399999999999998, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.63it/s] 20%|██        | 15/75 [00:00<00:02, 23.01it/s]                                               {'loss': 2.2695, 'grad_norm': 15.979368209838867, 'learning_rate': 0.000122, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.01it/s]                                               {'loss': 2.2907, 'grad_norm': 3.5570385456085205, 'learning_rate': 0.00011999999999999999, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 23.01it/s]                                               {'loss': 2.2277, 'grad_norm': 4.044322967529297, 'learning_rate': 0.00011799999999999998, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 23.01it/s] 24%|██▍       | 18/75 [00:00<00:02, 22.96it/s]                                               {'loss': 2.3329, 'grad_norm': 6.287386417388916, 'learning_rate': 0.00011599999999999999, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 22.96it/s]                                               {'loss': 2.389, 'grad_norm': 4.826357364654541, 'learning_rate': 0.00011399999999999999, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 22.96it/s]                                               {'loss': 2.3444, 'grad_norm': 3.751969337463379, 'learning_rate': 0.000112, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 22.96it/s] 28%|██▊       | 21/75 [00:00<00:02, 23.58it/s]                                               {'loss': 2.296, 'grad_norm': 3.3685600757598877, 'learning_rate': 0.00010999999999999998, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 23.58it/s]                                               {'loss': 2.3005, 'grad_norm': 4.422539234161377, 'learning_rate': 0.00010799999999999998, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 23.58it/s]                                               {'loss': 2.4288, 'grad_norm': 4.7373881340026855, 'learning_rate': 0.00010599999999999999, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 23.58it/s] 32%|███▏      | 24/75 [00:01<00:02, 23.96it/s]                                               {'loss': 2.4337, 'grad_norm': 4.869591236114502, 'learning_rate': 0.000104, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 23.96it/s]                                               {'loss': 2.3029, 'grad_norm': 3.9740469455718994, 'learning_rate': 0.000102, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.96it/s]                                               {'loss': 2.256, 'grad_norm': 3.498723268508911, 'learning_rate': 9.999999999999999e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.96it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.27it/s]                                               {'loss': 2.3009, 'grad_norm': 4.32334041595459, 'learning_rate': 9.799999999999998e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.27it/s]                                               {'loss': 2.4195, 'grad_norm': 4.1646552085876465, 'learning_rate': 9.599999999999999e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.27it/s]                                               {'loss': 2.4578, 'grad_norm': 5.249205589294434, 'learning_rate': 9.4e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.27it/s]                                               {'loss': 2.3193, 'grad_norm': 8.63963508605957, 'learning_rate': 9.199999999999999e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.27it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.17it/s]                                               {'loss': 2.4389, 'grad_norm': 4.381518840789795, 'learning_rate': 8.999999999999999e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.17it/s]                                               {'loss': 2.1805, 'grad_norm': 4.487451076507568, 'learning_rate': 8.8e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.17it/s]                                               {'loss': 2.1993, 'grad_norm': 4.8120341300964355, 'learning_rate': 8.6e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.17it/s] 45%|████▌     | 34/75 [00:01<00:01, 25.90it/s]                                               {'loss': 2.3759, 'grad_norm': 4.164672374725342, 'learning_rate': 8.4e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.90it/s]                                               {'loss': 2.4481, 'grad_norm': 5.573270797729492, 'learning_rate': 8.199999999999999e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.90it/s]                                               {'loss': 2.1589, 'grad_norm': 5.700244426727295, 'learning_rate': 7.999999999999999e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.90it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.48it/s]                                               {'loss': 2.2997, 'grad_norm': 2.9652674198150635, 'learning_rate': 7.8e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.48it/s]                                               {'loss': 2.2547, 'grad_norm': 4.484314918518066, 'learning_rate': 7.6e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.48it/s]                                               {'loss': 2.1434, 'grad_norm': 3.993464946746826, 'learning_rate': 7.4e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.48it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.4127, 'grad_norm': 4.474638938903809, 'learning_rate': 7.199999999999999e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.2247, 'grad_norm': 4.812503337860107, 'learning_rate': 7e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.4539, 'grad_norm': 4.147477626800537, 'learning_rate': 6.799999999999999e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.39it/s] 57%|█████▋    | 43/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.3966, 'grad_norm': 4.2280731201171875, 'learning_rate': 6.599999999999999e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.264, 'grad_norm': 5.260180950164795, 'learning_rate': 6.4e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.39it/s]                                               {'loss': 1.8545, 'grad_norm': 7.509382247924805, 'learning_rate': 6.199999999999999e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.3204, 'grad_norm': 4.497172832489014, 'learning_rate': 5.9999999999999995e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.39it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.67it/s]                                               {'loss': 2.2525, 'grad_norm': 3.7567667961120605, 'learning_rate': 5.7999999999999994e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.67it/s]                                               {'loss': 2.4149, 'grad_norm': 4.089384078979492, 'learning_rate': 5.6e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.67it/s]                                               {'loss': 2.3013, 'grad_norm': 4.697324752807617, 'learning_rate': 5.399999999999999e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.67it/s] 67%|██████▋   | 50/75 [00:01<00:00, 26.41it/s]                                               {'loss': 2.2802, 'grad_norm': 4.357570171356201, 'learning_rate': 5.2e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.41it/s]                                               {'loss': 2.1861, 'grad_norm': 3.8657383918762207, 'learning_rate': 4.9999999999999996e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 26.41it/s]                                               {'loss': 2.119, 'grad_norm': 4.085293292999268, 'learning_rate': 4.7999999999999994e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 26.41it/s] 71%|███████   | 53/75 [00:02<00:00, 26.38it/s]                                               {'loss': 2.2856, 'grad_norm': 4.867019176483154, 'learning_rate': 4.599999999999999e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 26.38it/s]                                               {'loss': 2.3666, 'grad_norm': 3.7639238834381104, 'learning_rate': 4.4e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 26.38it/s]                                               {'loss': 2.4571, 'grad_norm': 3.4906818866729736, 'learning_rate': 4.2e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 26.38it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.82it/s]                                               {'loss': 2.2501, 'grad_norm': 4.2548828125, 'learning_rate': 3.9999999999999996e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.82it/s]                                               {'loss': 2.2784, 'grad_norm': 4.413069248199463, 'learning_rate': 3.8e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.82it/s]                                               {'loss': 2.3322, 'grad_norm': 3.8506510257720947, 'learning_rate': 3.5999999999999994e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.82it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.3592, 'grad_norm': 5.509531021118164, 'learning_rate': 3.399999999999999e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.46it/s]                                               {'loss': 1.8816, 'grad_norm': 9.655145645141602, 'learning_rate': 3.2e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.1334, 'grad_norm': 4.232398986816406, 'learning_rate': 2.9999999999999997e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.263, 'grad_norm': 4.330652236938477, 'learning_rate': 2.8e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.46it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.81it/s]                                               {'loss': 2.4121, 'grad_norm': 3.8309438228607178, 'learning_rate': 2.6e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.81it/s]                                               {'loss': 2.251, 'grad_norm': 3.9498164653778076, 'learning_rate': 2.3999999999999997e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.81it/s]                                               {'loss': 2.1502, 'grad_norm': 3.5164310932159424, 'learning_rate': 2.2e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.81it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.20it/s]                                               {'loss': 2.3203, 'grad_norm': 3.8839128017425537, 'learning_rate': 1.9999999999999998e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.20it/s]                                               {'loss': 2.538, 'grad_norm': 4.4264726638793945, 'learning_rate': 1.7999999999999997e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.20it/s]                                               {'loss': 2.1434, 'grad_norm': 4.177818298339844, 'learning_rate': 1.6e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 26.20it/s] 92%|█████████▏| 69/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.3037, 'grad_norm': 3.7292287349700928, 'learning_rate': 1.4e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.3795, 'grad_norm': 3.5879745483398438, 'learning_rate': 1.1999999999999999e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.4205, 'grad_norm': 4.9751057624816895, 'learning_rate': 9.999999999999999e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.16it/s] 96%|█████████▌| 72/75 [00:02<00:00, 24.88it/s]                                               {'loss': 2.2451, 'grad_norm': 5.915584564208984, 'learning_rate': 8e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.88it/s]                                               {'loss': 2.2754, 'grad_norm': 4.974077224731445, 'learning_rate': 5.999999999999999e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.88it/s]                                               {'loss': 2.2696, 'grad_norm': 4.04877233505249, 'learning_rate': 4e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.88it/s]                                               {'loss': 2.5773, 'grad_norm': 14.120368957519531, 'learning_rate': 2e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.88it/s]                                               {'train_runtime': 3.0682, 'train_samples_per_second': 368.29, 'train_steps_per_second': 24.444, 'train_loss': 2.3177911901473998, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.88it/s]100%|██████████| 75/75 [00:03<00:00, 24.45it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1077, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(917, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(785, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(953, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1182, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(983, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1119, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1110, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1080, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(901, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1292, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(762, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 6/471 [00:00<00:08, 57.26it/s]  3%|▎         | 12/471 [00:00<00:08, 51.78it/s]  4%|▍         | 18/471 [00:00<00:08, 51.71it/s]  5%|▌         | 24/471 [00:00<00:08, 50.26it/s]  6%|▋         | 30/471 [00:00<00:08, 50.54it/s]  8%|▊         | 36/471 [00:00<00:08, 51.34it/s]  9%|▉         | 42/471 [00:00<00:08, 50.18it/s] 10%|█         | 48/471 [00:00<00:08, 51.43it/s] 11%|█▏        | 54/471 [00:01<00:08, 51.48it/s] 13%|█▎        | 60/471 [00:01<00:08, 50.27it/s] 14%|█▍        | 66/471 [00:01<00:07, 50.85it/s] 15%|█▌        | 72/471 [00:01<00:08, 49.23it/s] 16%|█▋        | 77/471 [00:01<00:07, 49.36it/s] 18%|█▊        | 83/471 [00:01<00:07, 50.27it/s] 19%|█▉        | 89/471 [00:01<00:07, 50.46it/s] 20%|██        | 95/471 [00:01<00:07, 48.87it/s] 21%|██▏       | 101/471 [00:01<00:07, 50.48it/s] 23%|██▎       | 107/471 [00:02<00:07, 49.28it/s] 24%|██▍       | 112/471 [00:02<00:07, 48.79it/s] 25%|██▌       | 118/471 [00:02<00:07, 49.52it/s] 26%|██▋       | 124/471 [00:02<00:06, 50.38it/s] 28%|██▊       | 130/471 [00:02<00:06, 50.94it/s] 29%|██▉       | 136/471 [00:02<00:06, 51.26it/s] 30%|███       | 142/471 [00:02<00:06, 51.45it/s] 31%|███▏      | 148/471 [00:02<00:06, 50.45it/s] 33%|███▎      | 154/471 [00:03<00:06, 51.07it/s] 34%|███▍      | 160/471 [00:03<00:06, 50.39it/s] 35%|███▌      | 166/471 [00:03<00:05, 51.52it/s] 37%|███▋      | 172/471 [00:03<00:05, 51.55it/s] 38%|███▊      | 178/471 [00:03<00:05, 51.78it/s] 39%|███▉      | 184/471 [00:03<00:05, 50.77it/s] 40%|████      | 190/471 [00:03<00:05, 51.41it/s] 42%|████▏     | 196/471 [00:03<00:05, 50.37it/s] 43%|████▎     | 202/471 [00:03<00:05, 51.47it/s] 44%|████▍     | 208/471 [00:04<00:05, 51.03it/s] 45%|████▌     | 214/471 [00:04<00:04, 51.52it/s] 47%|████▋     | 220/471 [00:04<00:04, 51.62it/s] 48%|████▊     | 226/471 [00:04<00:04, 51.02it/s] 49%|████▉     | 232/471 [00:04<00:04, 51.22it/s] 51%|█████     | 238/471 [00:04<00:04, 50.80it/s] 52%|█████▏    | 244/471 [00:04<00:04, 51.18it/s] 53%|█████▎    | 250/471 [00:04<00:04, 51.13it/s] 54%|█████▍    | 256/471 [00:05<00:04, 51.24it/s] 56%|█████▌    | 262/471 [00:05<00:04, 50.34it/s] 57%|█████▋    | 268/471 [00:05<00:03, 50.86it/s] 58%|█████▊    | 274/471 [00:05<00:03, 50.57it/s] 59%|█████▉    | 280/471 [00:05<00:03, 51.19it/s] 61%|██████    | 286/471 [00:05<00:03, 51.24it/s] 62%|██████▏   | 292/471 [00:05<00:03, 51.56it/s] 63%|██████▎   | 298/471 [00:05<00:03, 51.70it/s] 65%|██████▍   | 304/471 [00:05<00:03, 51.24it/s] 66%|██████▌   | 310/471 [00:06<00:03, 51.47it/s] 67%|██████▋   | 316/471 [00:06<00:03, 51.59it/s] 68%|██████▊   | 322/471 [00:06<00:03, 48.54it/s] 70%|██████▉   | 328/471 [00:06<00:02, 50.04it/s] 71%|███████   | 334/471 [00:06<00:02, 49.08it/s] 72%|███████▏  | 340/471 [00:06<00:02, 49.62it/s] 73%|███████▎  | 346/471 [00:06<00:02, 50.63it/s] 75%|███████▍  | 352/471 [00:06<00:02, 50.55it/s] 76%|███████▌  | 358/471 [00:07<00:02, 50.94it/s] 77%|███████▋  | 364/471 [00:07<00:02, 50.84it/s] 79%|███████▊  | 370/471 [00:07<00:01, 51.12it/s] 80%|███████▉  | 376/471 [00:07<00:01, 49.82it/s] 81%|████████  | 382/471 [00:07<00:01, 50.17it/s] 82%|████████▏ | 388/471 [00:07<00:01, 48.54it/s] 84%|████████▎ | 394/471 [00:07<00:01, 50.08it/s] 85%|████████▍ | 400/471 [00:07<00:01, 50.21it/s] 86%|████████▌ | 406/471 [00:08<00:01, 50.04it/s] 87%|████████▋ | 412/471 [00:08<00:01, 50.49it/s] 89%|████████▊ | 418/471 [00:08<00:01, 50.09it/s] 90%|█████████ | 424/471 [00:08<00:00, 48.37it/s] 91%|█████████ | 429/471 [00:08<00:00, 47.26it/s] 92%|█████████▏| 434/471 [00:08<00:00, 46.95it/s] 93%|█████████▎| 439/471 [00:08<00:00, 47.68it/s] 94%|█████████▍| 445/471 [00:08<00:00, 48.84it/s] 96%|█████████▌| 450/471 [00:08<00:00, 48.31it/s] 97%|█████████▋| 455/471 [00:09<00:00, 48.31it/s] 98%|█████████▊| 461/471 [00:09<00:00, 48.88it/s] 99%|█████████▉| 466/471 [00:09<00:00, 48.87it/s]100%|██████████| 471/471 [00:09<00:00, 50.37it/s]
{'eval_loss': 2.3099026679992676, 'eval_model_preparation_time': 0.0029, 'eval_acc': 0.3856877323420074, 'eval_runtime': 9.3774, 'eval_samples_per_second': 803.207, 'eval_steps_per_second': 50.227}
ROUND:11
CLIENT:7
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4168, 'grad_norm': 4.080916404724121, 'learning_rate': 0.0001464265445104546, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 16.38it/s]                                              {'loss': 2.4005, 'grad_norm': 4.640148639678955, 'learning_rate': 0.00014447419058364857, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 20.39it/s]  4%|▍         | 3/75 [00:00<00:03, 20.77it/s]                                              {'loss': 2.1412, 'grad_norm': 6.028745651245117, 'learning_rate': 0.0001425218366568425, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 20.77it/s]                                              {'loss': 2.3092, 'grad_norm': 4.069860935211182, 'learning_rate': 0.00014056948273003642, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 20.77it/s]                                              {'loss': 2.3456, 'grad_norm': 3.759371519088745, 'learning_rate': 0.00013861712880323037, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 20.77it/s]  8%|▊         | 6/75 [00:00<00:03, 22.76it/s]                                              {'loss': 2.3806, 'grad_norm': 5.928077697753906, 'learning_rate': 0.0001366647748764243, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 22.76it/s]                                              {'loss': 2.2392, 'grad_norm': 4.127727031707764, 'learning_rate': 0.00013471242094961826, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 22.76it/s]                                              {'loss': 2.093, 'grad_norm': 3.7149670124053955, 'learning_rate': 0.00013276006702281218, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 22.76it/s] 12%|█▏        | 9/75 [00:00<00:02, 23.99it/s]                                              {'loss': 2.4499, 'grad_norm': 4.611523151397705, 'learning_rate': 0.0001308077130960061, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.99it/s]                                              {'loss': 2.2754, 'grad_norm': 3.753478765487671, 'learning_rate': 0.00012885535916920006, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.99it/s]                                               {'loss': 2.1707, 'grad_norm': 3.7894887924194336, 'learning_rate': 0.000126903005242394, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.99it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.99it/s]                                               {'loss': 2.3945, 'grad_norm': 6.7923502922058105, 'learning_rate': 0.00012495065131558794, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.99it/s]                                               {'loss': 2.243, 'grad_norm': 4.4078145027160645, 'learning_rate': 0.00012299829738878187, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.99it/s]                                               {'loss': 2.0263, 'grad_norm': 3.0654144287109375, 'learning_rate': 0.00012104594346197581, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.99it/s]                                               {'loss': 2.5296, 'grad_norm': 12.535223960876465, 'learning_rate': 0.00011909358953516975, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.99it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.34it/s]                                               {'loss': 2.2208, 'grad_norm': 3.96244215965271, 'learning_rate': 0.00011714123560836369, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.34it/s]                                               {'loss': 2.3429, 'grad_norm': 4.927708625793457, 'learning_rate': 0.00011518888168155762, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.34it/s]                                               {'loss': 2.192, 'grad_norm': 4.252647876739502, 'learning_rate': 0.00011323652775475156, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.34it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.84it/s]                                               {'loss': 2.5601, 'grad_norm': 4.48506498336792, 'learning_rate': 0.0001112841738279455, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.84it/s]                                               {'loss': 2.1839, 'grad_norm': 4.633147716522217, 'learning_rate': 0.00010933181990113945, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.84it/s]                                               {'loss': 2.0836, 'grad_norm': 3.969677448272705, 'learning_rate': 0.00010737946597433338, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.84it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.16it/s]                                               {'loss': 2.0431, 'grad_norm': 3.5504143238067627, 'learning_rate': 0.00010542711204752732, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.16it/s]                                               {'loss': 2.0808, 'grad_norm': 4.014824867248535, 'learning_rate': 0.00010347475812072126, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.16it/s]                                               {'loss': 2.2795, 'grad_norm': 4.7843523025512695, 'learning_rate': 0.0001015224041939152, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 25.16it/s] 33%|███▎      | 25/75 [00:01<00:02, 24.12it/s]                                               {'loss': 2.4591, 'grad_norm': 4.265351295471191, 'learning_rate': 9.957005026710914e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.12it/s]                                               {'loss': 2.3075, 'grad_norm': 3.863468885421753, 'learning_rate': 9.761769634030307e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.12it/s]                                               {'loss': 2.1945, 'grad_norm': 3.9559619426727295, 'learning_rate': 9.566534241349701e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.12it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.32it/s]                                               {'loss': 2.0393, 'grad_norm': 3.7411890029907227, 'learning_rate': 9.371298848669095e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.32it/s]                                               {'loss': 2.0573, 'grad_norm': 3.982835054397583, 'learning_rate': 9.176063455988489e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.32it/s]                                               {'loss': 2.5035, 'grad_norm': 8.17375659942627, 'learning_rate': 8.980828063307882e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.32it/s]                                               {'loss': 2.1507, 'grad_norm': 3.6016650199890137, 'learning_rate': 8.785592670627276e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.32it/s] 43%|████▎     | 32/75 [00:01<00:01, 25.88it/s]                                               {'loss': 2.2819, 'grad_norm': 3.6093251705169678, 'learning_rate': 8.59035727794667e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.88it/s]                                               {'loss': 2.1064, 'grad_norm': 4.35561990737915, 'learning_rate': 8.395121885266065e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.88it/s]                                               {'loss': 2.1067, 'grad_norm': 4.373816967010498, 'learning_rate': 8.19988649258546e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.88it/s] 47%|████▋     | 35/75 [00:01<00:01, 25.30it/s]                                               {'loss': 1.8507, 'grad_norm': 3.402305841445923, 'learning_rate': 8.004651099904852e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.103, 'grad_norm': 3.8710858821868896, 'learning_rate': 7.809415707224246e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.3314, 'grad_norm': 3.45731258392334, 'learning_rate': 7.61418031454364e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.30it/s] 51%|█████     | 38/75 [00:01<00:01, 24.76it/s]                                               {'loss': 2.1809, 'grad_norm': 3.8816723823547363, 'learning_rate': 7.418944921863034e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.76it/s]                                               {'loss': 2.202, 'grad_norm': 4.633985996246338, 'learning_rate': 7.223709529182428e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.76it/s]                                               {'loss': 2.0898, 'grad_norm': 4.305783271789551, 'learning_rate': 7.028474136501821e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.76it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.12it/s]                                               {'loss': 2.2446, 'grad_norm': 4.3395280838012695, 'learning_rate': 6.833238743821215e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.12it/s]                                               {'loss': 2.1813, 'grad_norm': 3.436779499053955, 'learning_rate': 6.638003351140609e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.12it/s]                                               {'loss': 2.2216, 'grad_norm': 3.894256114959717, 'learning_rate': 6.442767958460003e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.12it/s] 59%|█████▊    | 44/75 [00:01<00:01, 24.11it/s]                                               {'loss': 2.3858, 'grad_norm': 4.850889205932617, 'learning_rate': 6.247532565779397e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.11it/s]                                               {'loss': 2.1376, 'grad_norm': 13.369548797607422, 'learning_rate': 6.0522971730987906e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.11it/s]                                               {'loss': 2.2214, 'grad_norm': 3.9304039478302, 'learning_rate': 5.8570617804181846e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.11it/s] 63%|██████▎   | 47/75 [00:01<00:01, 23.25it/s]                                               {'loss': 2.2773, 'grad_norm': 4.263123035430908, 'learning_rate': 5.661826387737578e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 23.25it/s]                                               {'loss': 2.2639, 'grad_norm': 3.7820680141448975, 'learning_rate': 5.466590995056973e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 23.25it/s]                                               {'loss': 2.0364, 'grad_norm': 3.8837664127349854, 'learning_rate': 5.271355602376366e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 23.25it/s] 67%|██████▋   | 50/75 [00:02<00:01, 23.90it/s]                                               {'loss': 2.1574, 'grad_norm': 2.997606039047241, 'learning_rate': 5.07612020969576e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 23.90it/s]                                               {'loss': 2.0547, 'grad_norm': 3.5361337661743164, 'learning_rate': 4.8808848170151535e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 23.90it/s]                                               {'loss': 2.1358, 'grad_norm': 3.510063886642456, 'learning_rate': 4.6856494243345476e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 23.90it/s] 71%|███████   | 53/75 [00:02<00:00, 23.90it/s]                                               {'loss': 2.1684, 'grad_norm': 4.089044094085693, 'learning_rate': 4.490414031653941e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 23.90it/s]                                               {'loss': 2.2837, 'grad_norm': 4.234923362731934, 'learning_rate': 4.295178638973335e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 23.90it/s]                                               {'loss': 2.1112, 'grad_norm': 3.750460386276245, 'learning_rate': 4.09994324629273e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 23.90it/s] 75%|███████▍  | 56/75 [00:02<00:00, 24.10it/s]                                               {'loss': 2.1427, 'grad_norm': 5.482604026794434, 'learning_rate': 3.904707853612123e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.10it/s]                                               {'loss': 2.2884, 'grad_norm': 3.8350582122802734, 'learning_rate': 3.709472460931517e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.10it/s]                                               {'loss': 2.0826, 'grad_norm': 2.7721176147460938, 'learning_rate': 3.5142370682509105e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.10it/s] 79%|███████▊  | 59/75 [00:02<00:00, 24.69it/s]                                               {'loss': 2.1802, 'grad_norm': 4.271350383758545, 'learning_rate': 3.3190016755703046e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.69it/s]                                               {'loss': 2.0275, 'grad_norm': 11.824167251586914, 'learning_rate': 3.1237662828896986e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.69it/s]                                               {'loss': 2.0101, 'grad_norm': 3.8375439643859863, 'learning_rate': 2.9285308902090923e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.69it/s]                                               {'loss': 2.3807, 'grad_norm': 4.81871223449707, 'learning_rate': 2.7332954975284864e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 24.69it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.11it/s]                                               {'loss': 2.174, 'grad_norm': 4.41277551651001, 'learning_rate': 2.53806010484788e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.11it/s]                                               {'loss': 2.1429, 'grad_norm': 4.373367786407471, 'learning_rate': 2.3428247121672738e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.11it/s]                                               {'loss': 2.0703, 'grad_norm': 2.6421353816986084, 'learning_rate': 2.1475893194866675e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.11it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.31it/s]                                               {'loss': 2.1148, 'grad_norm': 3.5603270530700684, 'learning_rate': 1.9523539268060615e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.31it/s]                                               {'loss': 2.1038, 'grad_norm': 3.480710983276367, 'learning_rate': 1.7571185341254553e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.31it/s]                                               {'loss': 2.1335, 'grad_norm': 4.4266486167907715, 'learning_rate': 1.5618831414448493e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.31it/s] 92%|█████████▏| 69/75 [00:02<00:00, 25.61it/s]                                               {'loss': 1.9991, 'grad_norm': 5.233405590057373, 'learning_rate': 1.3666477487642432e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.61it/s]                                               {'loss': 2.354, 'grad_norm': 5.089004039764404, 'learning_rate': 1.1714123560836369e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.61it/s]                                               {'loss': 2.2269, 'grad_norm': 3.6701433658599854, 'learning_rate': 9.761769634030308e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.61it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.2335, 'grad_norm': 4.6912970542907715, 'learning_rate': 7.809415707224247e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.2859, 'grad_norm': 4.385350704193115, 'learning_rate': 5.8570617804181845e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.0465, 'grad_norm': 4.0286993980407715, 'learning_rate': 3.904707853612123e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.5533, 'grad_norm': 12.987800598144531, 'learning_rate': 1.9523539268060616e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.46it/s]                                               {'train_runtime': 3.1277, 'train_samples_per_second': 361.293, 'train_steps_per_second': 23.98, 'train_loss': 2.2106411409378053, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.46it/s]100%|██████████| 75/75 [00:03<00:00, 24.00it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:44
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2547, 'grad_norm': 3.810777187347412, 'learning_rate': 0.0001464265445104546, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 22.57it/s]                                              {'loss': 2.3215, 'grad_norm': 3.739330768585205, 'learning_rate': 0.00014447419058364857, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 24.32it/s]  4%|▍         | 3/75 [00:00<00:02, 24.69it/s]                                              {'loss': 2.4898, 'grad_norm': 4.341529369354248, 'learning_rate': 0.0001425218366568425, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.69it/s]                                              {'loss': 2.3948, 'grad_norm': 3.9014673233032227, 'learning_rate': 0.00014056948273003642, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.69it/s]                                              {'loss': 2.3297, 'grad_norm': 3.756530284881592, 'learning_rate': 0.00013861712880323037, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.69it/s]  8%|▊         | 6/75 [00:00<00:02, 25.07it/s]                                              {'loss': 2.1391, 'grad_norm': 4.568918228149414, 'learning_rate': 0.0001366647748764243, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.07it/s]                                              {'loss': 2.171, 'grad_norm': 3.869176149368286, 'learning_rate': 0.00013471242094961826, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.07it/s]                                              {'loss': 2.426, 'grad_norm': 3.7235586643218994, 'learning_rate': 0.00013276006702281218, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.07it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.95it/s]                                              {'loss': 2.6317, 'grad_norm': 4.874462604522705, 'learning_rate': 0.0001308077130960061, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.95it/s]                                              {'loss': 2.1464, 'grad_norm': 3.406006336212158, 'learning_rate': 0.00012885535916920006, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.95it/s]                                               {'loss': 2.498, 'grad_norm': 4.445379734039307, 'learning_rate': 0.000126903005242394, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.95it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.05it/s]                                               {'loss': 2.3939, 'grad_norm': 3.5856435298919678, 'learning_rate': 0.00012495065131558794, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.05it/s]                                               {'loss': 2.2901, 'grad_norm': 4.795063018798828, 'learning_rate': 0.00012299829738878187, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.05it/s]                                               {'loss': 2.3209, 'grad_norm': 3.7894821166992188, 'learning_rate': 0.00012104594346197581, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.05it/s] 20%|██        | 15/75 [00:00<00:02, 26.49it/s]                                               {'loss': 2.4145, 'grad_norm': 11.009444236755371, 'learning_rate': 0.00011909358953516975, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 26.49it/s]                                               {'loss': 2.4302, 'grad_norm': 3.9918370246887207, 'learning_rate': 0.00011714123560836369, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.49it/s]                                               {'loss': 2.415, 'grad_norm': 3.9536006450653076, 'learning_rate': 0.00011518888168155762, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.49it/s] 24%|██▍       | 18/75 [00:00<00:02, 24.94it/s]                                               {'loss': 2.1895, 'grad_norm': 3.923828363418579, 'learning_rate': 0.00011323652775475156, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.94it/s]                                               {'loss': 2.2862, 'grad_norm': 3.4732747077941895, 'learning_rate': 0.0001112841738279455, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.94it/s]                                               {'loss': 2.3245, 'grad_norm': 5.453683376312256, 'learning_rate': 0.00010933181990113945, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.94it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.63it/s]                                               {'loss': 2.4714, 'grad_norm': 4.813335418701172, 'learning_rate': 0.00010737946597433338, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.63it/s]                                               {'loss': 2.3518, 'grad_norm': 4.660173416137695, 'learning_rate': 0.00010542711204752732, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.63it/s]                                               {'loss': 2.1661, 'grad_norm': 3.501258373260498, 'learning_rate': 0.00010347475812072126, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.63it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.31it/s]                                               {'loss': 2.2406, 'grad_norm': 3.510688304901123, 'learning_rate': 0.0001015224041939152, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.31it/s]                                               {'loss': 2.4244, 'grad_norm': 5.357306480407715, 'learning_rate': 9.957005026710914e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.31it/s]                                               {'loss': 2.0913, 'grad_norm': 4.210080623626709, 'learning_rate': 9.761769634030307e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.31it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.44it/s]                                               {'loss': 2.2047, 'grad_norm': 3.911829948425293, 'learning_rate': 9.566534241349701e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.44it/s]                                               {'loss': 2.3197, 'grad_norm': 4.956918239593506, 'learning_rate': 9.371298848669095e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.44it/s]                                               {'loss': 2.2574, 'grad_norm': 5.311135292053223, 'learning_rate': 9.176063455988489e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.44it/s]                                               {'loss': 1.7963, 'grad_norm': 7.755804538726807, 'learning_rate': 8.980828063307882e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.44it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.28it/s]                                               {'loss': 2.1806, 'grad_norm': 4.131298542022705, 'learning_rate': 8.785592670627276e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.28it/s]                                               {'loss': 2.4989, 'grad_norm': 4.4433112144470215, 'learning_rate': 8.59035727794667e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.28it/s]                                               {'loss': 2.0493, 'grad_norm': 4.03000545501709, 'learning_rate': 8.395121885266065e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.28it/s] 45%|████▌     | 34/75 [00:01<00:01, 25.94it/s]                                               {'loss': 2.3192, 'grad_norm': 3.7673680782318115, 'learning_rate': 8.19988649258546e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.94it/s]                                               {'loss': 2.2232, 'grad_norm': 5.075781345367432, 'learning_rate': 8.004651099904852e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.94it/s]                                               {'loss': 2.176, 'grad_norm': 3.611410617828369, 'learning_rate': 7.809415707224246e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.94it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.72it/s]                                               {'loss': 2.5796, 'grad_norm': 4.6711626052856445, 'learning_rate': 7.61418031454364e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.72it/s]                                               {'loss': 2.2034, 'grad_norm': 4.115286827087402, 'learning_rate': 7.418944921863034e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.72it/s]                                               {'loss': 2.2822, 'grad_norm': 3.9301598072052, 'learning_rate': 7.223709529182428e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.72it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.276, 'grad_norm': 4.186129093170166, 'learning_rate': 7.028474136501821e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.1969, 'grad_norm': 4.019512176513672, 'learning_rate': 6.833238743821215e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.2845, 'grad_norm': 3.48820161819458, 'learning_rate': 6.638003351140609e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.60it/s] 57%|█████▋    | 43/75 [00:01<00:01, 25.19it/s]                                               {'loss': 2.2464, 'grad_norm': 4.842536449432373, 'learning_rate': 6.442767958460003e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.19it/s]                                               {'loss': 2.13, 'grad_norm': 4.09550666809082, 'learning_rate': 6.247532565779397e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.19it/s]                                               {'loss': 2.6875, 'grad_norm': 16.624675750732422, 'learning_rate': 6.0522971730987906e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.19it/s]                                               {'loss': 2.1737, 'grad_norm': 5.245517730712891, 'learning_rate': 5.8570617804181846e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.19it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.64it/s]                                               {'loss': 2.0865, 'grad_norm': 3.6879403591156006, 'learning_rate': 5.661826387737578e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.64it/s]                                               {'loss': 2.2883, 'grad_norm': 4.3806681632995605, 'learning_rate': 5.466590995056973e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.64it/s]                                               {'loss': 2.2338, 'grad_norm': 3.90523362159729, 'learning_rate': 5.271355602376366e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.64it/s] 67%|██████▋   | 50/75 [00:01<00:00, 26.14it/s]                                               {'loss': 2.183, 'grad_norm': 3.6437582969665527, 'learning_rate': 5.07612020969576e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.14it/s]                                               {'loss': 2.4301, 'grad_norm': 3.755937099456787, 'learning_rate': 4.8808848170151535e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 26.14it/s]                                               {'loss': 1.9755, 'grad_norm': 4.162843227386475, 'learning_rate': 4.6856494243345476e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 26.14it/s] 71%|███████   | 53/75 [00:02<00:00, 25.81it/s]                                               {'loss': 2.1356, 'grad_norm': 5.526220798492432, 'learning_rate': 4.490414031653941e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.81it/s]                                               {'loss': 2.0762, 'grad_norm': 2.9965598583221436, 'learning_rate': 4.295178638973335e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.81it/s]                                               {'loss': 2.3156, 'grad_norm': 4.07892370223999, 'learning_rate': 4.09994324629273e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.81it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.70it/s]                                               {'loss': 2.2164, 'grad_norm': 3.576279878616333, 'learning_rate': 3.904707853612123e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.70it/s]                                               {'loss': 2.5396, 'grad_norm': 5.116130352020264, 'learning_rate': 3.709472460931517e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.70it/s]                                               {'loss': 2.3718, 'grad_norm': 4.198681831359863, 'learning_rate': 3.5142370682509105e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.70it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.30it/s]                                               {'loss': 2.1203, 'grad_norm': 3.872170925140381, 'learning_rate': 3.3190016755703046e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.30it/s]                                               {'loss': 2.2301, 'grad_norm': 8.464081764221191, 'learning_rate': 3.1237662828896986e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.30it/s]                                               {'loss': 2.2097, 'grad_norm': 4.084908485412598, 'learning_rate': 2.9285308902090923e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.30it/s] 83%|████████▎ | 62/75 [00:02<00:00, 25.62it/s]                                               {'loss': 2.1624, 'grad_norm': 5.405035972595215, 'learning_rate': 2.7332954975284864e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.62it/s]                                               {'loss': 1.8693, 'grad_norm': 3.6469640731811523, 'learning_rate': 2.53806010484788e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.62it/s]                                               {'loss': 2.1655, 'grad_norm': 3.432194232940674, 'learning_rate': 2.3428247121672738e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.62it/s] 87%|████████▋ | 65/75 [00:02<00:00, 24.95it/s]                                               {'loss': 2.2255, 'grad_norm': 3.7649385929107666, 'learning_rate': 2.1475893194866675e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.95it/s]                                               {'loss': 2.3286, 'grad_norm': 4.938488960266113, 'learning_rate': 1.9523539268060615e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.95it/s]                                               {'loss': 2.2126, 'grad_norm': 3.681438684463501, 'learning_rate': 1.7571185341254553e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.95it/s] 91%|█████████ | 68/75 [00:02<00:00, 24.11it/s]                                               {'loss': 2.3692, 'grad_norm': 4.666764259338379, 'learning_rate': 1.5618831414448493e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.11it/s]                                               {'loss': 2.272, 'grad_norm': 3.4972622394561768, 'learning_rate': 1.3666477487642432e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.11it/s]                                               {'loss': 2.3292, 'grad_norm': 3.450035572052002, 'learning_rate': 1.1714123560836369e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.11it/s] 95%|█████████▍| 71/75 [00:02<00:00, 24.45it/s]                                               {'loss': 1.9995, 'grad_norm': 4.130795478820801, 'learning_rate': 9.761769634030308e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.45it/s]                                               {'loss': 2.3295, 'grad_norm': 4.249151706695557, 'learning_rate': 7.809415707224247e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.45it/s]                                               {'loss': 2.1567, 'grad_norm': 4.517595291137695, 'learning_rate': 5.8570617804181845e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.45it/s] 99%|█████████▊| 74/75 [00:02<00:00, 25.07it/s]                                               {'loss': 2.3853, 'grad_norm': 4.786818504333496, 'learning_rate': 3.904707853612123e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.07it/s]                                               {'loss': 2.7696, 'grad_norm': 10.86094856262207, 'learning_rate': 1.9523539268060616e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.07it/s]                                               {'train_runtime': 3.0447, 'train_samples_per_second': 371.135, 'train_steps_per_second': 24.633, 'train_loss': 2.27580934047699, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.07it/s]100%|██████████| 75/75 [00:03<00:00, 24.64it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:43
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.418, 'grad_norm': 4.1664958000183105, 'learning_rate': 0.0001464265445104546, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 20.50it/s]                                              {'loss': 2.2303, 'grad_norm': 3.59643816947937, 'learning_rate': 0.00014447419058364857, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 22.70it/s]  4%|▍         | 3/75 [00:00<00:03, 21.90it/s]                                              {'loss': 2.3228, 'grad_norm': 3.364422559738159, 'learning_rate': 0.0001425218366568425, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 21.90it/s]                                              {'loss': 2.2497, 'grad_norm': 4.074527740478516, 'learning_rate': 0.00014056948273003642, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 21.90it/s]                                              {'loss': 2.2788, 'grad_norm': 4.526543617248535, 'learning_rate': 0.00013861712880323037, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 21.90it/s]  8%|▊         | 6/75 [00:00<00:03, 22.62it/s]                                              {'loss': 2.0699, 'grad_norm': 4.0184221267700195, 'learning_rate': 0.0001366647748764243, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 22.62it/s]                                              {'loss': 2.4162, 'grad_norm': 3.4967153072357178, 'learning_rate': 0.00013471242094961826, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 22.62it/s]                                              {'loss': 2.3409, 'grad_norm': 4.089892864227295, 'learning_rate': 0.00013276006702281218, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 22.62it/s] 12%|█▏        | 9/75 [00:00<00:02, 23.71it/s]                                              {'loss': 2.3074, 'grad_norm': 4.277656555175781, 'learning_rate': 0.0001308077130960061, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.71it/s]                                              {'loss': 2.4868, 'grad_norm': 4.305619239807129, 'learning_rate': 0.00012885535916920006, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.71it/s]                                               {'loss': 2.1184, 'grad_norm': 4.075994491577148, 'learning_rate': 0.000126903005242394, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.71it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.56it/s]                                               {'loss': 2.4458, 'grad_norm': 4.322190761566162, 'learning_rate': 0.00012495065131558794, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.56it/s]                                               {'loss': 2.2642, 'grad_norm': 4.2853779792785645, 'learning_rate': 0.00012299829738878187, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.56it/s]                                               {'loss': 2.5562, 'grad_norm': 7.16192102432251, 'learning_rate': 0.00012104594346197581, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.56it/s] 20%|██        | 15/75 [00:00<00:02, 25.40it/s]                                               {'loss': 2.1449, 'grad_norm': 15.740782737731934, 'learning_rate': 0.00011909358953516975, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.40it/s]                                               {'loss': 2.1387, 'grad_norm': 4.299558639526367, 'learning_rate': 0.00011714123560836369, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.40it/s]                                               {'loss': 2.3936, 'grad_norm': 4.308806896209717, 'learning_rate': 0.00011518888168155762, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.40it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.15it/s]                                               {'loss': 2.4489, 'grad_norm': 4.803475379943848, 'learning_rate': 0.00011323652775475156, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.15it/s]                                               {'loss': 2.3059, 'grad_norm': 4.260805606842041, 'learning_rate': 0.0001112841738279455, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.15it/s]                                               {'loss': 2.258, 'grad_norm': 3.846186876296997, 'learning_rate': 0.00010933181990113945, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.15it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.16it/s]                                               {'loss': 2.4977, 'grad_norm': 5.0042548179626465, 'learning_rate': 0.00010737946597433338, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.16it/s]                                               {'loss': 2.3018, 'grad_norm': 4.3234124183654785, 'learning_rate': 0.00010542711204752732, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.16it/s]                                               {'loss': 2.2246, 'grad_norm': 3.1767146587371826, 'learning_rate': 0.00010347475812072126, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.16it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.42it/s]                                               {'loss': 2.2368, 'grad_norm': 3.0451500415802, 'learning_rate': 0.0001015224041939152, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.42it/s]                                               {'loss': 1.9369, 'grad_norm': 3.5179545879364014, 'learning_rate': 9.957005026710914e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.42it/s]                                               {'loss': 2.1284, 'grad_norm': 3.215702533721924, 'learning_rate': 9.761769634030307e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.42it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.30it/s]                                               {'loss': 2.2992, 'grad_norm': 3.5190274715423584, 'learning_rate': 9.566534241349701e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.30it/s]                                               {'loss': 2.0298, 'grad_norm': 3.0673537254333496, 'learning_rate': 9.371298848669095e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.30it/s]                                               {'loss': 2.2016, 'grad_norm': 3.476691722869873, 'learning_rate': 9.176063455988489e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.30it/s] 40%|████      | 30/75 [00:01<00:01, 25.51it/s]                                               {'loss': 2.2252, 'grad_norm': 20.946250915527344, 'learning_rate': 8.980828063307882e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.51it/s]                                               {'loss': 2.1347, 'grad_norm': 3.0347306728363037, 'learning_rate': 8.785592670627276e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.51it/s]                                               {'loss': 2.394, 'grad_norm': 3.7557199001312256, 'learning_rate': 8.59035727794667e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.51it/s] 44%|████▍     | 33/75 [00:01<00:01, 25.18it/s]                                               {'loss': 2.3576, 'grad_norm': 5.131872177124023, 'learning_rate': 8.395121885266065e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.18it/s]                                               {'loss': 2.0873, 'grad_norm': 3.5050582885742188, 'learning_rate': 8.19988649258546e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.18it/s]                                               {'loss': 2.1535, 'grad_norm': 3.481688976287842, 'learning_rate': 8.004651099904852e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.18it/s] 48%|████▊     | 36/75 [00:01<00:01, 24.90it/s]                                               {'loss': 2.4262, 'grad_norm': 4.0256876945495605, 'learning_rate': 7.809415707224246e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.90it/s]                                               {'loss': 2.2033, 'grad_norm': 4.0048112869262695, 'learning_rate': 7.61418031454364e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.90it/s]                                               {'loss': 2.203, 'grad_norm': 4.469626426696777, 'learning_rate': 7.418944921863034e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.90it/s] 52%|█████▏    | 39/75 [00:01<00:01, 24.95it/s]                                               {'loss': 2.2207, 'grad_norm': 4.412182331085205, 'learning_rate': 7.223709529182428e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.95it/s]                                               {'loss': 2.1653, 'grad_norm': 3.2094719409942627, 'learning_rate': 7.028474136501821e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.95it/s]                                               {'loss': 2.4074, 'grad_norm': 4.160839557647705, 'learning_rate': 6.833238743821215e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.95it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.68it/s]                                               {'loss': 2.3392, 'grad_norm': 5.262994766235352, 'learning_rate': 6.638003351140609e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.68it/s]                                               {'loss': 2.158, 'grad_norm': 3.88090181350708, 'learning_rate': 6.442767958460003e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.68it/s]                                               {'loss': 2.1688, 'grad_norm': 3.329284906387329, 'learning_rate': 6.247532565779397e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.68it/s] 60%|██████    | 45/75 [00:01<00:01, 25.72it/s]                                               {'loss': 1.9794, 'grad_norm': 9.285430908203125, 'learning_rate': 6.0522971730987906e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.72it/s]                                               {'loss': 2.2282, 'grad_norm': 3.9571826457977295, 'learning_rate': 5.8570617804181846e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.72it/s]                                               {'loss': 2.1472, 'grad_norm': 3.075477123260498, 'learning_rate': 5.661826387737578e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.72it/s] 64%|██████▍   | 48/75 [00:01<00:01, 24.52it/s]                                               {'loss': 2.0956, 'grad_norm': 3.7072503566741943, 'learning_rate': 5.466590995056973e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 24.52it/s]                                               {'loss': 2.2345, 'grad_norm': 4.11801815032959, 'learning_rate': 5.271355602376366e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 24.52it/s]                                               {'loss': 2.1499, 'grad_norm': 4.0495991706848145, 'learning_rate': 5.07612020969576e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 24.52it/s] 68%|██████▊   | 51/75 [00:02<00:00, 24.61it/s]                                               {'loss': 2.3016, 'grad_norm': 3.4626107215881348, 'learning_rate': 4.8808848170151535e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 24.61it/s]                                               {'loss': 2.0541, 'grad_norm': 2.8448493480682373, 'learning_rate': 4.6856494243345476e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.61it/s]                                               {'loss': 2.34, 'grad_norm': 3.4833476543426514, 'learning_rate': 4.490414031653941e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.61it/s] 72%|███████▏  | 54/75 [00:02<00:00, 24.14it/s]                                               {'loss': 2.1069, 'grad_norm': 4.5663628578186035, 'learning_rate': 4.295178638973335e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.14it/s]                                               {'loss': 2.2063, 'grad_norm': 3.390934705734253, 'learning_rate': 4.09994324629273e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.14it/s]                                               {'loss': 2.229, 'grad_norm': 4.26375150680542, 'learning_rate': 3.904707853612123e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.14it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.04it/s]                                               {'loss': 2.1214, 'grad_norm': 3.684375762939453, 'learning_rate': 3.709472460931517e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.04it/s]                                               {'loss': 2.2823, 'grad_norm': 3.3808164596557617, 'learning_rate': 3.5142370682509105e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.04it/s]                                               {'loss': 2.4177, 'grad_norm': 5.395605087280273, 'learning_rate': 3.3190016755703046e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.04it/s] 80%|████████  | 60/75 [00:02<00:00, 24.74it/s]                                               {'loss': 2.4831, 'grad_norm': 11.811891555786133, 'learning_rate': 3.1237662828896986e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.74it/s]                                               {'loss': 2.304, 'grad_norm': 4.2946457862854, 'learning_rate': 2.9285308902090923e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.74it/s]                                               {'loss': 2.1042, 'grad_norm': 3.7334563732147217, 'learning_rate': 2.7332954975284864e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 24.74it/s] 84%|████████▍ | 63/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.5738, 'grad_norm': 4.388927459716797, 'learning_rate': 2.53806010484788e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.3304, 'grad_norm': 3.878558874130249, 'learning_rate': 2.3428247121672738e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.2578, 'grad_norm': 4.5189900398254395, 'learning_rate': 2.1475893194866675e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.33it/s] 88%|████████▊ | 66/75 [00:02<00:00, 24.90it/s]                                               {'loss': 2.0501, 'grad_norm': 6.106528282165527, 'learning_rate': 1.9523539268060615e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.90it/s]                                               {'loss': 1.9395, 'grad_norm': 2.8754639625549316, 'learning_rate': 1.7571185341254553e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.90it/s]                                               {'loss': 2.1399, 'grad_norm': 3.7291533946990967, 'learning_rate': 1.5618831414448493e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.90it/s] 92%|█████████▏| 69/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.1269, 'grad_norm': 3.948369264602661, 'learning_rate': 1.3666477487642432e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.1319, 'grad_norm': 3.9959805011749268, 'learning_rate': 1.1714123560836369e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.205, 'grad_norm': 3.938905715942383, 'learning_rate': 9.761769634030308e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.83it/s] 96%|█████████▌| 72/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.2525, 'grad_norm': 3.672785520553589, 'learning_rate': 7.809415707224247e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.1447, 'grad_norm': 3.7223122119903564, 'learning_rate': 5.8570617804181845e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.261, 'grad_norm': 3.8655433654785156, 'learning_rate': 3.904707853612123e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.83it/s]                                               {'loss': 2.8298, 'grad_norm': 14.12500286102295, 'learning_rate': 1.9523539268060616e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.83it/s]                                               {'train_runtime': 3.1386, 'train_samples_per_second': 360.029, 'train_steps_per_second': 23.896, 'train_loss': 2.2496677462259926, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.83it/s]100%|██████████| 75/75 [00:03<00:00, 23.90it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2687, 'grad_norm': 3.498095750808716, 'learning_rate': 0.0001464265445104546, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.65it/s]                                              {'loss': 2.5093, 'grad_norm': 3.9900245666503906, 'learning_rate': 0.00014447419058364857, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.37it/s]  4%|▍         | 3/75 [00:00<00:02, 24.65it/s]                                              {'loss': 2.2116, 'grad_norm': 4.17756462097168, 'learning_rate': 0.0001425218366568425, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.65it/s]                                              {'loss': 2.1031, 'grad_norm': 4.516848087310791, 'learning_rate': 0.00014056948273003642, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.65it/s]                                              {'loss': 2.2229, 'grad_norm': 3.6632940769195557, 'learning_rate': 0.00013861712880323037, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.65it/s]  8%|▊         | 6/75 [00:00<00:02, 23.31it/s]                                              {'loss': 2.5053, 'grad_norm': 4.065542697906494, 'learning_rate': 0.0001366647748764243, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.31it/s]                                              {'loss': 2.3693, 'grad_norm': 3.7527692317962646, 'learning_rate': 0.00013471242094961826, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.31it/s]                                              {'loss': 2.147, 'grad_norm': 3.919246196746826, 'learning_rate': 0.00013276006702281218, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.31it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.05it/s]                                              {'loss': 2.2382, 'grad_norm': 3.7343945503234863, 'learning_rate': 0.0001308077130960061, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.05it/s]                                              {'loss': 2.1733, 'grad_norm': 4.696538925170898, 'learning_rate': 0.00012885535916920006, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.05it/s]                                               {'loss': 2.3503, 'grad_norm': 4.282246112823486, 'learning_rate': 0.000126903005242394, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.05it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.68it/s]                                               {'loss': 2.2895, 'grad_norm': 3.9280846118927, 'learning_rate': 0.00012495065131558794, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.68it/s]                                               {'loss': 2.3687, 'grad_norm': 3.8104634284973145, 'learning_rate': 0.00012299829738878187, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.68it/s]                                               {'loss': 2.3307, 'grad_norm': 4.835028648376465, 'learning_rate': 0.00012104594346197581, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.68it/s] 20%|██        | 15/75 [00:00<00:02, 25.10it/s]                                               {'loss': 2.5766, 'grad_norm': 11.65849494934082, 'learning_rate': 0.00011909358953516975, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.10it/s]                                               {'loss': 2.2896, 'grad_norm': 4.119986534118652, 'learning_rate': 0.00011714123560836369, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.10it/s]                                               {'loss': 2.3831, 'grad_norm': 4.640582084655762, 'learning_rate': 0.00011518888168155762, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.10it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.08it/s]                                               {'loss': 2.3345, 'grad_norm': 4.824131488800049, 'learning_rate': 0.00011323652775475156, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.08it/s]                                               {'loss': 2.239, 'grad_norm': 3.342012882232666, 'learning_rate': 0.0001112841738279455, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.08it/s]                                               {'loss': 2.371, 'grad_norm': 3.8250653743743896, 'learning_rate': 0.00010933181990113945, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.08it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.29it/s]                                               {'loss': 2.3703, 'grad_norm': 3.800753355026245, 'learning_rate': 0.00010737946597433338, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.29it/s]                                               {'loss': 2.2484, 'grad_norm': 4.391905784606934, 'learning_rate': 0.00010542711204752732, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.29it/s]                                               {'loss': 2.3045, 'grad_norm': 3.9869472980499268, 'learning_rate': 0.00010347475812072126, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.29it/s] 32%|███▏      | 24/75 [00:00<00:02, 23.91it/s]                                               {'loss': 2.1625, 'grad_norm': 4.134521007537842, 'learning_rate': 0.0001015224041939152, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 23.91it/s]                                               {'loss': 2.2775, 'grad_norm': 4.103081703186035, 'learning_rate': 9.957005026710914e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.91it/s]                                               {'loss': 2.126, 'grad_norm': 4.178548336029053, 'learning_rate': 9.761769634030307e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.91it/s] 36%|███▌      | 27/75 [00:01<00:02, 23.93it/s]                                               {'loss': 2.1296, 'grad_norm': 4.260457515716553, 'learning_rate': 9.566534241349701e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 23.93it/s]                                               {'loss': 2.1614, 'grad_norm': 3.1930477619171143, 'learning_rate': 9.371298848669095e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 23.93it/s]                                               {'loss': 2.0558, 'grad_norm': 3.8069870471954346, 'learning_rate': 9.176063455988489e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 23.93it/s] 40%|████      | 30/75 [00:01<00:01, 25.52it/s]                                               {'loss': 2.8408, 'grad_norm': 13.163676261901855, 'learning_rate': 8.980828063307882e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.52it/s]                                               {'loss': 2.0786, 'grad_norm': 3.6117212772369385, 'learning_rate': 8.785592670627276e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.52it/s]                                               {'loss': 2.3584, 'grad_norm': 3.9501659870147705, 'learning_rate': 8.59035727794667e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.52it/s] 44%|████▍     | 33/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.3782, 'grad_norm': 3.8232786655426025, 'learning_rate': 8.395121885266065e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.273, 'grad_norm': 3.6393449306488037, 'learning_rate': 8.19988649258546e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.001, 'grad_norm': 2.96050763130188, 'learning_rate': 8.004651099904852e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.30it/s] 48%|████▊     | 36/75 [00:01<00:01, 25.61it/s]                                               {'loss': 2.1746, 'grad_norm': 4.347661972045898, 'learning_rate': 7.809415707224246e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.61it/s]                                               {'loss': 2.2545, 'grad_norm': 3.3939104080200195, 'learning_rate': 7.61418031454364e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.61it/s]                                               {'loss': 2.3179, 'grad_norm': 4.038731098175049, 'learning_rate': 7.418944921863034e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.61it/s] 52%|█████▏    | 39/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.1821, 'grad_norm': 3.6910793781280518, 'learning_rate': 7.223709529182428e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.1671, 'grad_norm': 3.65850830078125, 'learning_rate': 7.028474136501821e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.1489, 'grad_norm': 3.266460657119751, 'learning_rate': 6.833238743821215e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.34it/s] 56%|█████▌    | 42/75 [00:01<00:01, 25.08it/s]                                               {'loss': 2.2631, 'grad_norm': 3.553663730621338, 'learning_rate': 6.638003351140609e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.08it/s]                                               {'loss': 1.9736, 'grad_norm': 4.286458492279053, 'learning_rate': 6.442767958460003e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.08it/s]                                               {'loss': 2.1938, 'grad_norm': 3.3613786697387695, 'learning_rate': 6.247532565779397e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.08it/s] 60%|██████    | 45/75 [00:01<00:01, 26.18it/s]                                               {'loss': 2.8179, 'grad_norm': 13.162912368774414, 'learning_rate': 6.0522971730987906e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 26.18it/s]                                               {'loss': 2.2659, 'grad_norm': 3.2551865577697754, 'learning_rate': 5.8570617804181846e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.18it/s]                                               {'loss': 2.4212, 'grad_norm': 4.4130449295043945, 'learning_rate': 5.661826387737578e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.18it/s] 64%|██████▍   | 48/75 [00:01<00:01, 25.02it/s]                                               {'loss': 2.2573, 'grad_norm': 4.882500648498535, 'learning_rate': 5.466590995056973e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.02it/s]                                               {'loss': 2.1345, 'grad_norm': 3.4456279277801514, 'learning_rate': 5.271355602376366e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.02it/s]                                               {'loss': 2.0337, 'grad_norm': 3.051959276199341, 'learning_rate': 5.07612020969576e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.02it/s] 68%|██████▊   | 51/75 [00:02<00:01, 23.99it/s]                                               {'loss': 2.1502, 'grad_norm': 3.340327024459839, 'learning_rate': 4.8808848170151535e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 23.99it/s]                                               {'loss': 2.2203, 'grad_norm': 4.049910545349121, 'learning_rate': 4.6856494243345476e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 23.99it/s]                                               {'loss': 1.9495, 'grad_norm': 4.110718250274658, 'learning_rate': 4.490414031653941e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 23.99it/s] 72%|███████▏  | 54/75 [00:02<00:00, 23.36it/s]                                               {'loss': 2.497, 'grad_norm': 4.954830646514893, 'learning_rate': 4.295178638973335e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 23.36it/s]                                               {'loss': 2.0421, 'grad_norm': 3.6586837768554688, 'learning_rate': 4.09994324629273e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 23.36it/s]                                               {'loss': 2.1752, 'grad_norm': 3.5623838901519775, 'learning_rate': 3.904707853612123e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 23.36it/s] 76%|███████▌  | 57/75 [00:02<00:00, 23.93it/s]                                               {'loss': 2.115, 'grad_norm': 3.9510843753814697, 'learning_rate': 3.709472460931517e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 23.93it/s]                                               {'loss': 2.3224, 'grad_norm': 2.905410051345825, 'learning_rate': 3.5142370682509105e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 23.93it/s]                                               {'loss': 2.1029, 'grad_norm': 3.589897394180298, 'learning_rate': 3.3190016755703046e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 23.93it/s]                                               {'loss': 2.0748, 'grad_norm': 9.623113632202148, 'learning_rate': 3.1237662828896986e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 23.93it/s] 81%|████████▏ | 61/75 [00:02<00:00, 25.96it/s]                                               {'loss': 1.9937, 'grad_norm': 3.456519842147827, 'learning_rate': 2.9285308902090923e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.96it/s]                                               {'loss': 2.1269, 'grad_norm': 4.199491500854492, 'learning_rate': 2.7332954975284864e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.96it/s]                                               {'loss': 2.2827, 'grad_norm': 4.096591949462891, 'learning_rate': 2.53806010484788e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.96it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.84it/s]                                               {'loss': 2.059, 'grad_norm': 3.020670175552368, 'learning_rate': 2.3428247121672738e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.84it/s]                                               {'loss': 2.2151, 'grad_norm': 3.956867218017578, 'learning_rate': 2.1475893194866675e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.84it/s]                                               {'loss': 2.1466, 'grad_norm': 3.742095708847046, 'learning_rate': 1.9523539268060615e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.84it/s] 89%|████████▉ | 67/75 [00:02<00:00, 24.43it/s]                                               {'loss': 2.246, 'grad_norm': 4.646795749664307, 'learning_rate': 1.7571185341254553e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.43it/s]                                               {'loss': 2.2598, 'grad_norm': 3.971322536468506, 'learning_rate': 1.5618831414448493e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.43it/s]                                               {'loss': 2.3947, 'grad_norm': 4.251433849334717, 'learning_rate': 1.3666477487642432e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.43it/s] 93%|█████████▎| 70/75 [00:02<00:00, 23.73it/s]                                               {'loss': 2.0198, 'grad_norm': 2.8917312622070312, 'learning_rate': 1.1714123560836369e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 23.73it/s]                                               {'loss': 2.0403, 'grad_norm': 3.328057289123535, 'learning_rate': 9.761769634030308e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 23.73it/s]                                               {'loss': 2.2595, 'grad_norm': 3.855811595916748, 'learning_rate': 7.809415707224247e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 23.73it/s] 97%|█████████▋| 73/75 [00:02<00:00, 23.28it/s]                                               {'loss': 2.3741, 'grad_norm': 4.128379821777344, 'learning_rate': 5.8570617804181845e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 23.28it/s]                                               {'loss': 2.2154, 'grad_norm': 3.666370153427124, 'learning_rate': 3.904707853612123e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 23.28it/s]                                               {'loss': 1.8333, 'grad_norm': 10.689867973327637, 'learning_rate': 1.9523539268060616e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 23.28it/s]                                               {'train_runtime': 3.1607, 'train_samples_per_second': 357.515, 'train_steps_per_second': 23.729, 'train_loss': 2.2369372113545736, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 23.28it/s]100%|██████████| 75/75 [00:03<00:00, 23.73it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:14
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.1601, 'grad_norm': 4.959930896759033, 'learning_rate': 0.0001464265445104546, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 22.13it/s]                                              {'loss': 2.2627, 'grad_norm': 3.9179162979125977, 'learning_rate': 0.00014447419058364857, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.26it/s]  4%|▍         | 3/75 [00:00<00:02, 24.27it/s]                                              {'loss': 2.2172, 'grad_norm': 4.035315990447998, 'learning_rate': 0.0001425218366568425, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.27it/s]                                              {'loss': 2.232, 'grad_norm': 3.8608107566833496, 'learning_rate': 0.00014056948273003642, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.27it/s]                                              {'loss': 2.1701, 'grad_norm': 4.018940448760986, 'learning_rate': 0.00013861712880323037, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.27it/s]  8%|▊         | 6/75 [00:00<00:02, 23.42it/s]                                              {'loss': 2.1541, 'grad_norm': 4.213586330413818, 'learning_rate': 0.0001366647748764243, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.42it/s]                                              {'loss': 2.2473, 'grad_norm': 4.143962383270264, 'learning_rate': 0.00013471242094961826, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.42it/s]                                              {'loss': 2.2077, 'grad_norm': 4.383572578430176, 'learning_rate': 0.00013276006702281218, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.42it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.05it/s]                                              {'loss': 2.2372, 'grad_norm': 3.904449224472046, 'learning_rate': 0.0001308077130960061, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.05it/s]                                              {'loss': 2.2608, 'grad_norm': 3.88803768157959, 'learning_rate': 0.00012885535916920006, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.05it/s]                                               {'loss': 2.218, 'grad_norm': 5.849546909332275, 'learning_rate': 0.000126903005242394, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.05it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.42it/s]                                               {'loss': 2.2353, 'grad_norm': 4.283213138580322, 'learning_rate': 0.00012495065131558794, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.42it/s]                                               {'loss': 2.3619, 'grad_norm': 4.490012168884277, 'learning_rate': 0.00012299829738878187, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.42it/s]                                               {'loss': 2.3038, 'grad_norm': 5.181149005889893, 'learning_rate': 0.00012104594346197581, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.42it/s] 20%|██        | 15/75 [00:00<00:02, 26.15it/s]                                               {'loss': 2.3153, 'grad_norm': 19.541170120239258, 'learning_rate': 0.00011909358953516975, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 26.15it/s]                                               {'loss': 2.2299, 'grad_norm': 5.659525394439697, 'learning_rate': 0.00011714123560836369, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.15it/s]                                               {'loss': 2.2401, 'grad_norm': 4.308511734008789, 'learning_rate': 0.00011518888168155762, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.15it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.38it/s]                                               {'loss': 2.0857, 'grad_norm': 4.054260730743408, 'learning_rate': 0.00011323652775475156, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.38it/s]                                               {'loss': 2.2576, 'grad_norm': 4.523088455200195, 'learning_rate': 0.0001112841738279455, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.38it/s]                                               {'loss': 2.1866, 'grad_norm': 4.681704521179199, 'learning_rate': 0.00010933181990113945, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.38it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.30it/s]                                               {'loss': 2.0437, 'grad_norm': 3.513000011444092, 'learning_rate': 0.00010737946597433338, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.30it/s]                                               {'loss': 2.3576, 'grad_norm': 3.6851353645324707, 'learning_rate': 0.00010542711204752732, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.30it/s]                                               {'loss': 2.3688, 'grad_norm': 4.375795364379883, 'learning_rate': 0.00010347475812072126, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.30it/s] 32%|███▏      | 24/75 [00:00<00:02, 23.62it/s]                                               {'loss': 2.2877, 'grad_norm': 4.305335998535156, 'learning_rate': 0.0001015224041939152, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 23.62it/s]                                               {'loss': 1.921, 'grad_norm': 4.637398719787598, 'learning_rate': 9.957005026710914e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.62it/s]                                               {'loss': 2.0247, 'grad_norm': 3.4738802909851074, 'learning_rate': 9.761769634030307e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.62it/s] 36%|███▌      | 27/75 [00:01<00:02, 23.47it/s]                                               {'loss': 2.0035, 'grad_norm': 4.011693000793457, 'learning_rate': 9.566534241349701e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 23.47it/s]                                               {'loss': 2.2786, 'grad_norm': 4.514872074127197, 'learning_rate': 9.371298848669095e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 23.47it/s]                                               {'loss': 2.2062, 'grad_norm': 4.216403484344482, 'learning_rate': 9.176063455988489e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 23.47it/s]                                               {'loss': 1.9554, 'grad_norm': 13.341196060180664, 'learning_rate': 8.980828063307882e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.47it/s] 41%|████▏     | 31/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.3149, 'grad_norm': 3.640700101852417, 'learning_rate': 8.785592670627276e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.0363, 'grad_norm': 3.528308629989624, 'learning_rate': 8.59035727794667e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.1151, 'grad_norm': 3.3709354400634766, 'learning_rate': 8.395121885266065e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.00it/s] 45%|████▌     | 34/75 [00:01<00:01, 23.96it/s]                                               {'loss': 2.0954, 'grad_norm': 3.6887800693511963, 'learning_rate': 8.19988649258546e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 23.96it/s]                                               {'loss': 2.2159, 'grad_norm': 5.643962383270264, 'learning_rate': 8.004651099904852e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 23.96it/s]                                               {'loss': 1.9712, 'grad_norm': 3.7075815200805664, 'learning_rate': 7.809415707224246e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 23.96it/s] 49%|████▉     | 37/75 [00:01<00:01, 23.56it/s]                                               {'loss': 2.1252, 'grad_norm': 4.459619045257568, 'learning_rate': 7.61418031454364e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 23.56it/s]                                               {'loss': 2.3177, 'grad_norm': 3.776549816131592, 'learning_rate': 7.418944921863034e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 23.56it/s]                                               {'loss': 2.2238, 'grad_norm': 3.6783645153045654, 'learning_rate': 7.223709529182428e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.56it/s] 53%|█████▎    | 40/75 [00:01<00:01, 23.94it/s]                                               {'loss': 2.2142, 'grad_norm': 4.077240467071533, 'learning_rate': 7.028474136501821e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.94it/s]                                               {'loss': 2.1231, 'grad_norm': 4.8430562019348145, 'learning_rate': 6.833238743821215e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 23.94it/s]                                               {'loss': 2.0, 'grad_norm': 3.586864471435547, 'learning_rate': 6.638003351140609e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 23.94it/s] 57%|█████▋    | 43/75 [00:01<00:01, 23.64it/s]                                               {'loss': 2.1859, 'grad_norm': 3.609189033508301, 'learning_rate': 6.442767958460003e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 23.64it/s]                                               {'loss': 2.1511, 'grad_norm': 3.7883477210998535, 'learning_rate': 6.247532565779397e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 23.64it/s]                                               {'loss': 2.3906, 'grad_norm': 17.68319320678711, 'learning_rate': 6.0522971730987906e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 23.64it/s] 61%|██████▏   | 46/75 [00:01<00:01, 24.21it/s]                                               {'loss': 2.1031, 'grad_norm': 3.8370611667633057, 'learning_rate': 5.8570617804181846e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.21it/s]                                               {'loss': 2.1121, 'grad_norm': 4.1242876052856445, 'learning_rate': 5.661826387737578e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 24.21it/s]                                               {'loss': 2.0392, 'grad_norm': 3.7118732929229736, 'learning_rate': 5.466590995056973e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 24.21it/s] 65%|██████▌   | 49/75 [00:02<00:01, 24.19it/s]                                               {'loss': 2.1062, 'grad_norm': 5.057440280914307, 'learning_rate': 5.271355602376366e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 24.19it/s]                                               {'loss': 2.1734, 'grad_norm': 3.8625478744506836, 'learning_rate': 5.07612020969576e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 24.19it/s]                                               {'loss': 2.2543, 'grad_norm': 4.220641136169434, 'learning_rate': 4.8808848170151535e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 24.19it/s] 69%|██████▉   | 52/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.1365, 'grad_norm': 4.73085880279541, 'learning_rate': 4.6856494243345476e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.2195, 'grad_norm': 5.063293933868408, 'learning_rate': 4.490414031653941e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.0067, 'grad_norm': 3.8651692867279053, 'learning_rate': 4.295178638973335e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.53it/s] 73%|███████▎  | 55/75 [00:02<00:00, 24.79it/s]                                               {'loss': 2.2004, 'grad_norm': 3.5858423709869385, 'learning_rate': 4.09994324629273e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.79it/s]                                               {'loss': 2.1979, 'grad_norm': 3.477612257003784, 'learning_rate': 3.904707853612123e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.79it/s]                                               {'loss': 2.2368, 'grad_norm': 3.8087387084960938, 'learning_rate': 3.709472460931517e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.79it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.65it/s]                                               {'loss': 1.9533, 'grad_norm': 3.82038950920105, 'learning_rate': 3.5142370682509105e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.65it/s]                                               {'loss': 2.1712, 'grad_norm': 3.0147476196289062, 'learning_rate': 3.3190016755703046e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.65it/s]                                               {'loss': 1.8824, 'grad_norm': 8.501412391662598, 'learning_rate': 3.1237662828896986e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.65it/s]                                               {'loss': 2.183, 'grad_norm': 3.693763256072998, 'learning_rate': 2.9285308902090923e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.65it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.32it/s]                                               {'loss': 2.2968, 'grad_norm': 4.7340006828308105, 'learning_rate': 2.7332954975284864e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.32it/s]                                               {'loss': 1.9972, 'grad_norm': 3.9902570247650146, 'learning_rate': 2.53806010484788e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.32it/s]                                               {'loss': 2.3712, 'grad_norm': 6.235423564910889, 'learning_rate': 2.3428247121672738e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.32it/s] 87%|████████▋ | 65/75 [00:02<00:00, 26.21it/s]                                               {'loss': 2.0503, 'grad_norm': 3.735246419906616, 'learning_rate': 2.1475893194866675e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.21it/s]                                               {'loss': 1.914, 'grad_norm': 2.868557929992676, 'learning_rate': 1.9523539268060615e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.21it/s]                                               {'loss': 2.0609, 'grad_norm': 3.624720573425293, 'learning_rate': 1.7571185341254553e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.21it/s] 91%|█████████ | 68/75 [00:02<00:00, 25.95it/s]                                               {'loss': 1.9981, 'grad_norm': 4.3530144691467285, 'learning_rate': 1.5618831414448493e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.95it/s]                                               {'loss': 2.1862, 'grad_norm': 3.6468658447265625, 'learning_rate': 1.3666477487642432e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.95it/s]                                               {'loss': 2.2911, 'grad_norm': 4.054370403289795, 'learning_rate': 1.1714123560836369e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.95it/s] 95%|█████████▍| 71/75 [00:02<00:00, 25.18it/s]                                               {'loss': 2.0758, 'grad_norm': 4.473254680633545, 'learning_rate': 9.761769634030308e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.18it/s]                                               {'loss': 2.0121, 'grad_norm': 3.262843608856201, 'learning_rate': 7.809415707224247e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.18it/s]                                               {'loss': 1.9401, 'grad_norm': 4.245229721069336, 'learning_rate': 5.8570617804181845e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.18it/s] 99%|█████████▊| 74/75 [00:03<00:00, 25.09it/s]                                               {'loss': 2.1153, 'grad_norm': 3.441580295562744, 'learning_rate': 3.904707853612123e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 25.09it/s]                                               {'loss': 2.1466, 'grad_norm': 7.662574291229248, 'learning_rate': 1.9523539268060616e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.09it/s]                                               {'train_runtime': 3.1216, 'train_samples_per_second': 361.997, 'train_steps_per_second': 24.026, 'train_loss': 2.1592332299550376, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.09it/s]100%|██████████| 75/75 [00:03<00:00, 24.03it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1050, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(925, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(789, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(918, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1162, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(955, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1069, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1035, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1060, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(791, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1272, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(688, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 64.53it/s]  3%|▎         | 14/471 [00:00<00:08, 56.78it/s]  4%|▍         | 20/471 [00:00<00:08, 55.13it/s]  6%|▌         | 26/471 [00:00<00:08, 51.45it/s]  7%|▋         | 32/471 [00:00<00:08, 51.31it/s]  8%|▊         | 38/471 [00:00<00:08, 51.04it/s]  9%|▉         | 44/471 [00:00<00:08, 49.80it/s] 10%|█         | 49/471 [00:00<00:08, 47.16it/s] 11%|█▏        | 54/471 [00:01<00:08, 47.42it/s] 13%|█▎        | 60/471 [00:01<00:08, 48.26it/s] 14%|█▍        | 66/471 [00:01<00:08, 49.01it/s] 15%|█▌        | 72/471 [00:01<00:07, 50.01it/s] 17%|█▋        | 78/471 [00:01<00:07, 49.52it/s] 18%|█▊        | 83/471 [00:01<00:07, 49.20it/s] 19%|█▉        | 89/471 [00:01<00:07, 50.16it/s] 20%|██        | 95/471 [00:01<00:07, 50.69it/s] 21%|██▏       | 101/471 [00:02<00:07, 50.78it/s] 23%|██▎       | 107/471 [00:02<00:07, 51.39it/s] 24%|██▍       | 113/471 [00:02<00:06, 51.25it/s] 25%|██▌       | 119/471 [00:02<00:07, 50.02it/s] 27%|██▋       | 125/471 [00:02<00:06, 51.11it/s] 28%|██▊       | 131/471 [00:02<00:06, 51.29it/s] 29%|██▉       | 137/471 [00:02<00:06, 50.49it/s] 30%|███       | 143/471 [00:02<00:06, 50.52it/s] 32%|███▏      | 149/471 [00:02<00:06, 51.06it/s] 33%|███▎      | 155/471 [00:03<00:06, 51.45it/s] 34%|███▍      | 161/471 [00:03<00:06, 51.52it/s] 35%|███▌      | 167/471 [00:03<00:05, 51.45it/s] 37%|███▋      | 173/471 [00:03<00:05, 51.05it/s] 38%|███▊      | 179/471 [00:03<00:05, 51.27it/s] 39%|███▉      | 185/471 [00:03<00:05, 51.40it/s] 41%|████      | 191/471 [00:03<00:05, 51.47it/s] 42%|████▏     | 197/471 [00:03<00:05, 51.80it/s] 43%|████▎     | 203/471 [00:03<00:05, 51.84it/s] 44%|████▍     | 209/471 [00:04<00:05, 51.87it/s] 46%|████▌     | 215/471 [00:04<00:04, 51.73it/s] 47%|████▋     | 221/471 [00:04<00:04, 51.07it/s] 48%|████▊     | 227/471 [00:04<00:04, 51.53it/s] 49%|████▉     | 233/471 [00:04<00:04, 50.93it/s] 51%|█████     | 239/471 [00:04<00:04, 51.15it/s] 52%|█████▏    | 245/471 [00:04<00:04, 51.35it/s] 53%|█████▎    | 251/471 [00:04<00:04, 51.46it/s] 55%|█████▍    | 257/471 [00:05<00:04, 50.54it/s] 56%|█████▌    | 263/471 [00:05<00:04, 50.67it/s] 57%|█████▋    | 269/471 [00:05<00:04, 50.49it/s] 58%|█████▊    | 275/471 [00:05<00:03, 49.75it/s] 59%|█████▉    | 280/471 [00:05<00:03, 49.65it/s] 61%|██████    | 285/471 [00:05<00:03, 48.55it/s] 62%|██████▏   | 291/471 [00:05<00:03, 49.97it/s] 63%|██████▎   | 297/471 [00:05<00:03, 50.30it/s] 64%|██████▍   | 303/471 [00:05<00:03, 50.75it/s] 66%|██████▌   | 309/471 [00:06<00:03, 51.06it/s] 67%|██████▋   | 315/471 [00:06<00:03, 51.31it/s] 68%|██████▊   | 321/471 [00:06<00:02, 51.29it/s] 69%|██████▉   | 327/471 [00:06<00:02, 50.22it/s] 71%|███████   | 333/471 [00:06<00:02, 50.55it/s] 72%|███████▏  | 339/471 [00:06<00:02, 49.99it/s] 73%|███████▎  | 345/471 [00:06<00:02, 49.29it/s] 75%|███████▍  | 351/471 [00:06<00:02, 49.79it/s] 76%|███████▌  | 357/471 [00:07<00:02, 50.39it/s] 77%|███████▋  | 363/471 [00:07<00:02, 50.85it/s] 78%|███████▊  | 369/471 [00:07<00:02, 50.63it/s] 80%|███████▉  | 375/471 [00:07<00:01, 50.24it/s] 81%|████████  | 381/471 [00:07<00:01, 50.95it/s] 82%|████████▏ | 387/471 [00:07<00:01, 51.17it/s] 83%|████████▎ | 393/471 [00:07<00:01, 49.93it/s] 85%|████████▍ | 399/471 [00:07<00:01, 50.81it/s] 86%|████████▌ | 405/471 [00:07<00:01, 51.07it/s] 87%|████████▋ | 411/471 [00:08<00:01, 51.28it/s] 89%|████████▊ | 417/471 [00:08<00:01, 51.59it/s] 90%|████████▉ | 423/471 [00:08<00:00, 51.43it/s] 91%|█████████ | 429/471 [00:08<00:00, 51.30it/s] 92%|█████████▏| 435/471 [00:08<00:00, 49.98it/s] 94%|█████████▎| 441/471 [00:08<00:00, 49.39it/s] 95%|█████████▍| 447/471 [00:08<00:00, 49.77it/s] 96%|█████████▌| 453/471 [00:08<00:00, 49.39it/s] 97%|█████████▋| 459/471 [00:09<00:00, 49.75it/s] 99%|█████████▊| 464/471 [00:09<00:00, 48.89it/s]100%|█████████▉| 470/471 [00:09<00:00, 49.88it/s]100%|██████████| 471/471 [00:09<00:00, 50.65it/s]
{'eval_loss': 2.3015143871307373, 'eval_model_preparation_time': 0.0024, 'eval_acc': 0.3870154009559214, 'eval_runtime': 9.3195, 'eval_samples_per_second': 808.197, 'eval_steps_per_second': 50.539}
ROUND:12
CLIENT:27
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2319, 'grad_norm': 4.01456880569458, 'learning_rate': 0.00014316767251549832, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 15.37it/s]  3%|▎         | 2/75 [00:00<00:03, 19.24it/s]                                              {'loss': 2.4382, 'grad_norm': 4.302015781402588, 'learning_rate': 0.00014125877021529167, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 19.24it/s]                                              {'loss': 2.189, 'grad_norm': 4.80349063873291, 'learning_rate': 0.00013934986791508503, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 19.24it/s]                                              {'loss': 2.1994, 'grad_norm': 3.59472918510437, 'learning_rate': 0.00013744096561487838, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 19.24it/s]  7%|▋         | 5/75 [00:00<00:03, 22.06it/s]                                              {'loss': 2.2886, 'grad_norm': 4.320800304412842, 'learning_rate': 0.00013553206331467173, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.06it/s]                                              {'loss': 2.1729, 'grad_norm': 3.9688026905059814, 'learning_rate': 0.0001336231610144651, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 22.06it/s]                                              {'loss': 2.3079, 'grad_norm': 3.860778570175171, 'learning_rate': 0.00013171425871425846, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 22.06it/s] 11%|█         | 8/75 [00:00<00:02, 22.77it/s]                                              {'loss': 2.4034, 'grad_norm': 4.111429214477539, 'learning_rate': 0.00012980535641405182, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 22.77it/s]                                              {'loss': 2.2616, 'grad_norm': 3.8986237049102783, 'learning_rate': 0.00012789645411384517, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 22.77it/s]                                              {'loss': 2.411, 'grad_norm': 3.162905216217041, 'learning_rate': 0.00012598755181363852, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 22.77it/s] 15%|█▍        | 11/75 [00:00<00:02, 23.51it/s]                                               {'loss': 2.4199, 'grad_norm': 6.282540798187256, 'learning_rate': 0.00012407864951343188, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.51it/s]                                               {'loss': 2.3022, 'grad_norm': 3.53844952583313, 'learning_rate': 0.00012216974721322523, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.51it/s]                                               {'loss': 2.2035, 'grad_norm': 4.068609714508057, 'learning_rate': 0.00012026084491301858, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.51it/s] 19%|█▊        | 14/75 [00:00<00:02, 23.91it/s]                                               {'loss': 2.4822, 'grad_norm': 4.818439960479736, 'learning_rate': 0.00011835194261281195, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.91it/s]                                               {'loss': 2.2087, 'grad_norm': 13.026480674743652, 'learning_rate': 0.0001164430403126053, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.91it/s]                                               {'loss': 2.2118, 'grad_norm': 3.1617937088012695, 'learning_rate': 0.00011453413801239867, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 23.91it/s]                                               {'loss': 2.2106, 'grad_norm': 3.4289913177490234, 'learning_rate': 0.000112625235712192, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 23.91it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.60it/s]                                               {'loss': 2.3522, 'grad_norm': 4.383339881896973, 'learning_rate': 0.00011071633341198536, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.60it/s]                                               {'loss': 2.0558, 'grad_norm': 3.8049705028533936, 'learning_rate': 0.00010880743111177873, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.60it/s]                                               {'loss': 2.2778, 'grad_norm': 3.862865924835205, 'learning_rate': 0.00010689852881157208, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.60it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.85it/s]                                               {'loss': 2.4192, 'grad_norm': 3.5976619720458984, 'learning_rate': 0.00010498962651136543, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.85it/s]                                               {'loss': 2.3748, 'grad_norm': 3.768066167831421, 'learning_rate': 0.00010308072421115878, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.85it/s]                                               {'loss': 2.3842, 'grad_norm': 5.198188304901123, 'learning_rate': 0.00010117182191095215, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.85it/s] 32%|███▏      | 24/75 [00:01<00:02, 23.76it/s]                                               {'loss': 2.1964, 'grad_norm': 3.3624517917633057, 'learning_rate': 9.92629196107455e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 23.76it/s]                                               {'loss': 2.0803, 'grad_norm': 2.9427928924560547, 'learning_rate': 9.735401731053887e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.76it/s]                                               {'loss': 2.3336, 'grad_norm': 5.053984642028809, 'learning_rate': 9.544511501033221e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.76it/s] 36%|███▌      | 27/75 [00:01<00:02, 22.98it/s]                                               {'loss': 2.1553, 'grad_norm': 4.11760139465332, 'learning_rate': 9.353621271012556e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 22.98it/s]                                               {'loss': 2.0816, 'grad_norm': 4.748116970062256, 'learning_rate': 9.162731040991893e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 22.98it/s]                                               {'loss': 2.4908, 'grad_norm': 3.9373881816864014, 'learning_rate': 8.971840810971228e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 22.98it/s] 40%|████      | 30/75 [00:01<00:01, 24.45it/s]                                               {'loss': 1.4565, 'grad_norm': 6.539121150970459, 'learning_rate': 8.780950580950563e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.45it/s]                                               {'loss': 2.0132, 'grad_norm': 3.626380681991577, 'learning_rate': 8.590060350929899e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.45it/s]                                               {'loss': 2.1263, 'grad_norm': 3.7609386444091797, 'learning_rate': 8.399170120909235e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.45it/s] 44%|████▍     | 33/75 [00:01<00:01, 24.76it/s]                                               {'loss': 2.1814, 'grad_norm': 4.284518241882324, 'learning_rate': 8.20827989088857e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.76it/s]                                               {'loss': 2.1808, 'grad_norm': 3.2061142921447754, 'learning_rate': 8.017389660867907e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.76it/s]                                               {'loss': 2.3021, 'grad_norm': 3.332016706466675, 'learning_rate': 7.826499430847241e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.76it/s] 48%|████▊     | 36/75 [00:01<00:01, 24.77it/s]                                               {'loss': 2.2728, 'grad_norm': 4.159202575683594, 'learning_rate': 7.635609200826576e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.77it/s]                                               {'loss': 2.3689, 'grad_norm': 4.62026309967041, 'learning_rate': 7.444718970805913e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.77it/s]                                               {'loss': 2.247, 'grad_norm': 4.071951389312744, 'learning_rate': 7.253828740785248e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.77it/s] 52%|█████▏    | 39/75 [00:01<00:01, 24.66it/s]                                               {'loss': 2.2467, 'grad_norm': 3.8328256607055664, 'learning_rate': 7.062938510764584e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.66it/s]                                               {'loss': 2.1816, 'grad_norm': 3.4918315410614014, 'learning_rate': 6.872048280743919e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.66it/s]                                               {'loss': 2.1173, 'grad_norm': 4.430039405822754, 'learning_rate': 6.681158050723256e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.66it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.47it/s]                                               {'loss': 2.3999, 'grad_norm': 3.659360408782959, 'learning_rate': 6.490267820702591e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.47it/s]                                               {'loss': 2.0507, 'grad_norm': 3.7005696296691895, 'learning_rate': 6.299377590681926e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.47it/s]                                               {'loss': 2.4102, 'grad_norm': 4.098900318145752, 'learning_rate': 6.108487360661261e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.47it/s]                                               {'loss': 2.0096, 'grad_norm': 10.227970123291016, 'learning_rate': 5.9175971306405974e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.47it/s] 61%|██████▏   | 46/75 [00:01<00:01, 26.01it/s]                                               {'loss': 2.2424, 'grad_norm': 3.4807724952697754, 'learning_rate': 5.7267069006199334e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.01it/s]                                               {'loss': 2.1664, 'grad_norm': 4.1684112548828125, 'learning_rate': 5.535816670599268e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.01it/s]                                               {'loss': 2.185, 'grad_norm': 4.271663188934326, 'learning_rate': 5.344926440578604e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.01it/s] 65%|██████▌   | 49/75 [00:02<00:01, 25.17it/s]                                               {'loss': 2.3203, 'grad_norm': 4.595397472381592, 'learning_rate': 5.154036210557939e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 25.17it/s]                                               {'loss': 2.0821, 'grad_norm': 3.9916815757751465, 'learning_rate': 4.963145980537275e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.17it/s]                                               {'loss': 2.1664, 'grad_norm': 3.548759937286377, 'learning_rate': 4.7722557505166105e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.17it/s] 69%|██████▉   | 52/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.1935, 'grad_norm': 3.9663357734680176, 'learning_rate': 4.5813655204959464e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.1374, 'grad_norm': 3.0129764080047607, 'learning_rate': 4.390475290475282e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.0003, 'grad_norm': 3.8709750175476074, 'learning_rate': 4.199585060454618e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.83it/s] 73%|███████▎  | 55/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.184, 'grad_norm': 3.136701822280884, 'learning_rate': 4.0086948304339536e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.1227, 'grad_norm': 3.3684325218200684, 'learning_rate': 3.817804600413288e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.4391, 'grad_norm': 4.8027262687683105, 'learning_rate': 3.626914370392624e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.63it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.09it/s]                                               {'loss': 2.3599, 'grad_norm': 4.132306098937988, 'learning_rate': 3.4360241403719595e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.09it/s]                                               {'loss': 2.141, 'grad_norm': 3.5938727855682373, 'learning_rate': 3.2451339103512954e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.09it/s]                                               {'loss': 2.4385, 'grad_norm': 13.413512229919434, 'learning_rate': 3.054243680330631e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.09it/s]                                               {'loss': 2.2294, 'grad_norm': 4.038523197174072, 'learning_rate': 2.8633534503099667e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.09it/s] 83%|████████▎ | 62/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.171, 'grad_norm': 4.27197265625, 'learning_rate': 2.672463220289302e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.0341, 'grad_norm': 3.0063936710357666, 'learning_rate': 2.4815729902686376e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.63it/s]                                               {'loss': 1.9868, 'grad_norm': 4.791476726531982, 'learning_rate': 2.2906827602479732e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.63it/s] 87%|████████▋ | 65/75 [00:02<00:00, 24.80it/s]                                               {'loss': 2.3783, 'grad_norm': 3.906123638153076, 'learning_rate': 2.099792530227309e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.80it/s]                                               {'loss': 2.3652, 'grad_norm': 3.597487449645996, 'learning_rate': 1.908902300206644e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.80it/s]                                               {'loss': 2.0963, 'grad_norm': 5.018366813659668, 'learning_rate': 1.7180120701859797e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.80it/s] 91%|█████████ | 68/75 [00:02<00:00, 24.40it/s]                                               {'loss': 2.0127, 'grad_norm': 3.754304885864258, 'learning_rate': 1.5271218401653154e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.40it/s]                                               {'loss': 2.1831, 'grad_norm': 3.694328546524048, 'learning_rate': 1.336231610144651e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.40it/s]                                               {'loss': 2.3453, 'grad_norm': 3.9973561763763428, 'learning_rate': 1.1453413801239866e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.40it/s] 95%|█████████▍| 71/75 [00:02<00:00, 24.28it/s]                                               {'loss': 2.0674, 'grad_norm': 3.797656774520874, 'learning_rate': 9.54451150103322e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.28it/s]                                               {'loss': 2.1984, 'grad_norm': 4.112669944763184, 'learning_rate': 7.635609200826577e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.28it/s]                                               {'loss': 2.2297, 'grad_norm': 5.088049411773682, 'learning_rate': 5.726706900619933e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.28it/s] 99%|█████████▊| 74/75 [00:03<00:00, 24.30it/s]                                               {'loss': 2.4221, 'grad_norm': 5.528994560241699, 'learning_rate': 3.817804600413288e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.30it/s]                                               {'loss': 2.1911, 'grad_norm': 11.517584800720215, 'learning_rate': 1.908902300206644e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.30it/s]                                               {'train_runtime': 3.2113, 'train_samples_per_second': 351.888, 'train_steps_per_second': 23.355, 'train_loss': 2.2239996417363486, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.30it/s]100%|██████████| 75/75 [00:03<00:00, 23.36it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:35
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2098, 'grad_norm': 3.8528218269348145, 'learning_rate': 0.00014316767251549832, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 16.93it/s]                                              {'loss': 2.2651, 'grad_norm': 3.8605504035949707, 'learning_rate': 0.00014125877021529167, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 20.55it/s]  4%|▍         | 3/75 [00:00<00:03, 21.57it/s]                                              {'loss': 2.0679, 'grad_norm': 3.9329559803009033, 'learning_rate': 0.00013934986791508503, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 21.57it/s]                                              {'loss': 2.2262, 'grad_norm': 3.763136625289917, 'learning_rate': 0.00013744096561487838, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 21.57it/s]                                              {'loss': 2.3948, 'grad_norm': 5.422021389007568, 'learning_rate': 0.00013553206331467173, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 21.57it/s]  8%|▊         | 6/75 [00:00<00:02, 23.63it/s]                                              {'loss': 2.1607, 'grad_norm': 3.0667998790740967, 'learning_rate': 0.0001336231610144651, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.63it/s]                                              {'loss': 2.3643, 'grad_norm': 4.179618835449219, 'learning_rate': 0.00013171425871425846, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.63it/s]                                              {'loss': 2.4175, 'grad_norm': 4.811656951904297, 'learning_rate': 0.00012980535641405182, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.63it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.48it/s]                                              {'loss': 2.4298, 'grad_norm': 4.618941783905029, 'learning_rate': 0.00012789645411384517, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.48it/s]                                              {'loss': 2.1445, 'grad_norm': 3.877858877182007, 'learning_rate': 0.00012598755181363852, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.48it/s]                                               {'loss': 2.2241, 'grad_norm': 3.403351068496704, 'learning_rate': 0.00012407864951343188, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.48it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.67it/s]                                               {'loss': 2.0961, 'grad_norm': 3.698029041290283, 'learning_rate': 0.00012216974721322523, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.67it/s]                                               {'loss': 2.3908, 'grad_norm': 4.4657697677612305, 'learning_rate': 0.00012026084491301858, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.67it/s]                                               {'loss': 2.3569, 'grad_norm': 4.581246376037598, 'learning_rate': 0.00011835194261281195, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.67it/s]                                               {'loss': 1.9904, 'grad_norm': 7.508768081665039, 'learning_rate': 0.0001164430403126053, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.67it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.48it/s]                                               {'loss': 2.3751, 'grad_norm': 3.8730578422546387, 'learning_rate': 0.00011453413801239867, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.48it/s]                                               {'loss': 2.1746, 'grad_norm': 4.165066242218018, 'learning_rate': 0.000112625235712192, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.48it/s]                                               {'loss': 2.0857, 'grad_norm': 4.124760627746582, 'learning_rate': 0.00011071633341198536, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.48it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.17it/s]                                               {'loss': 2.058, 'grad_norm': 4.030721187591553, 'learning_rate': 0.00010880743111177873, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.17it/s]                                               {'loss': 2.2617, 'grad_norm': 3.7520110607147217, 'learning_rate': 0.00010689852881157208, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.17it/s]                                               {'loss': 2.1368, 'grad_norm': 3.689535617828369, 'learning_rate': 0.00010498962651136543, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.17it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.78it/s]                                               {'loss': 2.2283, 'grad_norm': 3.531040668487549, 'learning_rate': 0.00010308072421115878, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.78it/s]                                               {'loss': 2.2913, 'grad_norm': 3.978651762008667, 'learning_rate': 0.00010117182191095215, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.78it/s]                                               {'loss': 2.1833, 'grad_norm': 3.8912837505340576, 'learning_rate': 9.92629196107455e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.78it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.52it/s]                                               {'loss': 2.133, 'grad_norm': 3.8607561588287354, 'learning_rate': 9.735401731053887e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.52it/s]                                               {'loss': 2.3885, 'grad_norm': 4.923305511474609, 'learning_rate': 9.544511501033221e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.52it/s]                                               {'loss': 2.1978, 'grad_norm': 3.8144636154174805, 'learning_rate': 9.353621271012556e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.52it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.38it/s]                                               {'loss': 2.1269, 'grad_norm': 3.4889044761657715, 'learning_rate': 9.162731040991893e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.38it/s]                                               {'loss': 2.3099, 'grad_norm': 3.750121831893921, 'learning_rate': 8.971840810971228e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.38it/s]                                               {'loss': 2.1098, 'grad_norm': 11.94842529296875, 'learning_rate': 8.780950580950563e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.38it/s]                                               {'loss': 2.286, 'grad_norm': 3.6584815979003906, 'learning_rate': 8.590060350929899e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.38it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.90it/s]                                               {'loss': 1.998, 'grad_norm': 3.178321599960327, 'learning_rate': 8.399170120909235e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.90it/s]                                               {'loss': 2.1717, 'grad_norm': 3.858583450317383, 'learning_rate': 8.20827989088857e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.90it/s]                                               {'loss': 2.1639, 'grad_norm': 3.153671979904175, 'learning_rate': 8.017389660867907e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.90it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.45it/s]                                               {'loss': 2.1523, 'grad_norm': 4.309695243835449, 'learning_rate': 7.826499430847241e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.45it/s]                                               {'loss': 2.2049, 'grad_norm': 3.7540228366851807, 'learning_rate': 7.635609200826576e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.45it/s]                                               {'loss': 2.3068, 'grad_norm': 3.396949529647827, 'learning_rate': 7.444718970805913e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.45it/s] 51%|█████     | 38/75 [00:01<00:01, 26.32it/s]                                               {'loss': 2.0993, 'grad_norm': 3.840688943862915, 'learning_rate': 7.253828740785248e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.32it/s]                                               {'loss': 2.1432, 'grad_norm': 3.6001996994018555, 'learning_rate': 7.062938510764584e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.32it/s]                                               {'loss': 2.2408, 'grad_norm': 3.1217615604400635, 'learning_rate': 6.872048280743919e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 26.32it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.59it/s]                                               {'loss': 2.0561, 'grad_norm': 3.61356520652771, 'learning_rate': 6.681158050723256e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.59it/s]                                               {'loss': 2.1639, 'grad_norm': 3.8079442977905273, 'learning_rate': 6.490267820702591e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.59it/s]                                               {'loss': 2.3746, 'grad_norm': 4.820221900939941, 'learning_rate': 6.299377590681926e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.59it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.1094, 'grad_norm': 3.4859092235565186, 'learning_rate': 6.108487360661261e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.15, 'grad_norm': 11.794968605041504, 'learning_rate': 5.9175971306405974e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.3782, 'grad_norm': 4.207964897155762, 'learning_rate': 5.7267069006199334e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.00it/s] 63%|██████▎   | 47/75 [00:01<00:01, 25.48it/s]                                               {'loss': 2.0064, 'grad_norm': 2.95678973197937, 'learning_rate': 5.535816670599268e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.48it/s]                                               {'loss': 2.1318, 'grad_norm': 4.288459777832031, 'learning_rate': 5.344926440578604e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.48it/s]                                               {'loss': 2.1872, 'grad_norm': 4.168051242828369, 'learning_rate': 5.154036210557939e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.48it/s] 67%|██████▋   | 50/75 [00:01<00:00, 25.09it/s]                                               {'loss': 2.2845, 'grad_norm': 4.088627338409424, 'learning_rate': 4.963145980537275e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.09it/s]                                               {'loss': 1.9496, 'grad_norm': 3.6158597469329834, 'learning_rate': 4.7722557505166105e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.09it/s]                                               {'loss': 2.0081, 'grad_norm': 4.38178014755249, 'learning_rate': 4.5813655204959464e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.09it/s] 71%|███████   | 53/75 [00:02<00:00, 25.12it/s]                                               {'loss': 2.1623, 'grad_norm': 4.3945746421813965, 'learning_rate': 4.390475290475282e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.12it/s]                                               {'loss': 2.0563, 'grad_norm': 4.069839954376221, 'learning_rate': 4.199585060454618e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.12it/s]                                               {'loss': 2.4263, 'grad_norm': 3.219197988510132, 'learning_rate': 4.0086948304339536e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.12it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.2108, 'grad_norm': 4.436804294586182, 'learning_rate': 3.817804600413288e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.025, 'grad_norm': 3.2405641078948975, 'learning_rate': 3.626914370392624e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.23it/s]                                               {'loss': 1.9359, 'grad_norm': 3.128628969192505, 'learning_rate': 3.4360241403719595e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.23it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.3614, 'grad_norm': 3.8210885524749756, 'learning_rate': 3.2451339103512954e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.16it/s]                                               {'loss': 1.8876, 'grad_norm': 7.062598705291748, 'learning_rate': 3.054243680330631e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.0838, 'grad_norm': 3.0619699954986572, 'learning_rate': 2.8633534503099667e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.0262, 'grad_norm': 4.464110374450684, 'learning_rate': 2.672463220289302e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.16it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.61it/s]                                               {'loss': 2.0827, 'grad_norm': 3.4839677810668945, 'learning_rate': 2.4815729902686376e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.61it/s]                                               {'loss': 2.1683, 'grad_norm': 3.397970676422119, 'learning_rate': 2.2906827602479732e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.61it/s]                                               {'loss': 2.1662, 'grad_norm': 4.911391735076904, 'learning_rate': 2.099792530227309e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.61it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.40it/s]                                               {'loss': 2.1739, 'grad_norm': 5.082881450653076, 'learning_rate': 1.908902300206644e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.40it/s]                                               {'loss': 2.0283, 'grad_norm': 3.5302305221557617, 'learning_rate': 1.7180120701859797e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.40it/s]                                               {'loss': 1.8729, 'grad_norm': 3.2614400386810303, 'learning_rate': 1.5271218401653154e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.40it/s] 92%|█████████▏| 69/75 [00:02<00:00, 24.50it/s]                                               {'loss': 2.0565, 'grad_norm': 3.2312474250793457, 'learning_rate': 1.336231610144651e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.50it/s]                                               {'loss': 2.2991, 'grad_norm': 4.6013994216918945, 'learning_rate': 1.1453413801239866e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.50it/s]                                               {'loss': 2.3239, 'grad_norm': 4.059488773345947, 'learning_rate': 9.54451150103322e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.50it/s] 96%|█████████▌| 72/75 [00:02<00:00, 24.26it/s]                                               {'loss': 2.3195, 'grad_norm': 3.2218735218048096, 'learning_rate': 7.635609200826577e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.26it/s]                                               {'loss': 2.221, 'grad_norm': 4.329148769378662, 'learning_rate': 5.726706900619933e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.26it/s]                                               {'loss': 2.1149, 'grad_norm': 3.57714581489563, 'learning_rate': 3.817804600413288e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.26it/s]100%|██████████| 75/75 [00:02<00:00, 25.44it/s]                                               {'loss': 2.533, 'grad_norm': 12.283182144165039, 'learning_rate': 1.908902300206644e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.44it/s]                                               {'train_runtime': 3.0774, 'train_samples_per_second': 367.196, 'train_steps_per_second': 24.371, 'train_loss': 2.1853598165512085, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.44it/s]100%|██████████| 75/75 [00:03<00:00, 24.41it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:40
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2499, 'grad_norm': 5.324923038482666, 'learning_rate': 0.00014316767251549832, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.56it/s]                                              {'loss': 2.431, 'grad_norm': 5.149814128875732, 'learning_rate': 0.00014125877021529167, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.49it/s]  4%|▍         | 3/75 [00:00<00:02, 24.91it/s]                                              {'loss': 2.114, 'grad_norm': 4.9787702560424805, 'learning_rate': 0.00013934986791508503, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.91it/s]                                              {'loss': 2.6292, 'grad_norm': 5.059731960296631, 'learning_rate': 0.00013744096561487838, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.91it/s]                                              {'loss': 2.2685, 'grad_norm': 3.734250545501709, 'learning_rate': 0.00013553206331467173, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.91it/s]  8%|▊         | 6/75 [00:00<00:02, 25.02it/s]                                              {'loss': 2.3487, 'grad_norm': 3.695467948913574, 'learning_rate': 0.0001336231610144651, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.02it/s]                                              {'loss': 2.4796, 'grad_norm': 4.601046562194824, 'learning_rate': 0.00013171425871425846, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.02it/s]                                              {'loss': 2.3957, 'grad_norm': 4.558900356292725, 'learning_rate': 0.00012980535641405182, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.02it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.25it/s]                                              {'loss': 2.4191, 'grad_norm': 3.868922710418701, 'learning_rate': 0.00012789645411384517, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.25it/s]                                              {'loss': 2.616, 'grad_norm': 4.176638603210449, 'learning_rate': 0.00012598755181363852, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.25it/s]                                               {'loss': 2.4046, 'grad_norm': 3.314026355743408, 'learning_rate': 0.00012407864951343188, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.25it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.55it/s]                                               {'loss': 2.3833, 'grad_norm': 4.397766590118408, 'learning_rate': 0.00012216974721322523, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.55it/s]                                               {'loss': 2.3585, 'grad_norm': 5.647940635681152, 'learning_rate': 0.00012026084491301858, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.55it/s]                                               {'loss': 2.1218, 'grad_norm': 3.7173714637756348, 'learning_rate': 0.00011835194261281195, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.55it/s]                                               {'loss': 2.2665, 'grad_norm': 11.81805419921875, 'learning_rate': 0.0001164430403126053, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.55it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.31it/s]                                               {'loss': 2.3306, 'grad_norm': 4.295301914215088, 'learning_rate': 0.00011453413801239867, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.31it/s]                                               {'loss': 2.3281, 'grad_norm': 5.064523220062256, 'learning_rate': 0.000112625235712192, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.31it/s]                                               {'loss': 2.309, 'grad_norm': 4.135315895080566, 'learning_rate': 0.00011071633341198536, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.31it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.92it/s]                                               {'loss': 2.2717, 'grad_norm': 3.8757193088531494, 'learning_rate': 0.00010880743111177873, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.92it/s]                                               {'loss': 2.2427, 'grad_norm': 4.950352191925049, 'learning_rate': 0.00010689852881157208, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.92it/s]                                               {'loss': 2.2509, 'grad_norm': 3.5629842281341553, 'learning_rate': 0.00010498962651136543, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.92it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.64it/s]                                               {'loss': 2.4019, 'grad_norm': 5.599597454071045, 'learning_rate': 0.00010308072421115878, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.64it/s]                                               {'loss': 2.2798, 'grad_norm': 4.287270545959473, 'learning_rate': 0.00010117182191095215, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.64it/s]                                               {'loss': 2.3179, 'grad_norm': 4.028745651245117, 'learning_rate': 9.92629196107455e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.64it/s] 33%|███▎      | 25/75 [00:00<00:02, 24.59it/s]                                               {'loss': 2.5572, 'grad_norm': 3.9445574283599854, 'learning_rate': 9.735401731053887e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:02, 24.59it/s]                                               {'loss': 2.2889, 'grad_norm': 4.959009647369385, 'learning_rate': 9.544511501033221e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.59it/s]                                               {'loss': 2.259, 'grad_norm': 4.8877644538879395, 'learning_rate': 9.353621271012556e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.59it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.98it/s]                                               {'loss': 2.5621, 'grad_norm': 5.456346035003662, 'learning_rate': 9.162731040991893e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.98it/s]                                               {'loss': 2.2346, 'grad_norm': 3.059720039367676, 'learning_rate': 8.971840810971228e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.98it/s]                                               {'loss': 1.797, 'grad_norm': 11.762614250183105, 'learning_rate': 8.780950580950563e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.98it/s]                                               {'loss': 2.31, 'grad_norm': 5.759111404418945, 'learning_rate': 8.590060350929899e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.98it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.31it/s]                                               {'loss': 2.4512, 'grad_norm': 4.505431175231934, 'learning_rate': 8.399170120909235e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.31it/s]                                               {'loss': 2.4499, 'grad_norm': 4.834017753601074, 'learning_rate': 8.20827989088857e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.31it/s]                                               {'loss': 2.0586, 'grad_norm': 4.624847888946533, 'learning_rate': 8.017389660867907e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.31it/s] 47%|████▋     | 35/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.1981, 'grad_norm': 3.5192224979400635, 'learning_rate': 7.826499430847241e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.2469, 'grad_norm': 4.044196128845215, 'learning_rate': 7.635609200826576e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.2093, 'grad_norm': 3.395667314529419, 'learning_rate': 7.444718970805913e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.34it/s] 51%|█████     | 38/75 [00:01<00:01, 24.79it/s]                                               {'loss': 2.285, 'grad_norm': 4.4344377517700195, 'learning_rate': 7.253828740785248e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.79it/s]                                               {'loss': 2.1849, 'grad_norm': 4.220405578613281, 'learning_rate': 7.062938510764584e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.79it/s]                                               {'loss': 2.1893, 'grad_norm': 4.535678386688232, 'learning_rate': 6.872048280743919e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.79it/s] 55%|█████▍    | 41/75 [00:01<00:01, 24.97it/s]                                               {'loss': 2.1718, 'grad_norm': 3.9092864990234375, 'learning_rate': 6.681158050723256e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.97it/s]                                               {'loss': 2.4611, 'grad_norm': 4.827866554260254, 'learning_rate': 6.490267820702591e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.97it/s]                                               {'loss': 2.1699, 'grad_norm': 4.025697231292725, 'learning_rate': 6.299377590681926e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.97it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.5677, 'grad_norm': 3.6885664463043213, 'learning_rate': 6.108487360661261e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.5851, 'grad_norm': 17.2064266204834, 'learning_rate': 5.9175971306405974e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.2449, 'grad_norm': 4.011833667755127, 'learning_rate': 5.7267069006199334e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.0585, 'grad_norm': 3.233213424682617, 'learning_rate': 5.535816670599268e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.00it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.51it/s]                                               {'loss': 2.2714, 'grad_norm': 3.7858939170837402, 'learning_rate': 5.344926440578604e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.51it/s]                                               {'loss': 2.4374, 'grad_norm': 6.587071895599365, 'learning_rate': 5.154036210557939e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.51it/s]                                               {'loss': 2.3808, 'grad_norm': 4.1198506355285645, 'learning_rate': 4.963145980537275e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.51it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.20it/s]                                               {'loss': 2.3182, 'grad_norm': 4.701873302459717, 'learning_rate': 4.7722557505166105e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 26.20it/s]                                               {'loss': 2.2242, 'grad_norm': 4.677779674530029, 'learning_rate': 4.5813655204959464e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 26.20it/s]                                               {'loss': 2.2305, 'grad_norm': 3.8256895542144775, 'learning_rate': 4.390475290475282e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 26.20it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.68it/s]                                               {'loss': 2.3159, 'grad_norm': 4.562222480773926, 'learning_rate': 4.199585060454618e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.68it/s]                                               {'loss': 2.3122, 'grad_norm': 3.9709413051605225, 'learning_rate': 4.0086948304339536e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.68it/s]                                               {'loss': 2.187, 'grad_norm': 4.193395614624023, 'learning_rate': 3.817804600413288e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.68it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.72it/s]                                               {'loss': 2.254, 'grad_norm': 4.331820487976074, 'learning_rate': 3.626914370392624e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.72it/s]                                               {'loss': 2.2987, 'grad_norm': 3.7654831409454346, 'learning_rate': 3.4360241403719595e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.72it/s]                                               {'loss': 2.2809, 'grad_norm': 4.7375993728637695, 'learning_rate': 3.2451339103512954e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.72it/s] 80%|████████  | 60/75 [00:02<00:00, 25.63it/s]                                               {'loss': 1.7764, 'grad_norm': 17.891582489013672, 'learning_rate': 3.054243680330631e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.394, 'grad_norm': 4.214961051940918, 'learning_rate': 2.8633534503099667e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.2073, 'grad_norm': 4.395005702972412, 'learning_rate': 2.672463220289302e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.63it/s] 84%|████████▍ | 63/75 [00:02<00:00, 25.34it/s]                                               {'loss': 2.1019, 'grad_norm': 3.5862715244293213, 'learning_rate': 2.4815729902686376e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.34it/s]                                               {'loss': 2.2154, 'grad_norm': 4.184301853179932, 'learning_rate': 2.2906827602479732e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.34it/s]                                               {'loss': 2.391, 'grad_norm': 4.121356010437012, 'learning_rate': 2.099792530227309e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.34it/s] 88%|████████▊ | 66/75 [00:02<00:00, 24.66it/s]                                               {'loss': 2.0516, 'grad_norm': 4.727954864501953, 'learning_rate': 1.908902300206644e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.66it/s]                                               {'loss': 2.0456, 'grad_norm': 3.963707208633423, 'learning_rate': 1.7180120701859797e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.66it/s]                                               {'loss': 2.1967, 'grad_norm': 3.5441954135894775, 'learning_rate': 1.5271218401653154e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.66it/s] 92%|█████████▏| 69/75 [00:02<00:00, 24.44it/s]                                               {'loss': 2.2532, 'grad_norm': 5.7049336433410645, 'learning_rate': 1.336231610144651e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.44it/s]                                               {'loss': 2.3194, 'grad_norm': 3.5385985374450684, 'learning_rate': 1.1453413801239866e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.44it/s]                                               {'loss': 2.2937, 'grad_norm': 4.927907943725586, 'learning_rate': 9.54451150103322e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.44it/s] 96%|█████████▌| 72/75 [00:02<00:00, 24.14it/s]                                               {'loss': 2.2428, 'grad_norm': 3.3696188926696777, 'learning_rate': 7.635609200826577e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.14it/s]                                               {'loss': 2.4364, 'grad_norm': 4.768275737762451, 'learning_rate': 5.726706900619933e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.14it/s]                                               {'loss': 2.3319, 'grad_norm': 4.792412757873535, 'learning_rate': 3.817804600413288e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.14it/s]                                               {'loss': 2.4026, 'grad_norm': 11.86455249786377, 'learning_rate': 1.908902300206644e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.14it/s]                                               {'train_runtime': 3.0671, 'train_samples_per_second': 368.423, 'train_steps_per_second': 24.453, 'train_loss': 2.292411847114563, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.14it/s]100%|██████████| 75/75 [00:03<00:00, 24.46it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:38
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2372, 'grad_norm': 4.416806697845459, 'learning_rate': 0.00014316767251549832, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 22.46it/s]                                              {'loss': 2.1642, 'grad_norm': 4.049747467041016, 'learning_rate': 0.00014125877021529167, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.47it/s]  4%|▍         | 3/75 [00:00<00:02, 25.15it/s]                                              {'loss': 2.4302, 'grad_norm': 5.182134628295898, 'learning_rate': 0.00013934986791508503, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.15it/s]                                              {'loss': 2.4279, 'grad_norm': 4.13886833190918, 'learning_rate': 0.00013744096561487838, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.15it/s]                                              {'loss': 2.2307, 'grad_norm': 4.317910194396973, 'learning_rate': 0.00013553206331467173, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.15it/s]  8%|▊         | 6/75 [00:00<00:02, 24.39it/s]                                              {'loss': 2.4113, 'grad_norm': 4.413886547088623, 'learning_rate': 0.0001336231610144651, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.39it/s]                                              {'loss': 2.226, 'grad_norm': 5.191200256347656, 'learning_rate': 0.00013171425871425846, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.39it/s]                                              {'loss': 2.3433, 'grad_norm': 3.88297438621521, 'learning_rate': 0.00012980535641405182, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.39it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.12it/s]                                              {'loss': 2.2353, 'grad_norm': 3.631051540374756, 'learning_rate': 0.00012789645411384517, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.12it/s]                                              {'loss': 2.1774, 'grad_norm': 3.5164759159088135, 'learning_rate': 0.00012598755181363852, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.12it/s]                                               {'loss': 2.4901, 'grad_norm': 3.6493611335754395, 'learning_rate': 0.00012407864951343188, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.12it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.69it/s]                                               {'loss': 2.2596, 'grad_norm': 7.4457926750183105, 'learning_rate': 0.00012216974721322523, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.69it/s]                                               {'loss': 2.4555, 'grad_norm': 6.5833563804626465, 'learning_rate': 0.00012026084491301858, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.69it/s]                                               {'loss': 2.7196, 'grad_norm': 5.689053058624268, 'learning_rate': 0.00011835194261281195, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.69it/s]                                               {'loss': 2.3934, 'grad_norm': 13.0269136428833, 'learning_rate': 0.0001164430403126053, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.69it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.03it/s]                                               {'loss': 2.4781, 'grad_norm': 3.4978928565979004, 'learning_rate': 0.00011453413801239867, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.03it/s]                                               {'loss': 2.2308, 'grad_norm': 4.830541133880615, 'learning_rate': 0.000112625235712192, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.03it/s]                                               {'loss': 2.2229, 'grad_norm': 4.723818302154541, 'learning_rate': 0.00011071633341198536, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.03it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.15it/s]                                               {'loss': 2.3541, 'grad_norm': 4.2819600105285645, 'learning_rate': 0.00010880743111177873, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.15it/s]                                               {'loss': 2.2531, 'grad_norm': 4.465366840362549, 'learning_rate': 0.00010689852881157208, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.15it/s]                                               {'loss': 2.4946, 'grad_norm': 4.15153169631958, 'learning_rate': 0.00010498962651136543, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.15it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.55it/s]                                               {'loss': 2.3005, 'grad_norm': 4.224287033081055, 'learning_rate': 0.00010308072421115878, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.55it/s]                                               {'loss': 2.2264, 'grad_norm': 2.796605348587036, 'learning_rate': 0.00010117182191095215, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.55it/s]                                               {'loss': 2.1818, 'grad_norm': 3.98884916305542, 'learning_rate': 9.92629196107455e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.55it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.60it/s]                                               {'loss': 2.2406, 'grad_norm': 4.450204372406006, 'learning_rate': 9.735401731053887e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.60it/s]                                               {'loss': 2.4014, 'grad_norm': 4.33494234085083, 'learning_rate': 9.544511501033221e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.2023, 'grad_norm': 4.753173828125, 'learning_rate': 9.353621271012556e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.60it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.49it/s]                                               {'loss': 2.1458, 'grad_norm': 4.115795612335205, 'learning_rate': 9.162731040991893e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.49it/s]                                               {'loss': 2.3398, 'grad_norm': 4.987691879272461, 'learning_rate': 8.971840810971228e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.49it/s]                                               {'loss': 2.5419, 'grad_norm': 11.46843147277832, 'learning_rate': 8.780950580950563e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.49it/s] 41%|████▏     | 31/75 [00:01<00:01, 24.16it/s]                                               {'loss': 2.3602, 'grad_norm': 4.45591926574707, 'learning_rate': 8.590060350929899e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.16it/s]                                               {'loss': 2.1606, 'grad_norm': 3.6831648349761963, 'learning_rate': 8.399170120909235e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.16it/s]                                               {'loss': 2.143, 'grad_norm': 4.1086201667785645, 'learning_rate': 8.20827989088857e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.16it/s] 45%|████▌     | 34/75 [00:01<00:01, 24.32it/s]                                               {'loss': 2.2646, 'grad_norm': 3.6434528827667236, 'learning_rate': 8.017389660867907e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.32it/s]                                               {'loss': 2.4357, 'grad_norm': 5.114991664886475, 'learning_rate': 7.826499430847241e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.32it/s]                                               {'loss': 2.2189, 'grad_norm': 4.995966911315918, 'learning_rate': 7.635609200826576e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.32it/s] 49%|████▉     | 37/75 [00:01<00:01, 24.28it/s]                                               {'loss': 2.2994, 'grad_norm': 4.648914337158203, 'learning_rate': 7.444718970805913e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.28it/s]                                               {'loss': 2.2851, 'grad_norm': 3.6270012855529785, 'learning_rate': 7.253828740785248e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.28it/s]                                               {'loss': 2.2274, 'grad_norm': 3.4743106365203857, 'learning_rate': 7.062938510764584e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.28it/s] 53%|█████▎    | 40/75 [00:01<00:01, 24.74it/s]                                               {'loss': 2.2019, 'grad_norm': 5.334512233734131, 'learning_rate': 6.872048280743919e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.74it/s]                                               {'loss': 2.1512, 'grad_norm': 3.5036423206329346, 'learning_rate': 6.681158050723256e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.74it/s]                                               {'loss': 2.2496, 'grad_norm': 3.8932993412017822, 'learning_rate': 6.490267820702591e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.74it/s] 57%|█████▋    | 43/75 [00:01<00:01, 24.81it/s]                                               {'loss': 2.1574, 'grad_norm': 4.656312942504883, 'learning_rate': 6.299377590681926e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.81it/s]                                               {'loss': 2.2039, 'grad_norm': 3.544620990753174, 'learning_rate': 6.108487360661261e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.81it/s]                                               {'loss': 2.6989, 'grad_norm': 11.95955753326416, 'learning_rate': 5.9175971306405974e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.81it/s] 61%|██████▏   | 46/75 [00:01<00:01, 25.94it/s]                                               {'loss': 2.319, 'grad_norm': 3.938650608062744, 'learning_rate': 5.7267069006199334e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.94it/s]                                               {'loss': 2.1522, 'grad_norm': 4.1790971755981445, 'learning_rate': 5.535816670599268e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.94it/s]                                               {'loss': 2.2517, 'grad_norm': 3.750588893890381, 'learning_rate': 5.344926440578604e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.94it/s] 65%|██████▌   | 49/75 [00:01<00:01, 25.88it/s]                                               {'loss': 2.095, 'grad_norm': 3.9050686359405518, 'learning_rate': 5.154036210557939e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.88it/s]                                               {'loss': 2.1157, 'grad_norm': 4.622340202331543, 'learning_rate': 4.963145980537275e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.88it/s]                                               {'loss': 2.1817, 'grad_norm': 4.084238052368164, 'learning_rate': 4.7722557505166105e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.88it/s] 69%|██████▉   | 52/75 [00:02<00:00, 24.74it/s]                                               {'loss': 2.5041, 'grad_norm': 3.6740496158599854, 'learning_rate': 4.5813655204959464e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.74it/s]                                               {'loss': 2.3457, 'grad_norm': 4.637723922729492, 'learning_rate': 4.390475290475282e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.74it/s]                                               {'loss': 2.1748, 'grad_norm': 3.8849239349365234, 'learning_rate': 4.199585060454618e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.74it/s] 73%|███████▎  | 55/75 [00:02<00:00, 24.41it/s]                                               {'loss': 2.1781, 'grad_norm': 3.495515823364258, 'learning_rate': 4.0086948304339536e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.41it/s]                                               {'loss': 2.2407, 'grad_norm': 3.7462785243988037, 'learning_rate': 3.817804600413288e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.41it/s]                                               {'loss': 2.1968, 'grad_norm': 3.7850801944732666, 'learning_rate': 3.626914370392624e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.41it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.42it/s]                                               {'loss': 2.2324, 'grad_norm': 3.2683491706848145, 'learning_rate': 3.4360241403719595e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.42it/s]                                               {'loss': 2.133, 'grad_norm': 3.98279070854187, 'learning_rate': 3.2451339103512954e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.42it/s]                                               {'loss': 2.3211, 'grad_norm': 12.39001750946045, 'learning_rate': 3.054243680330631e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.42it/s] 81%|████████▏ | 61/75 [00:02<00:00, 25.64it/s]                                               {'loss': 2.2452, 'grad_norm': 3.596662759780884, 'learning_rate': 2.8633534503099667e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.64it/s]                                               {'loss': 2.2895, 'grad_norm': 5.755931377410889, 'learning_rate': 2.672463220289302e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.64it/s]                                               {'loss': 2.2031, 'grad_norm': 3.6757256984710693, 'learning_rate': 2.4815729902686376e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.64it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.72it/s]                                               {'loss': 2.3083, 'grad_norm': 4.010442733764648, 'learning_rate': 2.2906827602479732e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.72it/s]                                               {'loss': 2.1133, 'grad_norm': 3.867889881134033, 'learning_rate': 2.099792530227309e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.72it/s]                                               {'loss': 2.1394, 'grad_norm': 4.016345500946045, 'learning_rate': 1.908902300206644e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.72it/s] 89%|████████▉ | 67/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.3161, 'grad_norm': 3.9768080711364746, 'learning_rate': 1.7180120701859797e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.099, 'grad_norm': 4.04416561126709, 'learning_rate': 1.5271218401653154e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.1179, 'grad_norm': 5.240652561187744, 'learning_rate': 1.336231610144651e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.63it/s] 93%|█████████▎| 70/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.124, 'grad_norm': 3.235544443130493, 'learning_rate': 1.1453413801239866e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.2407, 'grad_norm': 5.1019415855407715, 'learning_rate': 9.54451150103322e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.1903, 'grad_norm': 4.015408992767334, 'learning_rate': 7.635609200826577e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.45it/s] 97%|█████████▋| 73/75 [00:02<00:00, 24.89it/s]                                               {'loss': 2.4656, 'grad_norm': 4.03306245803833, 'learning_rate': 5.726706900619933e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.89it/s]                                               {'loss': 2.128, 'grad_norm': 4.064443111419678, 'learning_rate': 3.817804600413288e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.89it/s]                                               {'loss': 2.2931, 'grad_norm': 13.086945533752441, 'learning_rate': 1.908902300206644e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.89it/s]                                               {'train_runtime': 3.0807, 'train_samples_per_second': 366.795, 'train_steps_per_second': 24.345, 'train_loss': 2.2745245424906413, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.89it/s]100%|██████████| 75/75 [00:03<00:00, 24.35it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:2
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2528, 'grad_norm': 3.9677395820617676, 'learning_rate': 0.00014316767251549832, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.42it/s]                                              {'loss': 2.3759, 'grad_norm': 3.8450968265533447, 'learning_rate': 0.00014125877021529167, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.66it/s]  4%|▍         | 3/75 [00:00<00:02, 24.88it/s]                                              {'loss': 2.1263, 'grad_norm': 4.593853950500488, 'learning_rate': 0.00013934986791508503, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.88it/s]                                              {'loss': 2.2358, 'grad_norm': 4.541906356811523, 'learning_rate': 0.00013744096561487838, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.88it/s]                                              {'loss': 2.2468, 'grad_norm': 4.0764665603637695, 'learning_rate': 0.00013553206331467173, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.88it/s]  8%|▊         | 6/75 [00:00<00:02, 24.99it/s]                                              {'loss': 2.3676, 'grad_norm': 4.727825164794922, 'learning_rate': 0.0001336231610144651, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.99it/s]                                              {'loss': 2.4562, 'grad_norm': 4.426109313964844, 'learning_rate': 0.00013171425871425846, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.99it/s]                                              {'loss': 2.0744, 'grad_norm': 4.353827953338623, 'learning_rate': 0.00012980535641405182, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.99it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.98it/s]                                              {'loss': 2.4072, 'grad_norm': 3.8383948802948, 'learning_rate': 0.00012789645411384517, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.98it/s]                                              {'loss': 2.3437, 'grad_norm': 4.444821834564209, 'learning_rate': 0.00012598755181363852, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.98it/s]                                               {'loss': 2.2408, 'grad_norm': 4.523146152496338, 'learning_rate': 0.00012407864951343188, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.98it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.37it/s]                                               {'loss': 2.5584, 'grad_norm': 5.391110420227051, 'learning_rate': 0.00012216974721322523, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.37it/s]                                               {'loss': 2.2875, 'grad_norm': 3.7789947986602783, 'learning_rate': 0.00012026084491301858, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.37it/s]                                               {'loss': 2.2522, 'grad_norm': 4.825567722320557, 'learning_rate': 0.00011835194261281195, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.37it/s]                                               {'loss': 2.3231, 'grad_norm': 8.615496635437012, 'learning_rate': 0.0001164430403126053, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.37it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.73it/s]                                               {'loss': 2.3477, 'grad_norm': 4.528775691986084, 'learning_rate': 0.00011453413801239867, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.73it/s]                                               {'loss': 2.2419, 'grad_norm': 3.3326644897460938, 'learning_rate': 0.000112625235712192, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.73it/s]                                               {'loss': 2.2924, 'grad_norm': 2.831287384033203, 'learning_rate': 0.00011071633341198536, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.73it/s] 25%|██▌       | 19/75 [00:00<00:02, 24.56it/s]                                               {'loss': 2.2366, 'grad_norm': 4.118825435638428, 'learning_rate': 0.00010880743111177873, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.56it/s]                                               {'loss': 2.2395, 'grad_norm': 3.4426465034484863, 'learning_rate': 0.00010689852881157208, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.56it/s]                                               {'loss': 2.2742, 'grad_norm': 4.047956466674805, 'learning_rate': 0.00010498962651136543, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.56it/s] 29%|██▉       | 22/75 [00:00<00:02, 23.08it/s]                                               {'loss': 2.4417, 'grad_norm': 5.18983268737793, 'learning_rate': 0.00010308072421115878, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 23.08it/s]                                               {'loss': 2.3362, 'grad_norm': 4.150675296783447, 'learning_rate': 0.00010117182191095215, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 23.08it/s]                                               {'loss': 2.4043, 'grad_norm': 4.203161239624023, 'learning_rate': 9.92629196107455e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 23.08it/s] 33%|███▎      | 25/75 [00:01<00:02, 23.07it/s]                                               {'loss': 2.1559, 'grad_norm': 4.523402690887451, 'learning_rate': 9.735401731053887e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.07it/s]                                               {'loss': 2.2355, 'grad_norm': 4.752526760101318, 'learning_rate': 9.544511501033221e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.07it/s]                                               {'loss': 2.1231, 'grad_norm': 3.902726173400879, 'learning_rate': 9.353621271012556e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 23.07it/s] 37%|███▋      | 28/75 [00:01<00:02, 23.04it/s]                                               {'loss': 2.1815, 'grad_norm': 4.66931676864624, 'learning_rate': 9.162731040991893e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 23.04it/s]                                               {'loss': 2.2078, 'grad_norm': 3.1516127586364746, 'learning_rate': 8.971840810971228e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 23.04it/s]                                               {'loss': 2.6086, 'grad_norm': 10.981795310974121, 'learning_rate': 8.780950580950563e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.04it/s] 41%|████▏     | 31/75 [00:01<00:01, 24.75it/s]                                               {'loss': 2.1236, 'grad_norm': 3.706231117248535, 'learning_rate': 8.590060350929899e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.75it/s]                                               {'loss': 2.1933, 'grad_norm': 4.163215637207031, 'learning_rate': 8.399170120909235e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.75it/s]                                               {'loss': 2.2514, 'grad_norm': 4.264011383056641, 'learning_rate': 8.20827989088857e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.75it/s] 45%|████▌     | 34/75 [00:01<00:01, 24.34it/s]                                               {'loss': 2.084, 'grad_norm': 3.7928240299224854, 'learning_rate': 8.017389660867907e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.34it/s]                                               {'loss': 2.1763, 'grad_norm': 2.9368953704833984, 'learning_rate': 7.826499430847241e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.34it/s]                                               {'loss': 2.2346, 'grad_norm': 3.534318447113037, 'learning_rate': 7.635609200826576e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.34it/s] 49%|████▉     | 37/75 [00:01<00:01, 24.05it/s]                                               {'loss': 2.3828, 'grad_norm': 4.710294723510742, 'learning_rate': 7.444718970805913e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.05it/s]                                               {'loss': 2.2468, 'grad_norm': 3.42480731010437, 'learning_rate': 7.253828740785248e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.05it/s]                                               {'loss': 2.2337, 'grad_norm': 4.774987697601318, 'learning_rate': 7.062938510764584e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.05it/s] 53%|█████▎    | 40/75 [00:01<00:01, 24.42it/s]                                               {'loss': 2.2749, 'grad_norm': 4.600589752197266, 'learning_rate': 6.872048280743919e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.42it/s]                                               {'loss': 2.2211, 'grad_norm': 5.058842658996582, 'learning_rate': 6.681158050723256e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.42it/s]                                               {'loss': 2.1884, 'grad_norm': 4.33897590637207, 'learning_rate': 6.490267820702591e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.42it/s] 57%|█████▋    | 43/75 [00:01<00:01, 24.71it/s]                                               {'loss': 2.2825, 'grad_norm': 4.08737850189209, 'learning_rate': 6.299377590681926e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.71it/s]                                               {'loss': 2.2643, 'grad_norm': 4.121744155883789, 'learning_rate': 6.108487360661261e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.71it/s]                                               {'loss': 3.112, 'grad_norm': 30.45219612121582, 'learning_rate': 5.9175971306405974e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.71it/s] 61%|██████▏   | 46/75 [00:01<00:01, 23.68it/s]                                               {'loss': 2.3104, 'grad_norm': 4.417705535888672, 'learning_rate': 5.7267069006199334e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 23.68it/s]                                               {'loss': 2.2849, 'grad_norm': 3.8743085861206055, 'learning_rate': 5.535816670599268e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 23.68it/s]                                               {'loss': 2.2991, 'grad_norm': 5.165441513061523, 'learning_rate': 5.344926440578604e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 23.68it/s] 65%|██████▌   | 49/75 [00:02<00:01, 23.93it/s]                                               {'loss': 2.2376, 'grad_norm': 5.268152236938477, 'learning_rate': 5.154036210557939e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 23.93it/s]                                               {'loss': 2.0384, 'grad_norm': 3.1198065280914307, 'learning_rate': 4.963145980537275e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 23.93it/s]                                               {'loss': 2.17, 'grad_norm': 4.920475006103516, 'learning_rate': 4.7722557505166105e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 23.93it/s] 69%|██████▉   | 52/75 [00:02<00:00, 23.70it/s]                                               {'loss': 2.3203, 'grad_norm': 4.402766704559326, 'learning_rate': 4.5813655204959464e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 23.70it/s]                                               {'loss': 2.2612, 'grad_norm': 3.885647773742676, 'learning_rate': 4.390475290475282e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 23.70it/s]                                               {'loss': 2.3173, 'grad_norm': 4.673047065734863, 'learning_rate': 4.199585060454618e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 23.70it/s] 73%|███████▎  | 55/75 [00:02<00:00, 23.44it/s]                                               {'loss': 2.2101, 'grad_norm': 4.1784892082214355, 'learning_rate': 4.0086948304339536e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 23.44it/s]                                               {'loss': 2.3421, 'grad_norm': 3.3712894916534424, 'learning_rate': 3.817804600413288e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 23.44it/s]                                               {'loss': 2.2696, 'grad_norm': 4.739978313446045, 'learning_rate': 3.626914370392624e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 23.44it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.04it/s]                                               {'loss': 2.1641, 'grad_norm': 3.7535858154296875, 'learning_rate': 3.4360241403719595e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.04it/s]                                               {'loss': 2.1589, 'grad_norm': 3.9145328998565674, 'learning_rate': 3.2451339103512954e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.04it/s]                                               {'loss': 2.2967, 'grad_norm': 10.778482437133789, 'learning_rate': 3.054243680330631e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.04it/s] 81%|████████▏ | 61/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.1728, 'grad_norm': 4.697166919708252, 'learning_rate': 2.8633534503099667e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.3673, 'grad_norm': 4.828632354736328, 'learning_rate': 2.672463220289302e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 24.99it/s]                                               {'loss': 1.9724, 'grad_norm': 3.2319858074188232, 'learning_rate': 2.4815729902686376e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 24.99it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.20it/s]                                               {'loss': 2.2431, 'grad_norm': 3.5548598766326904, 'learning_rate': 2.2906827602479732e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.20it/s]                                               {'loss': 2.3075, 'grad_norm': 3.5343804359436035, 'learning_rate': 2.099792530227309e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.20it/s]                                               {'loss': 2.0379, 'grad_norm': 2.7568042278289795, 'learning_rate': 1.908902300206644e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.20it/s] 89%|████████▉ | 67/75 [00:02<00:00, 24.48it/s]                                               {'loss': 2.1147, 'grad_norm': 4.123499870300293, 'learning_rate': 1.7180120701859797e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.48it/s]                                               {'loss': 2.2258, 'grad_norm': 3.227201461791992, 'learning_rate': 1.5271218401653154e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.48it/s]                                               {'loss': 2.2939, 'grad_norm': 4.365425109863281, 'learning_rate': 1.336231610144651e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.48it/s] 93%|█████████▎| 70/75 [00:02<00:00, 24.28it/s]                                               {'loss': 2.1681, 'grad_norm': 3.734182834625244, 'learning_rate': 1.1453413801239866e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.28it/s]                                               {'loss': 2.3545, 'grad_norm': 4.2419657707214355, 'learning_rate': 9.54451150103322e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.28it/s]                                               {'loss': 2.2412, 'grad_norm': 5.2162346839904785, 'learning_rate': 7.635609200826577e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.28it/s] 97%|█████████▋| 73/75 [00:03<00:00, 24.75it/s]                                               {'loss': 2.2143, 'grad_norm': 3.523825168609619, 'learning_rate': 5.726706900619933e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 24.75it/s]                                               {'loss': 2.141, 'grad_norm': 3.9127769470214844, 'learning_rate': 3.817804600413288e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.75it/s]                                               {'loss': 2.1394, 'grad_norm': 10.022889137268066, 'learning_rate': 1.908902300206644e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.75it/s]                                               {'train_runtime': 3.1594, 'train_samples_per_second': 357.662, 'train_steps_per_second': 23.739, 'train_loss': 2.264266726175944, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.75it/s]100%|██████████| 75/75 [00:03<00:00, 23.74it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1032, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(937, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(771, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(942, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1193, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(954, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1053, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1006, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1020, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(731, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1228, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(653, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 65.57it/s]  3%|▎         | 14/471 [00:00<00:07, 57.72it/s]  4%|▍         | 20/471 [00:00<00:08, 55.60it/s]  6%|▌         | 26/471 [00:00<00:08, 54.35it/s]  7%|▋         | 32/471 [00:00<00:08, 51.58it/s]  8%|▊         | 38/471 [00:00<00:08, 51.53it/s]  9%|▉         | 44/471 [00:00<00:08, 51.33it/s] 11%|█         | 50/471 [00:00<00:08, 51.20it/s] 12%|█▏        | 56/471 [00:01<00:08, 47.53it/s] 13%|█▎        | 62/471 [00:01<00:08, 49.39it/s] 14%|█▍        | 67/471 [00:01<00:08, 47.78it/s] 15%|█▌        | 73/471 [00:01<00:07, 50.04it/s] 17%|█▋        | 79/471 [00:01<00:07, 50.55it/s] 18%|█▊        | 85/471 [00:01<00:07, 50.96it/s] 19%|█▉        | 91/471 [00:01<00:07, 51.26it/s] 21%|██        | 97/471 [00:01<00:07, 51.42it/s] 22%|██▏       | 103/471 [00:02<00:07, 51.34it/s] 23%|██▎       | 109/471 [00:02<00:07, 50.62it/s] 24%|██▍       | 115/471 [00:02<00:06, 51.37it/s] 26%|██▌       | 121/471 [00:02<00:06, 51.03it/s] 27%|██▋       | 127/471 [00:02<00:07, 48.60it/s] 28%|██▊       | 133/471 [00:02<00:06, 49.72it/s] 29%|██▉       | 138/471 [00:02<00:06, 49.30it/s] 31%|███       | 144/471 [00:02<00:06, 50.83it/s] 32%|███▏      | 150/471 [00:02<00:06, 50.42it/s] 33%|███▎      | 156/471 [00:03<00:06, 49.10it/s] 34%|███▍      | 162/471 [00:03<00:06, 49.29it/s] 35%|███▌      | 167/471 [00:03<00:06, 48.08it/s] 37%|███▋      | 172/471 [00:03<00:06, 46.16it/s] 38%|███▊      | 178/471 [00:03<00:06, 48.32it/s] 39%|███▉      | 183/471 [00:03<00:06, 47.81it/s] 40%|████      | 189/471 [00:03<00:05, 48.73it/s] 41%|████▏     | 195/471 [00:03<00:05, 49.73it/s] 43%|████▎     | 201/471 [00:03<00:05, 50.51it/s] 44%|████▍     | 207/471 [00:04<00:05, 49.06it/s] 45%|████▌     | 212/471 [00:04<00:05, 48.43it/s] 46%|████▋     | 218/471 [00:04<00:05, 49.36it/s] 47%|████▋     | 223/471 [00:04<00:05, 46.52it/s] 49%|████▊     | 229/471 [00:04<00:04, 48.86it/s] 50%|████▉     | 235/471 [00:04<00:04, 49.88it/s] 51%|█████     | 241/471 [00:04<00:04, 50.68it/s] 52%|█████▏    | 247/471 [00:04<00:04, 51.33it/s] 54%|█████▎    | 253/471 [00:05<00:04, 51.15it/s] 55%|█████▍    | 259/471 [00:05<00:04, 50.76it/s] 56%|█████▋    | 265/471 [00:05<00:04, 51.40it/s] 58%|█████▊    | 271/471 [00:05<00:03, 50.39it/s] 59%|█████▉    | 277/471 [00:05<00:03, 50.33it/s] 60%|██████    | 283/471 [00:05<00:03, 50.51it/s] 61%|██████▏   | 289/471 [00:05<00:03, 50.24it/s] 63%|██████▎   | 295/471 [00:05<00:03, 50.52it/s] 64%|██████▍   | 301/471 [00:05<00:03, 50.94it/s] 65%|██████▌   | 307/471 [00:06<00:03, 51.58it/s] 66%|██████▋   | 313/471 [00:06<00:03, 51.74it/s] 68%|██████▊   | 319/471 [00:06<00:02, 51.72it/s] 69%|██████▉   | 325/471 [00:06<00:02, 49.73it/s] 70%|███████   | 331/471 [00:06<00:02, 50.71it/s] 72%|███████▏  | 337/471 [00:06<00:02, 50.93it/s] 73%|███████▎  | 343/471 [00:06<00:02, 50.32it/s] 74%|███████▍  | 349/471 [00:06<00:02, 48.76it/s] 75%|███████▌  | 355/471 [00:07<00:02, 50.25it/s] 77%|███████▋  | 361/471 [00:07<00:02, 49.75it/s] 78%|███████▊  | 367/471 [00:07<00:02, 50.25it/s] 79%|███████▉  | 373/471 [00:07<00:01, 49.91it/s] 80%|████████  | 379/471 [00:07<00:01, 49.79it/s] 82%|████████▏ | 385/471 [00:07<00:01, 50.51it/s] 83%|████████▎ | 391/471 [00:07<00:01, 50.98it/s] 84%|████████▍ | 397/471 [00:07<00:01, 49.92it/s] 86%|████████▌ | 403/471 [00:08<00:01, 50.42it/s] 87%|████████▋ | 409/471 [00:08<00:01, 50.62it/s] 88%|████████▊ | 415/471 [00:08<00:01, 50.79it/s] 89%|████████▉ | 421/471 [00:08<00:00, 50.40it/s] 91%|█████████ | 427/471 [00:08<00:00, 50.73it/s] 92%|█████████▏| 433/471 [00:08<00:00, 50.30it/s] 93%|█████████▎| 439/471 [00:08<00:00, 50.89it/s] 94%|█████████▍| 445/471 [00:08<00:00, 50.78it/s] 96%|█████████▌| 451/471 [00:08<00:00, 50.04it/s] 97%|█████████▋| 457/471 [00:09<00:00, 49.72it/s] 98%|█████████▊| 463/471 [00:09<00:00, 50.85it/s]100%|█████████▉| 469/471 [00:09<00:00, 50.83it/s]100%|██████████| 471/471 [00:09<00:00, 50.36it/s]
{'eval_loss': 2.2945852279663086, 'eval_model_preparation_time': 0.0029, 'eval_acc': 0.38860860329261815, 'eval_runtime': 9.3743, 'eval_samples_per_second': 803.475, 'eval_steps_per_second': 50.244}
ROUND:13
CLIENT:16
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3391, 'grad_norm': 4.4822916984558105, 'learning_rate': 0.00014017542509913794, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 21.67it/s]                                              {'loss': 2.2995, 'grad_norm': 4.727336883544922, 'learning_rate': 0.00013830641943114944, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.88it/s]  4%|▍         | 3/75 [00:00<00:02, 24.31it/s]                                              {'loss': 2.191, 'grad_norm': 4.765097141265869, 'learning_rate': 0.00013643741376316094, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.31it/s]                                              {'loss': 2.356, 'grad_norm': 5.30910587310791, 'learning_rate': 0.00013456840809517243, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.31it/s]                                              {'loss': 2.3299, 'grad_norm': 4.808292865753174, 'learning_rate': 0.00013269940242718393, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.31it/s]  8%|▊         | 6/75 [00:00<00:02, 24.81it/s]                                              {'loss': 2.2416, 'grad_norm': 3.6634633541107178, 'learning_rate': 0.00013083039675919542, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.81it/s]                                              {'loss': 2.2886, 'grad_norm': 4.687731742858887, 'learning_rate': 0.00012896139109120692, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.81it/s]                                              {'loss': 2.3539, 'grad_norm': 3.835256576538086, 'learning_rate': 0.0001270923854232184, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.81it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.52it/s]                                              {'loss': 2.354, 'grad_norm': 5.174108028411865, 'learning_rate': 0.00012522337975522988, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.52it/s]                                              {'loss': 2.2255, 'grad_norm': 3.457322597503662, 'learning_rate': 0.00012335437408724138, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.52it/s]                                               {'loss': 2.1565, 'grad_norm': 3.9430503845214844, 'learning_rate': 0.00012148536841925289, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.52it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.32it/s]                                               {'loss': 2.3709, 'grad_norm': 3.658202886581421, 'learning_rate': 0.00011961636275126438, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.32it/s]                                               {'loss': 2.0678, 'grad_norm': 3.9192070960998535, 'learning_rate': 0.00011774735708327587, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.32it/s]                                               {'loss': 2.4559, 'grad_norm': 4.110408782958984, 'learning_rate': 0.00011587835141528736, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.32it/s]                                               {'loss': 2.4059, 'grad_norm': 8.301045417785645, 'learning_rate': 0.00011400934574729886, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.32it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.03it/s]                                               {'loss': 2.3422, 'grad_norm': 4.170464038848877, 'learning_rate': 0.00011214034007931035, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.03it/s]                                               {'loss': 2.4105, 'grad_norm': 4.853627681732178, 'learning_rate': 0.00011027133441132184, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.03it/s]                                               {'loss': 2.2874, 'grad_norm': 3.0754356384277344, 'learning_rate': 0.00010840232874333335, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.03it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.49it/s]                                               {'loss': 2.2539, 'grad_norm': 4.42344856262207, 'learning_rate': 0.00010653332307534484, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.49it/s]                                               {'loss': 2.1821, 'grad_norm': 3.9573235511779785, 'learning_rate': 0.00010466431740735634, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.49it/s]                                               {'loss': 2.3271, 'grad_norm': 5.0751543045043945, 'learning_rate': 0.00010279531173936782, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.49it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.34it/s]                                               {'loss': 2.2322, 'grad_norm': 3.6066431999206543, 'learning_rate': 0.00010092630607137932, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.34it/s]                                               {'loss': 2.232, 'grad_norm': 3.1904752254486084, 'learning_rate': 9.905730040339081e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.34it/s]                                               {'loss': 2.1768, 'grad_norm': 4.232363224029541, 'learning_rate': 9.718829473540231e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 25.34it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.61it/s]                                               {'loss': 2.1543, 'grad_norm': 3.8458521366119385, 'learning_rate': 9.53192890674138e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.61it/s]                                               {'loss': 2.1059, 'grad_norm': 3.633486270904541, 'learning_rate': 9.345028339942529e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.61it/s]                                               {'loss': 2.2698, 'grad_norm': 3.5791242122650146, 'learning_rate': 9.158127773143678e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.61it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.82it/s]                                               {'loss': 2.3714, 'grad_norm': 3.9342222213745117, 'learning_rate': 8.971227206344829e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.82it/s]                                               {'loss': 2.1065, 'grad_norm': 3.4536476135253906, 'learning_rate': 8.784326639545979e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.82it/s]                                               {'loss': 1.639, 'grad_norm': 7.058072566986084, 'learning_rate': 8.597426072747127e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.82it/s]                                               {'loss': 2.013, 'grad_norm': 4.44326639175415, 'learning_rate': 8.410525505948277e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.82it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.46it/s]                                               {'loss': 2.2396, 'grad_norm': 3.9410860538482666, 'learning_rate': 8.223624939149426e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.46it/s]                                               {'loss': 2.1852, 'grad_norm': 3.3732707500457764, 'learning_rate': 8.036724372350576e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.46it/s]                                               {'loss': 2.1944, 'grad_norm': 4.918731689453125, 'learning_rate': 7.849823805551725e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.46it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.28it/s]                                               {'loss': 2.2501, 'grad_norm': 3.6670854091644287, 'learning_rate': 7.662923238752874e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.28it/s]                                               {'loss': 2.294, 'grad_norm': 3.805812358856201, 'learning_rate': 7.476022671954023e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.28it/s]                                               {'loss': 2.2342, 'grad_norm': 4.7576985359191895, 'learning_rate': 7.289122105155173e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.28it/s] 51%|█████     | 38/75 [00:01<00:01, 25.56it/s]                                               {'loss': 2.2707, 'grad_norm': 3.7945733070373535, 'learning_rate': 7.102221538356324e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.56it/s]                                               {'loss': 2.1333, 'grad_norm': 3.5295004844665527, 'learning_rate': 6.915320971557472e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.56it/s]                                               {'loss': 2.2948, 'grad_norm': 3.9452075958251953, 'learning_rate': 6.728420404758622e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.56it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.52it/s]                                               {'loss': 1.9981, 'grad_norm': 3.5118649005889893, 'learning_rate': 6.541519837959771e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.52it/s]                                               {'loss': 2.4498, 'grad_norm': 4.788789749145508, 'learning_rate': 6.35461927116092e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.52it/s]                                               {'loss': 2.2711, 'grad_norm': 3.9457967281341553, 'learning_rate': 6.167718704362069e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.52it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.0814, 'grad_norm': 3.044105052947998, 'learning_rate': 5.980818137563219e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.30it/s]                                               {'loss': 1.8909, 'grad_norm': 15.819886207580566, 'learning_rate': 5.793917570764368e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.1765, 'grad_norm': 3.593653440475464, 'learning_rate': 5.607017003965518e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.1028, 'grad_norm': 4.39961051940918, 'learning_rate': 5.420116437166667e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.30it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.24it/s]                                               {'loss': 2.3183, 'grad_norm': 4.778872013092041, 'learning_rate': 5.233215870367817e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.24it/s]                                               {'loss': 2.0665, 'grad_norm': 3.3438034057617188, 'learning_rate': 5.046315303568966e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.24it/s]                                               {'loss': 2.3024, 'grad_norm': 4.500397682189941, 'learning_rate': 4.8594147367701154e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.24it/s] 68%|██████▊   | 51/75 [00:01<00:00, 25.72it/s]                                               {'loss': 2.292, 'grad_norm': 3.951756715774536, 'learning_rate': 4.672514169971264e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 25.72it/s]                                               {'loss': 2.4147, 'grad_norm': 4.989504337310791, 'learning_rate': 4.4856136031724146e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.72it/s]                                               {'loss': 2.0785, 'grad_norm': 3.800203323364258, 'learning_rate': 4.2987130363735635e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.72it/s] 72%|███████▏  | 54/75 [00:02<00:00, 24.80it/s]                                               {'loss': 2.1346, 'grad_norm': 4.213259696960449, 'learning_rate': 4.111812469574713e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.80it/s]                                               {'loss': 2.1019, 'grad_norm': 4.983219146728516, 'learning_rate': 3.924911902775863e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.80it/s]                                               {'loss': 2.1991, 'grad_norm': 3.6289916038513184, 'learning_rate': 3.7380113359770116e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.80it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.50it/s]                                               {'loss': 2.1083, 'grad_norm': 3.3664181232452393, 'learning_rate': 3.551110769178162e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.50it/s]                                               {'loss': 2.2744, 'grad_norm': 4.244848251342773, 'learning_rate': 3.364210202379311e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.50it/s]                                               {'loss': 2.1541, 'grad_norm': 3.2305943965911865, 'learning_rate': 3.17730963558046e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.50it/s]                                               {'loss': 1.7313, 'grad_norm': 9.049365997314453, 'learning_rate': 2.9904090687816096e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.50it/s] 81%|████████▏ | 61/75 [00:02<00:00, 26.04it/s]                                               {'loss': 2.2389, 'grad_norm': 4.411750316619873, 'learning_rate': 2.803508501982759e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.04it/s]                                               {'loss': 2.1471, 'grad_norm': 3.9388346672058105, 'learning_rate': 2.6166079351839085e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.04it/s]                                               {'loss': 1.931, 'grad_norm': 3.9254016876220703, 'learning_rate': 2.4297073683850577e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.04it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.85it/s]                                               {'loss': 2.4127, 'grad_norm': 4.485668659210205, 'learning_rate': 2.2428068015862073e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.85it/s]                                               {'loss': 2.1358, 'grad_norm': 3.2448010444641113, 'learning_rate': 2.0559062347873566e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.85it/s]                                               {'loss': 2.2881, 'grad_norm': 4.2945380210876465, 'learning_rate': 1.8690056679885058e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.85it/s] 89%|████████▉ | 67/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.4153, 'grad_norm': 4.129317283630371, 'learning_rate': 1.6821051011896554e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.1132, 'grad_norm': 3.8058369159698486, 'learning_rate': 1.4952045343908048e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.0863, 'grad_norm': 3.133225679397583, 'learning_rate': 1.3083039675919542e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.49it/s] 93%|█████████▎| 70/75 [00:02<00:00, 25.52it/s]                                               {'loss': 2.3257, 'grad_norm': 6.414373397827148, 'learning_rate': 1.1214034007931037e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.52it/s]                                               {'loss': 2.2004, 'grad_norm': 4.694856643676758, 'learning_rate': 9.345028339942529e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.52it/s]                                               {'loss': 1.9175, 'grad_norm': 3.2461037635803223, 'learning_rate': 7.476022671954024e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.52it/s] 97%|█████████▋| 73/75 [00:02<00:00, 24.35it/s]                                               {'loss': 1.9589, 'grad_norm': 4.611532211303711, 'learning_rate': 5.607017003965518e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.35it/s]                                               {'loss': 2.2554, 'grad_norm': 3.730616569519043, 'learning_rate': 3.738011335977012e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.35it/s]                                               {'loss': 2.038, 'grad_norm': 8.957175254821777, 'learning_rate': 1.869005667988506e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.35it/s]                                               {'train_runtime': 3.0579, 'train_samples_per_second': 369.531, 'train_steps_per_second': 24.526, 'train_loss': 2.2032955058415733, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.35it/s]100%|██████████| 75/75 [00:03<00:00, 24.53it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:39
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.0989, 'grad_norm': 4.032205104827881, 'learning_rate': 0.00014017542509913794, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 21.06it/s]                                              {'loss': 2.3838, 'grad_norm': 3.583491563796997, 'learning_rate': 0.00013830641943114944, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 21.34it/s]  4%|▍         | 3/75 [00:00<00:03, 22.65it/s]                                              {'loss': 2.2695, 'grad_norm': 4.13655948638916, 'learning_rate': 0.00013643741376316094, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 22.65it/s]                                              {'loss': 2.1286, 'grad_norm': 4.7606964111328125, 'learning_rate': 0.00013456840809517243, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 22.65it/s]                                              {'loss': 2.3702, 'grad_norm': 5.019089698791504, 'learning_rate': 0.00013269940242718393, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.65it/s]  8%|▊         | 6/75 [00:00<00:03, 22.53it/s]                                              {'loss': 2.4631, 'grad_norm': 4.041996955871582, 'learning_rate': 0.00013083039675919542, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 22.53it/s]                                              {'loss': 2.1783, 'grad_norm': 4.2147393226623535, 'learning_rate': 0.00012896139109120692, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 22.53it/s]                                              {'loss': 2.4482, 'grad_norm': 4.9283270835876465, 'learning_rate': 0.0001270923854232184, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 22.53it/s] 12%|█▏        | 9/75 [00:00<00:02, 23.44it/s]                                              {'loss': 2.4331, 'grad_norm': 3.9289448261260986, 'learning_rate': 0.00012522337975522988, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.44it/s]                                              {'loss': 2.3844, 'grad_norm': 4.12923002243042, 'learning_rate': 0.00012335437408724138, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.44it/s]                                               {'loss': 2.2161, 'grad_norm': 3.9092772006988525, 'learning_rate': 0.00012148536841925289, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.44it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.82it/s]                                               {'loss': 2.3937, 'grad_norm': 4.564446449279785, 'learning_rate': 0.00011961636275126438, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.82it/s]                                               {'loss': 1.9852, 'grad_norm': 3.5911166667938232, 'learning_rate': 0.00011774735708327587, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.82it/s]                                               {'loss': 2.337, 'grad_norm': 3.8441715240478516, 'learning_rate': 0.00011587835141528736, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.82it/s]                                               {'loss': 2.3135, 'grad_norm': 11.56161880493164, 'learning_rate': 0.00011400934574729886, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.82it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.03it/s]                                               {'loss': 2.4434, 'grad_norm': 4.446091651916504, 'learning_rate': 0.00011214034007931035, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.03it/s]                                               {'loss': 2.0141, 'grad_norm': 3.3959336280822754, 'learning_rate': 0.00011027133441132184, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.03it/s]                                               {'loss': 2.1824, 'grad_norm': 4.077065467834473, 'learning_rate': 0.00010840232874333335, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.03it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.83it/s]                                               {'loss': 2.1276, 'grad_norm': 3.426593780517578, 'learning_rate': 0.00010653332307534484, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.83it/s]                                               {'loss': 2.2538, 'grad_norm': 4.330020904541016, 'learning_rate': 0.00010466431740735634, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.83it/s]                                               {'loss': 2.2646, 'grad_norm': 4.0151190757751465, 'learning_rate': 0.00010279531173936782, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.83it/s] 29%|██▉       | 22/75 [00:00<00:02, 24.76it/s]                                               {'loss': 2.163, 'grad_norm': 3.772017478942871, 'learning_rate': 0.00010092630607137932, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.76it/s]                                               {'loss': 2.1245, 'grad_norm': 5.807886600494385, 'learning_rate': 9.905730040339081e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.76it/s]                                               {'loss': 2.095, 'grad_norm': 3.9582316875457764, 'learning_rate': 9.718829473540231e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.76it/s] 33%|███▎      | 25/75 [00:01<00:02, 24.24it/s]                                               {'loss': 2.2423, 'grad_norm': 3.8224406242370605, 'learning_rate': 9.53192890674138e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.24it/s]                                               {'loss': 2.2338, 'grad_norm': 3.594628095626831, 'learning_rate': 9.345028339942529e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.24it/s]                                               {'loss': 2.2784, 'grad_norm': 4.014691352844238, 'learning_rate': 9.158127773143678e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.24it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.66it/s]                                               {'loss': 2.4613, 'grad_norm': 5.40967321395874, 'learning_rate': 8.971227206344829e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.66it/s]                                               {'loss': 2.3, 'grad_norm': 3.3573110103607178, 'learning_rate': 8.784326639545979e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.66it/s]                                               {'loss': 2.4465, 'grad_norm': 20.029338836669922, 'learning_rate': 8.597426072747127e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.66it/s]                                               {'loss': 2.0779, 'grad_norm': 3.3079092502593994, 'learning_rate': 8.410525505948277e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.66it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.29it/s]                                               {'loss': 2.3265, 'grad_norm': 3.7142248153686523, 'learning_rate': 8.223624939149426e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.29it/s]                                               {'loss': 2.2111, 'grad_norm': 3.342316150665283, 'learning_rate': 8.036724372350576e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.29it/s]                                               {'loss': 2.403, 'grad_norm': 3.507216215133667, 'learning_rate': 7.849823805551725e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.29it/s] 47%|████▋     | 35/75 [00:01<00:01, 24.89it/s]                                               {'loss': 2.0659, 'grad_norm': 4.263768196105957, 'learning_rate': 7.662923238752874e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.89it/s]                                               {'loss': 2.119, 'grad_norm': 3.8614110946655273, 'learning_rate': 7.476022671954023e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.89it/s]                                               {'loss': 2.215, 'grad_norm': 4.347047328948975, 'learning_rate': 7.289122105155173e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.89it/s] 51%|█████     | 38/75 [00:01<00:01, 23.81it/s]                                               {'loss': 2.348, 'grad_norm': 4.6471405029296875, 'learning_rate': 7.102221538356324e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 23.81it/s]                                               {'loss': 2.094, 'grad_norm': 3.668546676635742, 'learning_rate': 6.915320971557472e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.81it/s]                                               {'loss': 2.0488, 'grad_norm': 4.828460216522217, 'learning_rate': 6.728420404758622e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.81it/s] 55%|█████▍    | 41/75 [00:01<00:01, 24.22it/s]                                               {'loss': 2.1829, 'grad_norm': 4.003704071044922, 'learning_rate': 6.541519837959771e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.22it/s]                                               {'loss': 2.2613, 'grad_norm': 3.486382484436035, 'learning_rate': 6.35461927116092e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.22it/s]                                               {'loss': 2.2962, 'grad_norm': 4.184253692626953, 'learning_rate': 6.167718704362069e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.22it/s] 59%|█████▊    | 44/75 [00:01<00:01, 23.83it/s]                                               {'loss': 2.1871, 'grad_norm': 3.326223611831665, 'learning_rate': 5.980818137563219e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 23.83it/s]                                               {'loss': 2.7501, 'grad_norm': 16.9099063873291, 'learning_rate': 5.793917570764368e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 23.83it/s]                                               {'loss': 2.0537, 'grad_norm': 3.1370787620544434, 'learning_rate': 5.607017003965518e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 23.83it/s] 63%|██████▎   | 47/75 [00:01<00:01, 24.91it/s]                                               {'loss': 2.2105, 'grad_norm': 3.7249181270599365, 'learning_rate': 5.420116437166667e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 24.91it/s]                                               {'loss': 2.184, 'grad_norm': 3.1763319969177246, 'learning_rate': 5.233215870367817e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 24.91it/s]                                               {'loss': 2.3347, 'grad_norm': 4.106523513793945, 'learning_rate': 5.046315303568966e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 24.91it/s] 67%|██████▋   | 50/75 [00:02<00:01, 23.90it/s]                                               {'loss': 2.4457, 'grad_norm': 4.390334606170654, 'learning_rate': 4.8594147367701154e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 23.90it/s]                                               {'loss': 2.1372, 'grad_norm': 4.693404197692871, 'learning_rate': 4.672514169971264e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 23.90it/s]                                               {'loss': 2.1755, 'grad_norm': 3.837233781814575, 'learning_rate': 4.4856136031724146e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 23.90it/s] 71%|███████   | 53/75 [00:02<00:00, 23.83it/s]                                               {'loss': 2.2584, 'grad_norm': 3.8784375190734863, 'learning_rate': 4.2987130363735635e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 23.83it/s]                                               {'loss': 2.1954, 'grad_norm': 4.807240009307861, 'learning_rate': 4.111812469574713e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 23.83it/s]                                               {'loss': 2.0596, 'grad_norm': 3.8491430282592773, 'learning_rate': 3.924911902775863e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 23.83it/s] 75%|███████▍  | 56/75 [00:02<00:00, 24.12it/s]                                               {'loss': 2.2313, 'grad_norm': 3.908698081970215, 'learning_rate': 3.7380113359770116e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.12it/s]                                               {'loss': 2.2505, 'grad_norm': 3.9988560676574707, 'learning_rate': 3.551110769178162e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.12it/s]                                               {'loss': 2.2578, 'grad_norm': 3.8955934047698975, 'learning_rate': 3.364210202379311e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.12it/s] 79%|███████▊  | 59/75 [00:02<00:00, 24.00it/s]                                               {'loss': 1.9917, 'grad_norm': 3.6137702465057373, 'learning_rate': 3.17730963558046e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.00it/s]                                               {'loss': 2.3999, 'grad_norm': 8.803912162780762, 'learning_rate': 2.9904090687816096e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.00it/s]                                               {'loss': 2.2571, 'grad_norm': 4.420283317565918, 'learning_rate': 2.803508501982759e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.00it/s] 83%|████████▎ | 62/75 [00:02<00:00, 21.37it/s]                                               {'loss': 2.3923, 'grad_norm': 3.9266197681427, 'learning_rate': 2.6166079351839085e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 21.37it/s]                                               {'loss': 2.0647, 'grad_norm': 3.551076650619507, 'learning_rate': 2.4297073683850577e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 21.37it/s]                                               {'loss': 2.0115, 'grad_norm': 4.246209621429443, 'learning_rate': 2.2428068015862073e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 21.37it/s] 87%|████████▋ | 65/75 [00:02<00:00, 17.69it/s]                                               {'loss': 2.0869, 'grad_norm': 3.450817823410034, 'learning_rate': 2.0559062347873566e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 17.69it/s]                                               {'loss': 2.3881, 'grad_norm': 4.586526393890381, 'learning_rate': 1.8690056679885058e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 17.69it/s] 89%|████████▉ | 67/75 [00:03<00:00, 15.26it/s]                                               {'loss': 2.1003, 'grad_norm': 4.083486557006836, 'learning_rate': 1.6821051011896554e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 15.26it/s]                                               {'loss': 2.1223, 'grad_norm': 3.8510708808898926, 'learning_rate': 1.4952045343908048e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 15.26it/s]                                               {'loss': 2.1582, 'grad_norm': 3.6303820610046387, 'learning_rate': 1.3083039675919542e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 15.26it/s] 93%|█████████▎| 70/75 [00:03<00:00, 16.05it/s]                                               {'loss': 2.3646, 'grad_norm': 4.953085899353027, 'learning_rate': 1.1214034007931037e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 16.05it/s]                                               {'loss': 1.9847, 'grad_norm': 3.6668412685394287, 'learning_rate': 9.345028339942529e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 16.05it/s] 96%|█████████▌| 72/75 [00:03<00:00, 14.58it/s]                                               {'loss': 2.0441, 'grad_norm': 3.565760374069214, 'learning_rate': 7.476022671954024e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 14.58it/s]                                               {'loss': 2.1483, 'grad_norm': 4.012966156005859, 'learning_rate': 5.607017003965518e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 14.58it/s] 99%|█████████▊| 74/75 [00:03<00:00, 14.48it/s]                                               {'loss': 2.3947, 'grad_norm': 4.334416389465332, 'learning_rate': 3.738011335977012e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 14.48it/s]                                               {'loss': 1.763, 'grad_norm': 7.306158542633057, 'learning_rate': 1.869005667988506e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 14.48it/s]                                               {'train_runtime': 3.9757, 'train_samples_per_second': 284.23, 'train_steps_per_second': 18.865, 'train_loss': 2.227841223080953, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 14.48it/s]100%|██████████| 75/75 [00:03<00:00, 18.87it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:1
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.1664, 'grad_norm': 4.039452075958252, 'learning_rate': 0.00014017542509913794, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 22.88it/s]                                              {'loss': 2.3026, 'grad_norm': 4.327311038970947, 'learning_rate': 0.00013830641943114944, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.38it/s]  4%|▍         | 3/75 [00:00<00:02, 24.70it/s]                                              {'loss': 2.4026, 'grad_norm': 4.3271965980529785, 'learning_rate': 0.00013643741376316094, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.70it/s]                                              {'loss': 2.5187, 'grad_norm': 5.164936542510986, 'learning_rate': 0.00013456840809517243, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.70it/s]                                              {'loss': 2.3292, 'grad_norm': 4.129355430603027, 'learning_rate': 0.00013269940242718393, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.70it/s]  8%|▊         | 6/75 [00:00<00:02, 25.20it/s]                                              {'loss': 2.202, 'grad_norm': 4.087203502655029, 'learning_rate': 0.00013083039675919542, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.20it/s]                                              {'loss': 2.3748, 'grad_norm': 5.250041961669922, 'learning_rate': 0.00012896139109120692, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.20it/s]                                              {'loss': 2.2458, 'grad_norm': 4.138788223266602, 'learning_rate': 0.0001270923854232184, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.20it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.68it/s]                                              {'loss': 2.5313, 'grad_norm': 4.532528400421143, 'learning_rate': 0.00012522337975522988, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.68it/s]                                              {'loss': 2.5553, 'grad_norm': 4.451376914978027, 'learning_rate': 0.00012335437408724138, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.68it/s]                                               {'loss': 2.1593, 'grad_norm': 4.443119049072266, 'learning_rate': 0.00012148536841925289, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.68it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.27it/s]                                               {'loss': 2.2388, 'grad_norm': 4.046418190002441, 'learning_rate': 0.00011961636275126438, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.27it/s]                                               {'loss': 2.4077, 'grad_norm': 4.416529178619385, 'learning_rate': 0.00011774735708327587, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.27it/s]                                               {'loss': 2.4039, 'grad_norm': 5.048421859741211, 'learning_rate': 0.00011587835141528736, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.27it/s]                                               {'loss': 2.3447, 'grad_norm': 10.111001968383789, 'learning_rate': 0.00011400934574729886, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.27it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.99it/s]                                               {'loss': 2.285, 'grad_norm': 3.3236780166625977, 'learning_rate': 0.00011214034007931035, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.99it/s]                                               {'loss': 2.3818, 'grad_norm': 4.583737850189209, 'learning_rate': 0.00011027133441132184, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.99it/s]                                               {'loss': 2.2654, 'grad_norm': 4.602301597595215, 'learning_rate': 0.00010840232874333335, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.99it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.66it/s]                                               {'loss': 2.2219, 'grad_norm': 4.3912272453308105, 'learning_rate': 0.00010653332307534484, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.66it/s]                                               {'loss': 2.051, 'grad_norm': 4.122189998626709, 'learning_rate': 0.00010466431740735634, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.66it/s]                                               {'loss': 2.2843, 'grad_norm': 3.9904375076293945, 'learning_rate': 0.00010279531173936782, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.66it/s] 29%|██▉       | 22/75 [00:00<00:02, 24.67it/s]                                               {'loss': 2.3071, 'grad_norm': 3.7441940307617188, 'learning_rate': 0.00010092630607137932, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.67it/s]                                               {'loss': 2.507, 'grad_norm': 5.161864757537842, 'learning_rate': 9.905730040339081e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.67it/s]                                               {'loss': 2.1737, 'grad_norm': 4.2369585037231445, 'learning_rate': 9.718829473540231e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.67it/s] 33%|███▎      | 25/75 [00:01<00:02, 24.13it/s]                                               {'loss': 2.1552, 'grad_norm': 3.9408092498779297, 'learning_rate': 9.53192890674138e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.13it/s]                                               {'loss': 2.2885, 'grad_norm': 4.160257339477539, 'learning_rate': 9.345028339942529e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.13it/s]                                               {'loss': 2.2146, 'grad_norm': 3.974592685699463, 'learning_rate': 9.158127773143678e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.13it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.35it/s]                                               {'loss': 2.2932, 'grad_norm': 3.848151206970215, 'learning_rate': 8.971227206344829e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.35it/s]                                               {'loss': 2.2305, 'grad_norm': 4.966492176055908, 'learning_rate': 8.784326639545979e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.35it/s]                                               {'loss': 2.3653, 'grad_norm': 8.840164184570312, 'learning_rate': 8.597426072747127e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.35it/s]                                               {'loss': 2.1876, 'grad_norm': 3.353468179702759, 'learning_rate': 8.410525505948277e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.35it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.35it/s]                                               {'loss': 2.3012, 'grad_norm': 4.679839134216309, 'learning_rate': 8.223624939149426e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.35it/s]                                               {'loss': 2.1591, 'grad_norm': 4.353989601135254, 'learning_rate': 8.036724372350576e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.35it/s]                                               {'loss': 2.2493, 'grad_norm': 4.835078239440918, 'learning_rate': 7.849823805551725e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.35it/s] 47%|████▋     | 35/75 [00:01<00:01, 25.87it/s]                                               {'loss': 2.1417, 'grad_norm': 4.326303482055664, 'learning_rate': 7.662923238752874e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.87it/s]                                               {'loss': 2.2575, 'grad_norm': 4.772733688354492, 'learning_rate': 7.476022671954023e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.87it/s]                                               {'loss': 2.257, 'grad_norm': 4.86035680770874, 'learning_rate': 7.289122105155173e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.87it/s] 51%|█████     | 38/75 [00:01<00:01, 25.61it/s]                                               {'loss': 2.2708, 'grad_norm': 3.5736031532287598, 'learning_rate': 7.102221538356324e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.61it/s]                                               {'loss': 2.1687, 'grad_norm': 5.089613437652588, 'learning_rate': 6.915320971557472e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.61it/s]                                               {'loss': 2.1704, 'grad_norm': 3.3414394855499268, 'learning_rate': 6.728420404758622e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.61it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.66it/s]                                               {'loss': 2.232, 'grad_norm': 3.9094624519348145, 'learning_rate': 6.541519837959771e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.66it/s]                                               {'loss': 2.2811, 'grad_norm': 4.034713268280029, 'learning_rate': 6.35461927116092e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.66it/s]                                               {'loss': 2.325, 'grad_norm': 4.225096702575684, 'learning_rate': 6.167718704362069e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.66it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.2452, 'grad_norm': 3.4100444316864014, 'learning_rate': 5.980818137563219e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.1633, 'grad_norm': 8.070918083190918, 'learning_rate': 5.793917570764368e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.3221, 'grad_norm': 3.7570013999938965, 'learning_rate': 5.607017003965518e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.3949, 'grad_norm': 4.2007036209106445, 'learning_rate': 5.420116437166667e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.24it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.77it/s]                                               {'loss': 2.3822, 'grad_norm': 4.801649570465088, 'learning_rate': 5.233215870367817e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.77it/s]                                               {'loss': 2.3905, 'grad_norm': 4.340201377868652, 'learning_rate': 5.046315303568966e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.77it/s]                                               {'loss': 2.2366, 'grad_norm': 4.121417999267578, 'learning_rate': 4.8594147367701154e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.77it/s] 68%|██████▊   | 51/75 [00:02<00:00, 25.05it/s]                                               {'loss': 2.0719, 'grad_norm': 3.7735471725463867, 'learning_rate': 4.672514169971264e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.05it/s]                                               {'loss': 2.1646, 'grad_norm': 4.613376617431641, 'learning_rate': 4.4856136031724146e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.05it/s]                                               {'loss': 2.0953, 'grad_norm': 3.810493230819702, 'learning_rate': 4.2987130363735635e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.05it/s] 72%|███████▏  | 54/75 [00:02<00:00, 24.98it/s]                                               {'loss': 2.2888, 'grad_norm': 4.00100040435791, 'learning_rate': 4.111812469574713e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.98it/s]                                               {'loss': 1.9651, 'grad_norm': 3.1416938304901123, 'learning_rate': 3.924911902775863e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.98it/s]                                               {'loss': 2.0301, 'grad_norm': 4.205716609954834, 'learning_rate': 3.7380113359770116e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.98it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.96it/s]                                               {'loss': 1.9659, 'grad_norm': 4.182744026184082, 'learning_rate': 3.551110769178162e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.2944, 'grad_norm': 3.837161064147949, 'learning_rate': 3.364210202379311e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.306, 'grad_norm': 4.578786849975586, 'learning_rate': 3.17730963558046e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.0438, 'grad_norm': 12.260504722595215, 'learning_rate': 2.9904090687816096e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.96it/s] 81%|████████▏ | 61/75 [00:02<00:00, 26.28it/s]                                               {'loss': 2.3288, 'grad_norm': 4.119729042053223, 'learning_rate': 2.803508501982759e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.28it/s]                                               {'loss': 2.2612, 'grad_norm': 3.9744224548339844, 'learning_rate': 2.6166079351839085e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.28it/s]                                               {'loss': 2.2582, 'grad_norm': 3.6730868816375732, 'learning_rate': 2.4297073683850577e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.28it/s] 85%|████████▌ | 64/75 [00:02<00:00, 24.75it/s]                                               {'loss': 2.25, 'grad_norm': 3.3459935188293457, 'learning_rate': 2.2428068015862073e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 24.75it/s]                                               {'loss': 2.1791, 'grad_norm': 3.1351518630981445, 'learning_rate': 2.0559062347873566e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.75it/s]                                               {'loss': 2.1116, 'grad_norm': 4.253262519836426, 'learning_rate': 1.8690056679885058e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.75it/s] 89%|████████▉ | 67/75 [00:02<00:00, 24.14it/s]                                               {'loss': 2.2226, 'grad_norm': 3.939241409301758, 'learning_rate': 1.6821051011896554e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.14it/s]                                               {'loss': 2.1181, 'grad_norm': 3.69014573097229, 'learning_rate': 1.4952045343908048e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.14it/s]                                               {'loss': 1.9211, 'grad_norm': 3.0397496223449707, 'learning_rate': 1.3083039675919542e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.14it/s] 93%|█████████▎| 70/75 [00:02<00:00, 24.03it/s]                                               {'loss': 2.0618, 'grad_norm': 4.368070602416992, 'learning_rate': 1.1214034007931037e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.03it/s]                                               {'loss': 2.1946, 'grad_norm': 3.8504550457000732, 'learning_rate': 9.345028339942529e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.03it/s]                                               {'loss': 2.272, 'grad_norm': 3.8605268001556396, 'learning_rate': 7.476022671954024e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.03it/s] 97%|█████████▋| 73/75 [00:02<00:00, 24.36it/s]                                               {'loss': 2.3067, 'grad_norm': 4.718966484069824, 'learning_rate': 5.607017003965518e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.36it/s]                                               {'loss': 2.0688, 'grad_norm': 3.889498472213745, 'learning_rate': 3.738011335977012e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.36it/s]                                               {'loss': 2.5934, 'grad_norm': 13.6546630859375, 'learning_rate': 1.869005667988506e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.36it/s]                                               {'train_runtime': 3.0665, 'train_samples_per_second': 368.498, 'train_steps_per_second': 24.458, 'train_loss': 2.24956383228302, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.36it/s]100%|██████████| 75/75 [00:03<00:00, 24.46it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4007, 'grad_norm': 4.556586265563965, 'learning_rate': 0.00014017542509913794, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.83it/s]                                              {'loss': 2.3346, 'grad_norm': 4.218209266662598, 'learning_rate': 0.00013830641943114944, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.80it/s]  4%|▍         | 3/75 [00:00<00:03, 23.55it/s]                                              {'loss': 2.2659, 'grad_norm': 3.490145683288574, 'learning_rate': 0.00013643741376316094, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.55it/s]                                              {'loss': 2.2002, 'grad_norm': 4.828983306884766, 'learning_rate': 0.00013456840809517243, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 23.55it/s]                                              {'loss': 2.4813, 'grad_norm': 5.5228095054626465, 'learning_rate': 0.00013269940242718393, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.55it/s]  8%|▊         | 6/75 [00:00<00:02, 23.70it/s]                                              {'loss': 2.1749, 'grad_norm': 3.387354612350464, 'learning_rate': 0.00013083039675919542, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.70it/s]                                              {'loss': 2.2395, 'grad_norm': 4.015957355499268, 'learning_rate': 0.00012896139109120692, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.70it/s]                                              {'loss': 2.1261, 'grad_norm': 3.881917715072632, 'learning_rate': 0.0001270923854232184, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.70it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.51it/s]                                              {'loss': 2.2499, 'grad_norm': 3.861370086669922, 'learning_rate': 0.00012522337975522988, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.51it/s]                                              {'loss': 2.0739, 'grad_norm': 3.8515100479125977, 'learning_rate': 0.00012335437408724138, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.51it/s]                                               {'loss': 2.4483, 'grad_norm': 4.722615718841553, 'learning_rate': 0.00012148536841925289, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.51it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.73it/s]                                               {'loss': 2.2822, 'grad_norm': 3.753977060317993, 'learning_rate': 0.00011961636275126438, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.73it/s]                                               {'loss': 2.2698, 'grad_norm': 4.170660018920898, 'learning_rate': 0.00011774735708327587, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.73it/s]                                               {'loss': 2.2625, 'grad_norm': 3.3568570613861084, 'learning_rate': 0.00011587835141528736, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.73it/s]                                               {'loss': 2.166, 'grad_norm': 6.6722493171691895, 'learning_rate': 0.00011400934574729886, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.73it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.05it/s]                                               {'loss': 2.1922, 'grad_norm': 3.407907724380493, 'learning_rate': 0.00011214034007931035, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.05it/s]                                               {'loss': 2.3737, 'grad_norm': 5.1869330406188965, 'learning_rate': 0.00011027133441132184, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.05it/s]                                               {'loss': 2.3812, 'grad_norm': 3.7047438621520996, 'learning_rate': 0.00010840232874333335, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.05it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.23it/s]                                               {'loss': 2.19, 'grad_norm': 3.0580339431762695, 'learning_rate': 0.00010653332307534484, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.23it/s]                                               {'loss': 1.9959, 'grad_norm': 3.0374066829681396, 'learning_rate': 0.00010466431740735634, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.23it/s]                                               {'loss': 2.2393, 'grad_norm': 3.4028077125549316, 'learning_rate': 0.00010279531173936782, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.23it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.73it/s]                                               {'loss': 2.1451, 'grad_norm': 4.680479526519775, 'learning_rate': 0.00010092630607137932, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.73it/s]                                               {'loss': 1.983, 'grad_norm': 2.7599236965179443, 'learning_rate': 9.905730040339081e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.73it/s]                                               {'loss': 2.2182, 'grad_norm': 4.15800666809082, 'learning_rate': 9.718829473540231e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.73it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.24it/s]                                               {'loss': 2.1544, 'grad_norm': 3.939188003540039, 'learning_rate': 9.53192890674138e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.24it/s]                                               {'loss': 2.2984, 'grad_norm': 2.830000877380371, 'learning_rate': 9.345028339942529e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.3748, 'grad_norm': 4.038539409637451, 'learning_rate': 9.158127773143678e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.24it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.54it/s]                                               {'loss': 2.2535, 'grad_norm': 3.768228054046631, 'learning_rate': 8.971227206344829e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.54it/s]                                               {'loss': 2.2464, 'grad_norm': 4.089636325836182, 'learning_rate': 8.784326639545979e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.54it/s]                                               {'loss': 2.9461, 'grad_norm': 10.23221206665039, 'learning_rate': 8.597426072747127e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.54it/s]                                               {'loss': 2.3208, 'grad_norm': 4.661379814147949, 'learning_rate': 8.410525505948277e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.54it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.45it/s]                                               {'loss': 2.0212, 'grad_norm': 3.7027010917663574, 'learning_rate': 8.223624939149426e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.45it/s]                                               {'loss': 2.0443, 'grad_norm': 3.8604650497436523, 'learning_rate': 8.036724372350576e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.45it/s]                                               {'loss': 2.1995, 'grad_norm': 4.0901079177856445, 'learning_rate': 7.849823805551725e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.45it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.03it/s]                                               {'loss': 2.2649, 'grad_norm': 3.390646457672119, 'learning_rate': 7.662923238752874e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.03it/s]                                               {'loss': 2.295, 'grad_norm': 3.9571878910064697, 'learning_rate': 7.476022671954023e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.03it/s]                                               {'loss': 2.0601, 'grad_norm': 4.036198139190674, 'learning_rate': 7.289122105155173e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.03it/s] 51%|█████     | 38/75 [00:01<00:01, 25.72it/s]                                               {'loss': 2.2701, 'grad_norm': 3.747011184692383, 'learning_rate': 7.102221538356324e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.72it/s]                                               {'loss': 2.1138, 'grad_norm': 3.2350072860717773, 'learning_rate': 6.915320971557472e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.72it/s]                                               {'loss': 2.0363, 'grad_norm': 3.2807226181030273, 'learning_rate': 6.728420404758622e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.72it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.05it/s]                                               {'loss': 2.4183, 'grad_norm': 4.689787864685059, 'learning_rate': 6.541519837959771e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.05it/s]                                               {'loss': 2.4029, 'grad_norm': 3.971010684967041, 'learning_rate': 6.35461927116092e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.05it/s]                                               {'loss': 2.2186, 'grad_norm': 3.343665838241577, 'learning_rate': 6.167718704362069e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.05it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.037, 'grad_norm': 2.81996750831604, 'learning_rate': 5.980818137563219e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.1561, 'grad_norm': 11.411680221557617, 'learning_rate': 5.793917570764368e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.216, 'grad_norm': 3.326526641845703, 'learning_rate': 5.607017003965518e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.1591, 'grad_norm': 2.7417945861816406, 'learning_rate': 5.420116437166667e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.39it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.91it/s]                                               {'loss': 2.0986, 'grad_norm': 3.437058925628662, 'learning_rate': 5.233215870367817e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.91it/s]                                               {'loss': 2.3389, 'grad_norm': 4.408729553222656, 'learning_rate': 5.046315303568966e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.91it/s]                                               {'loss': 2.1258, 'grad_norm': 4.268811225891113, 'learning_rate': 4.8594147367701154e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.91it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.24it/s]                                               {'loss': 1.9949, 'grad_norm': 3.5396487712860107, 'learning_rate': 4.672514169971264e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 26.24it/s]                                               {'loss': 2.256, 'grad_norm': 3.677659273147583, 'learning_rate': 4.4856136031724146e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 26.24it/s]                                               {'loss': 2.3158, 'grad_norm': 4.097659587860107, 'learning_rate': 4.2987130363735635e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 26.24it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.2288, 'grad_norm': 4.594021797180176, 'learning_rate': 4.111812469574713e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.1208, 'grad_norm': 3.4534614086151123, 'learning_rate': 3.924911902775863e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.1167, 'grad_norm': 3.6056878566741943, 'learning_rate': 3.7380113359770116e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.02it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.01it/s]                                               {'loss': 2.1498, 'grad_norm': 3.9975085258483887, 'learning_rate': 3.551110769178162e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.01it/s]                                               {'loss': 2.2192, 'grad_norm': 4.671388626098633, 'learning_rate': 3.364210202379311e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.01it/s]                                               {'loss': 2.1046, 'grad_norm': 4.340256214141846, 'learning_rate': 3.17730963558046e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.01it/s]                                               {'loss': 1.6821, 'grad_norm': 6.525681495666504, 'learning_rate': 2.9904090687816096e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.01it/s] 81%|████████▏ | 61/75 [00:02<00:00, 25.79it/s]                                               {'loss': 2.307, 'grad_norm': 4.043921947479248, 'learning_rate': 2.803508501982759e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.79it/s]                                               {'loss': 2.3527, 'grad_norm': 4.052233695983887, 'learning_rate': 2.6166079351839085e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.79it/s]                                               {'loss': 2.0865, 'grad_norm': 3.3496809005737305, 'learning_rate': 2.4297073683850577e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.79it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.70it/s]                                               {'loss': 2.0103, 'grad_norm': 3.2045931816101074, 'learning_rate': 2.2428068015862073e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.70it/s]                                               {'loss': 2.1866, 'grad_norm': 3.2827630043029785, 'learning_rate': 2.0559062347873566e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.70it/s]                                               {'loss': 2.1472, 'grad_norm': 4.193002223968506, 'learning_rate': 1.8690056679885058e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.70it/s] 89%|████████▉ | 67/75 [00:02<00:00, 25.61it/s]                                               {'loss': 2.1281, 'grad_norm': 3.0198209285736084, 'learning_rate': 1.6821051011896554e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.61it/s]                                               {'loss': 2.1824, 'grad_norm': 3.5924031734466553, 'learning_rate': 1.4952045343908048e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.61it/s]                                               {'loss': 2.2879, 'grad_norm': 4.1782612800598145, 'learning_rate': 1.3083039675919542e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.61it/s] 93%|█████████▎| 70/75 [00:02<00:00, 24.88it/s]                                               {'loss': 2.2492, 'grad_norm': 4.0323357582092285, 'learning_rate': 1.1214034007931037e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.88it/s]                                               {'loss': 2.0344, 'grad_norm': 3.948467969894409, 'learning_rate': 9.345028339942529e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.88it/s]                                               {'loss': 2.0812, 'grad_norm': 3.9320755004882812, 'learning_rate': 7.476022671954024e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.88it/s] 97%|█████████▋| 73/75 [00:02<00:00, 24.51it/s]                                               {'loss': 2.1044, 'grad_norm': 3.541008710861206, 'learning_rate': 5.607017003965518e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.51it/s]                                               {'loss': 2.0964, 'grad_norm': 3.1633975505828857, 'learning_rate': 3.738011335977012e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.51it/s]                                               {'loss': 1.8265, 'grad_norm': 10.742569923400879, 'learning_rate': 1.869005667988506e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.51it/s]                                               {'train_runtime': 3.0447, 'train_samples_per_second': 371.135, 'train_steps_per_second': 24.633, 'train_loss': 2.200463228225708, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.51it/s]100%|██████████| 75/75 [00:03<00:00, 24.63it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.526, 'grad_norm': 3.975487232208252, 'learning_rate': 0.00014017542509913794, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 15.97it/s]  3%|▎         | 2/75 [00:00<00:03, 19.44it/s]                                              {'loss': 2.3596, 'grad_norm': 3.9908852577209473, 'learning_rate': 0.00013830641943114944, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 19.44it/s]                                              {'loss': 2.3434, 'grad_norm': 3.983555793762207, 'learning_rate': 0.00013643741376316094, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 19.44it/s]                                              {'loss': 2.2454, 'grad_norm': 4.0223283767700195, 'learning_rate': 0.00013456840809517243, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 19.44it/s]  7%|▋         | 5/75 [00:00<00:03, 21.61it/s]                                              {'loss': 2.1346, 'grad_norm': 4.556228160858154, 'learning_rate': 0.00013269940242718393, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 21.61it/s]                                              {'loss': 2.2861, 'grad_norm': 3.6144230365753174, 'learning_rate': 0.00013083039675919542, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 21.61it/s]                                              {'loss': 2.229, 'grad_norm': 3.536447763442993, 'learning_rate': 0.00012896139109120692, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 21.61it/s] 11%|█         | 8/75 [00:00<00:02, 22.91it/s]                                              {'loss': 2.373, 'grad_norm': 4.617103576660156, 'learning_rate': 0.0001270923854232184, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 22.91it/s]                                              {'loss': 2.3579, 'grad_norm': 4.4640631675720215, 'learning_rate': 0.00012522337975522988, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 22.91it/s]                                              {'loss': 2.2727, 'grad_norm': 4.490762710571289, 'learning_rate': 0.00012335437408724138, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 22.91it/s] 15%|█▍        | 11/75 [00:00<00:02, 23.67it/s]                                               {'loss': 2.503, 'grad_norm': 4.332307815551758, 'learning_rate': 0.00012148536841925289, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.67it/s]                                               {'loss': 2.3665, 'grad_norm': 4.209637641906738, 'learning_rate': 0.00011961636275126438, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.67it/s]                                               {'loss': 2.5373, 'grad_norm': 5.275619983673096, 'learning_rate': 0.00011774735708327587, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.67it/s] 19%|█▊        | 14/75 [00:00<00:02, 24.74it/s]                                               {'loss': 2.2099, 'grad_norm': 3.489393472671509, 'learning_rate': 0.00011587835141528736, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.74it/s]                                               {'loss': 1.9043, 'grad_norm': 10.77891731262207, 'learning_rate': 0.00011400934574729886, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.74it/s]                                               {'loss': 2.3553, 'grad_norm': 4.3090620040893555, 'learning_rate': 0.00011214034007931035, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 24.74it/s]                                               {'loss': 2.0707, 'grad_norm': 3.3363683223724365, 'learning_rate': 0.00011027133441132184, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.74it/s] 24%|██▍       | 18/75 [00:00<00:02, 26.63it/s]                                               {'loss': 2.2853, 'grad_norm': 5.3727030754089355, 'learning_rate': 0.00010840232874333335, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.63it/s]                                               {'loss': 2.2114, 'grad_norm': 3.79803204536438, 'learning_rate': 0.00010653332307534484, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.63it/s]                                               {'loss': 2.2862, 'grad_norm': 5.620054721832275, 'learning_rate': 0.00010466431740735634, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.63it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.67it/s]                                               {'loss': 2.3704, 'grad_norm': 3.6238930225372314, 'learning_rate': 0.00010279531173936782, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.67it/s]                                               {'loss': 2.5159, 'grad_norm': 3.6461262702941895, 'learning_rate': 0.00010092630607137932, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.67it/s]                                               {'loss': 2.0954, 'grad_norm': 3.567535161972046, 'learning_rate': 9.905730040339081e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.67it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.73it/s]                                               {'loss': 2.2661, 'grad_norm': 4.395792484283447, 'learning_rate': 9.718829473540231e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.73it/s]                                               {'loss': 2.3227, 'grad_norm': 4.323441505432129, 'learning_rate': 9.53192890674138e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.73it/s]                                               {'loss': 2.2213, 'grad_norm': 3.7209651470184326, 'learning_rate': 9.345028339942529e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.73it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.91it/s]                                               {'loss': 2.2133, 'grad_norm': 3.5996148586273193, 'learning_rate': 9.158127773143678e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.91it/s]                                               {'loss': 2.275, 'grad_norm': 3.682556629180908, 'learning_rate': 8.971227206344829e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.91it/s]                                               {'loss': 2.1359, 'grad_norm': 3.868206262588501, 'learning_rate': 8.784326639545979e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.91it/s]                                               {'loss': 1.9545, 'grad_norm': 8.47535228729248, 'learning_rate': 8.597426072747127e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.91it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.58it/s]                                               {'loss': 2.2, 'grad_norm': 4.312448978424072, 'learning_rate': 8.410525505948277e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.58it/s]                                               {'loss': 2.2829, 'grad_norm': 3.597504138946533, 'learning_rate': 8.223624939149426e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.58it/s]                                               {'loss': 2.1959, 'grad_norm': 3.539186477661133, 'learning_rate': 8.036724372350576e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.58it/s] 45%|████▌     | 34/75 [00:01<00:01, 26.40it/s]                                               {'loss': 2.0959, 'grad_norm': 3.9086005687713623, 'learning_rate': 7.849823805551725e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.40it/s]                                               {'loss': 2.4206, 'grad_norm': 3.6949257850646973, 'learning_rate': 7.662923238752874e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.40it/s]                                               {'loss': 2.2382, 'grad_norm': 3.3369076251983643, 'learning_rate': 7.476022671954023e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.40it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.36it/s]                                               {'loss': 2.3169, 'grad_norm': 3.862313747406006, 'learning_rate': 7.289122105155173e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.36it/s]                                               {'loss': 2.2849, 'grad_norm': 4.044930934906006, 'learning_rate': 7.102221538356324e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.36it/s]                                               {'loss': 2.2925, 'grad_norm': 3.8142309188842773, 'learning_rate': 6.915320971557472e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.36it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.23it/s]                                               {'loss': 2.1454, 'grad_norm': 3.7929792404174805, 'learning_rate': 6.728420404758622e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.23it/s]                                               {'loss': 2.4463, 'grad_norm': 4.3025360107421875, 'learning_rate': 6.541519837959771e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.23it/s]                                               {'loss': 2.0402, 'grad_norm': 3.9417781829833984, 'learning_rate': 6.35461927116092e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.23it/s] 57%|█████▋    | 43/75 [00:01<00:01, 25.49it/s]                                               {'loss': 2.0879, 'grad_norm': 4.273818016052246, 'learning_rate': 6.167718704362069e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.49it/s]                                               {'loss': 2.1578, 'grad_norm': 4.044132709503174, 'learning_rate': 5.980818137563219e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.49it/s]                                               {'loss': 2.5165, 'grad_norm': 24.255754470825195, 'learning_rate': 5.793917570764368e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.49it/s] 61%|██████▏   | 46/75 [00:01<00:01, 26.06it/s]                                               {'loss': 2.3773, 'grad_norm': 3.2473068237304688, 'learning_rate': 5.607017003965518e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.06it/s]                                               {'loss': 2.0796, 'grad_norm': 3.9993481636047363, 'learning_rate': 5.420116437166667e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.06it/s]                                               {'loss': 2.1853, 'grad_norm': 2.981095314025879, 'learning_rate': 5.233215870367817e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.06it/s] 65%|██████▌   | 49/75 [00:01<00:01, 25.66it/s]                                               {'loss': 2.4379, 'grad_norm': 4.356680393218994, 'learning_rate': 5.046315303568966e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.66it/s]                                               {'loss': 2.3255, 'grad_norm': 3.9379076957702637, 'learning_rate': 4.8594147367701154e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.66it/s]                                               {'loss': 2.2068, 'grad_norm': 3.9461114406585693, 'learning_rate': 4.672514169971264e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.66it/s] 69%|██████▉   | 52/75 [00:02<00:00, 25.58it/s]                                               {'loss': 2.027, 'grad_norm': 4.212587833404541, 'learning_rate': 4.4856136031724146e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.58it/s]                                               {'loss': 2.1357, 'grad_norm': 4.048077583312988, 'learning_rate': 4.2987130363735635e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.58it/s]                                               {'loss': 2.1642, 'grad_norm': 4.377682685852051, 'learning_rate': 4.111812469574713e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.58it/s] 73%|███████▎  | 55/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.2298, 'grad_norm': 5.379683971405029, 'learning_rate': 3.924911902775863e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.137, 'grad_norm': 4.25726842880249, 'learning_rate': 3.7380113359770116e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.2139, 'grad_norm': 5.091460704803467, 'learning_rate': 3.551110769178162e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.63it/s] 77%|███████▋  | 58/75 [00:02<00:00, 25.68it/s]                                               {'loss': 2.281, 'grad_norm': 3.8262758255004883, 'learning_rate': 3.364210202379311e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.68it/s]                                               {'loss': 2.2666, 'grad_norm': 4.093305587768555, 'learning_rate': 3.17730963558046e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.68it/s]                                               {'loss': 2.3795, 'grad_norm': 14.062514305114746, 'learning_rate': 2.9904090687816096e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.68it/s] 81%|████████▏ | 61/75 [00:02<00:00, 25.79it/s]                                               {'loss': 2.0588, 'grad_norm': 3.1651408672332764, 'learning_rate': 2.803508501982759e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.79it/s]                                               {'loss': 2.4187, 'grad_norm': 5.771020412445068, 'learning_rate': 2.6166079351839085e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.79it/s]                                               {'loss': 2.1856, 'grad_norm': 3.147536039352417, 'learning_rate': 2.4297073683850577e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.79it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.04it/s]                                               {'loss': 2.2034, 'grad_norm': 3.354757070541382, 'learning_rate': 2.2428068015862073e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.04it/s]                                               {'loss': 2.1695, 'grad_norm': 3.9687323570251465, 'learning_rate': 2.0559062347873566e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.04it/s]                                               {'loss': 2.1939, 'grad_norm': 4.401284694671631, 'learning_rate': 1.8690056679885058e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.04it/s] 89%|████████▉ | 67/75 [00:02<00:00, 24.87it/s]                                               {'loss': 2.1866, 'grad_norm': 2.8693554401397705, 'learning_rate': 1.6821051011896554e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.87it/s]                                               {'loss': 2.1935, 'grad_norm': 3.8853025436401367, 'learning_rate': 1.4952045343908048e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.87it/s]                                               {'loss': 2.2968, 'grad_norm': 3.6066501140594482, 'learning_rate': 1.3083039675919542e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.87it/s] 93%|█████████▎| 70/75 [00:02<00:00, 24.92it/s]                                               {'loss': 2.3594, 'grad_norm': 4.485109329223633, 'learning_rate': 1.1214034007931037e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.92it/s]                                               {'loss': 2.0677, 'grad_norm': 4.299304962158203, 'learning_rate': 9.345028339942529e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.92it/s]                                               {'loss': 2.3065, 'grad_norm': 3.7926700115203857, 'learning_rate': 7.476022671954024e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.92it/s] 97%|█████████▋| 73/75 [00:02<00:00, 25.13it/s]                                               {'loss': 2.1085, 'grad_norm': 5.7431321144104, 'learning_rate': 5.607017003965518e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.13it/s]                                               {'loss': 2.1231, 'grad_norm': 3.1432011127471924, 'learning_rate': 3.738011335977012e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.13it/s]                                               {'loss': 2.4259, 'grad_norm': 15.68238353729248, 'learning_rate': 1.869005667988506e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.13it/s]                                               {'train_runtime': 3.0914, 'train_samples_per_second': 365.532, 'train_steps_per_second': 24.261, 'train_loss': 2.2492641035715737, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.13it/s]100%|██████████| 75/75 [00:03<00:00, 24.26it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1043, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(915, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(767, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(924, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1123, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(911, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1037, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(975, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(998, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(696, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1202, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(608, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 63.66it/s]  3%|▎         | 14/471 [00:00<00:08, 56.41it/s]  4%|▍         | 20/471 [00:00<00:09, 48.95it/s]  6%|▌         | 26/471 [00:00<00:08, 50.86it/s]  7%|▋         | 32/471 [00:00<00:08, 51.12it/s]  8%|▊         | 38/471 [00:00<00:08, 50.66it/s]  9%|▉         | 44/471 [00:00<00:08, 50.68it/s] 11%|█         | 50/471 [00:01<00:09, 43.83it/s] 12%|█▏        | 56/471 [00:01<00:08, 46.96it/s] 13%|█▎        | 62/471 [00:01<00:08, 48.22it/s] 14%|█▍        | 68/471 [00:01<00:08, 49.51it/s] 16%|█▌        | 74/471 [00:01<00:07, 50.02it/s] 17%|█▋        | 80/471 [00:01<00:07, 51.12it/s] 18%|█▊        | 86/471 [00:01<00:07, 51.03it/s] 20%|█▉        | 92/471 [00:01<00:07, 51.33it/s] 21%|██        | 98/471 [00:01<00:07, 51.66it/s] 22%|██▏       | 104/471 [00:02<00:07, 51.36it/s] 23%|██▎       | 110/471 [00:02<00:06, 52.02it/s] 25%|██▍       | 116/471 [00:02<00:06, 51.76it/s] 26%|██▌       | 122/471 [00:02<00:06, 51.91it/s] 27%|██▋       | 128/471 [00:02<00:06, 50.73it/s] 28%|██▊       | 134/471 [00:02<00:06, 51.21it/s] 30%|██▉       | 140/471 [00:02<00:06, 51.73it/s] 31%|███       | 146/471 [00:02<00:06, 51.81it/s] 32%|███▏      | 152/471 [00:02<00:06, 51.06it/s] 34%|███▎      | 158/471 [00:03<00:06, 49.69it/s] 35%|███▍      | 164/471 [00:03<00:06, 51.08it/s] 36%|███▌      | 170/471 [00:03<00:05, 51.20it/s] 37%|███▋      | 176/471 [00:03<00:05, 51.51it/s] 39%|███▊      | 182/471 [00:03<00:05, 51.95it/s] 40%|███▉      | 188/471 [00:03<00:05, 52.01it/s] 41%|████      | 194/471 [00:03<00:05, 52.07it/s] 42%|████▏     | 200/471 [00:03<00:05, 52.23it/s] 44%|████▎     | 206/471 [00:04<00:05, 52.42it/s] 45%|████▌     | 212/471 [00:04<00:04, 52.01it/s] 46%|████▋     | 218/471 [00:04<00:04, 52.07it/s] 48%|████▊     | 224/471 [00:04<00:04, 52.16it/s] 49%|████▉     | 230/471 [00:04<00:04, 52.18it/s] 50%|█████     | 236/471 [00:04<00:04, 51.56it/s] 51%|█████▏    | 242/471 [00:04<00:04, 51.78it/s] 53%|█████▎    | 248/471 [00:04<00:04, 51.60it/s] 54%|█████▍    | 254/471 [00:04<00:04, 51.75it/s] 55%|█████▌    | 260/471 [00:05<00:04, 52.09it/s] 56%|█████▋    | 266/471 [00:05<00:04, 51.10it/s] 58%|█████▊    | 272/471 [00:05<00:03, 51.45it/s] 59%|█████▉    | 278/471 [00:05<00:03, 51.58it/s] 60%|██████    | 284/471 [00:05<00:03, 51.67it/s] 62%|██████▏   | 290/471 [00:05<00:03, 51.80it/s] 63%|██████▎   | 296/471 [00:05<00:03, 52.14it/s] 64%|██████▍   | 302/471 [00:05<00:03, 52.11it/s] 65%|██████▌   | 308/471 [00:06<00:03, 51.59it/s] 67%|██████▋   | 314/471 [00:06<00:03, 51.83it/s] 68%|██████▊   | 320/471 [00:06<00:02, 51.35it/s] 69%|██████▉   | 326/471 [00:06<00:02, 48.73it/s] 70%|███████   | 331/471 [00:06<00:02, 48.14it/s] 72%|███████▏  | 337/471 [00:06<00:02, 49.80it/s] 73%|███████▎  | 343/471 [00:06<00:02, 50.12it/s] 74%|███████▍  | 349/471 [00:06<00:02, 50.17it/s] 75%|███████▌  | 355/471 [00:06<00:02, 50.49it/s] 77%|███████▋  | 361/471 [00:07<00:02, 48.29it/s] 78%|███████▊  | 367/471 [00:07<00:02, 49.53it/s] 79%|███████▉  | 373/471 [00:07<00:01, 50.48it/s] 80%|████████  | 379/471 [00:07<00:01, 50.36it/s] 82%|████████▏ | 385/471 [00:07<00:01, 50.71it/s] 83%|████████▎ | 391/471 [00:07<00:01, 50.43it/s] 84%|████████▍ | 397/471 [00:07<00:01, 51.10it/s] 86%|████████▌ | 403/471 [00:07<00:01, 51.34it/s] 87%|████████▋ | 409/471 [00:08<00:01, 51.50it/s] 88%|████████▊ | 415/471 [00:08<00:01, 51.16it/s] 89%|████████▉ | 421/471 [00:08<00:00, 50.51it/s] 91%|█████████ | 427/471 [00:08<00:00, 51.37it/s] 92%|█████████▏| 433/471 [00:08<00:00, 51.62it/s] 93%|█████████▎| 439/471 [00:08<00:00, 51.75it/s] 94%|█████████▍| 445/471 [00:08<00:00, 52.08it/s] 96%|█████████▌| 451/471 [00:08<00:00, 51.97it/s] 97%|█████████▋| 457/471 [00:08<00:00, 51.97it/s] 98%|█████████▊| 463/471 [00:09<00:00, 51.34it/s]100%|█████████▉| 469/471 [00:09<00:00, 51.56it/s]100%|██████████| 471/471 [00:09<00:00, 51.09it/s]
{'eval_loss': 2.288466691970825, 'eval_model_preparation_time': 0.0028, 'eval_acc': 0.3911311736590547, 'eval_runtime': 9.2398, 'eval_samples_per_second': 815.167, 'eval_steps_per_second': 50.975}
ROUND:14
CLIENT:43
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3814, 'grad_norm': 3.9597322940826416, 'learning_rate': 0.00013741196746494238, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 22.69it/s]                                              {'loss': 2.2148, 'grad_norm': 3.4803638458251953, 'learning_rate': 0.00013557980789874314, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.72it/s]  4%|▍         | 3/75 [00:00<00:03, 23.67it/s]                                              {'loss': 2.3033, 'grad_norm': 3.342503309249878, 'learning_rate': 0.0001337476483325439, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.67it/s]                                              {'loss': 2.2271, 'grad_norm': 3.675048828125, 'learning_rate': 0.00013191548876634467, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 23.67it/s]                                              {'loss': 2.2641, 'grad_norm': 4.254847049713135, 'learning_rate': 0.00013008332920014544, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.67it/s]  8%|▊         | 6/75 [00:00<00:02, 23.82it/s]                                              {'loss': 2.0598, 'grad_norm': 3.903428792953491, 'learning_rate': 0.00012825116963394623, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.82it/s]                                              {'loss': 2.4056, 'grad_norm': 3.4887821674346924, 'learning_rate': 0.000126419010067747, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.82it/s]                                              {'loss': 2.331, 'grad_norm': 4.103053092956543, 'learning_rate': 0.00012458685050154776, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.82it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.31it/s]                                              {'loss': 2.2947, 'grad_norm': 4.353551387786865, 'learning_rate': 0.00012275469093534852, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.31it/s]                                              {'loss': 2.4806, 'grad_norm': 4.05628776550293, 'learning_rate': 0.00012092253136914929, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.31it/s]                                               {'loss': 2.1041, 'grad_norm': 3.9882569313049316, 'learning_rate': 0.00011909037180295007, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.31it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.45it/s]                                               {'loss': 2.4269, 'grad_norm': 4.329051971435547, 'learning_rate': 0.00011725821223675083, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.45it/s]                                               {'loss': 2.2614, 'grad_norm': 4.2587890625, 'learning_rate': 0.0001154260526705516, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.45it/s]                                               {'loss': 2.5521, 'grad_norm': 7.043001651763916, 'learning_rate': 0.00011359389310435236, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.45it/s]                                               {'loss': 2.1341, 'grad_norm': 15.333169937133789, 'learning_rate': 0.00011176173353815314, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.45it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.68it/s]                                               {'loss': 2.1288, 'grad_norm': 4.226130962371826, 'learning_rate': 0.0001099295739719539, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.68it/s]                                               {'loss': 2.3824, 'grad_norm': 4.272648334503174, 'learning_rate': 0.00010809741440575466, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.68it/s]                                               {'loss': 2.4286, 'grad_norm': 4.538631916046143, 'learning_rate': 0.00010626525483955544, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.68it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.19it/s]                                               {'loss': 2.3064, 'grad_norm': 4.306995868682861, 'learning_rate': 0.0001044330952733562, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.19it/s]                                               {'loss': 2.2478, 'grad_norm': 3.733840227127075, 'learning_rate': 0.00010260093570715698, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.19it/s]                                               {'loss': 2.499, 'grad_norm': 4.908270359039307, 'learning_rate': 0.00010076877614095773, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.19it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.88it/s]                                               {'loss': 2.2999, 'grad_norm': 4.251938343048096, 'learning_rate': 9.893661657475851e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.88it/s]                                               {'loss': 2.2111, 'grad_norm': 3.10219144821167, 'learning_rate': 9.710445700855928e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.88it/s]                                               {'loss': 2.2312, 'grad_norm': 2.9880897998809814, 'learning_rate': 9.527229744236005e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.88it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.17it/s]                                               {'loss': 1.9311, 'grad_norm': 3.4882283210754395, 'learning_rate': 9.344013787616082e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.17it/s]                                               {'loss': 2.1303, 'grad_norm': 3.280277967453003, 'learning_rate': 9.160797830996158e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.17it/s]                                               {'loss': 2.2883, 'grad_norm': 3.412255048751831, 'learning_rate': 8.977581874376235e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.17it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.99it/s]                                               {'loss': 2.0265, 'grad_norm': 2.983078718185425, 'learning_rate': 8.794365917756313e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.99it/s]                                               {'loss': 2.1925, 'grad_norm': 3.3862218856811523, 'learning_rate': 8.611149961136389e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.99it/s]                                               {'loss': 2.2281, 'grad_norm': 20.456737518310547, 'learning_rate': 8.427934004516464e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.99it/s] 41%|████▏     | 31/75 [00:01<00:01, 25.88it/s]                                               {'loss': 2.1344, 'grad_norm': 3.0485892295837402, 'learning_rate': 8.244718047896542e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.88it/s]                                               {'loss': 2.3923, 'grad_norm': 3.726820468902588, 'learning_rate': 8.061502091276619e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.88it/s]                                               {'loss': 2.3519, 'grad_norm': 5.241461277008057, 'learning_rate': 7.878286134656697e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.88it/s] 45%|████▌     | 34/75 [00:01<00:01, 25.87it/s]                                               {'loss': 2.0819, 'grad_norm': 3.424575090408325, 'learning_rate': 7.695070178036773e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.87it/s]                                               {'loss': 2.1466, 'grad_norm': 3.42960786819458, 'learning_rate': 7.51185422141685e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.87it/s]                                               {'loss': 2.4222, 'grad_norm': 3.9314181804656982, 'learning_rate': 7.328638264796926e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.87it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.74it/s]                                               {'loss': 2.203, 'grad_norm': 3.997783660888672, 'learning_rate': 7.145422308177004e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.74it/s]                                               {'loss': 2.1952, 'grad_norm': 4.405157566070557, 'learning_rate': 6.96220635155708e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.74it/s]                                               {'loss': 2.2157, 'grad_norm': 4.256838798522949, 'learning_rate': 6.778990394937157e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.74it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.06it/s]                                               {'loss': 2.1606, 'grad_norm': 3.216600179672241, 'learning_rate': 6.595774438317234e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.06it/s]                                               {'loss': 2.4006, 'grad_norm': 4.0991315841674805, 'learning_rate': 6.412558481697311e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.06it/s]                                               {'loss': 2.3329, 'grad_norm': 5.191373825073242, 'learning_rate': 6.229342525077388e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.06it/s] 57%|█████▋    | 43/75 [00:01<00:01, 24.86it/s]                                               {'loss': 2.1578, 'grad_norm': 3.781480312347412, 'learning_rate': 6.0461265684574645e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.86it/s]                                               {'loss': 2.1688, 'grad_norm': 3.346522808074951, 'learning_rate': 5.8629106118375416e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.86it/s]                                               {'loss': 1.9752, 'grad_norm': 9.058279991149902, 'learning_rate': 5.679694655217618e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.86it/s]                                               {'loss': 2.2253, 'grad_norm': 3.8441009521484375, 'learning_rate': 5.496478698597695e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.86it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.72it/s]                                               {'loss': 2.1457, 'grad_norm': 2.996246576309204, 'learning_rate': 5.313262741977772e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.72it/s]                                               {'loss': 2.1, 'grad_norm': 3.6748464107513428, 'learning_rate': 5.130046785357849e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.72it/s]                                               {'loss': 2.2291, 'grad_norm': 4.1848859786987305, 'learning_rate': 4.9468308287379255e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.72it/s] 67%|██████▋   | 50/75 [00:01<00:00, 26.22it/s]                                               {'loss': 2.1421, 'grad_norm': 4.000412940979004, 'learning_rate': 4.763614872118003e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.22it/s]                                               {'loss': 2.3006, 'grad_norm': 3.394752264022827, 'learning_rate': 4.580398915498079e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 26.22it/s]                                               {'loss': 2.0526, 'grad_norm': 2.8092052936553955, 'learning_rate': 4.3971829588781564e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 26.22it/s] 71%|███████   | 53/75 [00:02<00:00, 25.96it/s]                                               {'loss': 2.3334, 'grad_norm': 3.4831008911132812, 'learning_rate': 4.213967002258232e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.96it/s]                                               {'loss': 2.1099, 'grad_norm': 4.631913185119629, 'learning_rate': 4.0307510456383094e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.96it/s]                                               {'loss': 2.2005, 'grad_norm': 3.3068010807037354, 'learning_rate': 3.8475350890183866e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.96it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.65it/s]                                               {'loss': 2.2258, 'grad_norm': 4.244145393371582, 'learning_rate': 3.664319132398463e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.65it/s]                                               {'loss': 2.1152, 'grad_norm': 3.5026042461395264, 'learning_rate': 3.48110317577854e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.65it/s]                                               {'loss': 2.2797, 'grad_norm': 3.2837045192718506, 'learning_rate': 3.297887219158617e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.65it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.4141, 'grad_norm': 5.343127250671387, 'learning_rate': 3.114671262538694e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.4805, 'grad_norm': 11.617280960083008, 'learning_rate': 2.9314553059187708e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.3006, 'grad_norm': 4.2115254402160645, 'learning_rate': 2.7482393492988477e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.26it/s] 83%|████████▎ | 62/75 [00:02<00:00, 25.40it/s]                                               {'loss': 2.1044, 'grad_norm': 3.790339946746826, 'learning_rate': 2.5650233926789245e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.40it/s]                                               {'loss': 2.572, 'grad_norm': 4.318631172180176, 'learning_rate': 2.3818074360590014e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.40it/s]                                               {'loss': 2.3321, 'grad_norm': 3.7503933906555176, 'learning_rate': 2.1985914794390782e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.40it/s] 87%|████████▋ | 65/75 [00:02<00:00, 25.50it/s]                                               {'loss': 2.2652, 'grad_norm': 4.525774955749512, 'learning_rate': 2.0153755228191547e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.50it/s]                                               {'loss': 2.0466, 'grad_norm': 6.0611042976379395, 'learning_rate': 1.8321595661992315e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.50it/s]                                               {'loss': 1.9408, 'grad_norm': 2.831317901611328, 'learning_rate': 1.6489436095793084e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.50it/s] 91%|█████████ | 68/75 [00:02<00:00, 24.88it/s]                                               {'loss': 2.1314, 'grad_norm': 3.663360118865967, 'learning_rate': 1.4657276529593854e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.88it/s]                                               {'loss': 2.1249, 'grad_norm': 4.013842582702637, 'learning_rate': 1.2825116963394623e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.88it/s]                                               {'loss': 2.1331, 'grad_norm': 3.903272867202759, 'learning_rate': 1.0992957397195391e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.88it/s] 95%|█████████▍| 71/75 [00:02<00:00, 24.92it/s]                                               {'loss': 2.2023, 'grad_norm': 3.915543556213379, 'learning_rate': 9.160797830996158e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.92it/s]                                               {'loss': 2.2515, 'grad_norm': 3.6187257766723633, 'learning_rate': 7.328638264796927e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.92it/s]                                               {'loss': 2.1439, 'grad_norm': 3.606963634490967, 'learning_rate': 5.4964786985976955e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.92it/s] 99%|█████████▊| 74/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.2566, 'grad_norm': 3.813326120376587, 'learning_rate': 3.6643191323984635e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.8313, 'grad_norm': 13.818893432617188, 'learning_rate': 1.8321595661992318e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.83it/s]                                               {'train_runtime': 3.0514, 'train_samples_per_second': 370.317, 'train_steps_per_second': 24.579, 'train_loss': 2.2440413808822632, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.83it/s]100%|██████████| 75/75 [00:03<00:00, 24.58it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:18
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2688, 'grad_norm': 4.6875457763671875, 'learning_rate': 0.00013741196746494238, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.65it/s]                                              {'loss': 2.4647, 'grad_norm': 3.8588101863861084, 'learning_rate': 0.00013557980789874314, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.94it/s]  4%|▍         | 3/75 [00:00<00:02, 25.63it/s]                                              {'loss': 2.38, 'grad_norm': 6.2964959144592285, 'learning_rate': 0.0001337476483325439, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.63it/s]                                              {'loss': 2.2148, 'grad_norm': 3.804708480834961, 'learning_rate': 0.00013191548876634467, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.63it/s]                                              {'loss': 2.4083, 'grad_norm': 4.313161849975586, 'learning_rate': 0.00013008332920014544, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.63it/s]  8%|▊         | 6/75 [00:00<00:02, 23.94it/s]                                              {'loss': 2.3482, 'grad_norm': 3.419799566268921, 'learning_rate': 0.00012825116963394623, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.94it/s]                                              {'loss': 2.3168, 'grad_norm': 3.3252692222595215, 'learning_rate': 0.000126419010067747, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.94it/s]                                              {'loss': 2.288, 'grad_norm': 4.113749980926514, 'learning_rate': 0.00012458685050154776, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.94it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.58it/s]                                              {'loss': 2.1831, 'grad_norm': 2.6950089931488037, 'learning_rate': 0.00012275469093534852, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.58it/s]                                              {'loss': 2.2539, 'grad_norm': 5.043186187744141, 'learning_rate': 0.00012092253136914929, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.58it/s]                                               {'loss': 2.2782, 'grad_norm': 3.6340506076812744, 'learning_rate': 0.00011909037180295007, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.58it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.16it/s]                                               {'loss': 2.2615, 'grad_norm': 3.8990840911865234, 'learning_rate': 0.00011725821223675083, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.16it/s]                                               {'loss': 2.3635, 'grad_norm': 4.4882283210754395, 'learning_rate': 0.0001154260526705516, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.16it/s]                                               {'loss': 2.2823, 'grad_norm': 4.684997081756592, 'learning_rate': 0.00011359389310435236, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.16it/s]                                               {'loss': 2.5611, 'grad_norm': 10.399307250976562, 'learning_rate': 0.00011176173353815314, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.16it/s] 21%|██▏       | 16/75 [00:00<00:02, 25.53it/s]                                               {'loss': 2.2432, 'grad_norm': 3.916249990463257, 'learning_rate': 0.0001099295739719539, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.53it/s]                                               {'loss': 2.1689, 'grad_norm': 3.7068533897399902, 'learning_rate': 0.00010809741440575466, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.53it/s]                                               {'loss': 2.3193, 'grad_norm': 4.627822399139404, 'learning_rate': 0.00010626525483955544, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.53it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.32it/s]                                               {'loss': 2.3268, 'grad_norm': 3.1657557487487793, 'learning_rate': 0.0001044330952733562, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.32it/s]                                               {'loss': 2.3587, 'grad_norm': 4.381780624389648, 'learning_rate': 0.00010260093570715698, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.32it/s]                                               {'loss': 2.0982, 'grad_norm': 3.527519941329956, 'learning_rate': 0.00010076877614095773, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.32it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.57it/s]                                               {'loss': 2.2237, 'grad_norm': 3.9894394874572754, 'learning_rate': 9.893661657475851e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.57it/s]                                               {'loss': 2.1772, 'grad_norm': 3.6805529594421387, 'learning_rate': 9.710445700855928e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.57it/s]                                               {'loss': 2.1563, 'grad_norm': 4.13361120223999, 'learning_rate': 9.527229744236005e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.57it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.70it/s]                                               {'loss': 2.1965, 'grad_norm': 3.3322296142578125, 'learning_rate': 9.344013787616082e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.70it/s]                                               {'loss': 2.3269, 'grad_norm': 3.7846901416778564, 'learning_rate': 9.160797830996158e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.70it/s]                                               {'loss': 2.4592, 'grad_norm': 3.963107109069824, 'learning_rate': 8.977581874376235e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.70it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.3189, 'grad_norm': 4.5251288414001465, 'learning_rate': 8.794365917756313e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.3533, 'grad_norm': 4.345881462097168, 'learning_rate': 8.611149961136389e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.5578, 'grad_norm': 11.946943283081055, 'learning_rate': 8.427934004516464e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.2472, 'grad_norm': 3.6874704360961914, 'learning_rate': 8.244718047896542e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.37it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.93it/s]                                               {'loss': 2.2515, 'grad_norm': 3.554619073867798, 'learning_rate': 8.061502091276619e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.93it/s]                                               {'loss': 2.3478, 'grad_norm': 3.627627372741699, 'learning_rate': 7.878286134656697e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.93it/s]                                               {'loss': 2.12, 'grad_norm': 3.530886650085449, 'learning_rate': 7.695070178036773e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.93it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.35it/s]                                               {'loss': 2.3342, 'grad_norm': 3.4801487922668457, 'learning_rate': 7.51185422141685e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.35it/s]                                               {'loss': 2.2721, 'grad_norm': 4.160407543182373, 'learning_rate': 7.328638264796926e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.35it/s]                                               {'loss': 2.0408, 'grad_norm': 3.1418657302856445, 'learning_rate': 7.145422308177004e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.35it/s] 51%|█████     | 38/75 [00:01<00:01, 20.57it/s]                                               {'loss': 2.1061, 'grad_norm': 4.094965934753418, 'learning_rate': 6.96220635155708e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 20.57it/s]                                               {'loss': 2.2552, 'grad_norm': 3.385828733444214, 'learning_rate': 6.778990394937157e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 20.57it/s]                                               {'loss': 2.1866, 'grad_norm': 4.149171352386475, 'learning_rate': 6.595774438317234e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 20.57it/s] 55%|█████▍    | 41/75 [00:01<00:01, 20.51it/s]                                               {'loss': 2.2466, 'grad_norm': 4.134652137756348, 'learning_rate': 6.412558481697311e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 20.51it/s]                                               {'loss': 2.3318, 'grad_norm': 3.7641515731811523, 'learning_rate': 6.229342525077388e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 20.51it/s]                                               {'loss': 2.328, 'grad_norm': 3.0397489070892334, 'learning_rate': 6.0461265684574645e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 20.51it/s] 59%|█████▊    | 44/75 [00:01<00:01, 21.88it/s]                                               {'loss': 2.159, 'grad_norm': 4.086765289306641, 'learning_rate': 5.8629106118375416e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 21.88it/s]                                               {'loss': 2.3205, 'grad_norm': 11.024700164794922, 'learning_rate': 5.679694655217618e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 21.88it/s]                                               {'loss': 2.0276, 'grad_norm': 3.8920860290527344, 'learning_rate': 5.496478698597695e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 21.88it/s]                                               {'loss': 2.0689, 'grad_norm': 2.933091878890991, 'learning_rate': 5.313262741977772e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 21.88it/s] 64%|██████▍   | 48/75 [00:01<00:01, 23.74it/s]                                               {'loss': 2.1232, 'grad_norm': 3.4297821521759033, 'learning_rate': 5.130046785357849e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 23.74it/s]                                               {'loss': 2.2286, 'grad_norm': 3.849076986312866, 'learning_rate': 4.9468308287379255e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 23.74it/s]                                               {'loss': 2.2584, 'grad_norm': 3.8084821701049805, 'learning_rate': 4.763614872118003e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 23.74it/s] 68%|██████▊   | 51/75 [00:02<00:01, 23.85it/s]                                               {'loss': 2.0212, 'grad_norm': 3.999277114868164, 'learning_rate': 4.580398915498079e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 23.85it/s]                                               {'loss': 2.3032, 'grad_norm': 4.196914196014404, 'learning_rate': 4.3971829588781564e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 23.85it/s]                                               {'loss': 2.3848, 'grad_norm': 5.109707832336426, 'learning_rate': 4.213967002258232e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 23.85it/s] 72%|███████▏  | 54/75 [00:02<00:00, 24.09it/s]                                               {'loss': 2.43, 'grad_norm': 4.350823402404785, 'learning_rate': 4.0307510456383094e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.09it/s]                                               {'loss': 2.3288, 'grad_norm': 4.080894947052002, 'learning_rate': 3.8475350890183866e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.09it/s]                                               {'loss': 2.2246, 'grad_norm': 3.379504919052124, 'learning_rate': 3.664319132398463e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.09it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.24it/s]                                               {'loss': 2.1382, 'grad_norm': 3.859607219696045, 'learning_rate': 3.48110317577854e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.24it/s]                                               {'loss': 2.1722, 'grad_norm': 3.914292097091675, 'learning_rate': 3.297887219158617e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.24it/s]                                               {'loss': 2.1644, 'grad_norm': 4.735715866088867, 'learning_rate': 3.114671262538694e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.24it/s]                                               {'loss': 2.4842, 'grad_norm': 10.327692031860352, 'learning_rate': 2.9314553059187708e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.24it/s] 81%|████████▏ | 61/75 [00:02<00:00, 25.71it/s]                                               {'loss': 2.1979, 'grad_norm': 3.5069894790649414, 'learning_rate': 2.7482393492988477e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.71it/s]                                               {'loss': 2.2461, 'grad_norm': 3.773991823196411, 'learning_rate': 2.5650233926789245e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.71it/s]                                               {'loss': 2.1428, 'grad_norm': 3.6539981365203857, 'learning_rate': 2.3818074360590014e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.71it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.72it/s]                                               {'loss': 2.1781, 'grad_norm': 3.7747113704681396, 'learning_rate': 2.1985914794390782e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.72it/s]                                               {'loss': 2.1737, 'grad_norm': 4.082311630249023, 'learning_rate': 2.0153755228191547e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.72it/s]                                               {'loss': 2.3427, 'grad_norm': 3.6542551517486572, 'learning_rate': 1.8321595661992315e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.72it/s] 89%|████████▉ | 67/75 [00:02<00:00, 25.40it/s]                                               {'loss': 2.1125, 'grad_norm': 2.836711883544922, 'learning_rate': 1.6489436095793084e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.40it/s]                                               {'loss': 2.183, 'grad_norm': 2.9304842948913574, 'learning_rate': 1.4657276529593854e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.40it/s]                                               {'loss': 2.2513, 'grad_norm': 3.714684247970581, 'learning_rate': 1.2825116963394623e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.40it/s] 93%|█████████▎| 70/75 [00:02<00:00, 24.26it/s]                                               {'loss': 2.1557, 'grad_norm': 3.703237533569336, 'learning_rate': 1.0992957397195391e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.26it/s]                                               {'loss': 2.2819, 'grad_norm': 3.9350855350494385, 'learning_rate': 9.160797830996158e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.26it/s]                                               {'loss': 2.1933, 'grad_norm': 3.7755115032196045, 'learning_rate': 7.328638264796927e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.26it/s] 97%|█████████▋| 73/75 [00:03<00:00, 24.29it/s]                                               {'loss': 2.2085, 'grad_norm': 3.20861554145813, 'learning_rate': 5.4964786985976955e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 24.29it/s]                                               {'loss': 2.171, 'grad_norm': 3.354823350906372, 'learning_rate': 3.6643191323984635e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.29it/s]                                               {'loss': 2.4798, 'grad_norm': 10.379491806030273, 'learning_rate': 1.8321595661992318e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.29it/s]                                               {'train_runtime': 3.1637, 'train_samples_per_second': 357.18, 'train_steps_per_second': 23.707, 'train_loss': 2.2584262625376383, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.29it/s]100%|██████████| 75/75 [00:03<00:00, 23.71it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:45
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2868, 'grad_norm': 4.002808570861816, 'learning_rate': 0.00013741196746494238, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 21.72it/s]                                              {'loss': 2.2392, 'grad_norm': 4.628646373748779, 'learning_rate': 0.00013557980789874314, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 22.78it/s]  4%|▍         | 3/75 [00:00<00:03, 23.82it/s]                                              {'loss': 2.4013, 'grad_norm': 5.2017998695373535, 'learning_rate': 0.0001337476483325439, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.82it/s]                                              {'loss': 2.1538, 'grad_norm': 4.143939018249512, 'learning_rate': 0.00013191548876634467, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 23.82it/s]                                              {'loss': 2.3044, 'grad_norm': 3.6747405529022217, 'learning_rate': 0.00013008332920014544, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.82it/s]  8%|▊         | 6/75 [00:00<00:02, 25.30it/s]                                              {'loss': 2.397, 'grad_norm': 4.433536529541016, 'learning_rate': 0.00012825116963394623, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.30it/s]                                              {'loss': 2.4891, 'grad_norm': 3.6760621070861816, 'learning_rate': 0.000126419010067747, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.30it/s]                                              {'loss': 2.193, 'grad_norm': 4.224601745605469, 'learning_rate': 0.00012458685050154776, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.30it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.07it/s]                                              {'loss': 2.4873, 'grad_norm': 5.131882190704346, 'learning_rate': 0.00012275469093534852, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.07it/s]                                              {'loss': 2.3658, 'grad_norm': 4.2897844314575195, 'learning_rate': 0.00012092253136914929, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.07it/s]                                               {'loss': 2.0979, 'grad_norm': 3.1679015159606934, 'learning_rate': 0.00011909037180295007, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.07it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.86it/s]                                               {'loss': 2.2529, 'grad_norm': 3.4740054607391357, 'learning_rate': 0.00011725821223675083, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.86it/s]                                               {'loss': 2.3328, 'grad_norm': 3.778005361557007, 'learning_rate': 0.0001154260526705516, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.86it/s]                                               {'loss': 2.2839, 'grad_norm': 5.0049028396606445, 'learning_rate': 0.00011359389310435236, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.86it/s]                                               {'loss': 2.333, 'grad_norm': 16.362075805664062, 'learning_rate': 0.00011176173353815314, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.86it/s] 21%|██▏       | 16/75 [00:00<00:02, 27.25it/s]                                               {'loss': 2.385, 'grad_norm': 4.14648962020874, 'learning_rate': 0.0001099295739719539, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 27.25it/s]                                               {'loss': 2.1885, 'grad_norm': 4.600956916809082, 'learning_rate': 0.00010809741440575466, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 27.25it/s]                                               {'loss': 2.1828, 'grad_norm': 3.736403226852417, 'learning_rate': 0.00010626525483955544, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 27.25it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.61it/s]                                               {'loss': 2.1836, 'grad_norm': 3.537121534347534, 'learning_rate': 0.0001044330952733562, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.61it/s]                                               {'loss': 2.3837, 'grad_norm': 3.7381935119628906, 'learning_rate': 0.00010260093570715698, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.61it/s]                                               {'loss': 2.2651, 'grad_norm': 5.8885602951049805, 'learning_rate': 0.00010076877614095773, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.61it/s] 29%|██▉       | 22/75 [00:00<00:02, 26.19it/s]                                               {'loss': 2.0161, 'grad_norm': 5.023041725158691, 'learning_rate': 9.893661657475851e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 26.19it/s]                                               {'loss': 2.2335, 'grad_norm': 4.135372638702393, 'learning_rate': 9.710445700855928e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 26.19it/s]                                               {'loss': 2.1064, 'grad_norm': 3.5882809162139893, 'learning_rate': 9.527229744236005e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 26.19it/s] 33%|███▎      | 25/75 [00:00<00:01, 26.22it/s]                                               {'loss': 2.2477, 'grad_norm': 3.287240982055664, 'learning_rate': 9.344013787616082e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 26.22it/s]                                               {'loss': 2.0285, 'grad_norm': 3.424964427947998, 'learning_rate': 9.160797830996158e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 26.22it/s]                                               {'loss': 2.2175, 'grad_norm': 3.807523727416992, 'learning_rate': 8.977581874376235e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 26.22it/s] 37%|███▋      | 28/75 [00:01<00:01, 26.00it/s]                                               {'loss': 2.6289, 'grad_norm': 5.835043430328369, 'learning_rate': 8.794365917756313e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 26.00it/s]                                               {'loss': 2.321, 'grad_norm': 3.458587408065796, 'learning_rate': 8.611149961136389e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 26.00it/s]                                               {'loss': 2.4396, 'grad_norm': 16.338409423828125, 'learning_rate': 8.427934004516464e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 26.00it/s]                                               {'loss': 2.2972, 'grad_norm': 4.450229644775391, 'learning_rate': 8.244718047896542e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.00it/s] 43%|████▎     | 32/75 [00:01<00:01, 27.38it/s]                                               {'loss': 2.2873, 'grad_norm': 5.345392227172852, 'learning_rate': 8.061502091276619e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 27.38it/s]                                               {'loss': 2.3075, 'grad_norm': 3.0492422580718994, 'learning_rate': 7.878286134656697e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 27.38it/s]                                               {'loss': 2.2886, 'grad_norm': 4.21436071395874, 'learning_rate': 7.695070178036773e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 27.38it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.79it/s]                                               {'loss': 1.9147, 'grad_norm': 3.7762198448181152, 'learning_rate': 7.51185422141685e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.79it/s]                                               {'loss': 2.0148, 'grad_norm': 5.470869064331055, 'learning_rate': 7.328638264796926e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.79it/s]                                               {'loss': 2.3938, 'grad_norm': 4.613368988037109, 'learning_rate': 7.145422308177004e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.79it/s] 51%|█████     | 38/75 [00:01<00:01, 26.47it/s]                                               {'loss': 2.297, 'grad_norm': 4.264157772064209, 'learning_rate': 6.96220635155708e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.47it/s]                                               {'loss': 2.242, 'grad_norm': 4.135937690734863, 'learning_rate': 6.778990394937157e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.47it/s]                                               {'loss': 2.2287, 'grad_norm': 4.523591995239258, 'learning_rate': 6.595774438317234e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 26.47it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.72it/s]                                               {'loss': 2.2157, 'grad_norm': 3.117880344390869, 'learning_rate': 6.412558481697311e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.72it/s]                                               {'loss': 2.3141, 'grad_norm': 4.643850326538086, 'learning_rate': 6.229342525077388e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.72it/s]                                               {'loss': 2.1699, 'grad_norm': 4.324044704437256, 'learning_rate': 6.0461265684574645e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.72it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.67it/s]                                               {'loss': 2.2439, 'grad_norm': 6.300149917602539, 'learning_rate': 5.8629106118375416e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.67it/s]                                               {'loss': 1.9847, 'grad_norm': 8.27592945098877, 'learning_rate': 5.679694655217618e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.67it/s]                                               {'loss': 2.2518, 'grad_norm': 4.048883438110352, 'learning_rate': 5.496478698597695e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.67it/s]                                               {'loss': 2.1107, 'grad_norm': 4.14557409286499, 'learning_rate': 5.313262741977772e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.67it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.30it/s]                                               {'loss': 2.218, 'grad_norm': 4.984649658203125, 'learning_rate': 5.130046785357849e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 27.30it/s]                                               {'loss': 2.3218, 'grad_norm': 3.338322162628174, 'learning_rate': 4.9468308287379255e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 27.30it/s]                                               {'loss': 1.9736, 'grad_norm': 3.8588151931762695, 'learning_rate': 4.763614872118003e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 27.30it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.75it/s]                                               {'loss': 2.1509, 'grad_norm': 4.211784839630127, 'learning_rate': 4.580398915498079e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 26.75it/s]                                               {'loss': 1.9887, 'grad_norm': 3.1246423721313477, 'learning_rate': 4.3971829588781564e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 26.75it/s]                                               {'loss': 2.2616, 'grad_norm': 3.6948626041412354, 'learning_rate': 4.213967002258232e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 26.75it/s] 72%|███████▏  | 54/75 [00:02<00:00, 26.21it/s]                                               {'loss': 2.1817, 'grad_norm': 4.435728549957275, 'learning_rate': 4.0307510456383094e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 26.21it/s]                                               {'loss': 2.3798, 'grad_norm': 4.062268257141113, 'learning_rate': 3.8475350890183866e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 26.21it/s]                                               {'loss': 2.2382, 'grad_norm': 3.9180006980895996, 'learning_rate': 3.664319132398463e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 26.21it/s] 76%|███████▌  | 57/75 [00:02<00:00, 25.53it/s]                                               {'loss': 2.3009, 'grad_norm': 4.1294941902160645, 'learning_rate': 3.48110317577854e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.53it/s]                                               {'loss': 2.1707, 'grad_norm': 3.3858935832977295, 'learning_rate': 3.297887219158617e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.53it/s]                                               {'loss': 2.2287, 'grad_norm': 5.355820655822754, 'learning_rate': 3.114671262538694e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.53it/s] 80%|████████  | 60/75 [00:02<00:00, 25.44it/s]                                               {'loss': 2.1134, 'grad_norm': 14.947093963623047, 'learning_rate': 2.9314553059187708e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.44it/s]                                               {'loss': 2.1272, 'grad_norm': 4.268704414367676, 'learning_rate': 2.7482393492988477e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.44it/s]                                               {'loss': 2.2955, 'grad_norm': 4.253407955169678, 'learning_rate': 2.5650233926789245e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.44it/s] 84%|████████▍ | 63/75 [00:02<00:00, 25.42it/s]                                               {'loss': 2.2072, 'grad_norm': 3.494274854660034, 'learning_rate': 2.3818074360590014e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.42it/s]                                               {'loss': 2.2102, 'grad_norm': 3.7783236503601074, 'learning_rate': 2.1985914794390782e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.42it/s]                                               {'loss': 2.1725, 'grad_norm': 2.9890780448913574, 'learning_rate': 2.0153755228191547e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.42it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.1224, 'grad_norm': 3.489497661590576, 'learning_rate': 1.8321595661992315e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.3175, 'grad_norm': 4.7265825271606445, 'learning_rate': 1.6489436095793084e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.1547, 'grad_norm': 4.303959369659424, 'learning_rate': 1.4657276529593854e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.45it/s] 92%|█████████▏| 69/75 [00:02<00:00, 25.27it/s]                                               {'loss': 2.0636, 'grad_norm': 4.1131134033203125, 'learning_rate': 1.2825116963394623e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.27it/s]                                               {'loss': 2.1102, 'grad_norm': 3.5340518951416016, 'learning_rate': 1.0992957397195391e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.27it/s]                                               {'loss': 2.15, 'grad_norm': 4.484616756439209, 'learning_rate': 9.160797830996158e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.27it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.01it/s]                                               {'loss': 2.2437, 'grad_norm': 3.484830379486084, 'learning_rate': 7.328638264796927e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.01it/s]                                               {'loss': 2.1071, 'grad_norm': 3.2024378776550293, 'learning_rate': 5.4964786985976955e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.01it/s]                                               {'loss': 2.3712, 'grad_norm': 3.23933482170105, 'learning_rate': 3.6643191323984635e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.01it/s]100%|██████████| 75/75 [00:02<00:00, 26.02it/s]                                               {'loss': 1.8337, 'grad_norm': 9.482096672058105, 'learning_rate': 1.8321595661992318e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.02it/s]                                               {'train_runtime': 2.995, 'train_samples_per_second': 377.292, 'train_steps_per_second': 25.041, 'train_loss': 2.2269715054829917, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.02it/s]100%|██████████| 75/75 [00:02<00:00, 25.04it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:23
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.1721, 'grad_norm': 3.975923776626587, 'learning_rate': 0.00013741196746494238, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.70it/s]                                              {'loss': 2.4145, 'grad_norm': 3.5872161388397217, 'learning_rate': 0.00013557980789874314, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.75it/s]  4%|▍         | 3/75 [00:00<00:02, 25.14it/s]                                              {'loss': 2.2626, 'grad_norm': 4.880478382110596, 'learning_rate': 0.0001337476483325439, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.14it/s]                                              {'loss': 2.4303, 'grad_norm': 4.214066505432129, 'learning_rate': 0.00013191548876634467, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.14it/s]                                              {'loss': 2.2143, 'grad_norm': 3.483365058898926, 'learning_rate': 0.00013008332920014544, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.14it/s]  8%|▊         | 6/75 [00:00<00:02, 25.47it/s]                                              {'loss': 2.1243, 'grad_norm': 4.2701849937438965, 'learning_rate': 0.00012825116963394623, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.47it/s]                                              {'loss': 2.448, 'grad_norm': 4.68330717086792, 'learning_rate': 0.000126419010067747, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.47it/s]                                              {'loss': 2.3409, 'grad_norm': 3.495198965072632, 'learning_rate': 0.00012458685050154776, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.47it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.94it/s]                                              {'loss': 2.2747, 'grad_norm': 3.349377155303955, 'learning_rate': 0.00012275469093534852, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.94it/s]                                              {'loss': 2.0876, 'grad_norm': 3.2863142490386963, 'learning_rate': 0.00012092253136914929, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.94it/s]                                               {'loss': 2.1562, 'grad_norm': 3.2901103496551514, 'learning_rate': 0.00011909037180295007, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.94it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.38it/s]                                               {'loss': 2.1463, 'grad_norm': 4.614271640777588, 'learning_rate': 0.00011725821223675083, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.38it/s]                                               {'loss': 2.2334, 'grad_norm': 3.580289125442505, 'learning_rate': 0.0001154260526705516, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.38it/s]                                               {'loss': 2.3793, 'grad_norm': 4.410000801086426, 'learning_rate': 0.00011359389310435236, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.38it/s]                                               {'loss': 1.7261, 'grad_norm': 8.798834800720215, 'learning_rate': 0.00011176173353815314, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.38it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.66it/s]                                               {'loss': 2.2974, 'grad_norm': 3.3463685512542725, 'learning_rate': 0.0001099295739719539, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.66it/s]                                               {'loss': 2.3421, 'grad_norm': 3.2605369091033936, 'learning_rate': 0.00010809741440575466, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.66it/s]                                               {'loss': 2.1548, 'grad_norm': 3.4451630115509033, 'learning_rate': 0.00010626525483955544, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.66it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.49it/s]                                               {'loss': 2.2758, 'grad_norm': 4.054858207702637, 'learning_rate': 0.0001044330952733562, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.49it/s]                                               {'loss': 2.0034, 'grad_norm': 3.649989604949951, 'learning_rate': 0.00010260093570715698, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.49it/s]                                               {'loss': 2.1389, 'grad_norm': 4.039541721343994, 'learning_rate': 0.00010076877614095773, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.49it/s] 29%|██▉       | 22/75 [00:00<00:02, 26.27it/s]                                               {'loss': 2.4096, 'grad_norm': 4.514434814453125, 'learning_rate': 9.893661657475851e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 26.27it/s]                                               {'loss': 2.2828, 'grad_norm': 3.8912081718444824, 'learning_rate': 9.710445700855928e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 26.27it/s]                                               {'loss': 2.2917, 'grad_norm': 4.133504390716553, 'learning_rate': 9.527229744236005e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 26.27it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.40it/s]                                               {'loss': 2.2771, 'grad_norm': 3.6996371746063232, 'learning_rate': 9.344013787616082e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.40it/s]                                               {'loss': 2.2597, 'grad_norm': 3.463754415512085, 'learning_rate': 9.160797830996158e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.40it/s]                                               {'loss': 2.3778, 'grad_norm': 5.077038764953613, 'learning_rate': 8.977581874376235e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.40it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.65it/s]                                               {'loss': 1.8794, 'grad_norm': 3.2138640880584717, 'learning_rate': 8.794365917756313e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.65it/s]                                               {'loss': 2.008, 'grad_norm': 3.7964491844177246, 'learning_rate': 8.611149961136389e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.65it/s]                                               {'loss': 2.0649, 'grad_norm': 10.042632102966309, 'learning_rate': 8.427934004516464e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.65it/s]                                               {'loss': 2.2974, 'grad_norm': 3.2111589908599854, 'learning_rate': 8.244718047896542e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.65it/s] 43%|████▎     | 32/75 [00:01<00:01, 27.20it/s]                                               {'loss': 2.0877, 'grad_norm': 4.376789569854736, 'learning_rate': 8.061502091276619e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 27.20it/s]                                               {'loss': 2.0949, 'grad_norm': 5.327873706817627, 'learning_rate': 7.878286134656697e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 27.20it/s]                                               {'loss': 2.1652, 'grad_norm': 4.194468975067139, 'learning_rate': 7.695070178036773e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 27.20it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.89it/s]                                               {'loss': 2.2289, 'grad_norm': 3.4063079357147217, 'learning_rate': 7.51185422141685e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.89it/s]                                               {'loss': 2.3546, 'grad_norm': 3.811436176300049, 'learning_rate': 7.328638264796926e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.89it/s]                                               {'loss': 2.047, 'grad_norm': 3.1611602306365967, 'learning_rate': 7.145422308177004e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.89it/s] 51%|█████     | 38/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.178, 'grad_norm': 3.698608636856079, 'learning_rate': 6.96220635155708e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.2442, 'grad_norm': 4.109137535095215, 'learning_rate': 6.778990394937157e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.4217, 'grad_norm': 3.8339462280273438, 'learning_rate': 6.595774438317234e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.62it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.107, 'grad_norm': 3.3498568534851074, 'learning_rate': 6.412558481697311e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.0582, 'grad_norm': 3.03037428855896, 'learning_rate': 6.229342525077388e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.1168, 'grad_norm': 3.2270374298095703, 'learning_rate': 6.0461265684574645e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.13it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.2492, 'grad_norm': 4.379333019256592, 'learning_rate': 5.8629106118375416e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.2376, 'grad_norm': 9.1688814163208, 'learning_rate': 5.679694655217618e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.1291, 'grad_norm': 3.674163341522217, 'learning_rate': 5.496478698597695e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.1325, 'grad_norm': 3.9080240726470947, 'learning_rate': 5.313262741977772e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.00it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.47it/s]                                               {'loss': 2.0008, 'grad_norm': 3.560858726501465, 'learning_rate': 5.130046785357849e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.47it/s]                                               {'loss': 2.1003, 'grad_norm': 3.651911973953247, 'learning_rate': 4.9468308287379255e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.47it/s]                                               {'loss': 2.1428, 'grad_norm': 3.8847944736480713, 'learning_rate': 4.763614872118003e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.47it/s] 68%|██████▊   | 51/75 [00:01<00:00, 25.21it/s]                                               {'loss': 2.3834, 'grad_norm': 4.377033710479736, 'learning_rate': 4.580398915498079e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 25.21it/s]                                               {'loss': 2.2232, 'grad_norm': 3.9142820835113525, 'learning_rate': 4.3971829588781564e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.21it/s]                                               {'loss': 2.3193, 'grad_norm': 4.943408012390137, 'learning_rate': 4.213967002258232e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.21it/s] 72%|███████▏  | 54/75 [00:02<00:00, 24.65it/s]                                               {'loss': 2.3344, 'grad_norm': 4.2774271965026855, 'learning_rate': 4.0307510456383094e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.65it/s]                                               {'loss': 2.0442, 'grad_norm': 4.136715888977051, 'learning_rate': 3.8475350890183866e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.65it/s]                                               {'loss': 2.2085, 'grad_norm': 4.171804904937744, 'learning_rate': 3.664319132398463e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.65it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.75it/s]                                               {'loss': 2.2243, 'grad_norm': 3.787198543548584, 'learning_rate': 3.48110317577854e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.75it/s]                                               {'loss': 2.0536, 'grad_norm': 3.797290563583374, 'learning_rate': 3.297887219158617e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.75it/s]                                               {'loss': 2.1258, 'grad_norm': 4.1644673347473145, 'learning_rate': 3.114671262538694e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.75it/s]                                               {'loss': 2.1378, 'grad_norm': 9.830682754516602, 'learning_rate': 2.9314553059187708e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.75it/s] 81%|████████▏ | 61/75 [00:02<00:00, 26.31it/s]                                               {'loss': 2.1368, 'grad_norm': 4.5450029373168945, 'learning_rate': 2.7482393492988477e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.31it/s]                                               {'loss': 2.0612, 'grad_norm': 2.913609027862549, 'learning_rate': 2.5650233926789245e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.31it/s]                                               {'loss': 2.0491, 'grad_norm': 3.827531099319458, 'learning_rate': 2.3818074360590014e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.31it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.69it/s]                                               {'loss': 2.0653, 'grad_norm': 3.018092632293701, 'learning_rate': 2.1985914794390782e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.69it/s]                                               {'loss': 2.2538, 'grad_norm': 4.325839519500732, 'learning_rate': 2.0153755228191547e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.69it/s]                                               {'loss': 2.3069, 'grad_norm': 4.374340534210205, 'learning_rate': 1.8321595661992315e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.69it/s] 89%|████████▉ | 67/75 [00:02<00:00, 24.98it/s]                                               {'loss': 2.0483, 'grad_norm': 3.1643478870391846, 'learning_rate': 1.6489436095793084e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.98it/s]                                               {'loss': 2.0652, 'grad_norm': 3.5900444984436035, 'learning_rate': 1.4657276529593854e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.98it/s]                                               {'loss': 2.3044, 'grad_norm': 3.963364839553833, 'learning_rate': 1.2825116963394623e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.98it/s] 93%|█████████▎| 70/75 [00:02<00:00, 23.18it/s]                                               {'loss': 2.1568, 'grad_norm': 3.99316668510437, 'learning_rate': 1.0992957397195391e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 23.18it/s]                                               {'loss': 2.2635, 'grad_norm': 3.8577122688293457, 'learning_rate': 9.160797830996158e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 23.18it/s]                                               {'loss': 2.1247, 'grad_norm': 3.1350691318511963, 'learning_rate': 7.328638264796927e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 23.18it/s] 97%|█████████▋| 73/75 [00:02<00:00, 22.78it/s]                                               {'loss': 2.2428, 'grad_norm': 5.537866115570068, 'learning_rate': 5.4964786985976955e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 22.78it/s]                                               {'loss': 2.1532, 'grad_norm': 4.032898902893066, 'learning_rate': 3.6643191323984635e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 22.78it/s]                                               {'loss': 1.6581, 'grad_norm': 8.910076141357422, 'learning_rate': 1.8321595661992318e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 22.78it/s]                                               {'train_runtime': 3.0935, 'train_samples_per_second': 365.277, 'train_steps_per_second': 24.244, 'train_loss': 2.184172004063924, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 22.78it/s]100%|██████████| 75/75 [00:03<00:00, 24.25it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:10
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3645, 'grad_norm': 5.494688034057617, 'learning_rate': 0.00013741196746494238, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 19.14it/s]                                              {'loss': 2.2701, 'grad_norm': 4.196670055389404, 'learning_rate': 0.00013557980789874314, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 21.69it/s]  4%|▍         | 3/75 [00:00<00:03, 22.90it/s]                                              {'loss': 2.1622, 'grad_norm': 3.990617513656616, 'learning_rate': 0.0001337476483325439, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 22.90it/s]                                              {'loss': 2.1493, 'grad_norm': 4.16509485244751, 'learning_rate': 0.00013191548876634467, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 22.90it/s]                                              {'loss': 2.3653, 'grad_norm': 3.8042454719543457, 'learning_rate': 0.00013008332920014544, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.90it/s]  8%|▊         | 6/75 [00:00<00:02, 24.10it/s]                                              {'loss': 2.4487, 'grad_norm': 4.317508697509766, 'learning_rate': 0.00012825116963394623, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.10it/s]                                              {'loss': 2.4756, 'grad_norm': 4.957284450531006, 'learning_rate': 0.000126419010067747, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.10it/s]                                              {'loss': 2.5778, 'grad_norm': 4.152597427368164, 'learning_rate': 0.00012458685050154776, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.10it/s] 12%|█▏        | 9/75 [00:00<00:02, 23.92it/s]                                              {'loss': 2.2217, 'grad_norm': 4.271284103393555, 'learning_rate': 0.00012275469093534852, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.92it/s]                                              {'loss': 2.173, 'grad_norm': 4.174534797668457, 'learning_rate': 0.00012092253136914929, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.92it/s]                                               {'loss': 2.3577, 'grad_norm': 4.642778396606445, 'learning_rate': 0.00011909037180295007, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.92it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.75it/s]                                               {'loss': 2.4162, 'grad_norm': 6.092030048370361, 'learning_rate': 0.00011725821223675083, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.75it/s]                                               {'loss': 2.2301, 'grad_norm': 4.1878862380981445, 'learning_rate': 0.0001154260526705516, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.75it/s]                                               {'loss': 2.1597, 'grad_norm': 3.745826482772827, 'learning_rate': 0.00011359389310435236, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.75it/s] 20%|██        | 15/75 [00:00<00:02, 26.24it/s]                                               {'loss': 1.9857, 'grad_norm': 10.729649543762207, 'learning_rate': 0.00011176173353815314, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 26.24it/s]                                               {'loss': 2.269, 'grad_norm': 3.8534717559814453, 'learning_rate': 0.0001099295739719539, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.24it/s]                                               {'loss': 2.2735, 'grad_norm': 4.474687576293945, 'learning_rate': 0.00010809741440575466, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.24it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.80it/s]                                               {'loss': 2.3061, 'grad_norm': 3.8932979106903076, 'learning_rate': 0.00010626525483955544, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.80it/s]                                               {'loss': 2.1877, 'grad_norm': 3.4029645919799805, 'learning_rate': 0.0001044330952733562, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.80it/s]                                               {'loss': 2.39, 'grad_norm': 3.8153188228607178, 'learning_rate': 0.00010260093570715698, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.80it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.78it/s]                                               {'loss': 2.1789, 'grad_norm': 4.511816501617432, 'learning_rate': 0.00010076877614095773, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.78it/s]                                               {'loss': 2.2782, 'grad_norm': 3.6547248363494873, 'learning_rate': 9.893661657475851e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.78it/s]                                               {'loss': 2.2149, 'grad_norm': 4.686969757080078, 'learning_rate': 9.710445700855928e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.78it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.37it/s]                                               {'loss': 2.0491, 'grad_norm': 3.024224281311035, 'learning_rate': 9.527229744236005e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.37it/s]                                               {'loss': 2.25, 'grad_norm': 3.9724230766296387, 'learning_rate': 9.344013787616082e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.37it/s]                                               {'loss': 2.2061, 'grad_norm': 3.6693551540374756, 'learning_rate': 9.160797830996158e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.37it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.34it/s]                                               {'loss': 2.5094, 'grad_norm': 4.798442840576172, 'learning_rate': 8.977581874376235e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.34it/s]                                               {'loss': 2.2268, 'grad_norm': 3.589261531829834, 'learning_rate': 8.794365917756313e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.34it/s]                                               {'loss': 2.2573, 'grad_norm': 4.112771511077881, 'learning_rate': 8.611149961136389e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.34it/s] 40%|████      | 30/75 [00:01<00:01, 25.59it/s]                                               {'loss': 2.3395, 'grad_norm': 11.275978088378906, 'learning_rate': 8.427934004516464e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.59it/s]                                               {'loss': 2.2139, 'grad_norm': 3.827796459197998, 'learning_rate': 8.244718047896542e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.59it/s]                                               {'loss': 2.1994, 'grad_norm': 3.5998411178588867, 'learning_rate': 8.061502091276619e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.59it/s] 44%|████▍     | 33/75 [00:01<00:01, 25.03it/s]                                               {'loss': 2.3878, 'grad_norm': 4.698415279388428, 'learning_rate': 7.878286134656697e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.03it/s]                                               {'loss': 2.452, 'grad_norm': 4.56076192855835, 'learning_rate': 7.695070178036773e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.03it/s]                                               {'loss': 2.258, 'grad_norm': 4.278529644012451, 'learning_rate': 7.51185422141685e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.03it/s] 48%|████▊     | 36/75 [00:01<00:01, 24.14it/s]                                               {'loss': 2.4206, 'grad_norm': 4.309744358062744, 'learning_rate': 7.328638264796926e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.14it/s]                                               {'loss': 2.3664, 'grad_norm': 6.395837783813477, 'learning_rate': 7.145422308177004e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.14it/s]                                               {'loss': 2.01, 'grad_norm': 4.074687480926514, 'learning_rate': 6.96220635155708e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.14it/s] 52%|█████▏    | 39/75 [00:01<00:01, 23.78it/s]                                               {'loss': 2.0691, 'grad_norm': 3.4544641971588135, 'learning_rate': 6.778990394937157e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.78it/s]                                               {'loss': 2.3057, 'grad_norm': 3.935579299926758, 'learning_rate': 6.595774438317234e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.78it/s]                                               {'loss': 2.198, 'grad_norm': 5.333765029907227, 'learning_rate': 6.412558481697311e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 23.78it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.06it/s]                                               {'loss': 2.2514, 'grad_norm': 3.6301450729370117, 'learning_rate': 6.229342525077388e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.06it/s]                                               {'loss': 2.1793, 'grad_norm': 3.8643131256103516, 'learning_rate': 6.0461265684574645e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.06it/s]                                               {'loss': 2.0329, 'grad_norm': 3.634357452392578, 'learning_rate': 5.8629106118375416e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.06it/s]                                               {'loss': 1.9901, 'grad_norm': 10.942224502563477, 'learning_rate': 5.679694655217618e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.06it/s] 61%|██████▏   | 46/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.259, 'grad_norm': 4.721944808959961, 'learning_rate': 5.496478698597695e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.1047, 'grad_norm': 4.440525054931641, 'learning_rate': 5.313262741977772e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.1444, 'grad_norm': 4.689273834228516, 'learning_rate': 5.130046785357849e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.30it/s] 65%|██████▌   | 49/75 [00:01<00:01, 25.09it/s]                                               {'loss': 2.2898, 'grad_norm': 4.620163917541504, 'learning_rate': 4.9468308287379255e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.09it/s]                                               {'loss': 2.4229, 'grad_norm': 4.260795593261719, 'learning_rate': 4.763614872118003e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.09it/s]                                               {'loss': 2.4254, 'grad_norm': 4.466798305511475, 'learning_rate': 4.580398915498079e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.09it/s] 69%|██████▉   | 52/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.0936, 'grad_norm': 3.3136565685272217, 'learning_rate': 4.3971829588781564e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.2241, 'grad_norm': 4.742856502532959, 'learning_rate': 4.213967002258232e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.49it/s]                                               {'loss': 1.9362, 'grad_norm': 3.6991043090820312, 'learning_rate': 4.0307510456383094e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.49it/s] 73%|███████▎  | 55/75 [00:02<00:00, 24.71it/s]                                               {'loss': 2.1636, 'grad_norm': 3.691236972808838, 'learning_rate': 3.8475350890183866e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.71it/s]                                               {'loss': 2.2117, 'grad_norm': 4.1113667488098145, 'learning_rate': 3.664319132398463e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.71it/s]                                               {'loss': 2.0929, 'grad_norm': 4.204540252685547, 'learning_rate': 3.48110317577854e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.71it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.17it/s]                                               {'loss': 2.2637, 'grad_norm': 3.252540111541748, 'learning_rate': 3.297887219158617e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.17it/s]                                               {'loss': 2.269, 'grad_norm': 3.206376075744629, 'learning_rate': 3.114671262538694e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.17it/s]                                               {'loss': 2.5353, 'grad_norm': 16.93755531311035, 'learning_rate': 2.9314553059187708e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.17it/s] 81%|████████▏ | 61/75 [00:02<00:00, 24.97it/s]                                               {'loss': 2.2289, 'grad_norm': 3.915961503982544, 'learning_rate': 2.7482393492988477e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.97it/s]                                               {'loss': 2.1241, 'grad_norm': 3.85650897026062, 'learning_rate': 2.5650233926789245e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 24.97it/s]                                               {'loss': 2.1758, 'grad_norm': 3.7598791122436523, 'learning_rate': 2.3818074360590014e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 24.97it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.21it/s]                                               {'loss': 2.1044, 'grad_norm': 4.342527389526367, 'learning_rate': 2.1985914794390782e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.21it/s]                                               {'loss': 2.0359, 'grad_norm': 3.554454803466797, 'learning_rate': 2.0153755228191547e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.21it/s]                                               {'loss': 2.2814, 'grad_norm': 3.357656478881836, 'learning_rate': 1.8321595661992315e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.21it/s] 89%|████████▉ | 67/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.185, 'grad_norm': 3.5934534072875977, 'learning_rate': 1.6489436095793084e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.0976, 'grad_norm': 5.4692792892456055, 'learning_rate': 1.4657276529593854e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.3057, 'grad_norm': 3.644728899002075, 'learning_rate': 1.2825116963394623e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.45it/s] 93%|█████████▎| 70/75 [00:02<00:00, 25.36it/s]                                               {'loss': 2.3357, 'grad_norm': 3.4481167793273926, 'learning_rate': 1.0992957397195391e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.36it/s]                                               {'loss': 2.3592, 'grad_norm': 4.680078029632568, 'learning_rate': 9.160797830996158e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.36it/s]                                               {'loss': 2.2397, 'grad_norm': 4.258735656738281, 'learning_rate': 7.328638264796927e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.36it/s] 97%|█████████▋| 73/75 [00:02<00:00, 23.99it/s]                                               {'loss': 1.9187, 'grad_norm': 4.044825553894043, 'learning_rate': 5.4964786985976955e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 23.99it/s]                                               {'loss': 2.291, 'grad_norm': 4.752401828765869, 'learning_rate': 3.6643191323984635e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 23.99it/s]                                               {'loss': 2.5348, 'grad_norm': 9.191604614257812, 'learning_rate': 1.8321595661992318e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 23.99it/s]                                               {'train_runtime': 3.1225, 'train_samples_per_second': 361.894, 'train_steps_per_second': 24.02, 'train_loss': 2.243774905204773, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 23.99it/s]100%|██████████| 75/75 [00:03<00:00, 24.02it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(989, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(926, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(754, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(909, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1130, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(878, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1039, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(933, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(966, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(650, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1120, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(575, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 66.19it/s]  3%|▎         | 14/471 [00:00<00:08, 55.80it/s]  4%|▍         | 20/471 [00:00<00:08, 53.93it/s]  6%|▌         | 26/471 [00:00<00:08, 52.76it/s]  7%|▋         | 32/471 [00:00<00:08, 52.73it/s]  8%|▊         | 38/471 [00:00<00:08, 51.69it/s]  9%|▉         | 44/471 [00:00<00:08, 51.78it/s] 11%|█         | 50/471 [00:00<00:08, 52.07it/s] 12%|█▏        | 56/471 [00:01<00:07, 52.27it/s] 13%|█▎        | 62/471 [00:01<00:07, 51.30it/s] 14%|█▍        | 68/471 [00:01<00:07, 51.46it/s] 16%|█▌        | 74/471 [00:01<00:07, 51.44it/s] 17%|█▋        | 80/471 [00:01<00:07, 51.61it/s] 18%|█▊        | 86/471 [00:01<00:07, 51.93it/s] 20%|█▉        | 92/471 [00:01<00:07, 51.96it/s] 21%|██        | 98/471 [00:01<00:07, 51.30it/s] 22%|██▏       | 104/471 [00:01<00:07, 51.74it/s] 23%|██▎       | 110/471 [00:02<00:06, 52.11it/s] 25%|██▍       | 116/471 [00:02<00:06, 51.66it/s] 26%|██▌       | 122/471 [00:02<00:06, 51.99it/s] 27%|██▋       | 128/471 [00:02<00:06, 50.30it/s] 28%|██▊       | 134/471 [00:02<00:06, 50.66it/s] 30%|██▉       | 140/471 [00:02<00:06, 51.02it/s] 31%|███       | 146/471 [00:02<00:06, 51.04it/s] 32%|███▏      | 152/471 [00:02<00:06, 51.24it/s] 34%|███▎      | 158/471 [00:03<00:06, 51.76it/s] 35%|███▍      | 164/471 [00:03<00:05, 52.01it/s] 36%|███▌      | 170/471 [00:03<00:05, 52.25it/s] 37%|███▋      | 176/471 [00:03<00:05, 52.08it/s] 39%|███▊      | 182/471 [00:03<00:05, 52.05it/s] 40%|███▉      | 188/471 [00:03<00:05, 52.27it/s] 41%|████      | 194/471 [00:03<00:05, 52.31it/s] 42%|████▏     | 200/471 [00:03<00:05, 52.17it/s] 44%|████▎     | 206/471 [00:03<00:05, 51.85it/s] 45%|████▌     | 212/471 [00:04<00:04, 52.06it/s] 46%|████▋     | 218/471 [00:04<00:05, 50.32it/s] 48%|████▊     | 224/471 [00:04<00:04, 51.06it/s] 49%|████▉     | 230/471 [00:04<00:04, 51.56it/s] 50%|█████     | 236/471 [00:04<00:04, 52.03it/s] 51%|█████▏    | 242/471 [00:04<00:04, 52.17it/s] 53%|█████▎    | 248/471 [00:04<00:04, 51.61it/s] 54%|█████▍    | 254/471 [00:04<00:04, 50.91it/s] 55%|█████▌    | 260/471 [00:05<00:04, 50.50it/s] 56%|█████▋    | 266/471 [00:05<00:03, 51.33it/s] 58%|█████▊    | 272/471 [00:05<00:03, 51.86it/s] 59%|█████▉    | 278/471 [00:05<00:03, 51.95it/s] 60%|██████    | 284/471 [00:05<00:03, 51.83it/s] 62%|██████▏   | 290/471 [00:05<00:03, 51.23it/s] 63%|██████▎   | 296/471 [00:05<00:03, 51.45it/s] 64%|██████▍   | 302/471 [00:05<00:03, 51.72it/s] 65%|██████▌   | 308/471 [00:05<00:03, 51.30it/s] 67%|██████▋   | 314/471 [00:06<00:03, 51.61it/s] 68%|██████▊   | 320/471 [00:06<00:02, 51.90it/s] 69%|██████▉   | 326/471 [00:06<00:02, 51.76it/s] 70%|███████   | 332/471 [00:06<00:02, 51.57it/s] 72%|███████▏  | 338/471 [00:06<00:02, 49.08it/s] 73%|███████▎  | 344/471 [00:06<00:02, 49.78it/s] 74%|███████▍  | 349/471 [00:06<00:02, 49.19it/s] 75%|███████▌  | 355/471 [00:06<00:02, 49.67it/s] 77%|███████▋  | 361/471 [00:07<00:02, 49.93it/s] 78%|███████▊  | 366/471 [00:07<00:02, 49.80it/s] 79%|███████▉  | 372/471 [00:07<00:02, 49.41it/s] 80%|████████  | 377/471 [00:07<00:01, 49.13it/s] 81%|████████▏ | 383/471 [00:07<00:01, 50.70it/s] 83%|████████▎ | 389/471 [00:07<00:01, 51.15it/s] 84%|████████▍ | 395/471 [00:07<00:01, 50.26it/s] 85%|████████▌ | 401/471 [00:07<00:01, 51.29it/s] 86%|████████▋ | 407/471 [00:07<00:01, 50.17it/s] 88%|████████▊ | 413/471 [00:08<00:01, 51.38it/s] 89%|████████▉ | 419/471 [00:08<00:01, 51.49it/s] 90%|█████████ | 425/471 [00:08<00:00, 51.54it/s] 92%|█████████▏| 431/471 [00:08<00:00, 51.32it/s] 93%|█████████▎| 437/471 [00:08<00:00, 51.66it/s] 94%|█████████▍| 443/471 [00:08<00:00, 52.10it/s] 95%|█████████▌| 449/471 [00:08<00:00, 51.89it/s] 97%|█████████▋| 455/471 [00:08<00:00, 51.72it/s] 98%|█████████▊| 461/471 [00:08<00:00, 51.12it/s] 99%|█████████▉| 467/471 [00:09<00:00, 51.00it/s]100%|██████████| 471/471 [00:09<00:00, 51.43it/s]
{'eval_loss': 2.2836029529571533, 'eval_model_preparation_time': 0.003, 'eval_acc': 0.3916622411046203, 'eval_runtime': 9.1824, 'eval_samples_per_second': 820.267, 'eval_steps_per_second': 51.294}
ROUND:15
CLIENT:32
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.1662, 'grad_norm': 4.093303203582764, 'learning_rate': 0.00013484692283495344, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.72it/s]                                              {'loss': 2.4655, 'grad_norm': 3.838793992996216, 'learning_rate': 0.00013304896386382074, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 25.32it/s]  4%|▍         | 3/75 [00:00<00:02, 25.90it/s]                                              {'loss': 2.2806, 'grad_norm': 4.098992824554443, 'learning_rate': 0.00013125100489268801, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.90it/s]                                              {'loss': 2.3356, 'grad_norm': 4.70195198059082, 'learning_rate': 0.0001294530459215553, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.90it/s]                                              {'loss': 2.3501, 'grad_norm': 3.6614439487457275, 'learning_rate': 0.0001276550869504226, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.90it/s]  8%|▊         | 6/75 [00:00<00:02, 25.14it/s]                                              {'loss': 2.1386, 'grad_norm': 3.632096290588379, 'learning_rate': 0.00012585712797928987, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.14it/s]                                              {'loss': 2.3526, 'grad_norm': 4.889062404632568, 'learning_rate': 0.00012405916900815717, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.14it/s]                                              {'loss': 2.4465, 'grad_norm': 4.320581436157227, 'learning_rate': 0.00012226121003702445, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.14it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.42it/s]                                              {'loss': 2.2518, 'grad_norm': 2.64544939994812, 'learning_rate': 0.00012046325106589174, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.42it/s]                                              {'loss': 2.2477, 'grad_norm': 3.671295404434204, 'learning_rate': 0.00011866529209475903, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.42it/s]                                               {'loss': 2.266, 'grad_norm': 3.636920690536499, 'learning_rate': 0.00011686733312362632, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.42it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.23it/s]                                               {'loss': 2.4073, 'grad_norm': 3.859111785888672, 'learning_rate': 0.00011506937415249361, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.23it/s]                                               {'loss': 2.2548, 'grad_norm': 4.395226955413818, 'learning_rate': 0.00011327141518136088, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.23it/s]                                               {'loss': 2.2416, 'grad_norm': 4.573200225830078, 'learning_rate': 0.00011147345621022817, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.23it/s] 20%|██        | 15/75 [00:00<00:02, 25.92it/s]                                               {'loss': 1.7829, 'grad_norm': 7.376585006713867, 'learning_rate': 0.00010967549723909546, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.92it/s]                                               {'loss': 2.2528, 'grad_norm': 3.4760921001434326, 'learning_rate': 0.00010787753826796275, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.92it/s]                                               {'loss': 2.2101, 'grad_norm': 3.478241443634033, 'learning_rate': 0.00010607957929683003, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.92it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.75it/s]                                               {'loss': 2.1446, 'grad_norm': 3.6180686950683594, 'learning_rate': 0.00010428162032569732, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.75it/s]                                               {'loss': 2.2414, 'grad_norm': 4.973060607910156, 'learning_rate': 0.0001024836613545646, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.75it/s]                                               {'loss': 2.373, 'grad_norm': 4.137751579284668, 'learning_rate': 0.00010068570238343191, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.75it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.33it/s]                                               {'loss': 2.0583, 'grad_norm': 3.3832767009735107, 'learning_rate': 9.888774341229919e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.33it/s]                                               {'loss': 2.0649, 'grad_norm': 3.1666219234466553, 'learning_rate': 9.708978444116647e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.33it/s]                                               {'loss': 2.25, 'grad_norm': 4.017735958099365, 'learning_rate': 9.529182547003376e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.33it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.61it/s]                                               {'loss': 2.3574, 'grad_norm': 4.226058006286621, 'learning_rate': 9.349386649890105e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.61it/s]                                               {'loss': 2.3516, 'grad_norm': 4.27477502822876, 'learning_rate': 9.169590752776834e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:02, 24.61it/s]                                               {'loss': 2.3405, 'grad_norm': 3.7607972621917725, 'learning_rate': 8.989794855663562e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.61it/s] 36%|███▌      | 27/75 [00:01<00:01, 25.09it/s]                                               {'loss': 2.1493, 'grad_norm': 4.533234596252441, 'learning_rate': 8.809998958550291e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.09it/s]                                               {'loss': 2.1795, 'grad_norm': 5.576320648193359, 'learning_rate': 8.63020306143702e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.09it/s]                                               {'loss': 2.1952, 'grad_norm': 3.5542783737182617, 'learning_rate': 8.450407164323749e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.09it/s] 40%|████      | 30/75 [00:01<00:01, 26.29it/s]                                               {'loss': 1.7626, 'grad_norm': 8.559416770935059, 'learning_rate': 8.270611267210476e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 26.29it/s]                                               {'loss': 2.2525, 'grad_norm': 3.6878249645233154, 'learning_rate': 8.090815370097205e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.29it/s]                                               {'loss': 2.0455, 'grad_norm': 3.3129537105560303, 'learning_rate': 7.911019472983934e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.29it/s] 44%|████▍     | 33/75 [00:01<00:01, 25.75it/s]                                               {'loss': 2.4124, 'grad_norm': 4.200194358825684, 'learning_rate': 7.731223575870665e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.75it/s]                                               {'loss': 2.1767, 'grad_norm': 3.9472615718841553, 'learning_rate': 7.551427678757394e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.75it/s]                                               {'loss': 2.0799, 'grad_norm': 3.305063247680664, 'learning_rate': 7.371631781644121e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.75it/s] 48%|████▊     | 36/75 [00:01<00:01, 24.97it/s]                                               {'loss': 2.2082, 'grad_norm': 4.5270490646362305, 'learning_rate': 7.19183588453085e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.97it/s]                                               {'loss': 2.0139, 'grad_norm': 3.9426045417785645, 'learning_rate': 7.012039987417579e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.97it/s]                                               {'loss': 2.2785, 'grad_norm': 4.048557281494141, 'learning_rate': 6.832244090304308e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.97it/s] 52%|█████▏    | 39/75 [00:01<00:01, 25.18it/s]                                               {'loss': 2.2954, 'grad_norm': 5.877810001373291, 'learning_rate': 6.652448193191037e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.18it/s]                                               {'loss': 2.3243, 'grad_norm': 3.788825035095215, 'learning_rate': 6.472652296077765e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.18it/s]                                               {'loss': 2.0709, 'grad_norm': 3.1855309009552, 'learning_rate': 6.292856398964493e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.18it/s] 56%|█████▌    | 42/75 [00:01<00:01, 25.20it/s]                                               {'loss': 2.0611, 'grad_norm': 4.979519367218018, 'learning_rate': 6.113060501851222e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.20it/s]                                               {'loss': 2.1392, 'grad_norm': 4.096867561340332, 'learning_rate': 5.9332646047379514e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.20it/s]                                               {'loss': 2.156, 'grad_norm': 2.7482237815856934, 'learning_rate': 5.7534687076246803e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.20it/s]                                               {'loss': 2.1426, 'grad_norm': 10.87952995300293, 'learning_rate': 5.5736728105114086e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.20it/s] 61%|██████▏   | 46/75 [00:01<00:01, 26.84it/s]                                               {'loss': 2.0779, 'grad_norm': 3.6190614700317383, 'learning_rate': 5.3938769133981376e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.84it/s]                                               {'loss': 2.1267, 'grad_norm': 3.761361598968506, 'learning_rate': 5.214081016284866e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.84it/s]                                               {'loss': 2.1854, 'grad_norm': 3.790872097015381, 'learning_rate': 5.0342851191715955e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.84it/s] 65%|██████▌   | 49/75 [00:01<00:00, 26.32it/s]                                               {'loss': 2.2146, 'grad_norm': 3.6904687881469727, 'learning_rate': 4.854489222058324e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.32it/s]                                               {'loss': 2.1711, 'grad_norm': 3.57987904548645, 'learning_rate': 4.674693324945053e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.32it/s]                                               {'loss': 2.0806, 'grad_norm': 4.5329976081848145, 'learning_rate': 4.494897427831781e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 26.32it/s] 69%|██████▉   | 52/75 [00:02<00:00, 25.87it/s]                                               {'loss': 2.0363, 'grad_norm': 4.437418460845947, 'learning_rate': 4.31510153071851e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.87it/s]                                               {'loss': 2.1098, 'grad_norm': 3.820596218109131, 'learning_rate': 4.135305633605238e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.87it/s]                                               {'loss': 2.4247, 'grad_norm': 4.250713348388672, 'learning_rate': 3.955509736491967e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.87it/s] 73%|███████▎  | 55/75 [00:02<00:00, 25.60it/s]                                               {'loss': 2.1229, 'grad_norm': 3.7296669483184814, 'learning_rate': 3.775713839378697e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.60it/s]                                               {'loss': 2.4432, 'grad_norm': 3.9112961292266846, 'learning_rate': 3.595917942265425e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.60it/s]                                               {'loss': 2.0189, 'grad_norm': 4.886312484741211, 'learning_rate': 3.416122045152154e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.60it/s] 77%|███████▋  | 58/75 [00:02<00:00, 25.43it/s]                                               {'loss': 2.1428, 'grad_norm': 3.7704527378082275, 'learning_rate': 3.236326148038882e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.43it/s]                                               {'loss': 2.1661, 'grad_norm': 3.5706963539123535, 'learning_rate': 3.056530250925611e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.43it/s]                                               {'loss': 2.3303, 'grad_norm': 14.569961547851562, 'learning_rate': 2.8767343538123402e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.43it/s] 81%|████████▏ | 61/75 [00:02<00:00, 26.01it/s]                                               {'loss': 2.3192, 'grad_norm': 5.100015163421631, 'learning_rate': 2.6969384566990688e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.01it/s]                                               {'loss': 2.0333, 'grad_norm': 3.522930383682251, 'learning_rate': 2.5171425595857977e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.01it/s]                                               {'loss': 2.1279, 'grad_norm': 3.5336380004882812, 'learning_rate': 2.3373466624725263e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.01it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.53it/s]                                               {'loss': 2.0693, 'grad_norm': 4.546695232391357, 'learning_rate': 2.157550765359255e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.53it/s]                                               {'loss': 2.5141, 'grad_norm': 5.1730451583862305, 'learning_rate': 1.9777548682459836e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.53it/s]                                               {'loss': 2.2208, 'grad_norm': 4.202423095703125, 'learning_rate': 1.7979589711327125e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.53it/s] 89%|████████▉ | 67/75 [00:02<00:00, 24.87it/s]                                               {'loss': 2.2269, 'grad_norm': 3.7244105339050293, 'learning_rate': 1.618163074019441e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.87it/s]                                               {'loss': 2.0243, 'grad_norm': 3.366809606552124, 'learning_rate': 1.4383671769061701e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.87it/s]                                               {'loss': 2.2284, 'grad_norm': 3.302888870239258, 'learning_rate': 1.2585712797928989e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.87it/s] 93%|█████████▎| 70/75 [00:02<00:00, 24.73it/s]                                               {'loss': 2.152, 'grad_norm': 3.740663766860962, 'learning_rate': 1.0787753826796275e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.73it/s]                                               {'loss': 1.9991, 'grad_norm': 3.660468101501465, 'learning_rate': 8.989794855663563e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.73it/s]                                               {'loss': 2.1795, 'grad_norm': 3.137723445892334, 'learning_rate': 7.1918358845308504e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.73it/s] 97%|█████████▋| 73/75 [00:02<00:00, 24.77it/s]                                               {'loss': 2.0283, 'grad_norm': 3.146120309829712, 'learning_rate': 5.393876913398137e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.77it/s]                                               {'loss': 2.1164, 'grad_norm': 4.6358771324157715, 'learning_rate': 3.5959179422654252e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.77it/s]                                               {'loss': 1.8913, 'grad_norm': 8.05424690246582, 'learning_rate': 1.7979589711327126e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.77it/s]                                               {'train_runtime': 3.0358, 'train_samples_per_second': 372.226, 'train_steps_per_second': 24.705, 'train_loss': 2.191437921524048, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.77it/s]100%|██████████| 75/75 [00:03<00:00, 24.71it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4824, 'grad_norm': 4.068589210510254, 'learning_rate': 0.00013484692283495344, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.48it/s]                                              {'loss': 2.4568, 'grad_norm': 4.892375469207764, 'learning_rate': 0.00013304896386382074, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.14it/s]  4%|▍         | 3/75 [00:00<00:03, 23.66it/s]                                              {'loss': 2.4176, 'grad_norm': 4.74183464050293, 'learning_rate': 0.00013125100489268801, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.66it/s]                                              {'loss': 2.2978, 'grad_norm': 4.702042102813721, 'learning_rate': 0.0001294530459215553, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 23.66it/s]                                              {'loss': 2.4281, 'grad_norm': 3.8930423259735107, 'learning_rate': 0.0001276550869504226, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.66it/s]  8%|▊         | 6/75 [00:00<00:02, 25.21it/s]                                              {'loss': 2.0923, 'grad_norm': 3.551135540008545, 'learning_rate': 0.00012585712797928987, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.21it/s]                                              {'loss': 2.4232, 'grad_norm': 3.7632322311401367, 'learning_rate': 0.00012405916900815717, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.21it/s]                                              {'loss': 2.2058, 'grad_norm': 3.531317949295044, 'learning_rate': 0.00012226121003702445, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.21it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.39it/s]                                              {'loss': 2.6121, 'grad_norm': 4.804333686828613, 'learning_rate': 0.00012046325106589174, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.39it/s]                                              {'loss': 2.2395, 'grad_norm': 4.317632675170898, 'learning_rate': 0.00011866529209475903, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.39it/s]                                               {'loss': 2.3485, 'grad_norm': 3.4508440494537354, 'learning_rate': 0.00011686733312362632, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.39it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.02it/s]                                               {'loss': 2.0616, 'grad_norm': 3.621732711791992, 'learning_rate': 0.00011506937415249361, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.02it/s]                                               {'loss': 2.348, 'grad_norm': 4.64801549911499, 'learning_rate': 0.00011327141518136088, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.02it/s]                                               {'loss': 2.2799, 'grad_norm': 4.8055901527404785, 'learning_rate': 0.00011147345621022817, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.02it/s]                                               {'loss': 2.5316, 'grad_norm': 8.655827522277832, 'learning_rate': 0.00010967549723909546, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.02it/s] 21%|██▏       | 16/75 [00:00<00:02, 25.38it/s]                                               {'loss': 2.4615, 'grad_norm': 4.661594867706299, 'learning_rate': 0.00010787753826796275, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.38it/s]                                               {'loss': 2.269, 'grad_norm': 4.820140838623047, 'learning_rate': 0.00010607957929683003, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.38it/s]                                               {'loss': 2.232, 'grad_norm': 4.594708442687988, 'learning_rate': 0.00010428162032569732, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.38it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.02it/s]                                               {'loss': 2.5683, 'grad_norm': 4.702078342437744, 'learning_rate': 0.0001024836613545646, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.02it/s]                                               {'loss': 2.3535, 'grad_norm': 3.1477928161621094, 'learning_rate': 0.00010068570238343191, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.02it/s]                                               {'loss': 2.3938, 'grad_norm': 5.113506317138672, 'learning_rate': 9.888774341229919e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.02it/s] 29%|██▉       | 22/75 [00:00<00:02, 24.64it/s]                                               {'loss': 2.2895, 'grad_norm': 3.8258285522460938, 'learning_rate': 9.708978444116647e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.64it/s]                                               {'loss': 2.2844, 'grad_norm': 3.279945135116577, 'learning_rate': 9.529182547003376e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.64it/s]                                               {'loss': 2.1028, 'grad_norm': 4.290658950805664, 'learning_rate': 9.349386649890105e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.64it/s] 33%|███▎      | 25/75 [00:01<00:02, 24.37it/s]                                               {'loss': 2.259, 'grad_norm': 4.3026442527771, 'learning_rate': 9.169590752776834e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.37it/s]                                               {'loss': 2.3737, 'grad_norm': 4.063364505767822, 'learning_rate': 8.989794855663562e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.37it/s]                                               {'loss': 2.1946, 'grad_norm': 4.254602432250977, 'learning_rate': 8.809998958550291e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.37it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.1238, 'grad_norm': 3.936323404312134, 'learning_rate': 8.63020306143702e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.275, 'grad_norm': 4.738888263702393, 'learning_rate': 8.450407164323749e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.1758, 'grad_norm': 6.652272701263428, 'learning_rate': 8.270611267210476e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.57it/s] 41%|████▏     | 31/75 [00:01<00:01, 24.99it/s]                                               {'loss': 2.2558, 'grad_norm': 5.170799255371094, 'learning_rate': 8.090815370097205e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.99it/s]                                               {'loss': 2.4132, 'grad_norm': 4.216763019561768, 'learning_rate': 7.911019472983934e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.99it/s]                                               {'loss': 2.3225, 'grad_norm': 3.8944132328033447, 'learning_rate': 7.731223575870665e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.99it/s] 45%|████▌     | 34/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.4146, 'grad_norm': 4.728159427642822, 'learning_rate': 7.551427678757394e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.2741, 'grad_norm': 3.8074543476104736, 'learning_rate': 7.371631781644121e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.024, 'grad_norm': 3.6156411170959473, 'learning_rate': 7.19183588453085e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.13it/s] 49%|████▉     | 37/75 [00:01<00:01, 24.29it/s]                                               {'loss': 2.2802, 'grad_norm': 3.6364598274230957, 'learning_rate': 7.012039987417579e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.29it/s]                                               {'loss': 2.1895, 'grad_norm': 3.722095012664795, 'learning_rate': 6.832244090304308e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.29it/s]                                               {'loss': 2.2282, 'grad_norm': 3.490065574645996, 'learning_rate': 6.652448193191037e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.29it/s] 53%|█████▎    | 40/75 [00:01<00:01, 24.49it/s]                                               {'loss': 2.195, 'grad_norm': 4.165994644165039, 'learning_rate': 6.472652296077765e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.49it/s]                                               {'loss': 2.3174, 'grad_norm': 5.958255290985107, 'learning_rate': 6.292856398964493e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.49it/s]                                               {'loss': 2.1515, 'grad_norm': 3.1581757068634033, 'learning_rate': 6.113060501851222e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.49it/s] 57%|█████▋    | 43/75 [00:01<00:01, 24.58it/s]                                               {'loss': 2.2376, 'grad_norm': 3.97428822517395, 'learning_rate': 5.9332646047379514e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.58it/s]                                               {'loss': 2.4436, 'grad_norm': 4.8979315757751465, 'learning_rate': 5.7534687076246803e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.58it/s]                                               {'loss': 2.1805, 'grad_norm': 9.55727767944336, 'learning_rate': 5.5736728105114086e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.58it/s]                                               {'loss': 2.281, 'grad_norm': 3.598484754562378, 'learning_rate': 5.3938769133981376e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.58it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.60it/s]                                               {'loss': 2.3646, 'grad_norm': 4.478926658630371, 'learning_rate': 5.214081016284866e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.60it/s]                                               {'loss': 2.1822, 'grad_norm': 4.116509914398193, 'learning_rate': 5.0342851191715955e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.60it/s]                                               {'loss': 2.2459, 'grad_norm': 3.7184784412384033, 'learning_rate': 4.854489222058324e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.60it/s] 67%|██████▋   | 50/75 [00:01<00:00, 26.23it/s]                                               {'loss': 1.9666, 'grad_norm': 3.2182412147521973, 'learning_rate': 4.674693324945053e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.23it/s]                                               {'loss': 2.1313, 'grad_norm': 4.741962432861328, 'learning_rate': 4.494897427831781e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 26.23it/s]                                               {'loss': 2.3179, 'grad_norm': 3.787959575653076, 'learning_rate': 4.31510153071851e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 26.23it/s] 71%|███████   | 53/75 [00:02<00:00, 25.91it/s]                                               {'loss': 2.4865, 'grad_norm': 4.100539684295654, 'learning_rate': 4.135305633605238e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.91it/s]                                               {'loss': 2.3386, 'grad_norm': 4.491824626922607, 'learning_rate': 3.955509736491967e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.91it/s]                                               {'loss': 2.286, 'grad_norm': 3.8834376335144043, 'learning_rate': 3.775713839378697e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.91it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.43it/s]                                               {'loss': 2.1708, 'grad_norm': 4.318513870239258, 'learning_rate': 3.595917942265425e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.43it/s]                                               {'loss': 2.3535, 'grad_norm': 4.472209453582764, 'learning_rate': 3.416122045152154e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.43it/s]                                               {'loss': 2.3358, 'grad_norm': 5.5359625816345215, 'learning_rate': 3.236326148038882e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.43it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.17it/s]                                               {'loss': 2.0951, 'grad_norm': 5.8090291023254395, 'learning_rate': 3.056530250925611e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.17it/s]                                               {'loss': 2.2647, 'grad_norm': 17.659984588623047, 'learning_rate': 2.8767343538123402e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.17it/s]                                               {'loss': 2.1616, 'grad_norm': 3.4792404174804688, 'learning_rate': 2.6969384566990688e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.17it/s]                                               {'loss': 2.3262, 'grad_norm': 4.842857837677002, 'learning_rate': 2.5171425595857977e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.17it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.57it/s]                                               {'loss': 2.1997, 'grad_norm': 4.071529865264893, 'learning_rate': 2.3373466624725263e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.57it/s]                                               {'loss': 1.9828, 'grad_norm': 3.231933832168579, 'learning_rate': 2.157550765359255e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.57it/s]                                               {'loss': 2.3959, 'grad_norm': 4.041511535644531, 'learning_rate': 1.9777548682459836e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.57it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.22it/s]                                               {'loss': 2.284, 'grad_norm': 3.5913126468658447, 'learning_rate': 1.7979589711327125e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.22it/s]                                               {'loss': 2.1919, 'grad_norm': 3.7415530681610107, 'learning_rate': 1.618163074019441e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.22it/s]                                               {'loss': 2.2813, 'grad_norm': 4.130746841430664, 'learning_rate': 1.4383671769061701e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 26.22it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.04it/s]                                               {'loss': 2.1466, 'grad_norm': 4.051328659057617, 'learning_rate': 1.2585712797928989e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 26.04it/s]                                               {'loss': 2.1749, 'grad_norm': 4.558671474456787, 'learning_rate': 1.0787753826796275e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 26.04it/s]                                               {'loss': 2.3332, 'grad_norm': 5.428928852081299, 'learning_rate': 8.989794855663563e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 26.04it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.94it/s]                                               {'loss': 2.2991, 'grad_norm': 4.205250263214111, 'learning_rate': 7.1918358845308504e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.94it/s]                                               {'loss': 2.3479, 'grad_norm': 4.084489822387695, 'learning_rate': 5.393876913398137e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.94it/s]                                               {'loss': 2.0501, 'grad_norm': 3.6965601444244385, 'learning_rate': 3.5959179422654252e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.94it/s]                                               {'loss': 2.2753, 'grad_norm': 19.076635360717773, 'learning_rate': 1.7979589711327126e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.94it/s]                                               {'train_runtime': 3.0498, 'train_samples_per_second': 370.519, 'train_steps_per_second': 24.592, 'train_loss': 2.2770716826121014, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.94it/s]100%|██████████| 75/75 [00:03<00:00, 24.59it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2475, 'grad_norm': 3.2123165130615234, 'learning_rate': 0.00013484692283495344, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 22.31it/s]                                              {'loss': 2.4897, 'grad_norm': 3.9177944660186768, 'learning_rate': 0.00013304896386382074, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.16it/s]  4%|▍         | 3/75 [00:00<00:03, 23.31it/s]                                              {'loss': 2.1932, 'grad_norm': 4.033377170562744, 'learning_rate': 0.00013125100489268801, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.31it/s]                                              {'loss': 2.0815, 'grad_norm': 4.445708751678467, 'learning_rate': 0.0001294530459215553, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 23.31it/s]                                              {'loss': 2.2059, 'grad_norm': 3.507502317428589, 'learning_rate': 0.0001276550869504226, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 23.31it/s]  8%|▊         | 6/75 [00:00<00:02, 24.20it/s]                                              {'loss': 2.4842, 'grad_norm': 4.0115132331848145, 'learning_rate': 0.00012585712797928987, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.20it/s]                                              {'loss': 2.3484, 'grad_norm': 3.5863776206970215, 'learning_rate': 0.00012405916900815717, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.20it/s]                                              {'loss': 2.136, 'grad_norm': 3.8110902309417725, 'learning_rate': 0.00012226121003702445, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.20it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.71it/s]                                              {'loss': 2.2217, 'grad_norm': 3.518569231033325, 'learning_rate': 0.00012046325106589174, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.71it/s]                                              {'loss': 2.1679, 'grad_norm': 4.721151828765869, 'learning_rate': 0.00011866529209475903, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.71it/s]                                               {'loss': 2.3328, 'grad_norm': 4.151356220245361, 'learning_rate': 0.00011686733312362632, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.71it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.91it/s]                                               {'loss': 2.28, 'grad_norm': 3.801589012145996, 'learning_rate': 0.00011506937415249361, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.91it/s]                                               {'loss': 2.3637, 'grad_norm': 3.7817375659942627, 'learning_rate': 0.00011327141518136088, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.91it/s]                                               {'loss': 2.3128, 'grad_norm': 4.72264289855957, 'learning_rate': 0.00011147345621022817, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.91it/s]                                               {'loss': 2.5665, 'grad_norm': 11.56764030456543, 'learning_rate': 0.00010967549723909546, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.91it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.17it/s]                                               {'loss': 2.2752, 'grad_norm': 4.088305950164795, 'learning_rate': 0.00010787753826796275, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.17it/s]                                               {'loss': 2.3808, 'grad_norm': 4.576362133026123, 'learning_rate': 0.00010607957929683003, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.17it/s]                                               {'loss': 2.325, 'grad_norm': 4.727193832397461, 'learning_rate': 0.00010428162032569732, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.17it/s] 25%|██▌       | 19/75 [00:00<00:02, 24.73it/s]                                               {'loss': 2.2265, 'grad_norm': 3.2357335090637207, 'learning_rate': 0.0001024836613545646, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.73it/s]                                               {'loss': 2.3643, 'grad_norm': 3.741196870803833, 'learning_rate': 0.00010068570238343191, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.73it/s]                                               {'loss': 2.368, 'grad_norm': 3.703033208847046, 'learning_rate': 9.888774341229919e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.73it/s] 29%|██▉       | 22/75 [00:00<00:02, 24.90it/s]                                               {'loss': 2.2388, 'grad_norm': 4.146681785583496, 'learning_rate': 9.708978444116647e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.90it/s]                                               {'loss': 2.2968, 'grad_norm': 3.926565170288086, 'learning_rate': 9.529182547003376e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.90it/s]                                               {'loss': 2.1491, 'grad_norm': 4.226370811462402, 'learning_rate': 9.349386649890105e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.90it/s] 33%|███▎      | 25/75 [00:01<00:02, 24.93it/s]                                               {'loss': 2.2644, 'grad_norm': 3.8608686923980713, 'learning_rate': 9.169590752776834e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.93it/s]                                               {'loss': 2.1208, 'grad_norm': 4.093325614929199, 'learning_rate': 8.989794855663562e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.93it/s]                                               {'loss': 2.1232, 'grad_norm': 4.214817523956299, 'learning_rate': 8.809998958550291e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.93it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.26it/s]                                               {'loss': 2.1504, 'grad_norm': 3.1012721061706543, 'learning_rate': 8.63020306143702e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.26it/s]                                               {'loss': 2.0474, 'grad_norm': 3.714017152786255, 'learning_rate': 8.450407164323749e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.26it/s]                                               {'loss': 2.8379, 'grad_norm': 13.373106002807617, 'learning_rate': 8.270611267210476e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.26it/s]                                               {'loss': 2.0613, 'grad_norm': 3.4258222579956055, 'learning_rate': 8.090815370097205e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.26it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.67it/s]                                               {'loss': 2.3512, 'grad_norm': 3.8385486602783203, 'learning_rate': 7.911019472983934e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.67it/s]                                               {'loss': 2.3716, 'grad_norm': 3.825756311416626, 'learning_rate': 7.731223575870665e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.67it/s]                                               {'loss': 2.2604, 'grad_norm': 3.5223681926727295, 'learning_rate': 7.551427678757394e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.67it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.62it/s]                                               {'loss': 1.997, 'grad_norm': 2.8794186115264893, 'learning_rate': 7.371631781644121e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.62it/s]                                               {'loss': 2.1663, 'grad_norm': 4.336002349853516, 'learning_rate': 7.19183588453085e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.62it/s]                                               {'loss': 2.256, 'grad_norm': 3.281647205352783, 'learning_rate': 7.012039987417579e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.62it/s] 51%|█████     | 38/75 [00:01<00:01, 26.26it/s]                                               {'loss': 2.3204, 'grad_norm': 3.990872621536255, 'learning_rate': 6.832244090304308e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.26it/s]                                               {'loss': 2.1752, 'grad_norm': 3.6703455448150635, 'learning_rate': 6.652448193191037e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.26it/s]                                               {'loss': 2.161, 'grad_norm': 3.525799512863159, 'learning_rate': 6.472652296077765e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 26.26it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.71it/s]                                               {'loss': 2.1488, 'grad_norm': 3.208409547805786, 'learning_rate': 6.292856398964493e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.71it/s]                                               {'loss': 2.2564, 'grad_norm': 3.5946366786956787, 'learning_rate': 6.113060501851222e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.71it/s]                                               {'loss': 1.9637, 'grad_norm': 4.21779203414917, 'learning_rate': 5.9332646047379514e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.71it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.1889, 'grad_norm': 3.215925693511963, 'learning_rate': 5.7534687076246803e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.8011, 'grad_norm': 13.81602954864502, 'learning_rate': 5.5736728105114086e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.2648, 'grad_norm': 3.228916645050049, 'learning_rate': 5.3938769133981376e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.4166, 'grad_norm': 4.280325889587402, 'learning_rate': 5.214081016284866e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.31it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.88it/s]                                               {'loss': 2.2554, 'grad_norm': 5.0681281089782715, 'learning_rate': 5.0342851191715955e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.88it/s]                                               {'loss': 2.1415, 'grad_norm': 3.529597282409668, 'learning_rate': 4.854489222058324e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.88it/s]                                               {'loss': 2.0328, 'grad_norm': 3.028214693069458, 'learning_rate': 4.674693324945053e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.88it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.41it/s]                                               {'loss': 2.1479, 'grad_norm': 3.3067076206207275, 'learning_rate': 4.494897427831781e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 26.41it/s]                                               {'loss': 2.2112, 'grad_norm': 3.955922842025757, 'learning_rate': 4.31510153071851e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 26.41it/s]                                               {'loss': 1.9446, 'grad_norm': 4.038684844970703, 'learning_rate': 4.135305633605238e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 26.41it/s] 72%|███████▏  | 54/75 [00:02<00:00, 26.17it/s]                                               {'loss': 2.496, 'grad_norm': 4.732161045074463, 'learning_rate': 3.955509736491967e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 26.17it/s]                                               {'loss': 2.04, 'grad_norm': 3.6127119064331055, 'learning_rate': 3.775713839378697e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 26.17it/s]                                               {'loss': 2.1678, 'grad_norm': 3.565629243850708, 'learning_rate': 3.595917942265425e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 26.17it/s] 76%|███████▌  | 57/75 [00:02<00:00, 25.71it/s]                                               {'loss': 2.119, 'grad_norm': 3.968165874481201, 'learning_rate': 3.416122045152154e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.71it/s]                                               {'loss': 2.3156, 'grad_norm': 2.8250107765197754, 'learning_rate': 3.236326148038882e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.71it/s]                                               {'loss': 2.0965, 'grad_norm': 3.5855133533477783, 'learning_rate': 3.056530250925611e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.71it/s] 80%|████████  | 60/75 [00:02<00:00, 26.66it/s]                                               {'loss': 2.0742, 'grad_norm': 9.466031074523926, 'learning_rate': 2.8767343538123402e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 26.66it/s]                                               {'loss': 1.9929, 'grad_norm': 3.411151170730591, 'learning_rate': 2.6969384566990688e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.66it/s]                                               {'loss': 2.1232, 'grad_norm': 4.189870357513428, 'learning_rate': 2.5171425595857977e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.66it/s] 84%|████████▍ | 63/75 [00:02<00:00, 25.68it/s]                                               {'loss': 2.278, 'grad_norm': 3.9409005641937256, 'learning_rate': 2.3373466624725263e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.68it/s]                                               {'loss': 2.0558, 'grad_norm': 2.955691337585449, 'learning_rate': 2.157550765359255e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.68it/s]                                               {'loss': 2.213, 'grad_norm': 3.980484962463379, 'learning_rate': 1.9777548682459836e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.68it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.12it/s]                                               {'loss': 2.1417, 'grad_norm': 3.6046957969665527, 'learning_rate': 1.7979589711327125e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.12it/s]                                               {'loss': 2.2419, 'grad_norm': 4.573389530181885, 'learning_rate': 1.618163074019441e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.12it/s]                                               {'loss': 2.2596, 'grad_norm': 3.9295761585235596, 'learning_rate': 1.4383671769061701e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.12it/s] 92%|█████████▏| 69/75 [00:02<00:00, 25.27it/s]                                               {'loss': 2.3936, 'grad_norm': 4.12667989730835, 'learning_rate': 1.2585712797928989e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.27it/s]                                               {'loss': 2.0212, 'grad_norm': 2.8604111671447754, 'learning_rate': 1.0787753826796275e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.27it/s]                                               {'loss': 2.0323, 'grad_norm': 3.281132221221924, 'learning_rate': 8.989794855663563e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.27it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.64it/s]                                               {'loss': 2.2524, 'grad_norm': 3.741135835647583, 'learning_rate': 7.1918358845308504e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.64it/s]                                               {'loss': 2.3741, 'grad_norm': 3.998414993286133, 'learning_rate': 5.393876913398137e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.64it/s]                                               {'loss': 2.2186, 'grad_norm': 3.6104824542999268, 'learning_rate': 3.5959179422654252e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.64it/s]                                               {'loss': 1.8217, 'grad_norm': 10.45200252532959, 'learning_rate': 1.7979589711327126e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.64it/s]                                               {'train_runtime': 3.0143, 'train_samples_per_second': 374.886, 'train_steps_per_second': 24.882, 'train_loss': 2.229644462267558, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.64it/s]100%|██████████| 75/75 [00:03<00:00, 24.88it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:47
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2648, 'grad_norm': 4.680696964263916, 'learning_rate': 0.00013484692283495344, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.76it/s]                                              {'loss': 2.4161, 'grad_norm': 3.6926560401916504, 'learning_rate': 0.00013304896386382074, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.99it/s]  4%|▍         | 3/75 [00:00<00:02, 25.27it/s]                                              {'loss': 2.4721, 'grad_norm': 4.32813835144043, 'learning_rate': 0.00013125100489268801, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.27it/s]                                              {'loss': 2.3506, 'grad_norm': 4.33494758605957, 'learning_rate': 0.0001294530459215553, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.27it/s]                                              {'loss': 2.4644, 'grad_norm': 3.750274658203125, 'learning_rate': 0.0001276550869504226, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.27it/s]  8%|▊         | 6/75 [00:00<00:02, 24.27it/s]                                              {'loss': 2.2583, 'grad_norm': 4.309295177459717, 'learning_rate': 0.00012585712797928987, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.27it/s]                                              {'loss': 2.3704, 'grad_norm': 4.040919303894043, 'learning_rate': 0.00012405916900815717, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.27it/s]                                              {'loss': 2.2325, 'grad_norm': 4.255744934082031, 'learning_rate': 0.00012226121003702445, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.27it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.88it/s]                                              {'loss': 2.3586, 'grad_norm': 4.52219820022583, 'learning_rate': 0.00012046325106589174, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.88it/s]                                              {'loss': 2.4309, 'grad_norm': 3.5044195652008057, 'learning_rate': 0.00011866529209475903, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.88it/s]                                               {'loss': 2.5349, 'grad_norm': 4.052324295043945, 'learning_rate': 0.00011686733312362632, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.88it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.89it/s]                                               {'loss': 2.1688, 'grad_norm': 4.119884967803955, 'learning_rate': 0.00011506937415249361, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.89it/s]                                               {'loss': 2.4599, 'grad_norm': 4.544444561004639, 'learning_rate': 0.00011327141518136088, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.89it/s]                                               {'loss': 2.2958, 'grad_norm': 3.806962013244629, 'learning_rate': 0.00011147345621022817, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.89it/s]                                               {'loss': 2.6697, 'grad_norm': 10.838228225708008, 'learning_rate': 0.00010967549723909546, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.89it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.85it/s]                                               {'loss': 2.3326, 'grad_norm': 4.299829483032227, 'learning_rate': 0.00010787753826796275, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.85it/s]                                               {'loss': 2.3939, 'grad_norm': 4.720630168914795, 'learning_rate': 0.00010607957929683003, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.85it/s]                                               {'loss': 2.2922, 'grad_norm': 4.013779163360596, 'learning_rate': 0.00010428162032569732, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.85it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.47it/s]                                               {'loss': 2.155, 'grad_norm': 3.771672248840332, 'learning_rate': 0.0001024836613545646, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.47it/s]                                               {'loss': 2.5017, 'grad_norm': 5.357526779174805, 'learning_rate': 0.00010068570238343191, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.47it/s]                                               {'loss': 2.2221, 'grad_norm': 3.8770973682403564, 'learning_rate': 9.888774341229919e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.47it/s] 29%|██▉       | 22/75 [00:00<00:02, 26.22it/s]                                               {'loss': 2.2282, 'grad_norm': 4.362391471862793, 'learning_rate': 9.708978444116647e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 26.22it/s]                                               {'loss': 2.3904, 'grad_norm': 3.4574053287506104, 'learning_rate': 9.529182547003376e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 26.22it/s]                                               {'loss': 2.2423, 'grad_norm': 3.776916980743408, 'learning_rate': 9.349386649890105e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 26.22it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.82it/s]                                               {'loss': 2.4659, 'grad_norm': 4.380682468414307, 'learning_rate': 9.169590752776834e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.82it/s]                                               {'loss': 2.1048, 'grad_norm': 3.249246597290039, 'learning_rate': 8.989794855663562e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.82it/s]                                               {'loss': 2.2591, 'grad_norm': 4.73426628112793, 'learning_rate': 8.809998958550291e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.82it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.89it/s]                                               {'loss': 2.1949, 'grad_norm': 4.887542247772217, 'learning_rate': 8.63020306143702e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.89it/s]                                               {'loss': 2.3978, 'grad_norm': 3.86151123046875, 'learning_rate': 8.450407164323749e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.89it/s]                                               {'loss': 2.5196, 'grad_norm': 10.668631553649902, 'learning_rate': 8.270611267210476e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.89it/s]                                               {'loss': 2.0675, 'grad_norm': 3.748232364654541, 'learning_rate': 8.090815370097205e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.89it/s] 43%|████▎     | 32/75 [00:01<00:01, 27.01it/s]                                               {'loss': 2.195, 'grad_norm': 4.088025093078613, 'learning_rate': 7.911019472983934e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 27.01it/s]                                               {'loss': 2.404, 'grad_norm': 3.549955368041992, 'learning_rate': 7.731223575870665e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 27.01it/s]                                               {'loss': 2.2519, 'grad_norm': 6.15297269821167, 'learning_rate': 7.551427678757394e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 27.01it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.14it/s]                                               {'loss': 2.364, 'grad_norm': 4.518661022186279, 'learning_rate': 7.371631781644121e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.14it/s]                                               {'loss': 2.5349, 'grad_norm': 4.914524555206299, 'learning_rate': 7.19183588453085e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.14it/s]                                               {'loss': 2.1323, 'grad_norm': 3.550257444381714, 'learning_rate': 7.012039987417579e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.14it/s] 51%|█████     | 38/75 [00:01<00:01, 25.67it/s]                                               {'loss': 2.1181, 'grad_norm': 2.9929590225219727, 'learning_rate': 6.832244090304308e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.67it/s]                                               {'loss': 2.172, 'grad_norm': 3.911940097808838, 'learning_rate': 6.652448193191037e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.67it/s]                                               {'loss': 2.2397, 'grad_norm': 4.121105670928955, 'learning_rate': 6.472652296077765e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.67it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.79it/s]                                               {'loss': 2.2906, 'grad_norm': 3.73150897026062, 'learning_rate': 6.292856398964493e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.79it/s]                                               {'loss': 2.4593, 'grad_norm': 3.2557179927825928, 'learning_rate': 6.113060501851222e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.79it/s]                                               {'loss': 2.102, 'grad_norm': 4.3323516845703125, 'learning_rate': 5.9332646047379514e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.79it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.79it/s]                                               {'loss': 2.421, 'grad_norm': 5.645509719848633, 'learning_rate': 5.7534687076246803e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.79it/s]                                               {'loss': 2.3542, 'grad_norm': 15.586708068847656, 'learning_rate': 5.5736728105114086e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.79it/s]                                               {'loss': 2.2561, 'grad_norm': 3.3374078273773193, 'learning_rate': 5.3938769133981376e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.79it/s]                                               {'loss': 2.2609, 'grad_norm': 3.9128530025482178, 'learning_rate': 5.214081016284866e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.79it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.01it/s]                                               {'loss': 2.1425, 'grad_norm': 3.7505006790161133, 'learning_rate': 5.0342851191715955e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 27.01it/s]                                               {'loss': 2.2986, 'grad_norm': 4.715240955352783, 'learning_rate': 4.854489222058324e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 27.01it/s]                                               {'loss': 2.356, 'grad_norm': 5.114261150360107, 'learning_rate': 4.674693324945053e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 27.01it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.43it/s]                                               {'loss': 2.17, 'grad_norm': 4.008195400238037, 'learning_rate': 4.494897427831781e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 26.43it/s]                                               {'loss': 2.4659, 'grad_norm': 4.072745323181152, 'learning_rate': 4.31510153071851e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 26.43it/s]                                               {'loss': 2.1151, 'grad_norm': 3.2767839431762695, 'learning_rate': 4.135305633605238e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 26.43it/s] 72%|███████▏  | 54/75 [00:02<00:00, 24.43it/s]                                               {'loss': 2.2863, 'grad_norm': 4.629051208496094, 'learning_rate': 3.955509736491967e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.43it/s]                                               {'loss': 2.5016, 'grad_norm': 4.095224380493164, 'learning_rate': 3.775713839378697e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.43it/s]                                               {'loss': 2.2644, 'grad_norm': 4.44179105758667, 'learning_rate': 3.595917942265425e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.43it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.57it/s]                                               {'loss': 2.1614, 'grad_norm': 4.084072589874268, 'learning_rate': 3.416122045152154e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.57it/s]                                               {'loss': 2.1452, 'grad_norm': 3.7155325412750244, 'learning_rate': 3.236326148038882e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.57it/s]                                               {'loss': 2.318, 'grad_norm': 4.654490947723389, 'learning_rate': 3.056530250925611e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.57it/s] 80%|████████  | 60/75 [00:02<00:00, 25.75it/s]                                               {'loss': 1.8062, 'grad_norm': 13.103294372558594, 'learning_rate': 2.8767343538123402e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.75it/s]                                               {'loss': 2.3992, 'grad_norm': 5.24698543548584, 'learning_rate': 2.6969384566990688e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.75it/s]                                               {'loss': 2.219, 'grad_norm': 3.641697406768799, 'learning_rate': 2.5171425595857977e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.75it/s] 84%|████████▍ | 63/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.1572, 'grad_norm': 4.728588104248047, 'learning_rate': 2.3373466624725263e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.1752, 'grad_norm': 2.6076819896698, 'learning_rate': 2.157550765359255e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.1746, 'grad_norm': 3.703216075897217, 'learning_rate': 1.9777548682459836e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.99it/s] 88%|████████▊ | 66/75 [00:02<00:00, 24.62it/s]                                               {'loss': 2.0903, 'grad_norm': 3.0696842670440674, 'learning_rate': 1.7979589711327125e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.62it/s]                                               {'loss': 2.3034, 'grad_norm': 4.088699817657471, 'learning_rate': 1.618163074019441e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.62it/s]                                               {'loss': 2.0903, 'grad_norm': 5.262627124786377, 'learning_rate': 1.4383671769061701e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.62it/s] 92%|█████████▏| 69/75 [00:02<00:00, 24.84it/s]                                               {'loss': 2.2335, 'grad_norm': 3.827223539352417, 'learning_rate': 1.2585712797928989e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.84it/s]                                               {'loss': 2.1189, 'grad_norm': 4.198265552520752, 'learning_rate': 1.0787753826796275e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.84it/s]                                               {'loss': 2.2973, 'grad_norm': 4.097625255584717, 'learning_rate': 8.989794855663563e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.84it/s] 96%|█████████▌| 72/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.4545, 'grad_norm': 3.834996461868286, 'learning_rate': 7.1918358845308504e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.2697, 'grad_norm': 4.225191593170166, 'learning_rate': 5.393876913398137e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.244, 'grad_norm': 3.8829431533813477, 'learning_rate': 3.5959179422654252e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.96it/s]100%|██████████| 75/75 [00:02<00:00, 25.10it/s]                                               {'loss': 2.3431, 'grad_norm': 10.610225677490234, 'learning_rate': 1.7979589711327126e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.10it/s]                                               {'train_runtime': 3.065, 'train_samples_per_second': 368.683, 'train_steps_per_second': 24.47, 'train_loss': 2.288969519933065, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.10it/s]100%|██████████| 75/75 [00:03<00:00, 24.47it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3892, 'grad_norm': 5.444638252258301, 'learning_rate': 0.00013484692283495344, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 21.20it/s]                                              {'loss': 2.2586, 'grad_norm': 3.9388060569763184, 'learning_rate': 0.00013304896386382074, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 21.05it/s]  4%|▍         | 3/75 [00:00<00:03, 22.65it/s]                                              {'loss': 2.416, 'grad_norm': 5.487458229064941, 'learning_rate': 0.00013125100489268801, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 22.65it/s]                                              {'loss': 2.2292, 'grad_norm': 3.8496901988983154, 'learning_rate': 0.0001294530459215553, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 22.65it/s]                                              {'loss': 2.4528, 'grad_norm': 3.919259548187256, 'learning_rate': 0.0001276550869504226, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.65it/s]  8%|▊         | 6/75 [00:00<00:02, 24.08it/s]                                              {'loss': 2.2215, 'grad_norm': 3.7141849994659424, 'learning_rate': 0.00012585712797928987, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.08it/s]                                              {'loss': 2.1479, 'grad_norm': 3.9694101810455322, 'learning_rate': 0.00012405916900815717, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.08it/s]                                              {'loss': 2.4671, 'grad_norm': 4.060868740081787, 'learning_rate': 0.00012226121003702445, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.08it/s] 12%|█▏        | 9/75 [00:00<00:02, 23.97it/s]                                              {'loss': 2.4059, 'grad_norm': 4.018087387084961, 'learning_rate': 0.00012046325106589174, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.97it/s]                                              {'loss': 2.2281, 'grad_norm': 3.5743143558502197, 'learning_rate': 0.00011866529209475903, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.97it/s]                                               {'loss': 2.3049, 'grad_norm': 3.987877130508423, 'learning_rate': 0.00011686733312362632, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.97it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.09it/s]                                               {'loss': 2.3755, 'grad_norm': 3.7795767784118652, 'learning_rate': 0.00011506937415249361, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.09it/s]                                               {'loss': 2.3722, 'grad_norm': 3.232583522796631, 'learning_rate': 0.00011327141518136088, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.09it/s]                                               {'loss': 2.2422, 'grad_norm': 3.79504656791687, 'learning_rate': 0.00011147345621022817, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.09it/s]                                               {'loss': 3.0903, 'grad_norm': 20.20038414001465, 'learning_rate': 0.00010967549723909546, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.09it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.76it/s]                                               {'loss': 2.6261, 'grad_norm': 4.963923454284668, 'learning_rate': 0.00010787753826796275, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.76it/s]                                               {'loss': 2.2027, 'grad_norm': 4.183050632476807, 'learning_rate': 0.00010607957929683003, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.76it/s]                                               {'loss': 2.264, 'grad_norm': 2.9972407817840576, 'learning_rate': 0.00010428162032569732, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.76it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.35it/s]                                               {'loss': 2.3031, 'grad_norm': 4.395986080169678, 'learning_rate': 0.0001024836613545646, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.35it/s]                                               {'loss': 2.4771, 'grad_norm': 5.2555036544799805, 'learning_rate': 0.00010068570238343191, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.35it/s]                                               {'loss': 2.3817, 'grad_norm': 3.8945672512054443, 'learning_rate': 9.888774341229919e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.35it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.58it/s]                                               {'loss': 2.2729, 'grad_norm': 4.362180233001709, 'learning_rate': 9.708978444116647e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.58it/s]                                               {'loss': 2.1755, 'grad_norm': 3.4605908393859863, 'learning_rate': 9.529182547003376e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.58it/s]                                               {'loss': 2.1337, 'grad_norm': 3.6042442321777344, 'learning_rate': 9.349386649890105e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.58it/s] 33%|███▎      | 25/75 [00:00<00:02, 24.93it/s]                                               {'loss': 2.1891, 'grad_norm': 3.5550429821014404, 'learning_rate': 9.169590752776834e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.93it/s]                                               {'loss': 2.2864, 'grad_norm': 4.189309120178223, 'learning_rate': 8.989794855663562e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.93it/s]                                               {'loss': 2.2078, 'grad_norm': 3.833176851272583, 'learning_rate': 8.809998958550291e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.93it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.80it/s]                                               {'loss': 2.2634, 'grad_norm': 4.32578182220459, 'learning_rate': 8.63020306143702e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.80it/s]                                               {'loss': 2.1697, 'grad_norm': 3.6544573307037354, 'learning_rate': 8.450407164323749e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.80it/s]                                               {'loss': 2.6212, 'grad_norm': 9.222552299499512, 'learning_rate': 8.270611267210476e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.80it/s]                                               {'loss': 2.3121, 'grad_norm': 4.497533321380615, 'learning_rate': 8.090815370097205e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.80it/s] 43%|████▎     | 32/75 [00:01<00:01, 25.96it/s]                                               {'loss': 2.0435, 'grad_norm': 4.02318000793457, 'learning_rate': 7.911019472983934e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.96it/s]                                               {'loss': 2.1261, 'grad_norm': 3.5233092308044434, 'learning_rate': 7.731223575870665e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.96it/s]                                               {'loss': 2.2287, 'grad_norm': 3.431199789047241, 'learning_rate': 7.551427678757394e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.96it/s] 47%|████▋     | 35/75 [00:01<00:01, 24.98it/s]                                               {'loss': 2.3423, 'grad_norm': 4.206964015960693, 'learning_rate': 7.371631781644121e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.98it/s]                                               {'loss': 2.0485, 'grad_norm': 4.303011894226074, 'learning_rate': 7.19183588453085e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.98it/s]                                               {'loss': 2.454, 'grad_norm': 4.690633296966553, 'learning_rate': 7.012039987417579e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.98it/s] 51%|█████     | 38/75 [00:01<00:01, 24.70it/s]                                               {'loss': 2.4197, 'grad_norm': 3.948711633682251, 'learning_rate': 6.832244090304308e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.70it/s]                                               {'loss': 2.2343, 'grad_norm': 3.2877016067504883, 'learning_rate': 6.652448193191037e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.70it/s]                                               {'loss': 2.2719, 'grad_norm': 3.7192587852478027, 'learning_rate': 6.472652296077765e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.70it/s] 55%|█████▍    | 41/75 [00:01<00:01, 24.23it/s]                                               {'loss': 2.1194, 'grad_norm': 2.9296538829803467, 'learning_rate': 6.292856398964493e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.23it/s]                                               {'loss': 2.4407, 'grad_norm': 4.633761405944824, 'learning_rate': 6.113060501851222e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.23it/s]                                               {'loss': 2.302, 'grad_norm': 3.582271099090576, 'learning_rate': 5.9332646047379514e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.23it/s] 59%|█████▊    | 44/75 [00:01<00:01, 24.41it/s]                                               {'loss': 2.2442, 'grad_norm': 3.7728211879730225, 'learning_rate': 5.7534687076246803e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.41it/s]                                               {'loss': 2.0598, 'grad_norm': 9.71169376373291, 'learning_rate': 5.5736728105114086e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.41it/s]                                               {'loss': 2.4316, 'grad_norm': 3.936403751373291, 'learning_rate': 5.3938769133981376e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.41it/s]                                               {'loss': 2.2321, 'grad_norm': 2.8496274948120117, 'learning_rate': 5.214081016284866e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 24.41it/s] 64%|██████▍   | 48/75 [00:01<00:01, 25.92it/s]                                               {'loss': 2.3301, 'grad_norm': 3.881546974182129, 'learning_rate': 5.0342851191715955e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.92it/s]                                               {'loss': 2.3131, 'grad_norm': 4.3763885498046875, 'learning_rate': 4.854489222058324e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.92it/s]                                               {'loss': 2.4137, 'grad_norm': 3.8586297035217285, 'learning_rate': 4.674693324945053e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.92it/s] 68%|██████▊   | 51/75 [00:02<00:00, 25.55it/s]                                               {'loss': 2.1083, 'grad_norm': 4.283904075622559, 'learning_rate': 4.494897427831781e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.55it/s]                                               {'loss': 2.1412, 'grad_norm': 3.829348564147949, 'learning_rate': 4.31510153071851e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.55it/s]                                               {'loss': 2.1685, 'grad_norm': 3.4862663745880127, 'learning_rate': 4.135305633605238e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.55it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.61it/s]                                               {'loss': 2.2205, 'grad_norm': 4.259100437164307, 'learning_rate': 3.955509736491967e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.61it/s]                                               {'loss': 2.4365, 'grad_norm': 3.5240318775177, 'learning_rate': 3.775713839378697e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.61it/s]                                               {'loss': 2.2177, 'grad_norm': 3.9195187091827393, 'learning_rate': 3.595917942265425e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.61it/s] 76%|███████▌  | 57/75 [00:02<00:00, 25.66it/s]                                               {'loss': 2.1092, 'grad_norm': 4.632874965667725, 'learning_rate': 3.416122045152154e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.66it/s]                                               {'loss': 2.3451, 'grad_norm': 5.152449131011963, 'learning_rate': 3.236326148038882e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.66it/s]                                               {'loss': 2.0398, 'grad_norm': 3.345115900039673, 'learning_rate': 3.056530250925611e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.66it/s] 80%|████████  | 60/75 [00:02<00:00, 26.53it/s]                                               {'loss': 2.373, 'grad_norm': 20.09366226196289, 'learning_rate': 2.8767343538123402e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 26.53it/s]                                               {'loss': 2.2892, 'grad_norm': 4.016061782836914, 'learning_rate': 2.6969384566990688e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.53it/s]                                               {'loss': 2.1875, 'grad_norm': 3.0741167068481445, 'learning_rate': 2.5171425595857977e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.53it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.08it/s]                                               {'loss': 2.2016, 'grad_norm': 3.5156807899475098, 'learning_rate': 2.3373466624725263e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.08it/s]                                               {'loss': 2.2039, 'grad_norm': 3.6618940830230713, 'learning_rate': 2.157550765359255e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.08it/s]                                               {'loss': 2.0271, 'grad_norm': 3.237164258956909, 'learning_rate': 1.9777548682459836e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.08it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.86it/s]                                               {'loss': 2.2792, 'grad_norm': 5.138267517089844, 'learning_rate': 1.7979589711327125e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.86it/s]                                               {'loss': 2.1982, 'grad_norm': 4.19740629196167, 'learning_rate': 1.618163074019441e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.86it/s]                                               {'loss': 2.1933, 'grad_norm': 4.601531028747559, 'learning_rate': 1.4383671769061701e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.86it/s] 92%|█████████▏| 69/75 [00:02<00:00, 25.73it/s]                                               {'loss': 2.1716, 'grad_norm': 3.505049228668213, 'learning_rate': 1.2585712797928989e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.73it/s]                                               {'loss': 2.3327, 'grad_norm': 4.823398590087891, 'learning_rate': 1.0787753826796275e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.73it/s]                                               {'loss': 2.2017, 'grad_norm': 3.9288485050201416, 'learning_rate': 8.989794855663563e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.73it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.65it/s]                                               {'loss': 2.3274, 'grad_norm': 5.060141086578369, 'learning_rate': 7.1918358845308504e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.65it/s]                                               {'loss': 2.3008, 'grad_norm': 4.722682476043701, 'learning_rate': 5.393876913398137e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.65it/s]                                               {'loss': 2.253, 'grad_norm': 3.9266562461853027, 'learning_rate': 3.5959179422654252e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.65it/s]                                               {'loss': 2.7408, 'grad_norm': 9.142904281616211, 'learning_rate': 1.7979589711327126e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.65it/s]                                               {'train_runtime': 3.0473, 'train_samples_per_second': 370.826, 'train_steps_per_second': 24.612, 'train_loss': 2.288151372273763, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.65it/s]100%|██████████| 75/75 [00:03<00:00, 24.62it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1012, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(928, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(740, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(921, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1136, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(886, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1048, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(908, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(967, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(620, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1158, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(553, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 65.20it/s]  3%|▎         | 14/471 [00:00<00:08, 53.73it/s]  4%|▍         | 20/471 [00:00<00:08, 54.10it/s]  6%|▌         | 26/471 [00:00<00:08, 53.66it/s]  7%|▋         | 32/471 [00:00<00:08, 53.47it/s]  8%|▊         | 38/471 [00:00<00:08, 53.33it/s]  9%|▉         | 44/471 [00:00<00:08, 52.87it/s] 11%|█         | 50/471 [00:00<00:08, 52.16it/s] 12%|█▏        | 56/471 [00:01<00:07, 52.66it/s] 13%|█▎        | 62/471 [00:01<00:07, 52.74it/s] 14%|█▍        | 68/471 [00:01<00:07, 52.54it/s] 16%|█▌        | 74/471 [00:01<00:07, 51.81it/s] 17%|█▋        | 80/471 [00:01<00:07, 52.55it/s] 18%|█▊        | 86/471 [00:01<00:07, 51.37it/s] 20%|█▉        | 92/471 [00:01<00:07, 51.45it/s] 21%|██        | 98/471 [00:01<00:07, 51.21it/s] 22%|██▏       | 104/471 [00:01<00:07, 51.63it/s] 23%|██▎       | 110/471 [00:02<00:06, 52.06it/s] 25%|██▍       | 116/471 [00:02<00:06, 52.37it/s] 26%|██▌       | 122/471 [00:02<00:06, 52.07it/s] 27%|██▋       | 128/471 [00:02<00:06, 51.78it/s] 28%|██▊       | 134/471 [00:02<00:06, 51.26it/s] 30%|██▉       | 140/471 [00:02<00:06, 51.65it/s] 31%|███       | 146/471 [00:02<00:06, 51.12it/s] 32%|███▏      | 152/471 [00:02<00:06, 50.74it/s] 34%|███▎      | 158/471 [00:03<00:06, 51.59it/s] 35%|███▍      | 164/471 [00:03<00:05, 51.68it/s] 36%|███▌      | 170/471 [00:03<00:05, 51.83it/s] 37%|███▋      | 176/471 [00:03<00:05, 52.22it/s] 39%|███▊      | 182/471 [00:03<00:05, 51.47it/s] 40%|███▉      | 188/471 [00:03<00:05, 51.80it/s] 41%|████      | 194/471 [00:03<00:05, 52.07it/s] 42%|████▏     | 200/471 [00:03<00:05, 52.25it/s] 44%|████▎     | 206/471 [00:03<00:05, 52.49it/s] 45%|████▌     | 212/471 [00:04<00:04, 52.46it/s] 46%|████▋     | 218/471 [00:04<00:04, 51.33it/s] 48%|████▊     | 224/471 [00:04<00:04, 51.57it/s] 49%|████▉     | 230/471 [00:04<00:04, 52.30it/s] 50%|█████     | 236/471 [00:04<00:04, 52.02it/s] 51%|█████▏    | 242/471 [00:04<00:04, 51.69it/s] 53%|█████▎    | 248/471 [00:04<00:04, 51.98it/s] 54%|█████▍    | 254/471 [00:04<00:04, 51.03it/s] 55%|█████▌    | 260/471 [00:04<00:04, 51.60it/s] 56%|█████▋    | 266/471 [00:05<00:04, 51.21it/s] 58%|█████▊    | 272/471 [00:05<00:03, 51.49it/s] 59%|█████▉    | 278/471 [00:05<00:03, 51.76it/s] 60%|██████    | 284/471 [00:05<00:03, 51.54it/s] 62%|██████▏   | 290/471 [00:05<00:03, 51.53it/s] 63%|██████▎   | 296/471 [00:05<00:03, 52.03it/s] 64%|██████▍   | 302/471 [00:05<00:03, 52.10it/s] 65%|██████▌   | 308/471 [00:05<00:03, 51.98it/s] 67%|██████▋   | 314/471 [00:06<00:03, 50.87it/s] 68%|██████▊   | 320/471 [00:06<00:02, 51.17it/s] 69%|██████▉   | 326/471 [00:06<00:02, 50.94it/s] 70%|███████   | 332/471 [00:06<00:02, 51.68it/s] 72%|███████▏  | 338/471 [00:06<00:02, 51.93it/s] 73%|███████▎  | 344/471 [00:06<00:02, 52.07it/s] 74%|███████▍  | 350/471 [00:06<00:02, 51.54it/s] 76%|███████▌  | 356/471 [00:06<00:02, 51.97it/s] 77%|███████▋  | 362/471 [00:06<00:02, 52.12it/s] 78%|███████▊  | 368/471 [00:07<00:01, 52.22it/s] 79%|███████▉  | 374/471 [00:07<00:01, 52.20it/s] 81%|████████  | 380/471 [00:07<00:01, 52.42it/s] 82%|████████▏ | 386/471 [00:07<00:01, 52.57it/s] 83%|████████▎ | 392/471 [00:07<00:01, 52.43it/s] 85%|████████▍ | 398/471 [00:07<00:01, 51.89it/s] 86%|████████▌ | 404/471 [00:07<00:01, 52.42it/s] 87%|████████▋ | 410/471 [00:07<00:01, 50.76it/s] 88%|████████▊ | 416/471 [00:08<00:01, 51.75it/s] 90%|████████▉ | 422/471 [00:08<00:00, 51.49it/s] 91%|█████████ | 428/471 [00:08<00:00, 50.58it/s] 92%|█████████▏| 434/471 [00:08<00:00, 50.77it/s] 93%|█████████▎| 440/471 [00:08<00:00, 51.15it/s] 95%|█████████▍| 446/471 [00:08<00:00, 51.26it/s] 96%|█████████▌| 452/471 [00:08<00:00, 51.51it/s] 97%|█████████▋| 458/471 [00:08<00:00, 51.15it/s] 99%|█████████▊| 464/471 [00:08<00:00, 51.56it/s]100%|█████████▉| 470/471 [00:09<00:00, 51.77it/s]100%|██████████| 471/471 [00:09<00:00, 51.88it/s]
{'eval_loss': 2.2802910804748535, 'eval_model_preparation_time': 0.0032, 'eval_acc': 0.39418481147105683, 'eval_runtime': 9.1001, 'eval_samples_per_second': 827.682, 'eval_steps_per_second': 51.758}
ROUND:16
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3808, 'grad_norm': 4.441101551055908, 'learning_rate': 0.00013245553203367584, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.80it/s]                                              {'loss': 2.322, 'grad_norm': 4.059335708618164, 'learning_rate': 0.00013068945827322684, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.72it/s]  4%|▍         | 3/75 [00:00<00:02, 24.98it/s]                                              {'loss': 2.2414, 'grad_norm': 3.2565603256225586, 'learning_rate': 0.00012892338451277782, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.98it/s]                                              {'loss': 2.1686, 'grad_norm': 4.130576133728027, 'learning_rate': 0.0001271573107523288, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.98it/s]                                              {'loss': 2.4645, 'grad_norm': 5.625147819519043, 'learning_rate': 0.0001253912369918798, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.98it/s]  8%|▊         | 6/75 [00:00<00:02, 24.90it/s]                                              {'loss': 2.1601, 'grad_norm': 3.2816576957702637, 'learning_rate': 0.0001236251632314308, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.90it/s]                                              {'loss': 2.227, 'grad_norm': 3.940293312072754, 'learning_rate': 0.00012185908947098178, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.90it/s]                                              {'loss': 2.1211, 'grad_norm': 3.8210926055908203, 'learning_rate': 0.00012009301571053276, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.90it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.01it/s]                                              {'loss': 2.239, 'grad_norm': 3.8281195163726807, 'learning_rate': 0.00011832694195008375, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.01it/s]                                              {'loss': 2.0636, 'grad_norm': 3.8082966804504395, 'learning_rate': 0.00011656086818963474, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.01it/s]                                               {'loss': 2.4393, 'grad_norm': 4.665646076202393, 'learning_rate': 0.00011479479442918574, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.01it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.95it/s]                                               {'loss': 2.2805, 'grad_norm': 3.7498888969421387, 'learning_rate': 0.00011302872066873673, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.95it/s]                                               {'loss': 2.2556, 'grad_norm': 4.169567108154297, 'learning_rate': 0.0001112626469082877, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.95it/s]                                               {'loss': 2.2596, 'grad_norm': 3.3926403522491455, 'learning_rate': 0.00010949657314783869, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.95it/s] 20%|██        | 15/75 [00:00<00:02, 25.87it/s]                                               {'loss': 2.1516, 'grad_norm': 6.635638236999512, 'learning_rate': 0.00010773049938738969, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.87it/s]                                               {'loss': 2.1872, 'grad_norm': 3.383462905883789, 'learning_rate': 0.00010596442562694068, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.87it/s]                                               {'loss': 2.3664, 'grad_norm': 5.227296829223633, 'learning_rate': 0.00010419835186649165, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.87it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.93it/s]                                               {'loss': 2.38, 'grad_norm': 3.641801595687866, 'learning_rate': 0.00010243227810604265, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.93it/s]                                               {'loss': 2.1799, 'grad_norm': 2.98633074760437, 'learning_rate': 0.00010066620434559364, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.93it/s]                                               {'loss': 1.9851, 'grad_norm': 2.952026128768921, 'learning_rate': 9.890013058514463e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.93it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.49it/s]                                               {'loss': 2.2301, 'grad_norm': 3.3086442947387695, 'learning_rate': 9.71340568246956e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.49it/s]                                               {'loss': 2.1336, 'grad_norm': 4.598101615905762, 'learning_rate': 9.536798306424661e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.49it/s]                                               {'loss': 1.9759, 'grad_norm': 2.685426712036133, 'learning_rate': 9.36019093037976e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.49it/s] 32%|███▏      | 24/75 [00:00<00:02, 25.09it/s]                                               {'loss': 2.2037, 'grad_norm': 4.219545364379883, 'learning_rate': 9.183583554334858e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 25.09it/s]                                               {'loss': 2.1477, 'grad_norm': 3.9369618892669678, 'learning_rate': 9.006976178289957e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.09it/s]                                               {'loss': 2.294, 'grad_norm': 2.8205153942108154, 'learning_rate': 8.830368802245056e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.09it/s] 36%|███▌      | 27/75 [00:01<00:01, 25.43it/s]                                               {'loss': 2.3741, 'grad_norm': 4.134285926818848, 'learning_rate': 8.653761426200155e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.43it/s]                                               {'loss': 2.2475, 'grad_norm': 3.733612060546875, 'learning_rate': 8.477154050155254e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.43it/s]                                               {'loss': 2.2355, 'grad_norm': 4.046911239624023, 'learning_rate': 8.300546674110353e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.43it/s]                                               {'loss': 2.9406, 'grad_norm': 10.334681510925293, 'learning_rate': 8.123939298065451e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.43it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.95it/s]                                               {'loss': 2.3107, 'grad_norm': 4.636345863342285, 'learning_rate': 7.94733192202055e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.95it/s]                                               {'loss': 2.0162, 'grad_norm': 3.6648879051208496, 'learning_rate': 7.770724545975649e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.95it/s]                                               {'loss': 2.0409, 'grad_norm': 3.787532091140747, 'learning_rate': 7.594117169930749e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.95it/s] 45%|████▌     | 34/75 [00:01<00:01, 26.23it/s]                                               {'loss': 2.1841, 'grad_norm': 4.04432487487793, 'learning_rate': 7.417509793885848e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.23it/s]                                               {'loss': 2.2616, 'grad_norm': 3.4345896244049072, 'learning_rate': 7.240902417840946e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.23it/s]                                               {'loss': 2.2861, 'grad_norm': 3.9982094764709473, 'learning_rate': 7.064295041796044e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.23it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.80it/s]                                               {'loss': 2.0544, 'grad_norm': 4.013142108917236, 'learning_rate': 6.887687665751145e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.80it/s]                                               {'loss': 2.2665, 'grad_norm': 3.6385726928710938, 'learning_rate': 6.711080289706243e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.80it/s]                                               {'loss': 2.1075, 'grad_norm': 3.253572940826416, 'learning_rate': 6.534472913661342e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.80it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.0286, 'grad_norm': 3.2293152809143066, 'learning_rate': 6.35786553761644e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.4162, 'grad_norm': 4.687446117401123, 'learning_rate': 6.18125816157154e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.3968, 'grad_norm': 3.9655508995056152, 'learning_rate': 6.004650785526638e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.62it/s] 57%|█████▋    | 43/75 [00:01<00:01, 25.07it/s]                                               {'loss': 2.2094, 'grad_norm': 3.283902406692505, 'learning_rate': 5.828043409481737e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.07it/s]                                               {'loss': 2.0376, 'grad_norm': 2.8220388889312744, 'learning_rate': 5.651436033436836e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.07it/s]                                               {'loss': 2.1465, 'grad_norm': 11.451691627502441, 'learning_rate': 5.4748286573919344e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.07it/s]                                               {'loss': 2.2109, 'grad_norm': 3.3775720596313477, 'learning_rate': 5.298221281347034e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.07it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.62it/s]                                               {'loss': 2.1551, 'grad_norm': 2.737649440765381, 'learning_rate': 5.121613905302133e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.62it/s]                                               {'loss': 2.0963, 'grad_norm': 3.3717551231384277, 'learning_rate': 4.9450065292572316e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.62it/s]                                               {'loss': 2.3313, 'grad_norm': 4.357705593109131, 'learning_rate': 4.7683991532123304e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.62it/s] 67%|██████▋   | 50/75 [00:01<00:00, 25.87it/s]                                               {'loss': 2.1221, 'grad_norm': 4.165620803833008, 'learning_rate': 4.591791777167429e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.87it/s]                                               {'loss': 1.9958, 'grad_norm': 3.4738659858703613, 'learning_rate': 4.415184401122528e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 25.87it/s]                                               {'loss': 2.2544, 'grad_norm': 3.6311373710632324, 'learning_rate': 4.238577025077627e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.87it/s] 71%|███████   | 53/75 [00:02<00:00, 25.62it/s]                                               {'loss': 2.3071, 'grad_norm': 4.085299968719482, 'learning_rate': 4.061969649032726e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.62it/s]                                               {'loss': 2.2214, 'grad_norm': 4.489718437194824, 'learning_rate': 3.8853622729878245e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.62it/s]                                               {'loss': 2.119, 'grad_norm': 3.4008073806762695, 'learning_rate': 3.708754896942924e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.62it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.1196, 'grad_norm': 3.5685112476348877, 'learning_rate': 3.532147520898022e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.151, 'grad_norm': 3.9563679695129395, 'learning_rate': 3.355540144853122e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.2165, 'grad_norm': 4.69467306137085, 'learning_rate': 3.17893276880822e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.33it/s] 79%|███████▊  | 59/75 [00:02<00:00, 24.68it/s]                                               {'loss': 2.1023, 'grad_norm': 4.31920051574707, 'learning_rate': 3.002325392763319e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.68it/s]                                               {'loss': 1.6815, 'grad_norm': 6.350099563598633, 'learning_rate': 2.825718016718418e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.68it/s]                                               {'loss': 2.2976, 'grad_norm': 3.9705135822296143, 'learning_rate': 2.649110640673517e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.68it/s] 83%|████████▎ | 62/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.3534, 'grad_norm': 4.106808662414551, 'learning_rate': 2.4725032646286158e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.0882, 'grad_norm': 3.3528480529785156, 'learning_rate': 2.2958958885837146e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.007, 'grad_norm': 3.2236623764038086, 'learning_rate': 2.1192885125388134e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.49it/s] 87%|████████▋ | 65/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.1818, 'grad_norm': 3.2777440547943115, 'learning_rate': 1.9426811364939123e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.1438, 'grad_norm': 4.102578639984131, 'learning_rate': 1.766073760449011e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.1231, 'grad_norm': 3.0025484561920166, 'learning_rate': 1.58946638440411e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.14it/s] 91%|█████████ | 68/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.1845, 'grad_norm': 3.590435743331909, 'learning_rate': 1.412859008359209e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.2885, 'grad_norm': 4.151529312133789, 'learning_rate': 1.2362516323143079e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.2494, 'grad_norm': 3.966592311859131, 'learning_rate': 1.0596442562694067e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.16it/s] 95%|█████████▍| 71/75 [00:02<00:00, 24.91it/s]                                               {'loss': 2.0332, 'grad_norm': 3.940258502960205, 'learning_rate': 8.830368802245055e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.91it/s]                                               {'loss': 2.0797, 'grad_norm': 3.8501622676849365, 'learning_rate': 7.064295041796045e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.91it/s]                                               {'loss': 2.1036, 'grad_norm': 3.4224021434783936, 'learning_rate': 5.298221281347034e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.91it/s] 99%|█████████▊| 74/75 [00:02<00:00, 24.64it/s]                                               {'loss': 2.0903, 'grad_norm': 3.122612476348877, 'learning_rate': 3.5321475208980227e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.64it/s]                                               {'loss': 1.8292, 'grad_norm': 10.977327346801758, 'learning_rate': 1.7660737604490113e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.64it/s]                                               {'train_runtime': 3.0472, 'train_samples_per_second': 370.838, 'train_steps_per_second': 24.613, 'train_loss': 2.1940917030970257, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.64it/s]100%|██████████| 75/75 [00:03<00:00, 24.62it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:34
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3266, 'grad_norm': 3.5044755935668945, 'learning_rate': 0.00013245553203367584, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.85it/s]                                              {'loss': 2.1524, 'grad_norm': 2.953425645828247, 'learning_rate': 0.00013068945827322684, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.98it/s]  4%|▍         | 3/75 [00:00<00:02, 24.51it/s]                                              {'loss': 2.269, 'grad_norm': 3.5263190269470215, 'learning_rate': 0.00012892338451277782, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.51it/s]                                              {'loss': 2.032, 'grad_norm': 3.3782107830047607, 'learning_rate': 0.0001271573107523288, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.51it/s]                                              {'loss': 2.4129, 'grad_norm': 3.854015588760376, 'learning_rate': 0.0001253912369918798, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.51it/s]  8%|▊         | 6/75 [00:00<00:02, 24.83it/s]                                              {'loss': 2.2978, 'grad_norm': 4.025063514709473, 'learning_rate': 0.0001236251632314308, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.83it/s]                                              {'loss': 2.2992, 'grad_norm': 4.450211524963379, 'learning_rate': 0.00012185908947098178, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.83it/s]                                              {'loss': 2.5012, 'grad_norm': 3.952526330947876, 'learning_rate': 0.00012009301571053276, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.83it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.03it/s]                                              {'loss': 2.1508, 'grad_norm': 3.138857841491699, 'learning_rate': 0.00011832694195008375, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.03it/s]                                              {'loss': 2.0821, 'grad_norm': 3.1255178451538086, 'learning_rate': 0.00011656086818963474, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.03it/s]                                               {'loss': 2.3057, 'grad_norm': 3.453864336013794, 'learning_rate': 0.00011479479442918574, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.03it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.39it/s]                                               {'loss': 2.0383, 'grad_norm': 3.340599775314331, 'learning_rate': 0.00011302872066873673, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.39it/s]                                               {'loss': 2.3873, 'grad_norm': 4.996628284454346, 'learning_rate': 0.0001112626469082877, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.39it/s]                                               {'loss': 2.458, 'grad_norm': 5.626708507537842, 'learning_rate': 0.00010949657314783869, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.39it/s]                                               {'loss': 3.6218, 'grad_norm': 10.834479331970215, 'learning_rate': 0.00010773049938738969, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.39it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.31it/s]                                               {'loss': 2.1265, 'grad_norm': 4.196893692016602, 'learning_rate': 0.00010596442562694068, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.31it/s]                                               {'loss': 2.0766, 'grad_norm': 3.205831527709961, 'learning_rate': 0.00010419835186649165, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.31it/s]                                               {'loss': 2.3569, 'grad_norm': 3.8913192749023438, 'learning_rate': 0.00010243227810604265, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.31it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.07it/s]                                               {'loss': 2.2076, 'grad_norm': 4.089040279388428, 'learning_rate': 0.00010066620434559364, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.07it/s]                                               {'loss': 2.1816, 'grad_norm': 3.9003944396972656, 'learning_rate': 9.890013058514463e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.07it/s]                                               {'loss': 2.1929, 'grad_norm': 4.600602626800537, 'learning_rate': 9.71340568246956e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.07it/s] 29%|██▉       | 22/75 [00:00<00:02, 24.74it/s]                                               {'loss': 2.2486, 'grad_norm': 4.70488166809082, 'learning_rate': 9.536798306424661e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.74it/s]                                               {'loss': 2.3232, 'grad_norm': 3.6912450790405273, 'learning_rate': 9.36019093037976e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.74it/s]                                               {'loss': 2.3317, 'grad_norm': 3.3255386352539062, 'learning_rate': 9.183583554334858e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.74it/s] 33%|███▎      | 25/75 [00:01<00:02, 24.52it/s]                                               {'loss': 2.3362, 'grad_norm': 3.8668785095214844, 'learning_rate': 9.006976178289957e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.52it/s]                                               {'loss': 2.3907, 'grad_norm': 4.240605354309082, 'learning_rate': 8.830368802245056e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.52it/s]                                               {'loss': 2.2992, 'grad_norm': 3.493281841278076, 'learning_rate': 8.653761426200155e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.52it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.80it/s]                                               {'loss': 2.2292, 'grad_norm': 3.364223003387451, 'learning_rate': 8.477154050155254e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.80it/s]                                               {'loss': 2.1712, 'grad_norm': 3.485729932785034, 'learning_rate': 8.300546674110353e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.80it/s]                                               {'loss': 2.5948, 'grad_norm': 11.24110221862793, 'learning_rate': 8.123939298065451e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.80it/s]                                               {'loss': 2.0994, 'grad_norm': 3.521221399307251, 'learning_rate': 7.94733192202055e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.80it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.64it/s]                                               {'loss': 2.1835, 'grad_norm': 3.4474244117736816, 'learning_rate': 7.770724545975649e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.64it/s]                                               {'loss': 2.1283, 'grad_norm': 3.7078187465667725, 'learning_rate': 7.594117169930749e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.64it/s]                                               {'loss': 2.2235, 'grad_norm': 4.002468585968018, 'learning_rate': 7.417509793885848e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.64it/s] 47%|████▋     | 35/75 [00:01<00:01, 25.77it/s]                                               {'loss': 2.3472, 'grad_norm': 3.771470308303833, 'learning_rate': 7.240902417840946e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.77it/s]                                               {'loss': 2.1856, 'grad_norm': 3.0605673789978027, 'learning_rate': 7.064295041796044e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.77it/s]                                               {'loss': 2.1318, 'grad_norm': 3.8030846118927, 'learning_rate': 6.887687665751145e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.77it/s] 51%|█████     | 38/75 [00:01<00:01, 25.96it/s]                                               {'loss': 2.0364, 'grad_norm': 4.216424942016602, 'learning_rate': 6.711080289706243e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.96it/s]                                               {'loss': 2.2444, 'grad_norm': 3.4069793224334717, 'learning_rate': 6.534472913661342e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.96it/s]                                               {'loss': 2.2758, 'grad_norm': 4.5208234786987305, 'learning_rate': 6.35786553761644e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.96it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.3179, 'grad_norm': 4.030516147613525, 'learning_rate': 6.18125816157154e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.3124, 'grad_norm': 5.0256195068359375, 'learning_rate': 6.004650785526638e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.2406, 'grad_norm': 3.9522716999053955, 'learning_rate': 5.828043409481737e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.31it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.69it/s]                                               {'loss': 2.1915, 'grad_norm': 4.362198352813721, 'learning_rate': 5.651436033436836e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.69it/s]                                               {'loss': 2.4197, 'grad_norm': 13.688104629516602, 'learning_rate': 5.4748286573919344e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.69it/s]                                               {'loss': 2.2656, 'grad_norm': 3.4486255645751953, 'learning_rate': 5.298221281347034e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.69it/s]                                               {'loss': 2.3045, 'grad_norm': 3.8256278038024902, 'learning_rate': 5.121613905302133e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.69it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.68it/s]                                               {'loss': 2.2822, 'grad_norm': 4.372740268707275, 'learning_rate': 4.9450065292572316e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.68it/s]                                               {'loss': 2.2374, 'grad_norm': 4.305865287780762, 'learning_rate': 4.7683991532123304e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.68it/s]                                               {'loss': 2.1946, 'grad_norm': 3.312897205352783, 'learning_rate': 4.591791777167429e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.68it/s] 68%|██████▊   | 51/75 [00:02<00:00, 25.43it/s]                                               {'loss': 2.3337, 'grad_norm': 4.982008457183838, 'learning_rate': 4.415184401122528e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.43it/s]                                               {'loss': 2.306, 'grad_norm': 3.4381701946258545, 'learning_rate': 4.238577025077627e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.43it/s]                                               {'loss': 2.0939, 'grad_norm': 2.9026620388031006, 'learning_rate': 4.061969649032726e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.43it/s] 72%|███████▏  | 54/75 [00:02<00:00, 24.57it/s]                                               {'loss': 2.0236, 'grad_norm': 3.400596857070923, 'learning_rate': 3.8853622729878245e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.57it/s]                                               {'loss': 2.1199, 'grad_norm': 3.3051869869232178, 'learning_rate': 3.708754896942924e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.57it/s]                                               {'loss': 2.1466, 'grad_norm': 3.3229525089263916, 'learning_rate': 3.532147520898022e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.57it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.82it/s]                                               {'loss': 2.1671, 'grad_norm': 3.11879563331604, 'learning_rate': 3.355540144853122e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.82it/s]                                               {'loss': 2.1785, 'grad_norm': 3.333501100540161, 'learning_rate': 3.17893276880822e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.82it/s]                                               {'loss': 2.0022, 'grad_norm': 3.699108123779297, 'learning_rate': 3.002325392763319e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.82it/s]                                               {'loss': 1.7911, 'grad_norm': 8.817341804504395, 'learning_rate': 2.825718016718418e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.82it/s] 81%|████████▏ | 61/75 [00:02<00:00, 26.48it/s]                                               {'loss': 2.2017, 'grad_norm': 3.6789493560791016, 'learning_rate': 2.649110640673517e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.48it/s]                                               {'loss': 2.1291, 'grad_norm': 3.5602054595947266, 'learning_rate': 2.4725032646286158e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.48it/s]                                               {'loss': 2.2504, 'grad_norm': 3.599355459213257, 'learning_rate': 2.2958958885837146e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.48it/s] 85%|████████▌ | 64/75 [00:02<00:00, 26.11it/s]                                               {'loss': 2.0546, 'grad_norm': 3.817248582839966, 'learning_rate': 2.1192885125388134e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.11it/s]                                               {'loss': 2.2594, 'grad_norm': 4.58445930480957, 'learning_rate': 1.9426811364939123e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.11it/s]                                               {'loss': 2.1199, 'grad_norm': 3.4982717037200928, 'learning_rate': 1.766073760449011e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.11it/s] 89%|████████▉ | 67/75 [00:02<00:00, 25.64it/s]                                               {'loss': 2.0943, 'grad_norm': 3.194112539291382, 'learning_rate': 1.58946638440411e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.64it/s]                                               {'loss': 2.2007, 'grad_norm': 4.784841537475586, 'learning_rate': 1.412859008359209e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.64it/s]                                               {'loss': 2.008, 'grad_norm': 3.130560874938965, 'learning_rate': 1.2362516323143079e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.64it/s] 93%|█████████▎| 70/75 [00:02<00:00, 25.53it/s]                                               {'loss': 2.325, 'grad_norm': 4.068997383117676, 'learning_rate': 1.0596442562694067e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.53it/s]                                               {'loss': 2.3937, 'grad_norm': 4.355939865112305, 'learning_rate': 8.830368802245055e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.53it/s]                                               {'loss': 2.3463, 'grad_norm': 3.7974767684936523, 'learning_rate': 7.064295041796045e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.53it/s] 97%|█████████▋| 73/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.0954, 'grad_norm': 3.681640148162842, 'learning_rate': 5.298221281347034e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.0475, 'grad_norm': 3.6676533222198486, 'learning_rate': 3.5321475208980227e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.2357, 'grad_norm': 10.936259269714355, 'learning_rate': 1.7660737604490113e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.96it/s]                                               {'train_runtime': 3.0336, 'train_samples_per_second': 372.492, 'train_steps_per_second': 24.723, 'train_loss': 2.2393182849884035, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.96it/s]100%|██████████| 75/75 [00:03<00:00, 24.73it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2437, 'grad_norm': 3.2053651809692383, 'learning_rate': 0.00013245553203367584, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.63it/s]                                              {'loss': 2.4843, 'grad_norm': 3.8494713306427, 'learning_rate': 0.00013068945827322684, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 25.29it/s]  4%|▍         | 3/75 [00:00<00:02, 25.48it/s]                                              {'loss': 2.1908, 'grad_norm': 4.001838207244873, 'learning_rate': 0.00012892338451277782, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.48it/s]                                              {'loss': 2.0794, 'grad_norm': 4.51540470123291, 'learning_rate': 0.0001271573107523288, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.48it/s]                                              {'loss': 2.2014, 'grad_norm': 3.482078790664673, 'learning_rate': 0.0001253912369918798, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.48it/s]  8%|▊         | 6/75 [00:00<00:02, 24.84it/s]                                              {'loss': 2.4785, 'grad_norm': 4.018649101257324, 'learning_rate': 0.0001236251632314308, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.84it/s]                                              {'loss': 2.3471, 'grad_norm': 3.624135732650757, 'learning_rate': 0.00012185908947098178, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.84it/s]                                              {'loss': 2.1325, 'grad_norm': 3.7914984226226807, 'learning_rate': 0.00012009301571053276, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.84it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.10it/s]                                              {'loss': 2.2187, 'grad_norm': 3.4777958393096924, 'learning_rate': 0.00011832694195008375, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.10it/s]                                              {'loss': 2.1681, 'grad_norm': 4.742321968078613, 'learning_rate': 0.00011656086818963474, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.10it/s]                                               {'loss': 2.3281, 'grad_norm': 4.086431503295898, 'learning_rate': 0.00011479479442918574, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.10it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.42it/s]                                               {'loss': 2.279, 'grad_norm': 3.7820370197296143, 'learning_rate': 0.00011302872066873673, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.42it/s]                                               {'loss': 2.3616, 'grad_norm': 3.7767510414123535, 'learning_rate': 0.0001112626469082877, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.42it/s]                                               {'loss': 2.3083, 'grad_norm': 4.7201714515686035, 'learning_rate': 0.00010949657314783869, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.42it/s]                                               {'loss': 2.5541, 'grad_norm': 11.15402603149414, 'learning_rate': 0.00010773049938738969, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.42it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.80it/s]                                               {'loss': 2.2703, 'grad_norm': 4.009629726409912, 'learning_rate': 0.00010596442562694068, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.80it/s]                                               {'loss': 2.3797, 'grad_norm': 4.591802597045898, 'learning_rate': 0.00010419835186649165, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.80it/s]                                               {'loss': 2.321, 'grad_norm': 4.744609832763672, 'learning_rate': 0.00010243227810604265, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.80it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.59it/s]                                               {'loss': 2.2234, 'grad_norm': 3.2339847087860107, 'learning_rate': 0.00010066620434559364, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.59it/s]                                               {'loss': 2.3612, 'grad_norm': 3.779999017715454, 'learning_rate': 9.890013058514463e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.59it/s]                                               {'loss': 2.3674, 'grad_norm': 3.6959445476531982, 'learning_rate': 9.71340568246956e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.59it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.28it/s]                                               {'loss': 2.2381, 'grad_norm': 4.126071453094482, 'learning_rate': 9.536798306424661e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.28it/s]                                               {'loss': 2.2944, 'grad_norm': 3.949613094329834, 'learning_rate': 9.36019093037976e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.28it/s]                                               {'loss': 2.1446, 'grad_norm': 4.190683841705322, 'learning_rate': 9.183583554334858e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 25.28it/s] 33%|███▎      | 25/75 [00:00<00:02, 24.80it/s]                                               {'loss': 2.2636, 'grad_norm': 3.8804008960723877, 'learning_rate': 9.006976178289957e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.80it/s]                                               {'loss': 2.1183, 'grad_norm': 4.05911922454834, 'learning_rate': 8.830368802245056e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.80it/s]                                               {'loss': 2.1204, 'grad_norm': 4.195878505706787, 'learning_rate': 8.653761426200155e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.80it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.38it/s]                                               {'loss': 2.1492, 'grad_norm': 3.1394453048706055, 'learning_rate': 8.477154050155254e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.38it/s]                                               {'loss': 2.0443, 'grad_norm': 3.6738553047180176, 'learning_rate': 8.300546674110353e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.38it/s]                                               {'loss': 2.8337, 'grad_norm': 13.604020118713379, 'learning_rate': 8.123939298065451e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.38it/s]                                               {'loss': 2.0574, 'grad_norm': 3.417665481567383, 'learning_rate': 7.94733192202055e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.38it/s] 43%|████▎     | 32/75 [00:01<00:01, 25.51it/s]                                               {'loss': 2.3494, 'grad_norm': 3.835237979888916, 'learning_rate': 7.770724545975649e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.51it/s]                                               {'loss': 2.3697, 'grad_norm': 3.807979106903076, 'learning_rate': 7.594117169930749e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.51it/s]                                               {'loss': 2.2576, 'grad_norm': 3.501626968383789, 'learning_rate': 7.417509793885848e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.51it/s] 47%|████▋     | 35/75 [00:01<00:01, 25.66it/s]                                               {'loss': 1.9951, 'grad_norm': 2.8862662315368652, 'learning_rate': 7.240902417840946e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.66it/s]                                               {'loss': 2.165, 'grad_norm': 4.339739799499512, 'learning_rate': 7.064295041796044e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.66it/s]                                               {'loss': 2.2551, 'grad_norm': 3.2587907314300537, 'learning_rate': 6.887687665751145e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.66it/s] 51%|█████     | 38/75 [00:01<00:01, 25.84it/s]                                               {'loss': 2.3197, 'grad_norm': 3.94584059715271, 'learning_rate': 6.711080289706243e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.84it/s]                                               {'loss': 2.1756, 'grad_norm': 3.6570119857788086, 'learning_rate': 6.534472913661342e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.84it/s]                                               {'loss': 2.1594, 'grad_norm': 3.5016703605651855, 'learning_rate': 6.35786553761644e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.84it/s] 55%|█████▍    | 41/75 [00:01<00:01, 24.91it/s]                                               {'loss': 2.1481, 'grad_norm': 3.1804959774017334, 'learning_rate': 6.18125816157154e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.91it/s]                                               {'loss': 2.2535, 'grad_norm': 3.5968947410583496, 'learning_rate': 6.004650785526638e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.91it/s]                                               {'loss': 1.9627, 'grad_norm': 4.222934246063232, 'learning_rate': 5.828043409481737e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.91it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.23it/s]                                               {'loss': 2.1882, 'grad_norm': 3.2016682624816895, 'learning_rate': 5.651436033436836e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.23it/s]                                               {'loss': 2.7937, 'grad_norm': 13.989006042480469, 'learning_rate': 5.4748286573919344e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.23it/s]                                               {'loss': 2.2636, 'grad_norm': 3.222856044769287, 'learning_rate': 5.298221281347034e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.23it/s]                                               {'loss': 2.414, 'grad_norm': 4.2455315589904785, 'learning_rate': 5.121613905302133e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.23it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.57it/s]                                               {'loss': 2.2535, 'grad_norm': 5.0886454582214355, 'learning_rate': 4.9450065292572316e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.57it/s]                                               {'loss': 2.1411, 'grad_norm': 3.530684471130371, 'learning_rate': 4.7683991532123304e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.57it/s]                                               {'loss': 2.0315, 'grad_norm': 2.999528646469116, 'learning_rate': 4.591791777167429e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.57it/s] 68%|██████▊   | 51/75 [00:02<00:00, 25.92it/s]                                               {'loss': 2.1461, 'grad_norm': 3.2789716720581055, 'learning_rate': 4.415184401122528e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.92it/s]                                               {'loss': 2.2106, 'grad_norm': 3.931042194366455, 'learning_rate': 4.238577025077627e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.92it/s]                                               {'loss': 1.9431, 'grad_norm': 4.0306010246276855, 'learning_rate': 4.061969649032726e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.92it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.85it/s]                                               {'loss': 2.4968, 'grad_norm': 4.740200042724609, 'learning_rate': 3.8853622729878245e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.85it/s]                                               {'loss': 2.0388, 'grad_norm': 3.5818519592285156, 'learning_rate': 3.708754896942924e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.85it/s]                                               {'loss': 2.1658, 'grad_norm': 3.542585849761963, 'learning_rate': 3.532147520898022e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.85it/s] 76%|███████▌  | 57/75 [00:02<00:00, 25.18it/s]                                               {'loss': 2.1191, 'grad_norm': 3.963007688522339, 'learning_rate': 3.355540144853122e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.18it/s]                                               {'loss': 2.314, 'grad_norm': 2.814708709716797, 'learning_rate': 3.17893276880822e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.18it/s]                                               {'loss': 2.0966, 'grad_norm': 3.5856568813323975, 'learning_rate': 3.002325392763319e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.18it/s] 80%|████████  | 60/75 [00:02<00:00, 23.46it/s]                                               {'loss': 2.0765, 'grad_norm': 9.479307174682617, 'learning_rate': 2.825718016718418e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 23.46it/s]                                               {'loss': 1.9924, 'grad_norm': 3.3744664192199707, 'learning_rate': 2.649110640673517e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 23.46it/s]                                               {'loss': 2.1232, 'grad_norm': 4.215528964996338, 'learning_rate': 2.4725032646286158e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 23.46it/s] 84%|████████▍ | 63/75 [00:02<00:00, 23.55it/s]                                               {'loss': 2.278, 'grad_norm': 3.9452123641967773, 'learning_rate': 2.2958958885837146e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 23.55it/s]                                               {'loss': 2.0552, 'grad_norm': 2.9301767349243164, 'learning_rate': 2.1192885125388134e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 23.55it/s]                                               {'loss': 2.2115, 'grad_norm': 4.003672122955322, 'learning_rate': 1.9426811364939123e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 23.55it/s] 88%|████████▊ | 66/75 [00:02<00:00, 23.01it/s]                                               {'loss': 2.1411, 'grad_norm': 3.597604513168335, 'learning_rate': 1.766073760449011e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 23.01it/s]                                               {'loss': 2.2388, 'grad_norm': 4.553386688232422, 'learning_rate': 1.58946638440411e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 23.01it/s]                                               {'loss': 2.2595, 'grad_norm': 3.910675048828125, 'learning_rate': 1.412859008359209e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 23.01it/s] 92%|█████████▏| 69/75 [00:02<00:00, 22.88it/s]                                               {'loss': 2.3931, 'grad_norm': 4.093601703643799, 'learning_rate': 1.2362516323143079e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 22.88it/s]                                               {'loss': 2.02, 'grad_norm': 2.8602747917175293, 'learning_rate': 1.0596442562694067e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 22.88it/s]                                               {'loss': 2.0306, 'grad_norm': 3.249332904815674, 'learning_rate': 8.830368802245055e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 22.88it/s] 96%|█████████▌| 72/75 [00:02<00:00, 23.38it/s]                                               {'loss': 2.251, 'grad_norm': 3.688206434249878, 'learning_rate': 7.064295041796045e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 23.38it/s]                                               {'loss': 2.3719, 'grad_norm': 3.971099376678467, 'learning_rate': 5.298221281347034e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 23.38it/s]                                               {'loss': 2.2181, 'grad_norm': 3.6063499450683594, 'learning_rate': 3.5321475208980227e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 23.38it/s]                                               {'loss': 1.8222, 'grad_norm': 10.353242874145508, 'learning_rate': 1.7660737604490113e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 23.38it/s]                                               {'train_runtime': 3.1293, 'train_samples_per_second': 361.108, 'train_steps_per_second': 23.967, 'train_loss': 2.227640905380249, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 23.38it/s]100%|██████████| 75/75 [00:03<00:00, 23.97it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:36
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.264, 'grad_norm': 4.32680606842041, 'learning_rate': 0.00013245553203367584, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.52it/s]                                              {'loss': 2.3183, 'grad_norm': 3.453012704849243, 'learning_rate': 0.00013068945827322684, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.86it/s]  4%|▍         | 3/75 [00:00<00:02, 25.11it/s]                                              {'loss': 2.234, 'grad_norm': 3.6168270111083984, 'learning_rate': 0.00012892338451277782, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.11it/s]                                              {'loss': 2.2407, 'grad_norm': 3.9038279056549072, 'learning_rate': 0.0001271573107523288, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.11it/s]                                              {'loss': 2.3294, 'grad_norm': 4.822606563568115, 'learning_rate': 0.0001253912369918798, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.11it/s]  8%|▊         | 6/75 [00:00<00:02, 24.94it/s]                                              {'loss': 2.1982, 'grad_norm': 3.684612989425659, 'learning_rate': 0.0001236251632314308, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.94it/s]                                              {'loss': 2.3144, 'grad_norm': 3.9688467979431152, 'learning_rate': 0.00012185908947098178, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.94it/s]                                              {'loss': 2.2907, 'grad_norm': 4.676290988922119, 'learning_rate': 0.00012009301571053276, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.94it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.07it/s]                                              {'loss': 2.1932, 'grad_norm': 4.165255546569824, 'learning_rate': 0.00011832694195008375, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.07it/s]                                              {'loss': 2.0785, 'grad_norm': 3.7304437160491943, 'learning_rate': 0.00011656086818963474, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.07it/s]                                               {'loss': 2.163, 'grad_norm': 5.028759956359863, 'learning_rate': 0.00011479479442918574, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.07it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.72it/s]                                               {'loss': 1.9826, 'grad_norm': 3.058879852294922, 'learning_rate': 0.00011302872066873673, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.72it/s]                                               {'loss': 2.0609, 'grad_norm': 2.8464648723602295, 'learning_rate': 0.0001112626469082877, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.72it/s]                                               {'loss': 2.2662, 'grad_norm': 3.142758369445801, 'learning_rate': 0.00010949657314783869, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.72it/s]                                               {'loss': 2.0619, 'grad_norm': 7.774792194366455, 'learning_rate': 0.00010773049938738969, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.72it/s] 21%|██▏       | 16/75 [00:00<00:02, 27.13it/s]                                               {'loss': 2.3149, 'grad_norm': 4.243432521820068, 'learning_rate': 0.00010596442562694068, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 27.13it/s]                                               {'loss': 2.1364, 'grad_norm': 3.57795786857605, 'learning_rate': 0.00010419835186649165, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 27.13it/s]                                               {'loss': 2.1129, 'grad_norm': 4.555729866027832, 'learning_rate': 0.00010243227810604265, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 27.13it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.61it/s]                                               {'loss': 2.2323, 'grad_norm': 3.941955089569092, 'learning_rate': 0.00010066620434559364, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.61it/s]                                               {'loss': 2.2016, 'grad_norm': 4.905564785003662, 'learning_rate': 9.890013058514463e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.61it/s]                                               {'loss': 1.9532, 'grad_norm': 2.984323740005493, 'learning_rate': 9.71340568246956e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.61it/s] 29%|██▉       | 22/75 [00:00<00:02, 26.33it/s]                                               {'loss': 2.2649, 'grad_norm': 4.2239155769348145, 'learning_rate': 9.536798306424661e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 26.33it/s]                                               {'loss': 2.0718, 'grad_norm': 3.9588894844055176, 'learning_rate': 9.36019093037976e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 26.33it/s]                                               {'loss': 2.1763, 'grad_norm': 4.078423976898193, 'learning_rate': 9.183583554334858e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 26.33it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.92it/s]                                               {'loss': 2.2083, 'grad_norm': 3.2800650596618652, 'learning_rate': 9.006976178289957e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.92it/s]                                               {'loss': 2.0413, 'grad_norm': 3.608532667160034, 'learning_rate': 8.830368802245056e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.92it/s]                                               {'loss': 2.0392, 'grad_norm': 3.273653030395508, 'learning_rate': 8.653761426200155e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.92it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.17it/s]                                               {'loss': 2.2577, 'grad_norm': 5.308253288269043, 'learning_rate': 8.477154050155254e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.17it/s]                                               {'loss': 2.2642, 'grad_norm': 3.6113204956054688, 'learning_rate': 8.300546674110353e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.17it/s]                                               {'loss': 1.8417, 'grad_norm': 8.200736045837402, 'learning_rate': 8.123939298065451e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.17it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.12it/s]                                               {'loss': 2.1255, 'grad_norm': 3.3333323001861572, 'learning_rate': 7.94733192202055e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.12it/s]                                               {'loss': 2.1766, 'grad_norm': 3.0193564891815186, 'learning_rate': 7.770724545975649e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.12it/s]                                               {'loss': 2.0175, 'grad_norm': 3.462150812149048, 'learning_rate': 7.594117169930749e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.12it/s] 45%|████▌     | 34/75 [00:01<00:01, 26.26it/s]                                               {'loss': 2.0993, 'grad_norm': 3.425708293914795, 'learning_rate': 7.417509793885848e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.26it/s]                                               {'loss': 2.1045, 'grad_norm': 3.789276599884033, 'learning_rate': 7.240902417840946e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.26it/s]                                               {'loss': 1.8944, 'grad_norm': 3.735069990158081, 'learning_rate': 7.064295041796044e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.26it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.82it/s]                                               {'loss': 2.2758, 'grad_norm': 4.566610813140869, 'learning_rate': 6.887687665751145e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.82it/s]                                               {'loss': 2.0066, 'grad_norm': 3.079904794692993, 'learning_rate': 6.711080289706243e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.82it/s]                                               {'loss': 2.0707, 'grad_norm': 3.72379469871521, 'learning_rate': 6.534472913661342e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.82it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.2446, 'grad_norm': 3.5764973163604736, 'learning_rate': 6.35786553761644e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.1046, 'grad_norm': 3.5330069065093994, 'learning_rate': 6.18125816157154e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.2616, 'grad_norm': 2.601198673248291, 'learning_rate': 6.004650785526638e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.60it/s] 57%|█████▋    | 43/75 [00:01<00:01, 25.14it/s]                                               {'loss': 2.1289, 'grad_norm': 5.653541088104248, 'learning_rate': 5.828043409481737e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.14it/s]                                               {'loss': 2.3386, 'grad_norm': 4.222101211547852, 'learning_rate': 5.651436033436836e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.14it/s]                                               {'loss': 2.4447, 'grad_norm': 17.41158103942871, 'learning_rate': 5.4748286573919344e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.14it/s] 61%|██████▏   | 46/75 [00:01<00:01, 24.43it/s]                                               {'loss': 2.0019, 'grad_norm': 3.3566787242889404, 'learning_rate': 5.298221281347034e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.43it/s]                                               {'loss': 2.1482, 'grad_norm': 4.100178241729736, 'learning_rate': 5.121613905302133e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 24.43it/s]                                               {'loss': 1.9026, 'grad_norm': 2.946546792984009, 'learning_rate': 4.9450065292572316e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 24.43it/s] 65%|██████▌   | 49/75 [00:01<00:01, 24.04it/s]                                               {'loss': 2.0312, 'grad_norm': 3.324672222137451, 'learning_rate': 4.7683991532123304e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 24.04it/s]                                               {'loss': 2.0854, 'grad_norm': 3.5025346279144287, 'learning_rate': 4.591791777167429e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:01, 24.04it/s]                                               {'loss': 2.247, 'grad_norm': 3.522279739379883, 'learning_rate': 4.415184401122528e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 24.04it/s] 69%|██████▉   | 52/75 [00:02<00:00, 23.14it/s]                                               {'loss': 2.3095, 'grad_norm': 5.094314098358154, 'learning_rate': 4.238577025077627e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 23.14it/s]                                               {'loss': 2.0842, 'grad_norm': 3.1391828060150146, 'learning_rate': 4.061969649032726e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 23.14it/s]                                               {'loss': 2.3065, 'grad_norm': 3.323345422744751, 'learning_rate': 3.8853622729878245e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 23.14it/s] 73%|███████▎  | 55/75 [00:02<00:00, 23.65it/s]                                               {'loss': 1.9701, 'grad_norm': 3.1736197471618652, 'learning_rate': 3.708754896942924e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 23.65it/s]                                               {'loss': 1.9983, 'grad_norm': 4.284393787384033, 'learning_rate': 3.532147520898022e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 23.65it/s]                                               {'loss': 2.1335, 'grad_norm': 3.5306882858276367, 'learning_rate': 3.355540144853122e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 23.65it/s] 77%|███████▋  | 58/75 [00:02<00:00, 23.84it/s]                                               {'loss': 2.3992, 'grad_norm': 3.4430181980133057, 'learning_rate': 3.17893276880822e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 23.84it/s]                                               {'loss': 2.0677, 'grad_norm': 3.356858730316162, 'learning_rate': 3.002325392763319e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 23.84it/s]                                               {'loss': 1.8808, 'grad_norm': 8.439315795898438, 'learning_rate': 2.825718016718418e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 23.84it/s] 81%|████████▏ | 61/75 [00:02<00:00, 25.16it/s]                                               {'loss': 1.8549, 'grad_norm': 3.215935230255127, 'learning_rate': 2.649110640673517e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.0785, 'grad_norm': 2.7308685779571533, 'learning_rate': 2.4725032646286158e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.0834, 'grad_norm': 3.602098226547241, 'learning_rate': 2.2958958885837146e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.16it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.1208, 'grad_norm': 2.8641300201416016, 'learning_rate': 2.1192885125388134e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.0912, 'grad_norm': 3.4755637645721436, 'learning_rate': 1.9426811364939123e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.2193, 'grad_norm': 4.078632831573486, 'learning_rate': 1.766073760449011e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.46it/s] 89%|████████▉ | 67/75 [00:02<00:00, 24.68it/s]                                               {'loss': 2.2027, 'grad_norm': 2.9503555297851562, 'learning_rate': 1.58946638440411e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.68it/s]                                               {'loss': 2.4778, 'grad_norm': 5.337131023406982, 'learning_rate': 1.412859008359209e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.68it/s]                                               {'loss': 2.113, 'grad_norm': 3.3973333835601807, 'learning_rate': 1.2362516323143079e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.68it/s] 93%|█████████▎| 70/75 [00:02<00:00, 24.17it/s]                                               {'loss': 2.2523, 'grad_norm': 4.6076812744140625, 'learning_rate': 1.0596442562694067e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.17it/s]                                               {'loss': 1.8916, 'grad_norm': 3.692251443862915, 'learning_rate': 8.830368802245055e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.17it/s]                                               {'loss': 2.1392, 'grad_norm': 3.4939486980438232, 'learning_rate': 7.064295041796045e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.17it/s] 97%|█████████▋| 73/75 [00:02<00:00, 24.21it/s]                                               {'loss': 2.114, 'grad_norm': 4.111584663391113, 'learning_rate': 5.298221281347034e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.21it/s]                                               {'loss': 2.0029, 'grad_norm': 3.7617220878601074, 'learning_rate': 3.5321475208980227e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.21it/s]                                               {'loss': 1.3623, 'grad_norm': 5.678264141082764, 'learning_rate': 1.7660737604490113e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.21it/s]                                               {'train_runtime': 3.1029, 'train_samples_per_second': 364.177, 'train_steps_per_second': 24.171, 'train_loss': 2.1330173953374225, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.21it/s]100%|██████████| 75/75 [00:03<00:00, 24.17it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:20
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4253, 'grad_norm': 4.862011432647705, 'learning_rate': 0.00013245553203367584, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.65it/s]                                              {'loss': 2.2934, 'grad_norm': 3.951101303100586, 'learning_rate': 0.00013068945827322684, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.55it/s]  4%|▍         | 3/75 [00:00<00:02, 24.89it/s]                                              {'loss': 2.4115, 'grad_norm': 4.096745491027832, 'learning_rate': 0.00012892338451277782, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.89it/s]                                              {'loss': 2.24, 'grad_norm': 5.18683385848999, 'learning_rate': 0.0001271573107523288, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.89it/s]                                              {'loss': 2.4008, 'grad_norm': 3.957855701446533, 'learning_rate': 0.0001253912369918798, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.89it/s]  8%|▊         | 6/75 [00:00<00:02, 24.92it/s]                                              {'loss': 2.4292, 'grad_norm': 3.3758838176727295, 'learning_rate': 0.0001236251632314308, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.92it/s]                                              {'loss': 2.3772, 'grad_norm': 4.036861419677734, 'learning_rate': 0.00012185908947098178, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.92it/s]                                              {'loss': 2.4413, 'grad_norm': 4.407439231872559, 'learning_rate': 0.00012009301571053276, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.92it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.55it/s]                                              {'loss': 2.1181, 'grad_norm': 3.2912278175354004, 'learning_rate': 0.00011832694195008375, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.55it/s]                                              {'loss': 2.3517, 'grad_norm': 4.911542892456055, 'learning_rate': 0.00011656086818963474, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.55it/s]                                               {'loss': 2.6003, 'grad_norm': 4.242712020874023, 'learning_rate': 0.00011479479442918574, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.55it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.76it/s]                                               {'loss': 2.3484, 'grad_norm': 4.407774448394775, 'learning_rate': 0.00011302872066873673, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.76it/s]                                               {'loss': 2.0571, 'grad_norm': 5.561676979064941, 'learning_rate': 0.0001112626469082877, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.76it/s]                                               {'loss': 2.0714, 'grad_norm': 3.12792706489563, 'learning_rate': 0.00010949657314783869, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.76it/s] 20%|██        | 15/75 [00:00<00:02, 25.23it/s]                                               {'loss': 2.5986, 'grad_norm': 10.715228080749512, 'learning_rate': 0.00010773049938738969, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.23it/s]                                               {'loss': 2.1941, 'grad_norm': 3.9328231811523438, 'learning_rate': 0.00010596442562694068, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.23it/s]                                               {'loss': 2.2826, 'grad_norm': 4.648440837860107, 'learning_rate': 0.00010419835186649165, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.23it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.04it/s]                                               {'loss': 2.0816, 'grad_norm': 4.021644115447998, 'learning_rate': 0.00010243227810604265, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.04it/s]                                               {'loss': 2.4483, 'grad_norm': 4.0698466300964355, 'learning_rate': 0.00010066620434559364, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.04it/s]                                               {'loss': 2.3599, 'grad_norm': 3.7297682762145996, 'learning_rate': 9.890013058514463e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.04it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.79it/s]                                               {'loss': 2.3009, 'grad_norm': 3.9412739276885986, 'learning_rate': 9.71340568246956e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.79it/s]                                               {'loss': 2.2819, 'grad_norm': 4.0691118240356445, 'learning_rate': 9.536798306424661e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.79it/s]                                               {'loss': 2.1611, 'grad_norm': 4.11741828918457, 'learning_rate': 9.36019093037976e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.79it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.62it/s]                                               {'loss': 2.2129, 'grad_norm': 4.309831619262695, 'learning_rate': 9.183583554334858e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.62it/s]                                               {'loss': 2.1175, 'grad_norm': 4.802643775939941, 'learning_rate': 9.006976178289957e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.62it/s]                                               {'loss': 2.4452, 'grad_norm': 3.1026692390441895, 'learning_rate': 8.830368802245056e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.62it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.98it/s]                                               {'loss': 2.192, 'grad_norm': 3.774132013320923, 'learning_rate': 8.653761426200155e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.98it/s]                                               {'loss': 2.3999, 'grad_norm': 3.8304247856140137, 'learning_rate': 8.477154050155254e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.98it/s]                                               {'loss': 2.1796, 'grad_norm': 3.35109806060791, 'learning_rate': 8.300546674110353e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.98it/s] 40%|████      | 30/75 [00:01<00:01, 24.90it/s]                                               {'loss': 2.7339, 'grad_norm': 14.642969131469727, 'learning_rate': 8.123939298065451e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.90it/s]                                               {'loss': 2.2154, 'grad_norm': 4.439399719238281, 'learning_rate': 7.94733192202055e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.90it/s]                                               {'loss': 2.3609, 'grad_norm': 4.6852264404296875, 'learning_rate': 7.770724545975649e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.90it/s] 44%|████▍     | 33/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.1435, 'grad_norm': 3.3541321754455566, 'learning_rate': 7.594117169930749e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.5053, 'grad_norm': 4.24608039855957, 'learning_rate': 7.417509793885848e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.063, 'grad_norm': 3.6740851402282715, 'learning_rate': 7.240902417840946e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.57it/s] 48%|████▊     | 36/75 [00:01<00:01, 25.21it/s]                                               {'loss': 2.2284, 'grad_norm': 3.3113954067230225, 'learning_rate': 7.064295041796044e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.21it/s]                                               {'loss': 2.412, 'grad_norm': 4.8339762687683105, 'learning_rate': 6.887687665751145e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.21it/s]                                               {'loss': 2.2375, 'grad_norm': 3.178964853286743, 'learning_rate': 6.711080289706243e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.21it/s] 52%|█████▏    | 39/75 [00:01<00:01, 24.92it/s]                                               {'loss': 2.0856, 'grad_norm': 3.659149646759033, 'learning_rate': 6.534472913661342e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.92it/s]                                               {'loss': 2.4748, 'grad_norm': 4.164175510406494, 'learning_rate': 6.35786553761644e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.92it/s]                                               {'loss': 2.0918, 'grad_norm': 4.40055513381958, 'learning_rate': 6.18125816157154e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.92it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.3993, 'grad_norm': 4.88770866394043, 'learning_rate': 6.004650785526638e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.1616, 'grad_norm': 3.4453930854797363, 'learning_rate': 5.828043409481737e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.1146, 'grad_norm': 3.742063045501709, 'learning_rate': 5.651436033436836e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.57it/s] 60%|██████    | 45/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.3939, 'grad_norm': 14.425963401794434, 'learning_rate': 5.4748286573919344e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.2235, 'grad_norm': 4.563340187072754, 'learning_rate': 5.298221281347034e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.1969, 'grad_norm': 4.230530738830566, 'learning_rate': 5.121613905302133e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.24it/s] 64%|██████▍   | 48/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.4285, 'grad_norm': 4.356956481933594, 'learning_rate': 4.9450065292572316e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.3348, 'grad_norm': 3.8843138217926025, 'learning_rate': 4.7683991532123304e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.4147, 'grad_norm': 6.525835990905762, 'learning_rate': 4.591791777167429e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.31it/s] 68%|██████▊   | 51/75 [00:02<00:00, 25.20it/s]                                               {'loss': 2.0392, 'grad_norm': 3.653496265411377, 'learning_rate': 4.415184401122528e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.20it/s]                                               {'loss': 2.1723, 'grad_norm': 3.6779139041900635, 'learning_rate': 4.238577025077627e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.20it/s]                                               {'loss': 2.2024, 'grad_norm': 4.925724029541016, 'learning_rate': 4.061969649032726e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.20it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.12it/s]                                               {'loss': 2.3093, 'grad_norm': 3.320706844329834, 'learning_rate': 3.8853622729878245e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.12it/s]                                               {'loss': 2.1563, 'grad_norm': 3.8341071605682373, 'learning_rate': 3.708754896942924e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.12it/s]                                               {'loss': 2.2439, 'grad_norm': 3.7816717624664307, 'learning_rate': 3.532147520898022e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.12it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.89it/s]                                               {'loss': 2.3267, 'grad_norm': 4.288650989532471, 'learning_rate': 3.355540144853122e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.89it/s]                                               {'loss': 2.1768, 'grad_norm': 4.805664539337158, 'learning_rate': 3.17893276880822e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.89it/s]                                               {'loss': 2.1185, 'grad_norm': 4.1445112228393555, 'learning_rate': 3.002325392763319e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.89it/s] 80%|████████  | 60/75 [00:02<00:00, 25.87it/s]                                               {'loss': 2.624, 'grad_norm': 13.233298301696777, 'learning_rate': 2.825718016718418e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.87it/s]                                               {'loss': 2.3259, 'grad_norm': 3.3938589096069336, 'learning_rate': 2.649110640673517e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.87it/s]                                               {'loss': 2.3053, 'grad_norm': 4.626832008361816, 'learning_rate': 2.4725032646286158e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.87it/s] 84%|████████▍ | 63/75 [00:02<00:00, 24.66it/s]                                               {'loss': 1.9755, 'grad_norm': 3.935702085494995, 'learning_rate': 2.2958958885837146e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 24.66it/s]                                               {'loss': 2.2142, 'grad_norm': 3.8988776206970215, 'learning_rate': 2.1192885125388134e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 24.66it/s]                                               {'loss': 1.8813, 'grad_norm': 3.6526596546173096, 'learning_rate': 1.9426811364939123e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.66it/s] 88%|████████▊ | 66/75 [00:02<00:00, 24.86it/s]                                               {'loss': 2.1453, 'grad_norm': 3.794339418411255, 'learning_rate': 1.766073760449011e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.86it/s]                                               {'loss': 2.3795, 'grad_norm': 4.018055438995361, 'learning_rate': 1.58946638440411e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.86it/s]                                               {'loss': 2.1409, 'grad_norm': 4.181568145751953, 'learning_rate': 1.412859008359209e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.86it/s] 92%|█████████▏| 69/75 [00:02<00:00, 24.95it/s]                                               {'loss': 2.2621, 'grad_norm': 4.183399200439453, 'learning_rate': 1.2362516323143079e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.95it/s]                                               {'loss': 2.3507, 'grad_norm': 3.206622362136841, 'learning_rate': 1.0596442562694067e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.95it/s]                                               {'loss': 2.239, 'grad_norm': 4.057283401489258, 'learning_rate': 8.830368802245055e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.95it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.1995, 'grad_norm': 3.746713161468506, 'learning_rate': 7.064295041796045e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.3092, 'grad_norm': 4.623250484466553, 'learning_rate': 5.298221281347034e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.3037, 'grad_norm': 3.837136745452881, 'learning_rate': 3.5321475208980227e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.3512, 'grad_norm': 17.996227264404297, 'learning_rate': 1.7660737604490113e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.02it/s]                                               {'train_runtime': 3.0895, 'train_samples_per_second': 365.76, 'train_steps_per_second': 24.276, 'train_loss': 2.2772565412521364, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.02it/s]100%|██████████| 75/75 [00:03<00:00, 24.28it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(981, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(916, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(736, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(890, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1125, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(885, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1019, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(865, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(918, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(607, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1116, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(536, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 61.33it/s]  3%|▎         | 14/471 [00:00<00:08, 52.10it/s]  4%|▍         | 20/471 [00:00<00:09, 49.50it/s]  6%|▌         | 26/471 [00:00<00:08, 50.51it/s]  7%|▋         | 32/471 [00:00<00:08, 50.22it/s]  8%|▊         | 38/471 [00:00<00:08, 51.26it/s]  9%|▉         | 44/471 [00:00<00:08, 51.88it/s] 11%|█         | 50/471 [00:00<00:08, 51.02it/s] 12%|█▏        | 56/471 [00:01<00:08, 49.88it/s] 13%|█▎        | 62/471 [00:01<00:07, 51.40it/s] 14%|█▍        | 68/471 [00:01<00:07, 51.52it/s] 16%|█▌        | 74/471 [00:01<00:07, 51.96it/s] 17%|█▋        | 80/471 [00:01<00:07, 52.36it/s] 18%|█▊        | 86/471 [00:01<00:07, 50.97it/s] 20%|█▉        | 92/471 [00:01<00:07, 52.03it/s] 21%|██        | 98/471 [00:01<00:07, 51.88it/s] 22%|██▏       | 104/471 [00:02<00:07, 52.23it/s] 23%|██▎       | 110/471 [00:02<00:07, 51.52it/s] 25%|██▍       | 116/471 [00:02<00:06, 51.41it/s] 26%|██▌       | 122/471 [00:02<00:06, 51.25it/s] 27%|██▋       | 128/471 [00:02<00:06, 51.63it/s] 28%|██▊       | 134/471 [00:02<00:06, 51.33it/s] 30%|██▉       | 140/471 [00:02<00:06, 51.62it/s] 31%|███       | 146/471 [00:02<00:06, 52.22it/s] 32%|███▏      | 152/471 [00:02<00:06, 51.90it/s] 34%|███▎      | 158/471 [00:03<00:06, 50.27it/s] 35%|███▍      | 164/471 [00:03<00:06, 50.26it/s] 36%|███▌      | 170/471 [00:03<00:06, 49.03it/s] 37%|███▋      | 176/471 [00:03<00:05, 50.65it/s] 39%|███▊      | 182/471 [00:03<00:05, 51.24it/s] 40%|███▉      | 188/471 [00:03<00:05, 51.62it/s] 41%|████      | 194/471 [00:03<00:05, 52.03it/s] 42%|████▏     | 200/471 [00:03<00:05, 51.85it/s] 44%|████▎     | 206/471 [00:04<00:05, 51.43it/s] 45%|████▌     | 212/471 [00:04<00:05, 51.41it/s] 46%|████▋     | 218/471 [00:04<00:04, 51.41it/s] 48%|████▊     | 224/471 [00:04<00:05, 48.61it/s] 49%|████▉     | 230/471 [00:04<00:04, 49.92it/s] 50%|█████     | 236/471 [00:04<00:04, 49.91it/s] 51%|█████▏    | 242/471 [00:04<00:04, 50.26it/s] 53%|█████▎    | 248/471 [00:04<00:05, 44.27it/s] 54%|█████▍    | 254/471 [00:05<00:04, 46.41it/s] 55%|█████▍    | 259/471 [00:05<00:04, 46.60it/s] 56%|█████▌    | 264/471 [00:05<00:04, 45.57it/s] 57%|█████▋    | 269/471 [00:05<00:04, 46.05it/s] 58%|█████▊    | 274/471 [00:05<00:04, 46.57it/s] 59%|█████▉    | 280/471 [00:05<00:03, 48.04it/s] 61%|██████    | 286/471 [00:05<00:03, 48.84it/s] 62%|██████▏   | 291/471 [00:05<00:03, 48.71it/s] 63%|██████▎   | 297/471 [00:05<00:03, 50.14it/s] 64%|██████▍   | 303/471 [00:06<00:03, 50.80it/s] 66%|██████▌   | 309/471 [00:06<00:03, 49.56it/s] 67%|██████▋   | 314/471 [00:06<00:03, 48.43it/s] 68%|██████▊   | 320/471 [00:06<00:03, 49.67it/s] 69%|██████▉   | 325/471 [00:06<00:02, 49.50it/s] 70%|███████   | 331/471 [00:06<00:02, 50.49it/s] 72%|███████▏  | 337/471 [00:06<00:02, 50.51it/s] 73%|███████▎  | 343/471 [00:06<00:02, 50.24it/s] 74%|███████▍  | 349/471 [00:06<00:02, 50.98it/s] 75%|███████▌  | 355/471 [00:07<00:02, 50.42it/s] 77%|███████▋  | 361/471 [00:07<00:02, 49.69it/s] 78%|███████▊  | 366/471 [00:07<00:02, 49.67it/s] 79%|███████▉  | 371/471 [00:07<00:02, 49.41it/s] 80%|███████▉  | 376/471 [00:07<00:01, 49.22it/s] 81%|████████  | 382/471 [00:07<00:01, 50.48it/s] 82%|████████▏ | 388/471 [00:07<00:01, 50.37it/s] 84%|████████▎ | 394/471 [00:07<00:01, 49.74it/s] 85%|████████▍ | 399/471 [00:07<00:01, 49.75it/s] 86%|████████▌ | 405/471 [00:08<00:01, 50.69it/s] 87%|████████▋ | 411/471 [00:08<00:01, 50.26it/s] 89%|████████▊ | 417/471 [00:08<00:01, 50.68it/s] 90%|████████▉ | 423/471 [00:08<00:00, 51.16it/s] 91%|█████████ | 429/471 [00:08<00:00, 51.33it/s] 92%|█████████▏| 435/471 [00:08<00:00, 50.83it/s] 94%|█████████▎| 441/471 [00:08<00:00, 51.17it/s] 95%|█████████▍| 447/471 [00:08<00:00, 49.31it/s] 96%|█████████▌| 452/471 [00:09<00:00, 49.18it/s] 97%|█████████▋| 458/471 [00:09<00:00, 49.83it/s] 98%|█████████▊| 463/471 [00:09<00:00, 47.76it/s]100%|█████████▉| 469/471 [00:09<00:00, 49.16it/s]100%|██████████| 471/471 [00:09<00:00, 50.21it/s]
{'eval_loss': 2.2759671211242676, 'eval_model_preparation_time': 0.0031, 'eval_acc': 0.39564524694636216, 'eval_runtime': 9.4027, 'eval_samples_per_second': 801.047, 'eval_steps_per_second': 50.092}
ROUND:17
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2074, 'grad_norm': 4.572240829467773, 'learning_rate': 0.00013021734901736987, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 19.60it/s]                                              {'loss': 2.2465, 'grad_norm': 4.623925685882568, 'learning_rate': 0.00012848111769713828, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 21.56it/s]  4%|▍         | 3/75 [00:00<00:03, 21.93it/s]                                              {'loss': 2.1393, 'grad_norm': 4.518580436706543, 'learning_rate': 0.0001267448863769067, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 21.93it/s]                                              {'loss': 2.2548, 'grad_norm': 3.6795151233673096, 'learning_rate': 0.00012500865505667507, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 21.93it/s]                                              {'loss': 2.17, 'grad_norm': 4.295724391937256, 'learning_rate': 0.00012327242373644348, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 21.93it/s]  8%|▊         | 6/75 [00:00<00:02, 23.65it/s]                                              {'loss': 2.3171, 'grad_norm': 3.856025218963623, 'learning_rate': 0.00012153619241621188, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.65it/s]                                              {'loss': 2.3335, 'grad_norm': 4.11488676071167, 'learning_rate': 0.00011979996109598029, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.65it/s]                                              {'loss': 2.2469, 'grad_norm': 4.253942966461182, 'learning_rate': 0.00011806372977574867, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.65it/s] 12%|█▏        | 9/75 [00:00<00:02, 23.74it/s]                                              {'loss': 2.3236, 'grad_norm': 3.49102783203125, 'learning_rate': 0.00011632749845551708, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.74it/s]                                              {'loss': 2.3684, 'grad_norm': 4.245337963104248, 'learning_rate': 0.00011459126713528549, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.74it/s]                                               {'loss': 2.2341, 'grad_norm': 4.0809502601623535, 'learning_rate': 0.00011285503581505389, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.74it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.15it/s]                                               {'loss': 2.201, 'grad_norm': 4.658288955688477, 'learning_rate': 0.0001111188044948223, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.15it/s]                                               {'loss': 2.3448, 'grad_norm': 4.116631984710693, 'learning_rate': 0.0001093825731745907, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.15it/s]                                               {'loss': 2.1406, 'grad_norm': 3.9152140617370605, 'learning_rate': 0.00010764634185435909, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.15it/s]                                               {'loss': 2.343, 'grad_norm': 13.33440113067627, 'learning_rate': 0.0001059101105341275, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.15it/s] 21%|██▏       | 16/75 [00:00<00:02, 25.87it/s]                                               {'loss': 2.0638, 'grad_norm': 2.845923662185669, 'learning_rate': 0.00010417387921389591, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.87it/s]                                               {'loss': 2.3477, 'grad_norm': 4.244011402130127, 'learning_rate': 0.00010243764789366429, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.87it/s]                                               {'loss': 2.2598, 'grad_norm': 3.450658082962036, 'learning_rate': 0.0001007014165734327, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.87it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.16it/s]                                               {'loss': 2.2484, 'grad_norm': 4.407414436340332, 'learning_rate': 9.896518525320111e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.16it/s]                                               {'loss': 2.3332, 'grad_norm': 3.90901780128479, 'learning_rate': 9.722895393296951e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.16it/s]                                               {'loss': 2.0541, 'grad_norm': 3.3403782844543457, 'learning_rate': 9.54927226127379e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.16it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.74it/s]                                               {'loss': 2.0479, 'grad_norm': 3.9701671600341797, 'learning_rate': 9.37564912925063e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.74it/s]                                               {'loss': 2.2312, 'grad_norm': 4.490180969238281, 'learning_rate': 9.202025997227471e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.74it/s]                                               {'loss': 2.2146, 'grad_norm': 3.210794448852539, 'learning_rate': 9.028402865204312e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.74it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.63it/s]                                               {'loss': 2.2261, 'grad_norm': 3.5635061264038086, 'learning_rate': 8.854779733181151e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.63it/s]                                               {'loss': 2.0885, 'grad_norm': 5.992856025695801, 'learning_rate': 8.681156601157991e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.63it/s]                                               {'loss': 2.1946, 'grad_norm': 3.970928907394409, 'learning_rate': 8.507533469134832e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.63it/s] 37%|███▋      | 28/75 [00:01<00:01, 23.81it/s]                                               {'loss': 2.2357, 'grad_norm': 4.441601753234863, 'learning_rate': 8.333910337111672e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 23.81it/s]                                               {'loss': 2.3012, 'grad_norm': 6.12552547454834, 'learning_rate': 8.160287205088513e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 23.81it/s]                                               {'loss': 2.0162, 'grad_norm': 12.835637092590332, 'learning_rate': 7.986664073065352e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.81it/s]                                               {'loss': 2.0934, 'grad_norm': 3.4224965572357178, 'learning_rate': 7.813040941042192e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 23.81it/s] 43%|████▎     | 32/75 [00:01<00:01, 25.67it/s]                                               {'loss': 2.2464, 'grad_norm': 5.466696739196777, 'learning_rate': 7.639417809019033e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.67it/s]                                               {'loss': 2.188, 'grad_norm': 4.907799243927002, 'learning_rate': 7.465794676995872e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.67it/s]                                               {'loss': 2.0486, 'grad_norm': 2.894029140472412, 'learning_rate': 7.292171544972713e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.67it/s] 47%|████▋     | 35/75 [00:01<00:01, 25.82it/s]                                               {'loss': 2.1924, 'grad_norm': 5.774167537689209, 'learning_rate': 7.118548412949553e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.82it/s]                                               {'loss': 2.1116, 'grad_norm': 4.236401081085205, 'learning_rate': 6.944925280926393e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.82it/s]                                               {'loss': 2.0316, 'grad_norm': 3.7260046005249023, 'learning_rate': 6.771302148903234e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.82it/s] 51%|█████     | 38/75 [00:01<00:01, 25.26it/s]                                               {'loss': 2.1556, 'grad_norm': 3.261190414428711, 'learning_rate': 6.597679016880074e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.26it/s]                                               {'loss': 2.0928, 'grad_norm': 4.610795497894287, 'learning_rate': 6.424055884856914e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.26it/s]                                               {'loss': 2.2303, 'grad_norm': 3.783914089202881, 'learning_rate': 6.250432752833754e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.26it/s] 55%|█████▍    | 41/75 [00:01<00:01, 24.42it/s]                                               {'loss': 2.0779, 'grad_norm': 4.280150890350342, 'learning_rate': 6.076809620810594e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.42it/s]                                               {'loss': 2.1721, 'grad_norm': 4.617460250854492, 'learning_rate': 5.9031864887874336e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.42it/s]                                               {'loss': 2.1599, 'grad_norm': 3.7173826694488525, 'learning_rate': 5.7295633567642746e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.42it/s] 59%|█████▊    | 44/75 [00:01<00:01, 24.46it/s]                                               {'loss': 2.2608, 'grad_norm': 3.271787643432617, 'learning_rate': 5.555940224741115e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.46it/s]                                               {'loss': 2.6868, 'grad_norm': 15.220884323120117, 'learning_rate': 5.3823170927179545e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.46it/s]                                               {'loss': 2.2687, 'grad_norm': 3.6673572063446045, 'learning_rate': 5.2086939606947954e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.46it/s] 63%|██████▎   | 47/75 [00:01<00:01, 25.43it/s]                                               {'loss': 2.4031, 'grad_norm': 5.016770839691162, 'learning_rate': 5.035070828671635e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.43it/s]                                               {'loss': 2.2003, 'grad_norm': 3.8316338062286377, 'learning_rate': 4.8614476966484753e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.43it/s]                                               {'loss': 2.2288, 'grad_norm': 3.8494348526000977, 'learning_rate': 4.687824564625315e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.43it/s] 67%|██████▋   | 50/75 [00:01<00:00, 25.46it/s]                                               {'loss': 2.2555, 'grad_norm': 5.236601829528809, 'learning_rate': 4.514201432602156e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.46it/s]                                               {'loss': 2.0776, 'grad_norm': 3.0199167728424072, 'learning_rate': 4.3405783005789955e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.0863, 'grad_norm': 3.4268062114715576, 'learning_rate': 4.166955168555836e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.46it/s] 71%|███████   | 53/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.0203, 'grad_norm': 3.832119941711426, 'learning_rate': 3.993332036532676e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.0981, 'grad_norm': 4.765934944152832, 'learning_rate': 3.8197089045095164e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.2823, 'grad_norm': 4.668705940246582, 'learning_rate': 3.646085772486357e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.16it/s] 75%|███████▍  | 56/75 [00:02<00:00, 24.48it/s]                                               {'loss': 2.1205, 'grad_norm': 4.217228889465332, 'learning_rate': 3.472462640463196e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.48it/s]                                               {'loss': 2.053, 'grad_norm': 4.0430755615234375, 'learning_rate': 3.298839508440037e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.48it/s]                                               {'loss': 2.1561, 'grad_norm': 4.173603057861328, 'learning_rate': 3.125216376416877e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.48it/s] 79%|███████▊  | 59/75 [00:02<00:00, 24.42it/s]                                               {'loss': 2.0078, 'grad_norm': 2.9576961994171143, 'learning_rate': 2.9515932443937168e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.42it/s]                                               {'loss': 1.8957, 'grad_norm': 8.244973182678223, 'learning_rate': 2.7779701123705574e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.42it/s]                                               {'loss': 2.0673, 'grad_norm': 3.7354068756103516, 'learning_rate': 2.6043469803473977e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.42it/s] 83%|████████▎ | 62/75 [00:02<00:00, 23.35it/s]                                               {'loss': 2.1383, 'grad_norm': 3.0575029850006104, 'learning_rate': 2.4307238483242377e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 23.35it/s]                                               {'loss': 2.1888, 'grad_norm': 5.1424560546875, 'learning_rate': 2.257100716301078e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 23.35it/s]                                               {'loss': 2.1743, 'grad_norm': 4.413637161254883, 'learning_rate': 2.083477584277918e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 23.35it/s] 87%|████████▋ | 65/75 [00:02<00:00, 23.83it/s]                                               {'loss': 2.1242, 'grad_norm': 4.595987796783447, 'learning_rate': 1.9098544522547582e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 23.83it/s]                                               {'loss': 2.1686, 'grad_norm': 3.9691162109375, 'learning_rate': 1.736231320231598e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 23.83it/s]                                               {'loss': 2.1807, 'grad_norm': 3.17938232421875, 'learning_rate': 1.5626081882084384e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 23.83it/s] 91%|█████████ | 68/75 [00:02<00:00, 24.08it/s]                                               {'loss': 2.1876, 'grad_norm': 5.3660383224487305, 'learning_rate': 1.3889850561852787e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.08it/s]                                               {'loss': 2.0767, 'grad_norm': 5.033926486968994, 'learning_rate': 1.2153619241621188e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.08it/s]                                               {'loss': 2.0178, 'grad_norm': 3.484837532043457, 'learning_rate': 1.041738792138959e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.08it/s] 95%|█████████▍| 71/75 [00:02<00:00, 24.15it/s]                                               {'loss': 2.1, 'grad_norm': 4.629432201385498, 'learning_rate': 8.68115660115799e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.15it/s]                                               {'loss': 2.2164, 'grad_norm': 3.8268046379089355, 'learning_rate': 6.944925280926394e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.15it/s]                                               {'loss': 2.2501, 'grad_norm': 3.288148880004883, 'learning_rate': 5.208693960694795e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.15it/s] 99%|█████████▊| 74/75 [00:02<00:00, 24.88it/s]                                               {'loss': 2.2004, 'grad_norm': 4.032986164093018, 'learning_rate': 3.472462640463197e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.88it/s]                                               {'loss': 1.7541, 'grad_norm': 9.878403663635254, 'learning_rate': 1.7362313202315984e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.88it/s]                                               {'train_runtime': 3.1198, 'train_samples_per_second': 362.205, 'train_steps_per_second': 24.04, 'train_loss': 2.179754050572713, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.88it/s]100%|██████████| 75/75 [00:03<00:00, 24.04it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:48
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3762, 'grad_norm': 5.000331401824951, 'learning_rate': 0.00013021734901736987, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 20.51it/s]                                              {'loss': 2.3136, 'grad_norm': 5.051638126373291, 'learning_rate': 0.00012848111769713828, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.08it/s]  4%|▍         | 3/75 [00:00<00:03, 23.98it/s]                                              {'loss': 2.4622, 'grad_norm': 3.867403268814087, 'learning_rate': 0.0001267448863769067, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.98it/s]                                              {'loss': 2.1188, 'grad_norm': 3.701298236846924, 'learning_rate': 0.00012500865505667507, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 23.98it/s]                                              {'loss': 2.2861, 'grad_norm': 3.7044804096221924, 'learning_rate': 0.00012327242373644348, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.98it/s]  8%|▊         | 6/75 [00:00<00:02, 24.00it/s]                                              {'loss': 2.2358, 'grad_norm': 4.263367652893066, 'learning_rate': 0.00012153619241621188, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.00it/s]                                              {'loss': 2.514, 'grad_norm': 4.667040824890137, 'learning_rate': 0.00011979996109598029, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.00it/s]                                              {'loss': 2.2043, 'grad_norm': 3.9248905181884766, 'learning_rate': 0.00011806372977574867, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.00it/s] 12%|█▏        | 9/75 [00:00<00:02, 23.46it/s]                                              {'loss': 2.2651, 'grad_norm': 4.944910526275635, 'learning_rate': 0.00011632749845551708, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.46it/s]                                              {'loss': 2.2478, 'grad_norm': 3.7963454723358154, 'learning_rate': 0.00011459126713528549, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.46it/s]                                               {'loss': 2.3743, 'grad_norm': 3.4885787963867188, 'learning_rate': 0.00011285503581505389, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.46it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.02it/s]                                               {'loss': 2.2536, 'grad_norm': 3.709935426712036, 'learning_rate': 0.0001111188044948223, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.02it/s]                                               {'loss': 2.6944, 'grad_norm': 6.309396743774414, 'learning_rate': 0.0001093825731745907, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.02it/s]                                               {'loss': 2.092, 'grad_norm': 3.7195775508880615, 'learning_rate': 0.00010764634185435909, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.02it/s]                                               {'loss': 1.8578, 'grad_norm': 9.109712600708008, 'learning_rate': 0.0001059101105341275, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.02it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.46it/s]                                               {'loss': 2.4209, 'grad_norm': 3.742295026779175, 'learning_rate': 0.00010417387921389591, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.46it/s]                                               {'loss': 2.2903, 'grad_norm': 3.808598041534424, 'learning_rate': 0.00010243764789366429, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.46it/s]                                               {'loss': 2.6836, 'grad_norm': 5.4478068351745605, 'learning_rate': 0.0001007014165734327, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.46it/s] 25%|██▌       | 19/75 [00:00<00:02, 24.90it/s]                                               {'loss': 2.214, 'grad_norm': 4.075634479522705, 'learning_rate': 9.896518525320111e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.90it/s]                                               {'loss': 2.1907, 'grad_norm': 4.356686592102051, 'learning_rate': 9.722895393296951e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.90it/s]                                               {'loss': 2.1928, 'grad_norm': 4.080817699432373, 'learning_rate': 9.54927226127379e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.90it/s] 29%|██▉       | 22/75 [00:00<00:02, 21.03it/s]                                               {'loss': 2.071, 'grad_norm': 4.0831403732299805, 'learning_rate': 9.37564912925063e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 21.03it/s]                                               {'loss': 2.1111, 'grad_norm': 3.5508272647857666, 'learning_rate': 9.202025997227471e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 21.03it/s]                                               {'loss': 2.2189, 'grad_norm': 3.3813259601593018, 'learning_rate': 9.028402865204312e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 21.03it/s] 33%|███▎      | 25/75 [00:01<00:02, 20.57it/s]                                               {'loss': 2.1423, 'grad_norm': 4.449933052062988, 'learning_rate': 8.854779733181151e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 20.57it/s]                                               {'loss': 2.1576, 'grad_norm': 4.880620002746582, 'learning_rate': 8.681156601157991e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 20.57it/s]                                               {'loss': 2.2957, 'grad_norm': 5.554994583129883, 'learning_rate': 8.507533469134832e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 20.57it/s] 37%|███▋      | 28/75 [00:01<00:02, 21.25it/s]                                               {'loss': 2.2246, 'grad_norm': 4.01203727722168, 'learning_rate': 8.333910337111672e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 21.25it/s]                                               {'loss': 2.2388, 'grad_norm': 4.76024055480957, 'learning_rate': 8.160287205088513e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 21.25it/s]                                               {'loss': 1.9672, 'grad_norm': 7.304728984832764, 'learning_rate': 7.986664073065352e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 21.25it/s] 41%|████▏     | 31/75 [00:01<00:02, 20.95it/s]                                               {'loss': 2.2525, 'grad_norm': 4.215663909912109, 'learning_rate': 7.813040941042192e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:02, 20.95it/s]                                               {'loss': 2.1723, 'grad_norm': 3.5668580532073975, 'learning_rate': 7.639417809019033e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:02, 20.95it/s]                                               {'loss': 2.3301, 'grad_norm': 4.189387798309326, 'learning_rate': 7.465794676995872e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:02, 20.95it/s] 45%|████▌     | 34/75 [00:01<00:01, 21.89it/s]                                               {'loss': 2.2183, 'grad_norm': 3.9781622886657715, 'learning_rate': 7.292171544972713e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 21.89it/s]                                               {'loss': 2.1755, 'grad_norm': 3.4749865531921387, 'learning_rate': 7.118548412949553e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 21.89it/s]                                               {'loss': 2.2089, 'grad_norm': 4.933432102203369, 'learning_rate': 6.944925280926393e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 21.89it/s] 49%|████▉     | 37/75 [00:01<00:01, 22.33it/s]                                               {'loss': 2.2039, 'grad_norm': 3.9733076095581055, 'learning_rate': 6.771302148903234e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 22.33it/s]                                               {'loss': 2.1847, 'grad_norm': 4.4651103019714355, 'learning_rate': 6.597679016880074e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 22.33it/s]                                               {'loss': 2.3329, 'grad_norm': 4.783064842224121, 'learning_rate': 6.424055884856914e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 22.33it/s] 53%|█████▎    | 40/75 [00:01<00:01, 22.95it/s]                                               {'loss': 2.3437, 'grad_norm': 4.102202892303467, 'learning_rate': 6.250432752833754e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 22.95it/s]                                               {'loss': 2.1049, 'grad_norm': 3.502753973007202, 'learning_rate': 6.076809620810594e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 22.95it/s]                                               {'loss': 2.0544, 'grad_norm': 6.312591075897217, 'learning_rate': 5.9031864887874336e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 22.95it/s] 57%|█████▋    | 43/75 [00:01<00:01, 23.45it/s]                                               {'loss': 2.3308, 'grad_norm': 5.236229419708252, 'learning_rate': 5.7295633567642746e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 23.45it/s]                                               {'loss': 2.3927, 'grad_norm': 3.548436403274536, 'learning_rate': 5.555940224741115e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 23.45it/s]                                               {'loss': 2.3825, 'grad_norm': 12.213879585266113, 'learning_rate': 5.3823170927179545e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 23.45it/s] 61%|██████▏   | 46/75 [00:02<00:01, 23.91it/s]                                               {'loss': 2.0564, 'grad_norm': 3.41917085647583, 'learning_rate': 5.2086939606947954e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 23.91it/s]                                               {'loss': 2.4212, 'grad_norm': 3.779017448425293, 'learning_rate': 5.035070828671635e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 23.91it/s]                                               {'loss': 2.2175, 'grad_norm': 5.53726863861084, 'learning_rate': 4.8614476966484753e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 23.91it/s] 65%|██████▌   | 49/75 [00:02<00:01, 24.54it/s]                                               {'loss': 2.1196, 'grad_norm': 5.895210266113281, 'learning_rate': 4.687824564625315e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 24.54it/s]                                               {'loss': 2.0845, 'grad_norm': 2.512211322784424, 'learning_rate': 4.514201432602156e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 24.54it/s]                                               {'loss': 2.1341, 'grad_norm': 4.054752826690674, 'learning_rate': 4.3405783005789955e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 24.54it/s] 69%|██████▉   | 52/75 [00:02<00:00, 24.52it/s]                                               {'loss': 2.4732, 'grad_norm': 4.067312717437744, 'learning_rate': 4.166955168555836e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.52it/s]                                               {'loss': 2.4533, 'grad_norm': 5.704606056213379, 'learning_rate': 3.993332036532676e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.52it/s]                                               {'loss': 2.3298, 'grad_norm': 5.173492908477783, 'learning_rate': 3.8197089045095164e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.52it/s] 73%|███████▎  | 55/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.2554, 'grad_norm': 4.797618865966797, 'learning_rate': 3.646085772486357e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.2455, 'grad_norm': 2.9815866947174072, 'learning_rate': 3.472462640463196e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.63it/s]                                               {'loss': 1.9516, 'grad_norm': 4.579465866088867, 'learning_rate': 3.298839508440037e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.63it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.1242, 'grad_norm': 4.23171329498291, 'learning_rate': 3.125216376416877e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.2178, 'grad_norm': 3.816732168197632, 'learning_rate': 2.9515932443937168e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.3973, 'grad_norm': 7.250980377197266, 'learning_rate': 2.7779701123705574e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.99it/s] 81%|████████▏ | 61/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.2025, 'grad_norm': 3.7130558490753174, 'learning_rate': 2.6043469803473977e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.3351, 'grad_norm': 4.014099597930908, 'learning_rate': 2.4307238483242377e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.2428, 'grad_norm': 4.467225551605225, 'learning_rate': 2.257100716301078e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.26it/s] 85%|████████▌ | 64/75 [00:02<00:00, 24.62it/s]                                               {'loss': 1.9668, 'grad_norm': 4.412024974822998, 'learning_rate': 2.083477584277918e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 24.62it/s]                                               {'loss': 2.0482, 'grad_norm': 3.641484022140503, 'learning_rate': 1.9098544522547582e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.62it/s]                                               {'loss': 2.3252, 'grad_norm': 4.167667388916016, 'learning_rate': 1.736231320231598e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.62it/s] 89%|████████▉ | 67/75 [00:02<00:00, 24.74it/s]                                               {'loss': 2.2232, 'grad_norm': 4.0870771408081055, 'learning_rate': 1.5626081882084384e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.74it/s]                                               {'loss': 2.0453, 'grad_norm': 4.980292320251465, 'learning_rate': 1.3889850561852787e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.74it/s]                                               {'loss': 2.2524, 'grad_norm': 4.915203094482422, 'learning_rate': 1.2153619241621188e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.74it/s] 93%|█████████▎| 70/75 [00:02<00:00, 24.77it/s]                                               {'loss': 2.2386, 'grad_norm': 4.426868438720703, 'learning_rate': 1.041738792138959e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.77it/s]                                               {'loss': 2.3259, 'grad_norm': 4.44171142578125, 'learning_rate': 8.68115660115799e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 24.77it/s]                                               {'loss': 2.3212, 'grad_norm': 4.281914234161377, 'learning_rate': 6.944925280926394e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 24.77it/s] 97%|█████████▋| 73/75 [00:03<00:00, 24.28it/s]                                               {'loss': 2.196, 'grad_norm': 3.5657873153686523, 'learning_rate': 5.208693960694795e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 24.28it/s]                                               {'loss': 2.099, 'grad_norm': 3.1724438667297363, 'learning_rate': 3.472462640463197e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.28it/s]                                               {'loss': 2.1993, 'grad_norm': 13.168198585510254, 'learning_rate': 1.7362313202315984e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.28it/s]                                               {'train_runtime': 3.2834, 'train_samples_per_second': 344.155, 'train_steps_per_second': 22.842, 'train_loss': 2.2387538798650106, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.28it/s]100%|██████████| 75/75 [00:03<00:00, 22.84it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:11
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2998, 'grad_norm': 4.251039981842041, 'learning_rate': 0.00013021734901736987, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.91it/s]                                              {'loss': 2.2668, 'grad_norm': 4.900989532470703, 'learning_rate': 0.00012848111769713828, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.90it/s]  4%|▍         | 3/75 [00:00<00:02, 25.22it/s]                                              {'loss': 2.291, 'grad_norm': 3.4759254455566406, 'learning_rate': 0.0001267448863769067, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.22it/s]                                              {'loss': 2.201, 'grad_norm': 4.157912254333496, 'learning_rate': 0.00012500865505667507, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.22it/s]                                              {'loss': 2.4942, 'grad_norm': 4.678056716918945, 'learning_rate': 0.00012327242373644348, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.22it/s]  8%|▊         | 6/75 [00:00<00:02, 25.13it/s]                                              {'loss': 2.195, 'grad_norm': 2.7234435081481934, 'learning_rate': 0.00012153619241621188, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.13it/s]                                              {'loss': 2.4461, 'grad_norm': 3.8740036487579346, 'learning_rate': 0.00011979996109598029, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.13it/s]                                              {'loss': 2.4911, 'grad_norm': 4.412345886230469, 'learning_rate': 0.00011806372977574867, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.13it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.24it/s]                                              {'loss': 2.2761, 'grad_norm': 4.455558776855469, 'learning_rate': 0.00011632749845551708, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.24it/s]                                              {'loss': 2.1705, 'grad_norm': 3.87558913230896, 'learning_rate': 0.00011459126713528549, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.24it/s]                                               {'loss': 2.3282, 'grad_norm': 6.552332401275635, 'learning_rate': 0.00011285503581505389, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.24it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.66it/s]                                               {'loss': 2.2376, 'grad_norm': 5.666177749633789, 'learning_rate': 0.0001111188044948223, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.66it/s]                                               {'loss': 2.3153, 'grad_norm': 3.852602243423462, 'learning_rate': 0.0001093825731745907, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.66it/s]                                               {'loss': 2.3833, 'grad_norm': 6.105276107788086, 'learning_rate': 0.00010764634185435909, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.66it/s] 20%|██        | 15/75 [00:00<00:02, 26.67it/s]                                               {'loss': 2.3067, 'grad_norm': 11.193601608276367, 'learning_rate': 0.0001059101105341275, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 26.67it/s]                                               {'loss': 2.4329, 'grad_norm': 4.243436336517334, 'learning_rate': 0.00010417387921389591, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.67it/s]                                               {'loss': 2.3506, 'grad_norm': 5.4474568367004395, 'learning_rate': 0.00010243764789366429, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.67it/s] 24%|██▍       | 18/75 [00:00<00:02, 24.61it/s]                                               {'loss': 2.1689, 'grad_norm': 3.296825408935547, 'learning_rate': 0.0001007014165734327, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.61it/s]                                               {'loss': 2.3225, 'grad_norm': 3.433694362640381, 'learning_rate': 9.896518525320111e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.61it/s]                                               {'loss': 2.1996, 'grad_norm': 4.01539945602417, 'learning_rate': 9.722895393296951e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.61it/s] 28%|██▊       | 21/75 [00:00<00:02, 23.56it/s]                                               {'loss': 2.1373, 'grad_norm': 3.5905656814575195, 'learning_rate': 9.54927226127379e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 23.56it/s]                                               {'loss': 2.415, 'grad_norm': 4.148953437805176, 'learning_rate': 9.37564912925063e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 23.56it/s]                                               {'loss': 2.1114, 'grad_norm': 4.528931617736816, 'learning_rate': 9.202025997227471e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 23.56it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.15it/s]                                               {'loss': 2.1733, 'grad_norm': 3.1144046783447266, 'learning_rate': 9.028402865204312e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.15it/s]                                               {'loss': 2.2692, 'grad_norm': 4.054201126098633, 'learning_rate': 8.854779733181151e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.15it/s]                                               {'loss': 2.2802, 'grad_norm': 3.351602554321289, 'learning_rate': 8.681156601157991e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.15it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.88it/s]                                               {'loss': 2.4281, 'grad_norm': 5.42403507232666, 'learning_rate': 8.507533469134832e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.88it/s]                                               {'loss': 2.1715, 'grad_norm': 3.0649843215942383, 'learning_rate': 8.333910337111672e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.88it/s]                                               {'loss': 2.3442, 'grad_norm': 4.748600482940674, 'learning_rate': 8.160287205088513e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.88it/s]                                               {'loss': 2.068, 'grad_norm': 13.563446998596191, 'learning_rate': 7.986664073065352e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.88it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.48it/s]                                               {'loss': 2.2103, 'grad_norm': 5.420076847076416, 'learning_rate': 7.813040941042192e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.48it/s]                                               {'loss': 2.36, 'grad_norm': 3.4655747413635254, 'learning_rate': 7.639417809019033e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.48it/s]                                               {'loss': 2.2313, 'grad_norm': 4.662176609039307, 'learning_rate': 7.465794676995872e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.48it/s] 45%|████▌     | 34/75 [00:01<00:01, 26.00it/s]                                               {'loss': 2.2484, 'grad_norm': 3.850236415863037, 'learning_rate': 7.292171544972713e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.00it/s]                                               {'loss': 2.3832, 'grad_norm': 4.637624740600586, 'learning_rate': 7.118548412949553e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.00it/s]                                               {'loss': 2.1511, 'grad_norm': 4.279363632202148, 'learning_rate': 6.944925280926393e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.00it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.01it/s]                                               {'loss': 2.3588, 'grad_norm': 3.209182024002075, 'learning_rate': 6.771302148903234e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.01it/s]                                               {'loss': 2.2238, 'grad_norm': 3.853721857070923, 'learning_rate': 6.597679016880074e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.01it/s]                                               {'loss': 2.025, 'grad_norm': 2.6249263286590576, 'learning_rate': 6.424055884856914e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.01it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.07it/s]                                               {'loss': 2.2469, 'grad_norm': 3.713857650756836, 'learning_rate': 6.250432752833754e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.07it/s]                                               {'loss': 2.2258, 'grad_norm': 4.681885242462158, 'learning_rate': 6.076809620810594e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.07it/s]                                               {'loss': 2.4892, 'grad_norm': 4.657618999481201, 'learning_rate': 5.9031864887874336e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.07it/s] 57%|█████▋    | 43/75 [00:01<00:01, 24.60it/s]                                               {'loss': 2.1923, 'grad_norm': 4.207746982574463, 'learning_rate': 5.7295633567642746e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.60it/s]                                               {'loss': 2.1786, 'grad_norm': 3.273486614227295, 'learning_rate': 5.555940224741115e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.60it/s]                                               {'loss': 2.3216, 'grad_norm': 19.523700714111328, 'learning_rate': 5.3823170927179545e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.60it/s] 61%|██████▏   | 46/75 [00:01<00:01, 24.33it/s]                                               {'loss': 2.4195, 'grad_norm': 4.87654447555542, 'learning_rate': 5.2086939606947954e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.33it/s]                                               {'loss': 2.0523, 'grad_norm': 3.44551420211792, 'learning_rate': 5.035070828671635e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 24.33it/s]                                               {'loss': 2.0389, 'grad_norm': 3.593302011489868, 'learning_rate': 4.8614476966484753e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 24.33it/s] 65%|██████▌   | 49/75 [00:01<00:01, 23.42it/s]                                               {'loss': 2.2813, 'grad_norm': 3.3136959075927734, 'learning_rate': 4.687824564625315e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 23.42it/s]                                               {'loss': 2.3308, 'grad_norm': 4.366106986999512, 'learning_rate': 4.514201432602156e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 23.42it/s]                                               {'loss': 2.0445, 'grad_norm': 3.553356170654297, 'learning_rate': 4.3405783005789955e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 23.42it/s] 69%|██████▉   | 52/75 [00:02<00:00, 23.27it/s]                                               {'loss': 2.2986, 'grad_norm': 4.362438678741455, 'learning_rate': 4.166955168555836e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 23.27it/s]                                               {'loss': 2.2906, 'grad_norm': 5.073653221130371, 'learning_rate': 3.993332036532676e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 23.27it/s]                                               {'loss': 2.1814, 'grad_norm': 3.6665937900543213, 'learning_rate': 3.8197089045095164e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 23.27it/s] 73%|███████▎  | 55/75 [00:02<00:00, 23.36it/s]                                               {'loss': 2.0981, 'grad_norm': 3.3192765712738037, 'learning_rate': 3.646085772486357e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 23.36it/s]                                               {'loss': 2.2027, 'grad_norm': 4.039930820465088, 'learning_rate': 3.472462640463196e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 23.36it/s]                                               {'loss': 2.1661, 'grad_norm': 4.36419677734375, 'learning_rate': 3.298839508440037e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 23.36it/s] 77%|███████▋  | 58/75 [00:02<00:00, 23.42it/s]                                               {'loss': 2.5133, 'grad_norm': 4.101251602172852, 'learning_rate': 3.125216376416877e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 23.42it/s]                                               {'loss': 2.0616, 'grad_norm': 3.824230432510376, 'learning_rate': 2.9515932443937168e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 23.42it/s]                                               {'loss': 2.4809, 'grad_norm': 12.265920639038086, 'learning_rate': 2.7779701123705574e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 23.42it/s] 81%|████████▏ | 61/75 [00:02<00:00, 24.42it/s]                                               {'loss': 2.1061, 'grad_norm': 3.1549015045166016, 'learning_rate': 2.6043469803473977e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.42it/s]                                               {'loss': 2.1158, 'grad_norm': 4.247574806213379, 'learning_rate': 2.4307238483242377e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 24.42it/s]                                               {'loss': 2.2302, 'grad_norm': 3.561343193054199, 'learning_rate': 2.257100716301078e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 24.42it/s] 85%|████████▌ | 64/75 [00:02<00:00, 24.02it/s]                                               {'loss': 2.1221, 'grad_norm': 4.027350902557373, 'learning_rate': 2.083477584277918e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 24.02it/s]                                               {'loss': 2.2223, 'grad_norm': 4.451693534851074, 'learning_rate': 1.9098544522547582e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.02it/s]                                               {'loss': 2.4872, 'grad_norm': 5.145015239715576, 'learning_rate': 1.736231320231598e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.02it/s] 89%|████████▉ | 67/75 [00:02<00:00, 24.54it/s]                                               {'loss': 2.1748, 'grad_norm': 3.2060580253601074, 'learning_rate': 1.5626081882084384e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.54it/s]                                               {'loss': 2.2434, 'grad_norm': 3.433928966522217, 'learning_rate': 1.3889850561852787e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.54it/s]                                               {'loss': 2.1506, 'grad_norm': 5.344192981719971, 'learning_rate': 1.2153619241621188e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.54it/s] 93%|█████████▎| 70/75 [00:02<00:00, 24.67it/s]                                               {'loss': 2.1527, 'grad_norm': 2.6038787364959717, 'learning_rate': 1.041738792138959e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.67it/s]                                               {'loss': 2.2696, 'grad_norm': 4.012714385986328, 'learning_rate': 8.68115660115799e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.67it/s]                                               {'loss': 2.2398, 'grad_norm': 3.847827434539795, 'learning_rate': 6.944925280926394e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.67it/s] 97%|█████████▋| 73/75 [00:02<00:00, 24.84it/s]                                               {'loss': 2.2198, 'grad_norm': 4.678499698638916, 'learning_rate': 5.208693960694795e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.84it/s]                                               {'loss': 2.1523, 'grad_norm': 4.043411731719971, 'learning_rate': 3.472462640463197e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.84it/s]                                               {'loss': 2.1586, 'grad_norm': 7.920310974121094, 'learning_rate': 1.7362313202315984e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.84it/s]                                               {'train_runtime': 3.1373, 'train_samples_per_second': 360.182, 'train_steps_per_second': 23.906, 'train_loss': 2.252369928359985, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.84it/s]100%|██████████| 75/75 [00:03<00:00, 23.91it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4722, 'grad_norm': 4.588751316070557, 'learning_rate': 0.00013021734901736987, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 15.03it/s]  3%|▎         | 2/75 [00:00<00:03, 19.52it/s]                                              {'loss': 2.0483, 'grad_norm': 3.4736380577087402, 'learning_rate': 0.00012848111769713828, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 19.52it/s]                                              {'loss': 2.2436, 'grad_norm': 4.7610344886779785, 'learning_rate': 0.0001267448863769067, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 19.52it/s]                                              {'loss': 2.447, 'grad_norm': 5.427060604095459, 'learning_rate': 0.00012500865505667507, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 19.52it/s]  7%|▋         | 5/75 [00:00<00:02, 23.55it/s]                                              {'loss': 2.4844, 'grad_norm': 5.289426803588867, 'learning_rate': 0.00012327242373644348, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.55it/s]                                              {'loss': 2.262, 'grad_norm': 3.2549965381622314, 'learning_rate': 0.00012153619241621188, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.55it/s]                                              {'loss': 2.4893, 'grad_norm': 4.882518291473389, 'learning_rate': 0.00011979996109598029, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.55it/s] 11%|█         | 8/75 [00:00<00:02, 24.77it/s]                                              {'loss': 2.1026, 'grad_norm': 4.855315208435059, 'learning_rate': 0.00011806372977574867, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.77it/s]                                              {'loss': 2.0204, 'grad_norm': 3.8049113750457764, 'learning_rate': 0.00011632749845551708, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.77it/s]                                              {'loss': 2.2128, 'grad_norm': 3.7985761165618896, 'learning_rate': 0.00011459126713528549, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.77it/s] 15%|█▍        | 11/75 [00:00<00:02, 25.09it/s]                                               {'loss': 2.2769, 'grad_norm': 4.479316234588623, 'learning_rate': 0.00011285503581505389, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.09it/s]                                               {'loss': 2.3778, 'grad_norm': 4.698726177215576, 'learning_rate': 0.0001111188044948223, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.09it/s]                                               {'loss': 2.4207, 'grad_norm': 5.205453395843506, 'learning_rate': 0.0001093825731745907, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.09it/s] 19%|█▊        | 14/75 [00:00<00:02, 25.37it/s]                                               {'loss': 2.3126, 'grad_norm': 5.168961524963379, 'learning_rate': 0.00010764634185435909, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.37it/s]                                               {'loss': 2.0708, 'grad_norm': 7.172348499298096, 'learning_rate': 0.0001059101105341275, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.37it/s]                                               {'loss': 1.9969, 'grad_norm': 3.368626356124878, 'learning_rate': 0.00010417387921389591, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.37it/s]                                               {'loss': 2.195, 'grad_norm': 5.276321887969971, 'learning_rate': 0.00010243764789366429, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.37it/s] 24%|██▍       | 18/75 [00:00<00:02, 26.88it/s]                                               {'loss': 2.5179, 'grad_norm': 4.2413153648376465, 'learning_rate': 0.0001007014165734327, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.88it/s]                                               {'loss': 2.1021, 'grad_norm': 4.1569671630859375, 'learning_rate': 9.896518525320111e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.88it/s]                                               {'loss': 2.3546, 'grad_norm': 4.052000999450684, 'learning_rate': 9.722895393296951e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.88it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.97it/s]                                               {'loss': 2.1998, 'grad_norm': 3.2201032638549805, 'learning_rate': 9.54927226127379e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.97it/s]                                               {'loss': 2.1636, 'grad_norm': 3.75821590423584, 'learning_rate': 9.37564912925063e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.97it/s]                                               {'loss': 2.085, 'grad_norm': 4.300746440887451, 'learning_rate': 9.202025997227471e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.97it/s] 32%|███▏      | 24/75 [00:00<00:01, 25.64it/s]                                               {'loss': 2.2594, 'grad_norm': 4.9078240394592285, 'learning_rate': 9.028402865204312e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.64it/s]                                               {'loss': 2.4284, 'grad_norm': 4.438844680786133, 'learning_rate': 8.854779733181151e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.64it/s]                                               {'loss': 2.3757, 'grad_norm': 3.8654754161834717, 'learning_rate': 8.681156601157991e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.64it/s] 36%|███▌      | 27/75 [00:01<00:01, 25.33it/s]                                               {'loss': 2.2174, 'grad_norm': 5.745235443115234, 'learning_rate': 8.507533469134832e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.33it/s]                                               {'loss': 2.1686, 'grad_norm': 4.435901641845703, 'learning_rate': 8.333910337111672e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.33it/s]                                               {'loss': 2.3131, 'grad_norm': 3.3889105319976807, 'learning_rate': 8.160287205088513e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.33it/s]                                               {'loss': 1.62, 'grad_norm': 8.64099407196045, 'learning_rate': 7.986664073065352e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.33it/s] 41%|████▏     | 31/75 [00:01<00:01, 27.02it/s]                                               {'loss': 2.3717, 'grad_norm': 4.452218532562256, 'learning_rate': 7.813040941042192e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 27.02it/s]                                               {'loss': 2.3108, 'grad_norm': 3.489057779312134, 'learning_rate': 7.639417809019033e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 27.02it/s]                                               {'loss': 2.1094, 'grad_norm': 4.419223308563232, 'learning_rate': 7.465794676995872e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 27.02it/s] 45%|████▌     | 34/75 [00:01<00:01, 26.50it/s]                                               {'loss': 2.1389, 'grad_norm': 3.506955623626709, 'learning_rate': 7.292171544972713e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.50it/s]                                               {'loss': 2.204, 'grad_norm': 4.385241985321045, 'learning_rate': 7.118548412949553e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.50it/s]                                               {'loss': 2.2229, 'grad_norm': 3.9882142543792725, 'learning_rate': 6.944925280926393e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.50it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.90it/s]                                               {'loss': 2.187, 'grad_norm': 3.9078404903411865, 'learning_rate': 6.771302148903234e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.90it/s]                                               {'loss': 2.324, 'grad_norm': 4.106011867523193, 'learning_rate': 6.597679016880074e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.90it/s]                                               {'loss': 2.2354, 'grad_norm': 3.77268385887146, 'learning_rate': 6.424055884856914e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.90it/s] 53%|█████▎    | 40/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.1574, 'grad_norm': 3.820303201675415, 'learning_rate': 6.250432752833754e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.3563, 'grad_norm': 5.3093366622924805, 'learning_rate': 6.076809620810594e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.1993, 'grad_norm': 4.186629772186279, 'learning_rate': 5.9031864887874336e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.56it/s] 57%|█████▋    | 43/75 [00:01<00:01, 24.49it/s]                                               {'loss': 2.0644, 'grad_norm': 4.009377956390381, 'learning_rate': 5.7295633567642746e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.49it/s]                                               {'loss': 2.1682, 'grad_norm': 3.804919719696045, 'learning_rate': 5.555940224741115e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.49it/s]                                               {'loss': 1.436, 'grad_norm': 10.721651077270508, 'learning_rate': 5.3823170927179545e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.49it/s]                                               {'loss': 2.3425, 'grad_norm': 3.240267753601074, 'learning_rate': 5.2086939606947954e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.49it/s] 63%|██████▎   | 47/75 [00:01<00:01, 25.87it/s]                                               {'loss': 2.3242, 'grad_norm': 3.6916329860687256, 'learning_rate': 5.035070828671635e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.87it/s]                                               {'loss': 1.9927, 'grad_norm': 3.261552572250366, 'learning_rate': 4.8614476966484753e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.87it/s]                                               {'loss': 2.2064, 'grad_norm': 5.097237586975098, 'learning_rate': 4.687824564625315e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.87it/s] 67%|██████▋   | 50/75 [00:01<00:01, 24.90it/s]                                               {'loss': 2.0273, 'grad_norm': 5.348911762237549, 'learning_rate': 4.514201432602156e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:01, 24.90it/s]                                               {'loss': 2.0989, 'grad_norm': 3.8942954540252686, 'learning_rate': 4.3405783005789955e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 24.90it/s]                                               {'loss': 2.4153, 'grad_norm': 4.725719451904297, 'learning_rate': 4.166955168555836e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.90it/s] 71%|███████   | 53/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.2765, 'grad_norm': 3.2884278297424316, 'learning_rate': 3.993332036532676e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.1298, 'grad_norm': 4.81234073638916, 'learning_rate': 3.8197089045095164e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.63it/s]                                               {'loss': 2.1859, 'grad_norm': 3.7645843029022217, 'learning_rate': 3.646085772486357e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.63it/s] 75%|███████▍  | 56/75 [00:02<00:00, 24.48it/s]                                               {'loss': 2.369, 'grad_norm': 5.393908500671387, 'learning_rate': 3.472462640463196e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.48it/s]                                               {'loss': 2.0882, 'grad_norm': 3.458056926727295, 'learning_rate': 3.298839508440037e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.48it/s]                                               {'loss': 2.3368, 'grad_norm': 3.791166305541992, 'learning_rate': 3.125216376416877e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.48it/s] 79%|███████▊  | 59/75 [00:02<00:00, 24.27it/s]                                               {'loss': 2.1144, 'grad_norm': 4.004620552062988, 'learning_rate': 2.9515932443937168e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.27it/s]                                               {'loss': 2.0645, 'grad_norm': 9.33003044128418, 'learning_rate': 2.7779701123705574e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.27it/s]                                               {'loss': 2.1475, 'grad_norm': 3.9873979091644287, 'learning_rate': 2.6043469803473977e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.27it/s]                                               {'loss': 2.3053, 'grad_norm': 4.1184892654418945, 'learning_rate': 2.4307238483242377e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 24.27it/s] 84%|████████▍ | 63/75 [00:02<00:00, 25.94it/s]                                               {'loss': 2.3108, 'grad_norm': 4.068089485168457, 'learning_rate': 2.257100716301078e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.94it/s]                                               {'loss': 2.293, 'grad_norm': 6.433312892913818, 'learning_rate': 2.083477584277918e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.94it/s]                                               {'loss': 2.2093, 'grad_norm': 3.6200497150421143, 'learning_rate': 1.9098544522547582e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.94it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.84it/s]                                               {'loss': 2.214, 'grad_norm': 4.334987163543701, 'learning_rate': 1.736231320231598e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.84it/s]                                               {'loss': 2.2017, 'grad_norm': 3.6044254302978516, 'learning_rate': 1.5626081882084384e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.84it/s]                                               {'loss': 2.3361, 'grad_norm': 5.086878776550293, 'learning_rate': 1.3889850561852787e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.84it/s] 92%|█████████▏| 69/75 [00:02<00:00, 25.80it/s]                                               {'loss': 2.1147, 'grad_norm': 3.6986467838287354, 'learning_rate': 1.2153619241621188e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.80it/s]                                               {'loss': 2.1301, 'grad_norm': 3.790607452392578, 'learning_rate': 1.041738792138959e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.80it/s]                                               {'loss': 2.0942, 'grad_norm': 3.3090426921844482, 'learning_rate': 8.68115660115799e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.80it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.32it/s]                                               {'loss': 1.9242, 'grad_norm': 3.2709767818450928, 'learning_rate': 6.944925280926394e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.32it/s]                                               {'loss': 2.3225, 'grad_norm': 4.507071018218994, 'learning_rate': 5.208693960694795e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.32it/s]                                               {'loss': 2.1042, 'grad_norm': 3.78621506690979, 'learning_rate': 3.472462640463197e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.32it/s]                                               {'loss': 1.5894, 'grad_norm': 8.456329345703125, 'learning_rate': 1.7362313202315984e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.32it/s]                                               {'train_runtime': 3.054, 'train_samples_per_second': 370.005, 'train_steps_per_second': 24.558, 'train_loss': 2.1999193811416626, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.32it/s]100%|██████████| 75/75 [00:03<00:00, 24.56it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.464, 'grad_norm': 3.915344715118408, 'learning_rate': 0.00013021734901736987, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 20.64it/s]                                              {'loss': 2.4466, 'grad_norm': 4.826641082763672, 'learning_rate': 0.00012848111769713828, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.56it/s]  4%|▍         | 3/75 [00:00<00:02, 24.81it/s]                                              {'loss': 2.4125, 'grad_norm': 4.773728847503662, 'learning_rate': 0.0001267448863769067, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.81it/s]                                              {'loss': 2.2893, 'grad_norm': 4.664008140563965, 'learning_rate': 0.00012500865505667507, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.81it/s]                                              {'loss': 2.4225, 'grad_norm': 3.8614509105682373, 'learning_rate': 0.00012327242373644348, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.81it/s]  8%|▊         | 6/75 [00:00<00:02, 25.41it/s]                                              {'loss': 2.0807, 'grad_norm': 3.4665298461914062, 'learning_rate': 0.00012153619241621188, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.41it/s]                                              {'loss': 2.4175, 'grad_norm': 3.7743794918060303, 'learning_rate': 0.00011979996109598029, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.41it/s]                                              {'loss': 2.1984, 'grad_norm': 3.5722196102142334, 'learning_rate': 0.00011806372977574867, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.41it/s] 12%|█▏        | 9/75 [00:00<00:02, 23.71it/s]                                              {'loss': 2.6151, 'grad_norm': 4.7631516456604, 'learning_rate': 0.00011632749845551708, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.71it/s]                                              {'loss': 2.2311, 'grad_norm': 4.195537567138672, 'learning_rate': 0.00011459126713528549, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.71it/s]                                               {'loss': 2.3458, 'grad_norm': 3.3630118370056152, 'learning_rate': 0.00011285503581505389, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.71it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.47it/s]                                               {'loss': 2.051, 'grad_norm': 3.546403169631958, 'learning_rate': 0.0001111188044948223, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.47it/s]                                               {'loss': 2.3385, 'grad_norm': 4.594139099121094, 'learning_rate': 0.0001093825731745907, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.47it/s]                                               {'loss': 2.2683, 'grad_norm': 4.8143463134765625, 'learning_rate': 0.00010764634185435909, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.47it/s] 20%|██        | 15/75 [00:00<00:02, 24.55it/s]                                               {'loss': 2.5284, 'grad_norm': 8.565716743469238, 'learning_rate': 0.0001059101105341275, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.55it/s]                                               {'loss': 2.4577, 'grad_norm': 4.584409713745117, 'learning_rate': 0.00010417387921389591, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 24.55it/s]                                               {'loss': 2.2601, 'grad_norm': 4.8091559410095215, 'learning_rate': 0.00010243764789366429, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.55it/s] 24%|██▍       | 18/75 [00:00<00:02, 23.66it/s]                                               {'loss': 2.2287, 'grad_norm': 4.500776290893555, 'learning_rate': 0.0001007014165734327, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 23.66it/s]                                               {'loss': 2.5629, 'grad_norm': 4.716848373413086, 'learning_rate': 9.896518525320111e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 23.66it/s]                                               {'loss': 2.3489, 'grad_norm': 3.1256861686706543, 'learning_rate': 9.722895393296951e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 23.66it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.23it/s]                                               {'loss': 2.3806, 'grad_norm': 4.915102958679199, 'learning_rate': 9.54927226127379e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.23it/s]                                               {'loss': 2.282, 'grad_norm': 3.8124451637268066, 'learning_rate': 9.37564912925063e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.23it/s]                                               {'loss': 2.2784, 'grad_norm': 3.275883197784424, 'learning_rate': 9.202025997227471e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.23it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.69it/s]                                               {'loss': 2.0975, 'grad_norm': 4.267612934112549, 'learning_rate': 9.028402865204312e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.69it/s]                                               {'loss': 2.2562, 'grad_norm': 4.256049633026123, 'learning_rate': 8.854779733181151e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.69it/s]                                               {'loss': 2.3724, 'grad_norm': 4.0356221199035645, 'learning_rate': 8.681156601157991e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.69it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.81it/s]                                               {'loss': 2.1895, 'grad_norm': 4.253154277801514, 'learning_rate': 8.507533469134832e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.81it/s]                                               {'loss': 2.1254, 'grad_norm': 3.9037461280822754, 'learning_rate': 8.333910337111672e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.81it/s]                                               {'loss': 2.2691, 'grad_norm': 4.764484882354736, 'learning_rate': 8.160287205088513e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.81it/s] 40%|████      | 30/75 [00:01<00:01, 24.84it/s]                                               {'loss': 2.1714, 'grad_norm': 6.585867881774902, 'learning_rate': 7.986664073065352e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.84it/s]                                               {'loss': 2.2537, 'grad_norm': 5.207461833953857, 'learning_rate': 7.813040941042192e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.84it/s]                                               {'loss': 2.4076, 'grad_norm': 4.226244926452637, 'learning_rate': 7.639417809019033e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.84it/s] 44%|████▍     | 33/75 [00:01<00:01, 24.66it/s]                                               {'loss': 2.319, 'grad_norm': 3.8134806156158447, 'learning_rate': 7.465794676995872e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.66it/s]                                               {'loss': 2.4161, 'grad_norm': 4.723766803741455, 'learning_rate': 7.292171544972713e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.66it/s]                                               {'loss': 2.2701, 'grad_norm': 3.733053684234619, 'learning_rate': 7.118548412949553e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.66it/s] 48%|████▊     | 36/75 [00:01<00:01, 24.82it/s]                                               {'loss': 2.0174, 'grad_norm': 3.548025369644165, 'learning_rate': 6.944925280926393e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.82it/s]                                               {'loss': 2.2807, 'grad_norm': 3.6807210445404053, 'learning_rate': 6.771302148903234e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.82it/s]                                               {'loss': 2.1848, 'grad_norm': 3.692213773727417, 'learning_rate': 6.597679016880074e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.82it/s] 52%|█████▏    | 39/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.2244, 'grad_norm': 3.478391170501709, 'learning_rate': 6.424055884856914e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.188, 'grad_norm': 4.1512861251831055, 'learning_rate': 6.250432752833754e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.3108, 'grad_norm': 6.085628986358643, 'learning_rate': 6.076809620810594e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.34it/s] 56%|█████▌    | 42/75 [00:01<00:01, 25.46it/s]                                               {'loss': 2.1494, 'grad_norm': 3.102588653564453, 'learning_rate': 5.9031864887874336e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.46it/s]                                               {'loss': 2.2359, 'grad_norm': 3.9702680110931396, 'learning_rate': 5.7295633567642746e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.46it/s]                                               {'loss': 2.4361, 'grad_norm': 4.902244567871094, 'learning_rate': 5.555940224741115e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.46it/s]                                               {'loss': 2.1828, 'grad_norm': 9.514751434326172, 'learning_rate': 5.3823170927179545e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.46it/s] 61%|██████▏   | 46/75 [00:01<00:01, 27.11it/s]                                               {'loss': 2.2796, 'grad_norm': 3.643139123916626, 'learning_rate': 5.2086939606947954e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 27.11it/s]                                               {'loss': 2.3636, 'grad_norm': 4.46050500869751, 'learning_rate': 5.035070828671635e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 27.11it/s]                                               {'loss': 2.1815, 'grad_norm': 4.151159763336182, 'learning_rate': 4.8614476966484753e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 27.11it/s] 65%|██████▌   | 49/75 [00:01<00:00, 26.77it/s]                                               {'loss': 2.2401, 'grad_norm': 3.7071287631988525, 'learning_rate': 4.687824564625315e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.77it/s]                                               {'loss': 1.9639, 'grad_norm': 3.1725077629089355, 'learning_rate': 4.514201432602156e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.77it/s]                                               {'loss': 2.1287, 'grad_norm': 4.796310901641846, 'learning_rate': 4.3405783005789955e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 26.77it/s] 69%|██████▉   | 52/75 [00:02<00:00, 25.71it/s]                                               {'loss': 2.3197, 'grad_norm': 3.793119192123413, 'learning_rate': 4.166955168555836e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.71it/s]                                               {'loss': 2.4835, 'grad_norm': 4.018857479095459, 'learning_rate': 3.993332036532676e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.71it/s]                                               {'loss': 2.3379, 'grad_norm': 4.368782043457031, 'learning_rate': 3.8197089045095164e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.71it/s] 73%|███████▎  | 55/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.2842, 'grad_norm': 3.78701114654541, 'learning_rate': 3.646085772486357e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.1712, 'grad_norm': 4.399886608123779, 'learning_rate': 3.472462640463196e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.3487, 'grad_norm': 4.470423698425293, 'learning_rate': 3.298839508440037e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.33it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.27it/s]                                               {'loss': 2.3317, 'grad_norm': 5.531282424926758, 'learning_rate': 3.125216376416877e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.27it/s]                                               {'loss': 2.0965, 'grad_norm': 5.8728203773498535, 'learning_rate': 2.9515932443937168e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.27it/s]                                               {'loss': 2.2606, 'grad_norm': 17.740989685058594, 'learning_rate': 2.7779701123705574e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.27it/s]                                               {'loss': 2.1607, 'grad_norm': 3.4544503688812256, 'learning_rate': 2.6043469803473977e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.27it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.30it/s]                                               {'loss': 2.3231, 'grad_norm': 4.727243423461914, 'learning_rate': 2.4307238483242377e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.30it/s]                                               {'loss': 2.2011, 'grad_norm': 4.095865726470947, 'learning_rate': 2.257100716301078e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.30it/s]                                               {'loss': 1.983, 'grad_norm': 3.2004857063293457, 'learning_rate': 2.083477584277918e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.30it/s] 87%|████████▋ | 65/75 [00:02<00:00, 26.18it/s]                                               {'loss': 2.3932, 'grad_norm': 4.0489020347595215, 'learning_rate': 1.9098544522547582e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.18it/s]                                               {'loss': 2.2821, 'grad_norm': 3.548670768737793, 'learning_rate': 1.736231320231598e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.18it/s]                                               {'loss': 2.1905, 'grad_norm': 3.7053585052490234, 'learning_rate': 1.5626081882084384e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.18it/s] 91%|█████████ | 68/75 [00:02<00:00, 26.15it/s]                                               {'loss': 2.2805, 'grad_norm': 4.135275363922119, 'learning_rate': 1.3889850561852787e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 26.15it/s]                                               {'loss': 2.1404, 'grad_norm': 3.960552930831909, 'learning_rate': 1.2153619241621188e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 26.15it/s]                                               {'loss': 2.1724, 'grad_norm': 4.578352451324463, 'learning_rate': 1.041738792138959e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 26.15it/s] 95%|█████████▍| 71/75 [00:02<00:00, 25.90it/s]                                               {'loss': 2.3318, 'grad_norm': 5.359118461608887, 'learning_rate': 8.68115660115799e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.90it/s]                                               {'loss': 2.3013, 'grad_norm': 4.290177822113037, 'learning_rate': 6.944925280926394e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.90it/s]                                               {'loss': 2.3457, 'grad_norm': 4.090785980224609, 'learning_rate': 5.208693960694795e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.90it/s] 99%|█████████▊| 74/75 [00:02<00:00, 25.75it/s]                                               {'loss': 2.0482, 'grad_norm': 3.695772886276245, 'learning_rate': 3.472462640463197e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.75it/s]                                               {'loss': 2.275, 'grad_norm': 18.62905502319336, 'learning_rate': 1.7362313202315984e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.75it/s]                                               {'train_runtime': 3.0568, 'train_samples_per_second': 369.673, 'train_steps_per_second': 24.536, 'train_loss': 2.2731657807032266, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.75it/s]100%|██████████| 75/75 [00:03<00:00, 24.54it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(983, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(905, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(725, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(894, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1103, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(878, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1003, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(837, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(907, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(561, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1084, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(514, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 63.15it/s]  3%|▎         | 14/471 [00:00<00:08, 52.90it/s]  4%|▍         | 20/471 [00:00<00:08, 50.14it/s]  6%|▌         | 26/471 [00:00<00:08, 52.85it/s]  7%|▋         | 32/471 [00:00<00:08, 53.50it/s]  8%|▊         | 38/471 [00:00<00:08, 53.16it/s]  9%|▉         | 44/471 [00:00<00:07, 53.58it/s] 11%|█         | 50/471 [00:00<00:07, 53.21it/s] 12%|█▏        | 56/471 [00:01<00:07, 53.24it/s] 13%|█▎        | 62/471 [00:01<00:07, 53.70it/s] 14%|█▍        | 68/471 [00:01<00:07, 53.78it/s] 16%|█▌        | 74/471 [00:01<00:07, 53.16it/s] 17%|█▋        | 80/471 [00:01<00:07, 52.86it/s] 18%|█▊        | 86/471 [00:01<00:07, 52.25it/s] 20%|█▉        | 92/471 [00:01<00:07, 51.79it/s] 21%|██        | 98/471 [00:01<00:07, 51.14it/s] 22%|██▏       | 104/471 [00:01<00:07, 51.80it/s] 23%|██▎       | 110/471 [00:02<00:06, 51.91it/s] 25%|██▍       | 116/471 [00:02<00:06, 52.11it/s] 26%|██▌       | 122/471 [00:02<00:06, 52.81it/s] 27%|██▋       | 128/471 [00:02<00:06, 51.75it/s] 28%|██▊       | 134/471 [00:02<00:06, 51.46it/s] 30%|██▉       | 140/471 [00:02<00:06, 51.34it/s] 31%|███       | 146/471 [00:02<00:06, 51.30it/s] 32%|███▏      | 152/471 [00:02<00:06, 52.10it/s] 34%|███▎      | 158/471 [00:03<00:06, 52.15it/s] 35%|███▍      | 164/471 [00:03<00:05, 52.96it/s] 36%|███▌      | 170/471 [00:03<00:05, 52.33it/s] 37%|███▋      | 176/471 [00:03<00:05, 52.02it/s] 39%|███▊      | 182/471 [00:03<00:05, 52.66it/s] 40%|███▉      | 188/471 [00:03<00:05, 50.99it/s] 41%|████      | 194/471 [00:03<00:05, 52.27it/s] 42%|████▏     | 200/471 [00:03<00:05, 52.73it/s] 44%|████▎     | 206/471 [00:03<00:05, 51.53it/s] 45%|████▌     | 212/471 [00:04<00:04, 52.69it/s] 46%|████▋     | 218/471 [00:04<00:04, 51.56it/s] 48%|████▊     | 224/471 [00:04<00:04, 51.63it/s] 49%|████▉     | 230/471 [00:04<00:04, 51.72it/s] 50%|█████     | 236/471 [00:04<00:04, 52.43it/s] 51%|█████▏    | 242/471 [00:04<00:04, 52.99it/s] 53%|█████▎    | 248/471 [00:04<00:04, 53.17it/s] 54%|█████▍    | 254/471 [00:04<00:04, 53.57it/s] 55%|█████▌    | 260/471 [00:04<00:03, 53.59it/s] 56%|█████▋    | 266/471 [00:05<00:03, 53.78it/s] 58%|█████▊    | 272/471 [00:05<00:03, 51.60it/s] 59%|█████▉    | 278/471 [00:05<00:03, 52.52it/s] 60%|██████    | 284/471 [00:05<00:03, 52.87it/s] 62%|██████▏   | 290/471 [00:05<00:03, 53.08it/s] 63%|██████▎   | 296/471 [00:05<00:03, 52.19it/s] 64%|██████▍   | 302/471 [00:05<00:03, 52.82it/s] 65%|██████▌   | 308/471 [00:05<00:03, 51.53it/s] 67%|██████▋   | 314/471 [00:05<00:03, 51.73it/s] 68%|██████▊   | 320/471 [00:06<00:02, 52.64it/s] 69%|██████▉   | 326/471 [00:06<00:02, 52.87it/s] 70%|███████   | 332/471 [00:06<00:02, 53.22it/s] 72%|███████▏  | 338/471 [00:06<00:02, 53.40it/s] 73%|███████▎  | 344/471 [00:06<00:02, 53.49it/s] 74%|███████▍  | 350/471 [00:06<00:02, 53.20it/s] 76%|███████▌  | 356/471 [00:06<00:02, 53.45it/s] 77%|███████▋  | 362/471 [00:06<00:02, 53.69it/s] 78%|███████▊  | 368/471 [00:06<00:01, 53.26it/s] 79%|███████▉  | 374/471 [00:07<00:01, 53.13it/s] 81%|████████  | 380/471 [00:07<00:01, 53.45it/s] 82%|████████▏ | 386/471 [00:07<00:01, 53.48it/s] 83%|████████▎ | 392/471 [00:07<00:01, 53.53it/s] 85%|████████▍ | 398/471 [00:07<00:01, 53.67it/s] 86%|████████▌ | 404/471 [00:07<00:01, 53.09it/s] 87%|████████▋ | 410/471 [00:07<00:01, 53.52it/s] 88%|████████▊ | 416/471 [00:07<00:01, 53.48it/s] 90%|████████▉ | 422/471 [00:08<00:00, 52.68it/s] 91%|█████████ | 428/471 [00:08<00:00, 52.52it/s] 92%|█████████▏| 434/471 [00:08<00:00, 52.65it/s] 93%|█████████▎| 440/471 [00:08<00:00, 52.95it/s] 95%|█████████▍| 446/471 [00:08<00:00, 51.40it/s] 96%|█████████▌| 452/471 [00:08<00:00, 52.14it/s] 97%|█████████▋| 458/471 [00:08<00:00, 52.38it/s] 99%|█████████▊| 464/471 [00:08<00:00, 52.59it/s]100%|█████████▉| 470/471 [00:08<00:00, 52.99it/s]100%|██████████| 471/471 [00:08<00:00, 52.69it/s]
{'eval_loss': 2.2730915546417236, 'eval_model_preparation_time': 0.0067, 'eval_acc': 0.39644184811471056, 'eval_runtime': 8.9628, 'eval_samples_per_second': 840.36, 'eval_steps_per_second': 52.55}
ROUND:18
CLIENT:7
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3864, 'grad_norm': 4.1971001625061035, 'learning_rate': 0.00012811529493745265, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.43it/s]                                              {'loss': 2.3266, 'grad_norm': 4.468866348266602, 'learning_rate': 0.00012640709100495329, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 25.04it/s]  4%|▍         | 3/75 [00:00<00:02, 25.61it/s]                                              {'loss': 2.0925, 'grad_norm': 5.825961112976074, 'learning_rate': 0.00012469888707245392, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.61it/s]                                              {'loss': 2.286, 'grad_norm': 3.804857015609741, 'learning_rate': 0.00012299068313995453, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.61it/s]                                              {'loss': 2.3184, 'grad_norm': 3.722716808319092, 'learning_rate': 0.00012128247920745518, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.61it/s]  8%|▊         | 6/75 [00:00<00:02, 24.82it/s]                                              {'loss': 2.3224, 'grad_norm': 5.476954460144043, 'learning_rate': 0.00011957427527495582, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.82it/s]                                              {'loss': 2.2223, 'grad_norm': 4.157096862792969, 'learning_rate': 0.00011786607134245644, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.82it/s]                                              {'loss': 2.079, 'grad_norm': 3.505352258682251, 'learning_rate': 0.00011615786740995706, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.82it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.57it/s]                                              {'loss': 2.4342, 'grad_norm': 4.464301586151123, 'learning_rate': 0.0001144496634774577, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.57it/s]                                              {'loss': 2.2638, 'grad_norm': 3.7059988975524902, 'learning_rate': 0.00011274145954495833, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.57it/s]                                               {'loss': 2.164, 'grad_norm': 3.6256933212280273, 'learning_rate': 0.00011103325561245897, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.57it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.67it/s]                                               {'loss': 2.3864, 'grad_norm': 7.052363872528076, 'learning_rate': 0.0001093250516799596, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.67it/s]                                               {'loss': 2.2146, 'grad_norm': 4.710059642791748, 'learning_rate': 0.00010761684774746022, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.67it/s]                                               {'loss': 2.006, 'grad_norm': 3.029689073562622, 'learning_rate': 0.00010590864381496086, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.67it/s]                                               {'loss': 2.5151, 'grad_norm': 12.839781761169434, 'learning_rate': 0.0001042004398824615, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.67it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.08it/s]                                               {'loss': 2.2001, 'grad_norm': 4.282257556915283, 'learning_rate': 0.00010249223594996213, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.08it/s]                                               {'loss': 2.3176, 'grad_norm': 4.465236663818359, 'learning_rate': 0.00010078403201746274, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.08it/s]                                               {'loss': 2.1837, 'grad_norm': 3.711428642272949, 'learning_rate': 9.907582808496338e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.08it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.35it/s]                                               {'loss': 2.5522, 'grad_norm': 4.231146812438965, 'learning_rate': 9.736762415246401e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.35it/s]                                               {'loss': 2.1567, 'grad_norm': 4.527524471282959, 'learning_rate': 9.565942021996465e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.35it/s]                                               {'loss': 2.0607, 'grad_norm': 3.8737845420837402, 'learning_rate': 9.395121628746527e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.35it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.42it/s]                                               {'loss': 2.0199, 'grad_norm': 3.345228672027588, 'learning_rate': 9.22430123549659e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.42it/s]                                               {'loss': 2.0545, 'grad_norm': 3.968135118484497, 'learning_rate': 9.053480842246654e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.42it/s]                                               {'loss': 2.2793, 'grad_norm': 4.9311113357543945, 'learning_rate': 8.882660448996718e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 25.42it/s] 33%|███▎      | 25/75 [00:01<00:02, 24.63it/s]                                               {'loss': 2.4427, 'grad_norm': 4.100559234619141, 'learning_rate': 8.711840055746781e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.63it/s]                                               {'loss': 2.2867, 'grad_norm': 3.7504642009735107, 'learning_rate': 8.541019662496843e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.63it/s]                                               {'loss': 2.1736, 'grad_norm': 3.5233829021453857, 'learning_rate': 8.370199269246907e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.63it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.04it/s]                                               {'loss': 2.0231, 'grad_norm': 3.62213397026062, 'learning_rate': 8.199378875996969e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.04it/s]                                               {'loss': 2.0368, 'grad_norm': 3.919755697250366, 'learning_rate': 8.028558482747033e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.04it/s]                                               {'loss': 2.4972, 'grad_norm': 7.949209213256836, 'learning_rate': 7.857738089497095e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.04it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.22it/s]                                               {'loss': 2.1444, 'grad_norm': 3.5571212768554688, 'learning_rate': 7.686917696247158e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.22it/s]                                               {'loss': 2.27, 'grad_norm': 3.518507480621338, 'learning_rate': 7.516097302997222e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.22it/s]                                               {'loss': 2.0876, 'grad_norm': 4.281824111938477, 'learning_rate': 7.345276909747286e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.22it/s] 45%|████▌     | 34/75 [00:01<00:01, 25.78it/s]                                               {'loss': 2.1039, 'grad_norm': 3.977006673812866, 'learning_rate': 7.174456516497349e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.78it/s]                                               {'loss': 1.8466, 'grad_norm': 3.2571229934692383, 'learning_rate': 7.003636123247411e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.78it/s]                                               {'loss': 2.0899, 'grad_norm': 3.772688627243042, 'learning_rate': 6.832815729997475e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.78it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.82it/s]                                               {'loss': 2.3278, 'grad_norm': 3.383906841278076, 'learning_rate': 6.661995336747539e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.82it/s]                                               {'loss': 2.1726, 'grad_norm': 3.819453001022339, 'learning_rate': 6.491174943497602e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.82it/s]                                               {'loss': 2.1864, 'grad_norm': 4.238586902618408, 'learning_rate': 6.320354550247664e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.82it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.0751, 'grad_norm': 4.47837495803833, 'learning_rate': 6.149534156997727e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.2366, 'grad_norm': 4.0620574951171875, 'learning_rate': 5.978713763747791e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.1721, 'grad_norm': 3.458467483520508, 'learning_rate': 5.807893370497853e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.62it/s] 57%|█████▋    | 43/75 [00:01<00:01, 25.56it/s]                                               {'loss': 2.2155, 'grad_norm': 3.8551089763641357, 'learning_rate': 5.6370729772479165e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.56it/s]                                               {'loss': 2.378, 'grad_norm': 4.762766361236572, 'learning_rate': 5.46625258399798e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.56it/s]                                               {'loss': 2.1495, 'grad_norm': 13.216541290283203, 'learning_rate': 5.295432190748043e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.56it/s] 61%|██████▏   | 46/75 [00:01<00:01, 26.42it/s]                                               {'loss': 2.2187, 'grad_norm': 3.771216869354248, 'learning_rate': 5.1246117974981066e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.42it/s]                                               {'loss': 2.27, 'grad_norm': 4.46356201171875, 'learning_rate': 4.953791404248169e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.42it/s]                                               {'loss': 2.2558, 'grad_norm': 3.729677677154541, 'learning_rate': 4.782971010998232e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.42it/s] 65%|██████▌   | 49/75 [00:01<00:01, 25.92it/s]                                               {'loss': 2.0294, 'grad_norm': 3.7631802558898926, 'learning_rate': 4.612150617748295e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.92it/s]                                               {'loss': 2.1509, 'grad_norm': 2.9547672271728516, 'learning_rate': 4.441330224498359e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.92it/s]                                               {'loss': 2.0561, 'grad_norm': 3.554647445678711, 'learning_rate': 4.270509831248422e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.92it/s] 69%|██████▉   | 52/75 [00:02<00:00, 24.80it/s]                                               {'loss': 2.1339, 'grad_norm': 3.194284439086914, 'learning_rate': 4.0996894379984846e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.80it/s]                                               {'loss': 2.17, 'grad_norm': 4.101949214935303, 'learning_rate': 3.9288690447485475e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.80it/s]                                               {'loss': 2.2744, 'grad_norm': 4.03166389465332, 'learning_rate': 3.758048651498611e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.80it/s] 73%|███████▎  | 55/75 [00:02<00:00, 25.00it/s]                                               {'loss': 2.1185, 'grad_norm': 3.7236900329589844, 'learning_rate': 3.5872282582486746e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.00it/s]                                               {'loss': 2.1339, 'grad_norm': 5.068652629852295, 'learning_rate': 3.4164078649987375e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.00it/s]                                               {'loss': 2.276, 'grad_norm': 3.711141347885132, 'learning_rate': 3.245587471748801e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.00it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.0803, 'grad_norm': 2.549804925918579, 'learning_rate': 3.074767078498863e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.1814, 'grad_norm': 3.8007609844207764, 'learning_rate': 2.9039466852489265e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.0265, 'grad_norm': 11.492990493774414, 'learning_rate': 2.73312629199899e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.96it/s] 81%|████████▏ | 61/75 [00:02<00:00, 26.27it/s]                                               {'loss': 2.0082, 'grad_norm': 3.7410318851470947, 'learning_rate': 2.5623058987490533e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.27it/s]                                               {'loss': 2.3701, 'grad_norm': 4.506629467010498, 'learning_rate': 2.391485505499116e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.27it/s]                                               {'loss': 2.184, 'grad_norm': 4.152698993682861, 'learning_rate': 2.2206651122491794e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.27it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.55it/s]                                               {'loss': 2.1359, 'grad_norm': 4.297034740447998, 'learning_rate': 2.0498447189992423e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.55it/s]                                               {'loss': 2.066, 'grad_norm': 2.5757362842559814, 'learning_rate': 1.8790243257493055e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.55it/s]                                               {'loss': 2.1145, 'grad_norm': 3.495915651321411, 'learning_rate': 1.7082039324993687e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.55it/s] 89%|████████▉ | 67/75 [00:02<00:00, 25.04it/s]                                               {'loss': 2.0906, 'grad_norm': 3.438678026199341, 'learning_rate': 1.5373835392494316e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.04it/s]                                               {'loss': 2.1311, 'grad_norm': 3.917539596557617, 'learning_rate': 1.366563145999495e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.04it/s]                                               {'loss': 1.9959, 'grad_norm': 5.094424247741699, 'learning_rate': 1.195742752749558e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.04it/s] 93%|█████████▎| 70/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.3541, 'grad_norm': 5.147979736328125, 'learning_rate': 1.0249223594996211e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.2271, 'grad_norm': 3.520230293273926, 'learning_rate': 8.541019662496844e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.2397, 'grad_norm': 4.706323623657227, 'learning_rate': 6.832815729997475e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.23it/s] 97%|█████████▋| 73/75 [00:02<00:00, 24.65it/s]                                               {'loss': 2.2864, 'grad_norm': 4.2802839279174805, 'learning_rate': 5.124611797498106e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.65it/s]                                               {'loss': 2.044, 'grad_norm': 3.81701397895813, 'learning_rate': 3.4164078649987376e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.65it/s]                                               {'loss': 2.5457, 'grad_norm': 13.368782043457031, 'learning_rate': 1.7082039324993688e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.65it/s]                                               {'train_runtime': 3.0633, 'train_samples_per_second': 368.886, 'train_steps_per_second': 24.484, 'train_loss': 2.1990316184361776, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.65it/s]100%|██████████| 75/75 [00:03<00:00, 24.48it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:22
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.154, 'grad_norm': 4.343080997467041, 'learning_rate': 0.00012811529493745265, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.43it/s]                                              {'loss': 2.1791, 'grad_norm': 4.083620548248291, 'learning_rate': 0.00012640709100495329, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.33it/s]  4%|▍         | 3/75 [00:00<00:03, 23.74it/s]                                              {'loss': 2.1041, 'grad_norm': 3.903182029724121, 'learning_rate': 0.00012469888707245392, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.74it/s]                                              {'loss': 2.0388, 'grad_norm': 4.639719009399414, 'learning_rate': 0.00012299068313995453, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 23.74it/s]                                              {'loss': 1.9613, 'grad_norm': 3.5229837894439697, 'learning_rate': 0.00012128247920745518, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.74it/s]  8%|▊         | 6/75 [00:00<00:02, 24.75it/s]                                              {'loss': 2.3298, 'grad_norm': 5.013001441955566, 'learning_rate': 0.00011957427527495582, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.75it/s]                                              {'loss': 2.1778, 'grad_norm': 5.447201728820801, 'learning_rate': 0.00011786607134245644, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.75it/s]                                              {'loss': 1.9261, 'grad_norm': 3.6912903785705566, 'learning_rate': 0.00011615786740995706, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.75it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.68it/s]                                              {'loss': 2.01, 'grad_norm': 3.6335179805755615, 'learning_rate': 0.0001144496634774577, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.68it/s]                                              {'loss': 2.1601, 'grad_norm': 3.466148614883423, 'learning_rate': 0.00011274145954495833, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.68it/s]                                               {'loss': 2.2689, 'grad_norm': 3.8060219287872314, 'learning_rate': 0.00011103325561245897, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.68it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.06it/s]                                               {'loss': 1.9335, 'grad_norm': 3.6594135761260986, 'learning_rate': 0.0001093250516799596, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.06it/s]                                               {'loss': 2.1029, 'grad_norm': 4.689208507537842, 'learning_rate': 0.00010761684774746022, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.06it/s]                                               {'loss': 2.0702, 'grad_norm': 3.6167218685150146, 'learning_rate': 0.00010590864381496086, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.06it/s]                                               {'loss': 1.8201, 'grad_norm': 8.347940444946289, 'learning_rate': 0.0001042004398824615, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.06it/s] 21%|██▏       | 16/75 [00:00<00:02, 27.02it/s]                                               {'loss': 1.9502, 'grad_norm': 3.160597801208496, 'learning_rate': 0.00010249223594996213, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 27.02it/s]                                               {'loss': 2.2174, 'grad_norm': 4.126814365386963, 'learning_rate': 0.00010078403201746274, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 27.02it/s]                                               {'loss': 2.1044, 'grad_norm': 4.305668354034424, 'learning_rate': 9.907582808496338e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 27.02it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.84it/s]                                               {'loss': 1.9965, 'grad_norm': 3.812211751937866, 'learning_rate': 9.736762415246401e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.84it/s]                                               {'loss': 2.1352, 'grad_norm': 4.622171401977539, 'learning_rate': 9.565942021996465e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.84it/s]                                               {'loss': 2.3389, 'grad_norm': 4.336463451385498, 'learning_rate': 9.395121628746527e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.84it/s] 29%|██▉       | 22/75 [00:00<00:01, 26.67it/s]                                               {'loss': 2.0963, 'grad_norm': 4.297891139984131, 'learning_rate': 9.22430123549659e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:01, 26.67it/s]                                               {'loss': 2.1328, 'grad_norm': 4.040130138397217, 'learning_rate': 9.053480842246654e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 26.67it/s]                                               {'loss': 2.0325, 'grad_norm': 4.334568500518799, 'learning_rate': 8.882660448996718e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 26.67it/s] 33%|███▎      | 25/75 [00:00<00:01, 26.14it/s]                                               {'loss': 1.9448, 'grad_norm': 3.4190657138824463, 'learning_rate': 8.711840055746781e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 26.14it/s]                                               {'loss': 1.8485, 'grad_norm': 4.490251541137695, 'learning_rate': 8.541019662496843e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 26.14it/s]                                               {'loss': 2.162, 'grad_norm': 3.569626808166504, 'learning_rate': 8.370199269246907e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 26.14it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.91it/s]                                               {'loss': 2.0576, 'grad_norm': 3.3006110191345215, 'learning_rate': 8.199378875996969e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.91it/s]                                               {'loss': 2.0989, 'grad_norm': 3.976600170135498, 'learning_rate': 8.028558482747033e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.91it/s]                                               {'loss': 1.5361, 'grad_norm': 7.819439888000488, 'learning_rate': 7.857738089497095e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.91it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.68it/s]                                               {'loss': 1.9127, 'grad_norm': 3.5721981525421143, 'learning_rate': 7.686917696247158e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.68it/s]                                               {'loss': 2.0138, 'grad_norm': 4.165071964263916, 'learning_rate': 7.516097302997222e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.68it/s]                                               {'loss': 2.1379, 'grad_norm': 3.996457099914551, 'learning_rate': 7.345276909747286e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.68it/s] 45%|████▌     | 34/75 [00:01<00:01, 26.19it/s]                                               {'loss': 2.0606, 'grad_norm': 4.383007049560547, 'learning_rate': 7.174456516497349e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.19it/s]                                               {'loss': 2.2441, 'grad_norm': 4.733361721038818, 'learning_rate': 7.003636123247411e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.19it/s]                                               {'loss': 1.9452, 'grad_norm': 3.9912283420562744, 'learning_rate': 6.832815729997475e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.19it/s] 49%|████▉     | 37/75 [00:01<00:01, 26.04it/s]                                               {'loss': 2.0746, 'grad_norm': 4.157949924468994, 'learning_rate': 6.661995336747539e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.04it/s]                                               {'loss': 2.1809, 'grad_norm': 3.8582041263580322, 'learning_rate': 6.491174943497602e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.04it/s]                                               {'loss': 2.0362, 'grad_norm': 3.5627949237823486, 'learning_rate': 6.320354550247664e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.04it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.83it/s]                                               {'loss': 2.0783, 'grad_norm': 4.45050048828125, 'learning_rate': 6.149534156997727e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.83it/s]                                               {'loss': 2.0703, 'grad_norm': 3.927173614501953, 'learning_rate': 5.978713763747791e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.83it/s]                                               {'loss': 1.9877, 'grad_norm': 3.823873281478882, 'learning_rate': 5.807893370497853e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.83it/s] 57%|█████▋    | 43/75 [00:01<00:01, 25.27it/s]                                               {'loss': 2.1042, 'grad_norm': 3.829005479812622, 'learning_rate': 5.6370729772479165e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.27it/s]                                               {'loss': 2.1306, 'grad_norm': 3.348848581314087, 'learning_rate': 5.46625258399798e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.27it/s]                                               {'loss': 1.9952, 'grad_norm': 13.402304649353027, 'learning_rate': 5.295432190748043e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.27it/s]                                               {'loss': 1.9567, 'grad_norm': 4.675126075744629, 'learning_rate': 5.1246117974981066e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.27it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.89it/s]                                               {'loss': 2.0914, 'grad_norm': 3.690683126449585, 'learning_rate': 4.953791404248169e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.89it/s]                                               {'loss': 2.0024, 'grad_norm': 4.079418182373047, 'learning_rate': 4.782971010998232e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.89it/s]                                               {'loss': 1.9633, 'grad_norm': 3.760850429534912, 'learning_rate': 4.612150617748295e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.89it/s] 67%|██████▋   | 50/75 [00:01<00:00, 26.69it/s]                                               {'loss': 2.2465, 'grad_norm': 5.861806869506836, 'learning_rate': 4.441330224498359e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.69it/s]                                               {'loss': 2.1513, 'grad_norm': 3.4133896827697754, 'learning_rate': 4.270509831248422e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 26.69it/s]                                               {'loss': 2.0498, 'grad_norm': 3.514275074005127, 'learning_rate': 4.0996894379984846e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 26.69it/s] 71%|███████   | 53/75 [00:02<00:00, 26.40it/s]                                               {'loss': 2.1443, 'grad_norm': 3.542522668838501, 'learning_rate': 3.9288690447485475e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 26.40it/s]                                               {'loss': 1.9904, 'grad_norm': 3.1303839683532715, 'learning_rate': 3.758048651498611e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 26.40it/s]                                               {'loss': 2.1062, 'grad_norm': 3.8788249492645264, 'learning_rate': 3.5872282582486746e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 26.40it/s] 75%|███████▍  | 56/75 [00:02<00:00, 26.36it/s]                                               {'loss': 1.934, 'grad_norm': 3.6986067295074463, 'learning_rate': 3.4164078649987375e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 26.36it/s]                                               {'loss': 2.0187, 'grad_norm': 4.8933281898498535, 'learning_rate': 3.245587471748801e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 26.36it/s]                                               {'loss': 2.0242, 'grad_norm': 3.3892643451690674, 'learning_rate': 3.074767078498863e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 26.36it/s] 79%|███████▊  | 59/75 [00:02<00:00, 26.17it/s]                                               {'loss': 2.2677, 'grad_norm': 4.094686985015869, 'learning_rate': 2.9039466852489265e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 26.17it/s]                                               {'loss': 2.1968, 'grad_norm': 13.027373313903809, 'learning_rate': 2.73312629199899e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 26.17it/s]                                               {'loss': 2.2088, 'grad_norm': 3.4875245094299316, 'learning_rate': 2.5623058987490533e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.17it/s]                                               {'loss': 1.9401, 'grad_norm': 3.2000882625579834, 'learning_rate': 2.391485505499116e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.17it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.27it/s]                                               {'loss': 2.1457, 'grad_norm': 3.8152077198028564, 'learning_rate': 2.2206651122491794e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 27.27it/s]                                               {'loss': 1.9679, 'grad_norm': 5.201708793640137, 'learning_rate': 2.0498447189992423e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 27.27it/s]                                               {'loss': 1.9408, 'grad_norm': 3.7653443813323975, 'learning_rate': 1.8790243257493055e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 27.27it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.92it/s]                                               {'loss': 1.947, 'grad_norm': 4.047135353088379, 'learning_rate': 1.7082039324993687e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.92it/s]                                               {'loss': 2.1387, 'grad_norm': 5.402745723724365, 'learning_rate': 1.5373835392494316e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.92it/s]                                               {'loss': 2.1799, 'grad_norm': 4.113804817199707, 'learning_rate': 1.366563145999495e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 26.92it/s] 92%|█████████▏| 69/75 [00:02<00:00, 25.91it/s]                                               {'loss': 1.9823, 'grad_norm': 3.302196741104126, 'learning_rate': 1.195742752749558e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.91it/s]                                               {'loss': 2.1551, 'grad_norm': 4.58331823348999, 'learning_rate': 1.0249223594996211e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.91it/s]                                               {'loss': 1.9883, 'grad_norm': 5.998293399810791, 'learning_rate': 8.541019662496844e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.91it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.0666, 'grad_norm': 3.8994650840759277, 'learning_rate': 6.832815729997475e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.49it/s]                                               {'loss': 1.9991, 'grad_norm': 3.3479163646698, 'learning_rate': 5.124611797498106e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.1124, 'grad_norm': 3.3646202087402344, 'learning_rate': 3.4164078649987376e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.3275, 'grad_norm': 7.66827917098999, 'learning_rate': 1.7082039324993688e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.49it/s]                                               {'train_runtime': 2.9664, 'train_samples_per_second': 380.929, 'train_steps_per_second': 25.283, 'train_loss': 2.068066199620565, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.49it/s]100%|██████████| 75/75 [00:02<00:00, 25.29it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:41
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2869, 'grad_norm': 4.1478047370910645, 'learning_rate': 0.00012811529493745265, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 21.57it/s]                                              {'loss': 2.1942, 'grad_norm': 4.151677131652832, 'learning_rate': 0.00012640709100495329, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.76it/s]  4%|▍         | 3/75 [00:00<00:02, 24.66it/s]                                              {'loss': 2.3843, 'grad_norm': 6.0404791831970215, 'learning_rate': 0.00012469888707245392, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.66it/s]                                              {'loss': 2.1944, 'grad_norm': 4.027298450469971, 'learning_rate': 0.00012299068313995453, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.66it/s]                                              {'loss': 2.2448, 'grad_norm': 3.325773239135742, 'learning_rate': 0.00012128247920745518, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.66it/s]  8%|▊         | 6/75 [00:00<00:02, 25.90it/s]                                              {'loss': 2.3751, 'grad_norm': 4.589799404144287, 'learning_rate': 0.00011957427527495582, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.90it/s]                                              {'loss': 2.2716, 'grad_norm': 3.6078133583068848, 'learning_rate': 0.00011786607134245644, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.90it/s]                                              {'loss': 2.3146, 'grad_norm': 2.776393413543701, 'learning_rate': 0.00011615786740995706, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.90it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.78it/s]                                              {'loss': 2.2483, 'grad_norm': 4.009195327758789, 'learning_rate': 0.0001144496634774577, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.78it/s]                                              {'loss': 2.431, 'grad_norm': 3.5463454723358154, 'learning_rate': 0.00011274145954495833, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.78it/s]                                               {'loss': 2.2819, 'grad_norm': 4.443380832672119, 'learning_rate': 0.00011103325561245897, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.78it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.57it/s]                                               {'loss': 2.2779, 'grad_norm': 4.424707889556885, 'learning_rate': 0.0001093250516799596, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.57it/s]                                               {'loss': 2.2462, 'grad_norm': 5.638967037200928, 'learning_rate': 0.00010761684774746022, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.57it/s]                                               {'loss': 2.4723, 'grad_norm': 4.069675445556641, 'learning_rate': 0.00010590864381496086, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.57it/s]                                               {'loss': 2.2808, 'grad_norm': 11.884976387023926, 'learning_rate': 0.0001042004398824615, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.57it/s] 21%|██▏       | 16/75 [00:00<00:02, 27.19it/s]                                               {'loss': 2.2937, 'grad_norm': 3.501295328140259, 'learning_rate': 0.00010249223594996213, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 27.19it/s]                                               {'loss': 2.3591, 'grad_norm': 4.8482666015625, 'learning_rate': 0.00010078403201746274, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 27.19it/s]                                               {'loss': 2.4295, 'grad_norm': 3.6748688220977783, 'learning_rate': 9.907582808496338e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 27.19it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.99it/s]                                               {'loss': 2.2001, 'grad_norm': 3.4800543785095215, 'learning_rate': 9.736762415246401e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.99it/s]                                               {'loss': 2.1311, 'grad_norm': 3.2900824546813965, 'learning_rate': 9.565942021996465e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.99it/s]                                               {'loss': 2.2861, 'grad_norm': 4.337215423583984, 'learning_rate': 9.395121628746527e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.99it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.76it/s]                                               {'loss': 2.3363, 'grad_norm': 4.301945209503174, 'learning_rate': 9.22430123549659e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.76it/s]                                               {'loss': 2.2549, 'grad_norm': 3.8327786922454834, 'learning_rate': 9.053480842246654e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.76it/s]                                               {'loss': 2.0679, 'grad_norm': 3.23718523979187, 'learning_rate': 8.882660448996718e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.76it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.78it/s]                                               {'loss': 2.2772, 'grad_norm': 3.66835618019104, 'learning_rate': 8.711840055746781e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.78it/s]                                               {'loss': 2.2052, 'grad_norm': 2.902327299118042, 'learning_rate': 8.541019662496843e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.78it/s]                                               {'loss': 2.0411, 'grad_norm': 3.544679641723633, 'learning_rate': 8.370199269246907e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.78it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.36it/s]                                               {'loss': 2.3578, 'grad_norm': 3.660017967224121, 'learning_rate': 8.199378875996969e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.36it/s]                                               {'loss': 2.376, 'grad_norm': 3.5891196727752686, 'learning_rate': 8.028558482747033e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.36it/s]                                               {'loss': 2.1218, 'grad_norm': 5.576681137084961, 'learning_rate': 7.857738089497095e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.36it/s]                                               {'loss': 2.2881, 'grad_norm': 3.3655874729156494, 'learning_rate': 7.686917696247158e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.36it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.94it/s]                                               {'loss': 2.1679, 'grad_norm': 5.054744720458984, 'learning_rate': 7.516097302997222e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.94it/s]                                               {'loss': 2.2525, 'grad_norm': 4.325881481170654, 'learning_rate': 7.345276909747286e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.94it/s]                                               {'loss': 2.1985, 'grad_norm': 3.6895124912261963, 'learning_rate': 7.174456516497349e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.94it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.63it/s]                                               {'loss': 2.1716, 'grad_norm': 3.83091402053833, 'learning_rate': 7.003636123247411e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.63it/s]                                               {'loss': 2.1567, 'grad_norm': 4.010517120361328, 'learning_rate': 6.832815729997475e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.63it/s]                                               {'loss': 2.2649, 'grad_norm': 4.447229862213135, 'learning_rate': 6.661995336747539e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.63it/s] 51%|█████     | 38/75 [00:01<00:01, 26.43it/s]                                               {'loss': 2.1747, 'grad_norm': 4.557892799377441, 'learning_rate': 6.491174943497602e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.43it/s]                                               {'loss': 2.2245, 'grad_norm': 4.076158046722412, 'learning_rate': 6.320354550247664e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.43it/s]                                               {'loss': 2.0857, 'grad_norm': 3.7203636169433594, 'learning_rate': 6.149534156997727e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 26.43it/s] 55%|█████▍    | 41/75 [00:01<00:01, 26.05it/s]                                               {'loss': 2.297, 'grad_norm': 3.616654396057129, 'learning_rate': 5.978713763747791e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 26.05it/s]                                               {'loss': 2.2255, 'grad_norm': 4.665344715118408, 'learning_rate': 5.807893370497853e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 26.05it/s]                                               {'loss': 2.1798, 'grad_norm': 3.8030312061309814, 'learning_rate': 5.6370729772479165e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 26.05it/s] 59%|█████▊    | 44/75 [00:01<00:01, 26.03it/s]                                               {'loss': 2.2908, 'grad_norm': 3.835371494293213, 'learning_rate': 5.46625258399798e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 26.03it/s]                                               {'loss': 2.1697, 'grad_norm': 8.026506423950195, 'learning_rate': 5.295432190748043e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 26.03it/s]                                               {'loss': 2.3751, 'grad_norm': 4.686310291290283, 'learning_rate': 5.1246117974981066e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.03it/s]                                               {'loss': 2.2494, 'grad_norm': 4.07356595993042, 'learning_rate': 4.953791404248169e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.03it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.65it/s]                                               {'loss': 2.177, 'grad_norm': 4.608443737030029, 'learning_rate': 4.782971010998232e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 27.65it/s]                                               {'loss': 2.2509, 'grad_norm': 4.2515549659729, 'learning_rate': 4.612150617748295e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 27.65it/s]                                               {'loss': 2.2691, 'grad_norm': 4.091099262237549, 'learning_rate': 4.441330224498359e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 27.65it/s] 68%|██████▊   | 51/75 [00:01<00:00, 27.12it/s]                                               {'loss': 2.2374, 'grad_norm': 3.9427742958068848, 'learning_rate': 4.270509831248422e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 27.12it/s]                                               {'loss': 2.17, 'grad_norm': 3.125333547592163, 'learning_rate': 4.0996894379984846e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 27.12it/s]                                               {'loss': 2.0112, 'grad_norm': 3.65982723236084, 'learning_rate': 3.9288690447485475e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 27.12it/s] 72%|███████▏  | 54/75 [00:02<00:00, 26.60it/s]                                               {'loss': 2.184, 'grad_norm': 3.430954933166504, 'learning_rate': 3.758048651498611e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 26.60it/s]                                               {'loss': 2.1982, 'grad_norm': 2.8068430423736572, 'learning_rate': 3.5872282582486746e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 26.60it/s]                                               {'loss': 2.2405, 'grad_norm': 3.5258662700653076, 'learning_rate': 3.4164078649987375e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 26.60it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.23it/s]                                               {'loss': 2.36, 'grad_norm': 4.463468074798584, 'learning_rate': 3.245587471748801e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 26.23it/s]                                               {'loss': 2.1146, 'grad_norm': 3.4578254222869873, 'learning_rate': 3.074767078498863e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 26.23it/s]                                               {'loss': 2.2247, 'grad_norm': 5.239828109741211, 'learning_rate': 2.9039466852489265e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 26.23it/s] 80%|████████  | 60/75 [00:02<00:00, 27.09it/s]                                               {'loss': 1.8817, 'grad_norm': 9.460816383361816, 'learning_rate': 2.73312629199899e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 27.09it/s]                                               {'loss': 2.1344, 'grad_norm': 3.215655565261841, 'learning_rate': 2.5623058987490533e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 27.09it/s]                                               {'loss': 2.1761, 'grad_norm': 3.1904184818267822, 'learning_rate': 2.391485505499116e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 27.09it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.04it/s]                                               {'loss': 2.3125, 'grad_norm': 4.275901794433594, 'learning_rate': 2.2206651122491794e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.04it/s]                                               {'loss': 2.1603, 'grad_norm': 4.727686405181885, 'learning_rate': 2.0498447189992423e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.04it/s]                                               {'loss': 2.1837, 'grad_norm': 3.874009609222412, 'learning_rate': 1.8790243257493055e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.04it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.74it/s]                                               {'loss': 2.308, 'grad_norm': 3.879133939743042, 'learning_rate': 1.7082039324993687e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.74it/s]                                               {'loss': 2.1265, 'grad_norm': 4.229337692260742, 'learning_rate': 1.5373835392494316e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.74it/s]                                               {'loss': 2.3423, 'grad_norm': 5.255896091461182, 'learning_rate': 1.366563145999495e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.74it/s] 92%|█████████▏| 69/75 [00:02<00:00, 25.76it/s]                                               {'loss': 2.3312, 'grad_norm': 4.132570743560791, 'learning_rate': 1.195742752749558e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.76it/s]                                               {'loss': 2.1502, 'grad_norm': 3.431276321411133, 'learning_rate': 1.0249223594996211e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.76it/s]                                               {'loss': 1.9575, 'grad_norm': 4.108404636383057, 'learning_rate': 8.541019662496844e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.76it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.58it/s]                                               {'loss': 2.4231, 'grad_norm': 6.263842582702637, 'learning_rate': 6.832815729997475e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.58it/s]                                               {'loss': 2.2034, 'grad_norm': 3.0041744709014893, 'learning_rate': 5.124611797498106e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.58it/s]                                               {'loss': 2.0756, 'grad_norm': 3.7006165981292725, 'learning_rate': 3.4164078649987376e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.58it/s]                                               {'loss': 2.1698, 'grad_norm': 6.266851902008057, 'learning_rate': 1.7082039324993688e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.58it/s]                                               {'train_runtime': 2.947, 'train_samples_per_second': 383.443, 'train_steps_per_second': 25.45, 'train_loss': 2.2318485085169475, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.58it/s]100%|██████████| 75/75 [00:02<00:00, 25.45it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:29
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2262, 'grad_norm': 4.816714286804199, 'learning_rate': 0.00012811529493745265, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 24.08it/s]                                              {'loss': 2.254, 'grad_norm': 4.099118232727051, 'learning_rate': 0.00012640709100495329, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.96it/s]  4%|▍         | 3/75 [00:00<00:02, 25.23it/s]                                              {'loss': 2.3808, 'grad_norm': 4.898525714874268, 'learning_rate': 0.00012469888707245392, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.23it/s]                                              {'loss': 2.167, 'grad_norm': 4.781215190887451, 'learning_rate': 0.00012299068313995453, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.23it/s]                                              {'loss': 2.2543, 'grad_norm': 5.150137424468994, 'learning_rate': 0.00012128247920745518, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.23it/s]  8%|▊         | 6/75 [00:00<00:02, 25.26it/s]                                              {'loss': 2.3681, 'grad_norm': 5.144608974456787, 'learning_rate': 0.00011957427527495582, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.26it/s]                                              {'loss': 2.1817, 'grad_norm': 3.6867308616638184, 'learning_rate': 0.00011786607134245644, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.26it/s]                                              {'loss': 2.3341, 'grad_norm': 3.5369081497192383, 'learning_rate': 0.00011615786740995706, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.26it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.20it/s]                                              {'loss': 2.146, 'grad_norm': 3.7012243270874023, 'learning_rate': 0.0001144496634774577, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.20it/s]                                              {'loss': 2.2217, 'grad_norm': 3.3925256729125977, 'learning_rate': 0.00011274145954495833, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.20it/s]                                               {'loss': 2.1842, 'grad_norm': 4.586488723754883, 'learning_rate': 0.00011103325561245897, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.20it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.59it/s]                                               {'loss': 2.1877, 'grad_norm': 4.383284091949463, 'learning_rate': 0.0001093250516799596, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.59it/s]                                               {'loss': 2.3285, 'grad_norm': 4.426137924194336, 'learning_rate': 0.00010761684774746022, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.59it/s]                                               {'loss': 2.4353, 'grad_norm': 4.033994674682617, 'learning_rate': 0.00010590864381496086, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.59it/s]                                               {'loss': 2.4084, 'grad_norm': 9.906286239624023, 'learning_rate': 0.0001042004398824615, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.59it/s] 21%|██▏       | 16/75 [00:00<00:02, 27.59it/s]                                               {'loss': 2.2767, 'grad_norm': 4.495861530303955, 'learning_rate': 0.00010249223594996213, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 27.59it/s]                                               {'loss': 2.1869, 'grad_norm': 3.991821765899658, 'learning_rate': 0.00010078403201746274, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 27.59it/s]                                               {'loss': 2.2367, 'grad_norm': 4.0497894287109375, 'learning_rate': 9.907582808496338e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 27.59it/s] 25%|██▌       | 19/75 [00:00<00:02, 27.39it/s]                                               {'loss': 2.1606, 'grad_norm': 4.793083190917969, 'learning_rate': 9.736762415246401e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 27.39it/s]                                               {'loss': 2.2255, 'grad_norm': 3.1671807765960693, 'learning_rate': 9.565942021996465e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 27.39it/s]                                               {'loss': 2.3424, 'grad_norm': 3.156162738800049, 'learning_rate': 9.395121628746527e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:01, 27.39it/s] 29%|██▉       | 22/75 [00:00<00:01, 26.75it/s]                                               {'loss': 2.4526, 'grad_norm': 4.24116325378418, 'learning_rate': 9.22430123549659e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:01, 26.75it/s]                                               {'loss': 2.2985, 'grad_norm': 4.854788780212402, 'learning_rate': 9.053480842246654e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 26.75it/s]                                               {'loss': 2.3285, 'grad_norm': 4.636886119842529, 'learning_rate': 8.882660448996718e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 26.75it/s] 33%|███▎      | 25/75 [00:00<00:01, 26.55it/s]                                               {'loss': 2.2804, 'grad_norm': 4.303359031677246, 'learning_rate': 8.711840055746781e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 26.55it/s]                                               {'loss': 2.2219, 'grad_norm': 3.6275861263275146, 'learning_rate': 8.541019662496843e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:00<00:01, 26.55it/s]                                               {'loss': 2.1138, 'grad_norm': 3.3728504180908203, 'learning_rate': 8.370199269246907e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 26.55it/s] 37%|███▋      | 28/75 [00:01<00:01, 26.33it/s]                                               {'loss': 2.1023, 'grad_norm': 3.9919114112854004, 'learning_rate': 8.199378875996969e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 26.33it/s]                                               {'loss': 2.0893, 'grad_norm': 3.641177177429199, 'learning_rate': 8.028558482747033e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 26.33it/s]                                               {'loss': 2.3835, 'grad_norm': 8.751155853271484, 'learning_rate': 7.857738089497095e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 26.33it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.59it/s]                                               {'loss': 2.1272, 'grad_norm': 3.056875467300415, 'learning_rate': 7.686917696247158e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.59it/s]                                               {'loss': 2.1579, 'grad_norm': 4.025806903839111, 'learning_rate': 7.516097302997222e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.59it/s]                                               {'loss': 2.1816, 'grad_norm': 4.156073570251465, 'learning_rate': 7.345276909747286e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.59it/s] 45%|████▌     | 34/75 [00:01<00:01, 26.45it/s]                                               {'loss': 2.1491, 'grad_norm': 3.379617929458618, 'learning_rate': 7.174456516497349e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.45it/s]                                               {'loss': 2.2052, 'grad_norm': 3.3926949501037598, 'learning_rate': 7.003636123247411e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.45it/s]                                               {'loss': 2.3164, 'grad_norm': 3.650641441345215, 'learning_rate': 6.832815729997475e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.45it/s] 49%|████▉     | 37/75 [00:01<00:01, 24.77it/s]                                               {'loss': 2.2264, 'grad_norm': 5.018136501312256, 'learning_rate': 6.661995336747539e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.77it/s]                                               {'loss': 2.219, 'grad_norm': 5.2321858406066895, 'learning_rate': 6.491174943497602e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.77it/s]                                               {'loss': 2.5601, 'grad_norm': 5.72203254699707, 'learning_rate': 6.320354550247664e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.77it/s] 53%|█████▎    | 40/75 [00:01<00:01, 24.98it/s]                                               {'loss': 2.1688, 'grad_norm': 4.011119365692139, 'learning_rate': 6.149534156997727e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.98it/s]                                               {'loss': 2.0737, 'grad_norm': 2.4774067401885986, 'learning_rate': 5.978713763747791e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.98it/s]                                               {'loss': 2.1518, 'grad_norm': 4.7057037353515625, 'learning_rate': 5.807893370497853e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.98it/s] 57%|█████▋    | 43/75 [00:01<00:01, 24.93it/s]                                               {'loss': 2.281, 'grad_norm': 4.826076507568359, 'learning_rate': 5.6370729772479165e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.93it/s]                                               {'loss': 2.3177, 'grad_norm': 3.407012939453125, 'learning_rate': 5.46625258399798e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.93it/s]                                               {'loss': 2.3058, 'grad_norm': 10.425846099853516, 'learning_rate': 5.295432190748043e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.93it/s] 61%|██████▏   | 46/75 [00:01<00:01, 25.50it/s]                                               {'loss': 2.1083, 'grad_norm': 3.17616868019104, 'learning_rate': 5.1246117974981066e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.50it/s]                                               {'loss': 2.3545, 'grad_norm': 3.637543201446533, 'learning_rate': 4.953791404248169e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.50it/s]                                               {'loss': 2.337, 'grad_norm': 3.9466118812561035, 'learning_rate': 4.782971010998232e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.50it/s] 65%|██████▌   | 49/75 [00:01<00:01, 25.69it/s]                                               {'loss': 2.2359, 'grad_norm': 4.404354572296143, 'learning_rate': 4.612150617748295e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.69it/s]                                               {'loss': 2.0407, 'grad_norm': 3.5002758502960205, 'learning_rate': 4.441330224498359e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.69it/s]                                               {'loss': 2.0775, 'grad_norm': 4.370438575744629, 'learning_rate': 4.270509831248422e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 25.69it/s] 69%|██████▉   | 52/75 [00:02<00:00, 25.82it/s]                                               {'loss': 2.0836, 'grad_norm': 3.4142372608184814, 'learning_rate': 4.0996894379984846e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.82it/s]                                               {'loss': 2.0778, 'grad_norm': 3.4480690956115723, 'learning_rate': 3.9288690447485475e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.82it/s]                                               {'loss': 2.2743, 'grad_norm': 5.315352439880371, 'learning_rate': 3.758048651498611e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.82it/s] 73%|███████▎  | 55/75 [00:02<00:00, 25.60it/s]                                               {'loss': 2.1903, 'grad_norm': 4.187002182006836, 'learning_rate': 3.5872282582486746e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.60it/s]                                               {'loss': 2.2432, 'grad_norm': 4.13771390914917, 'learning_rate': 3.4164078649987375e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.60it/s]                                               {'loss': 2.1504, 'grad_norm': 3.430640935897827, 'learning_rate': 3.245587471748801e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.60it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.93it/s]                                               {'loss': 2.2305, 'grad_norm': 3.3693630695343018, 'learning_rate': 3.074767078498863e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.93it/s]                                               {'loss': 2.339, 'grad_norm': 5.198879241943359, 'learning_rate': 2.9039466852489265e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.93it/s]                                               {'loss': 2.1611, 'grad_norm': 8.493063926696777, 'learning_rate': 2.73312629199899e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.93it/s]                                               {'loss': 2.2658, 'grad_norm': 4.3957133293151855, 'learning_rate': 2.5623058987490533e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.93it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.00it/s]                                               {'loss': 2.1196, 'grad_norm': 3.7085845470428467, 'learning_rate': 2.391485505499116e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.00it/s]                                               {'loss': 2.1157, 'grad_norm': 3.143941640853882, 'learning_rate': 2.2206651122491794e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.00it/s]                                               {'loss': 2.2226, 'grad_norm': 4.1872358322143555, 'learning_rate': 2.0498447189992423e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.00it/s] 87%|████████▋ | 65/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.3125, 'grad_norm': 4.0455732345581055, 'learning_rate': 1.8790243257493055e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.2261, 'grad_norm': 3.649395704269409, 'learning_rate': 1.7082039324993687e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.1017, 'grad_norm': 3.6982884407043457, 'learning_rate': 1.5373835392494316e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.46it/s] 91%|█████████ | 68/75 [00:02<00:00, 25.34it/s]                                               {'loss': 2.2167, 'grad_norm': 4.2403645515441895, 'learning_rate': 1.366563145999495e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.34it/s]                                               {'loss': 2.2354, 'grad_norm': 4.189017295837402, 'learning_rate': 1.195742752749558e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.34it/s]                                               {'loss': 2.2913, 'grad_norm': 3.3798844814300537, 'learning_rate': 1.0249223594996211e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.34it/s] 95%|█████████▍| 71/75 [00:02<00:00, 25.17it/s]                                               {'loss': 2.2036, 'grad_norm': 4.062912940979004, 'learning_rate': 8.541019662496844e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.17it/s]                                               {'loss': 2.2904, 'grad_norm': 4.8403401374816895, 'learning_rate': 6.832815729997475e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.17it/s]                                               {'loss': 2.0332, 'grad_norm': 4.162386894226074, 'learning_rate': 5.124611797498106e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.17it/s] 99%|█████████▊| 74/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.0887, 'grad_norm': 3.162085771560669, 'learning_rate': 3.4164078649987376e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.242, 'grad_norm': 17.473520278930664, 'learning_rate': 1.7082039324993688e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.23it/s]                                               {'train_runtime': 3.0289, 'train_samples_per_second': 373.075, 'train_steps_per_second': 24.762, 'train_loss': 2.227847162882487, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.23it/s]100%|██████████| 75/75 [00:03<00:00, 24.76it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:48
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3681, 'grad_norm': 4.930598735809326, 'learning_rate': 0.00012811529493745265, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 22.47it/s]                                              {'loss': 2.3078, 'grad_norm': 5.09324312210083, 'learning_rate': 0.00012640709100495329, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 24.17it/s]  4%|▍         | 3/75 [00:00<00:02, 24.75it/s]                                              {'loss': 2.4602, 'grad_norm': 3.818279981613159, 'learning_rate': 0.00012469888707245392, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.75it/s]                                              {'loss': 2.1128, 'grad_norm': 3.6334872245788574, 'learning_rate': 0.00012299068313995453, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.75it/s]                                              {'loss': 2.2837, 'grad_norm': 3.6778013706207275, 'learning_rate': 0.00012128247920745518, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.75it/s]  8%|▊         | 6/75 [00:00<00:02, 25.03it/s]                                              {'loss': 2.2314, 'grad_norm': 4.261397838592529, 'learning_rate': 0.00011957427527495582, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.03it/s]                                              {'loss': 2.5116, 'grad_norm': 4.677916049957275, 'learning_rate': 0.00011786607134245644, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.03it/s]                                              {'loss': 2.1997, 'grad_norm': 3.895151376724243, 'learning_rate': 0.00011615786740995706, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.03it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.54it/s]                                              {'loss': 2.259, 'grad_norm': 4.9105658531188965, 'learning_rate': 0.0001144496634774577, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.54it/s]                                              {'loss': 2.2425, 'grad_norm': 3.764785051345825, 'learning_rate': 0.00011274145954495833, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.54it/s]                                               {'loss': 2.3713, 'grad_norm': 3.426062822341919, 'learning_rate': 0.00011103325561245897, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.54it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.90it/s]                                               {'loss': 2.2485, 'grad_norm': 3.715005874633789, 'learning_rate': 0.0001093250516799596, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.90it/s]                                               {'loss': 2.6948, 'grad_norm': 6.335399627685547, 'learning_rate': 0.00010761684774746022, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.90it/s]                                               {'loss': 2.0893, 'grad_norm': 3.7318642139434814, 'learning_rate': 0.00010590864381496086, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.90it/s]                                               {'loss': 1.8549, 'grad_norm': 9.007670402526855, 'learning_rate': 0.0001042004398824615, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.90it/s] 21%|██▏       | 16/75 [00:00<00:02, 27.65it/s]                                               {'loss': 2.4179, 'grad_norm': 3.7426180839538574, 'learning_rate': 0.00010249223594996213, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 27.65it/s]                                               {'loss': 2.2879, 'grad_norm': 3.8184049129486084, 'learning_rate': 0.00010078403201746274, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 27.65it/s]                                               {'loss': 2.6846, 'grad_norm': 5.45985221862793, 'learning_rate': 9.907582808496338e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 27.65it/s] 25%|██▌       | 19/75 [00:00<00:02, 27.11it/s]                                               {'loss': 2.2093, 'grad_norm': 4.052614688873291, 'learning_rate': 9.736762415246401e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 27.11it/s]                                               {'loss': 2.1906, 'grad_norm': 4.259541034698486, 'learning_rate': 9.565942021996465e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 27.11it/s]                                               {'loss': 2.1892, 'grad_norm': 4.046812534332275, 'learning_rate': 9.395121628746527e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:01, 27.11it/s] 29%|██▉       | 22/75 [00:00<00:02, 26.27it/s]                                               {'loss': 2.0679, 'grad_norm': 4.024005889892578, 'learning_rate': 9.22430123549659e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 26.27it/s]                                               {'loss': 2.1077, 'grad_norm': 3.4933366775512695, 'learning_rate': 9.053480842246654e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 26.27it/s]                                               {'loss': 2.2159, 'grad_norm': 3.37082576751709, 'learning_rate': 8.882660448996718e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 26.27it/s] 33%|███▎      | 25/75 [00:00<00:01, 26.17it/s]                                               {'loss': 2.1387, 'grad_norm': 4.430947303771973, 'learning_rate': 8.711840055746781e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 26.17it/s]                                               {'loss': 2.1535, 'grad_norm': 4.71196174621582, 'learning_rate': 8.541019662496843e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:00<00:01, 26.17it/s]                                               {'loss': 2.2932, 'grad_norm': 5.564852237701416, 'learning_rate': 8.370199269246907e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 26.17it/s] 37%|███▋      | 28/75 [00:01<00:01, 26.38it/s]                                               {'loss': 2.2218, 'grad_norm': 3.9968929290771484, 'learning_rate': 8.199378875996969e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 26.38it/s]                                               {'loss': 2.235, 'grad_norm': 4.699699401855469, 'learning_rate': 8.028558482747033e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 26.38it/s]                                               {'loss': 1.9658, 'grad_norm': 7.245141506195068, 'learning_rate': 7.857738089497095e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 26.38it/s]                                               {'loss': 2.2511, 'grad_norm': 4.1955718994140625, 'learning_rate': 7.686917696247158e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.38it/s] 43%|████▎     | 32/75 [00:01<00:01, 28.21it/s]                                               {'loss': 2.17, 'grad_norm': 3.5615439414978027, 'learning_rate': 7.516097302997222e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 28.21it/s]                                               {'loss': 2.3287, 'grad_norm': 4.16441011428833, 'learning_rate': 7.345276909747286e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 28.21it/s]                                               {'loss': 2.2156, 'grad_norm': 4.015340805053711, 'learning_rate': 7.174456516497349e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 28.21it/s] 47%|████▋     | 35/75 [00:01<00:01, 27.69it/s]                                               {'loss': 2.1728, 'grad_norm': 3.447381019592285, 'learning_rate': 7.003636123247411e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 27.69it/s]                                               {'loss': 2.2045, 'grad_norm': 4.920790195465088, 'learning_rate': 6.832815729997475e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 27.69it/s]                                               {'loss': 2.2025, 'grad_norm': 3.984015464782715, 'learning_rate': 6.661995336747539e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 27.69it/s] 51%|█████     | 38/75 [00:01<00:01, 26.98it/s]                                               {'loss': 2.1832, 'grad_norm': 4.446846008300781, 'learning_rate': 6.491174943497602e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.98it/s]                                               {'loss': 2.3322, 'grad_norm': 4.777683734893799, 'learning_rate': 6.320354550247664e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.98it/s]                                               {'loss': 2.343, 'grad_norm': 4.169539451599121, 'learning_rate': 6.149534156997727e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 26.98it/s] 55%|█████▍    | 41/75 [00:01<00:01, 26.67it/s]                                               {'loss': 2.1024, 'grad_norm': 3.4699671268463135, 'learning_rate': 5.978713763747791e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 26.67it/s]                                               {'loss': 2.0517, 'grad_norm': 6.267935752868652, 'learning_rate': 5.807893370497853e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 26.67it/s]                                               {'loss': 2.3284, 'grad_norm': 5.200229644775391, 'learning_rate': 5.6370729772479165e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 26.67it/s] 59%|█████▊    | 44/75 [00:01<00:01, 26.76it/s]                                               {'loss': 2.3894, 'grad_norm': 3.5534543991088867, 'learning_rate': 5.46625258399798e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 26.76it/s]                                               {'loss': 2.3805, 'grad_norm': 12.276509284973145, 'learning_rate': 5.295432190748043e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 26.76it/s]                                               {'loss': 2.0553, 'grad_norm': 3.4103586673736572, 'learning_rate': 5.1246117974981066e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.76it/s]                                               {'loss': 2.4201, 'grad_norm': 3.7691664695739746, 'learning_rate': 4.953791404248169e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.76it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.63it/s]                                               {'loss': 2.2168, 'grad_norm': 5.541615962982178, 'learning_rate': 4.782971010998232e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 27.63it/s]                                               {'loss': 2.1172, 'grad_norm': 5.872567176818848, 'learning_rate': 4.612150617748295e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 27.63it/s]                                               {'loss': 2.0835, 'grad_norm': 2.48862624168396, 'learning_rate': 4.441330224498359e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 27.63it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.82it/s]                                               {'loss': 2.132, 'grad_norm': 4.111143589019775, 'learning_rate': 4.270509831248422e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 26.82it/s]                                               {'loss': 2.4727, 'grad_norm': 3.9786014556884766, 'learning_rate': 4.0996894379984846e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 26.82it/s]                                               {'loss': 2.4506, 'grad_norm': 5.725043296813965, 'learning_rate': 3.9288690447485475e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:01<00:00, 26.82it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.96it/s]                                               {'loss': 2.3266, 'grad_norm': 5.197640419006348, 'learning_rate': 3.758048651498611e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.96it/s]                                               {'loss': 2.2528, 'grad_norm': 4.801418304443359, 'learning_rate': 3.5872282582486746e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.96it/s]                                               {'loss': 2.243, 'grad_norm': 2.964939594268799, 'learning_rate': 3.4164078649987375e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.96it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.96it/s]                                               {'loss': 1.9491, 'grad_norm': 4.5750732421875, 'learning_rate': 3.245587471748801e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.1227, 'grad_norm': 4.165535926818848, 'learning_rate': 3.074767078498863e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.2155, 'grad_norm': 3.8372035026550293, 'learning_rate': 2.9039466852489265e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.3959, 'grad_norm': 7.216298580169678, 'learning_rate': 2.73312629199899e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.96it/s] 81%|████████▏ | 61/75 [00:02<00:00, 26.73it/s]                                               {'loss': 2.201, 'grad_norm': 3.694010019302368, 'learning_rate': 2.5623058987490533e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.73it/s]                                               {'loss': 2.3334, 'grad_norm': 3.9714505672454834, 'learning_rate': 2.391485505499116e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.73it/s]                                               {'loss': 2.2396, 'grad_norm': 4.438910484313965, 'learning_rate': 2.2206651122491794e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.73it/s] 85%|████████▌ | 64/75 [00:02<00:00, 26.70it/s]                                               {'loss': 1.966, 'grad_norm': 4.417796611785889, 'learning_rate': 2.0498447189992423e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.70it/s]                                               {'loss': 2.0469, 'grad_norm': 3.614851236343384, 'learning_rate': 1.8790243257493055e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.70it/s]                                               {'loss': 2.3243, 'grad_norm': 4.179853916168213, 'learning_rate': 1.7082039324993687e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.70it/s] 89%|████████▉ | 67/75 [00:02<00:00, 26.44it/s]                                               {'loss': 2.2229, 'grad_norm': 4.132649898529053, 'learning_rate': 1.5373835392494316e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.44it/s]                                               {'loss': 2.0419, 'grad_norm': 4.949425220489502, 'learning_rate': 1.366563145999495e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 26.44it/s]                                               {'loss': 2.2514, 'grad_norm': 4.960019111633301, 'learning_rate': 1.195742752749558e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 26.44it/s] 93%|█████████▎| 70/75 [00:02<00:00, 26.20it/s]                                               {'loss': 2.2371, 'grad_norm': 4.422528266906738, 'learning_rate': 1.0249223594996211e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 26.20it/s]                                               {'loss': 2.3251, 'grad_norm': 4.436733722686768, 'learning_rate': 8.541019662496844e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 26.20it/s]                                               {'loss': 2.3193, 'grad_norm': 4.289377689361572, 'learning_rate': 6.832815729997475e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 26.20it/s] 97%|█████████▋| 73/75 [00:02<00:00, 25.91it/s]                                               {'loss': 2.1936, 'grad_norm': 3.599815607070923, 'learning_rate': 5.124611797498106e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.91it/s]                                               {'loss': 2.0974, 'grad_norm': 3.170476198196411, 'learning_rate': 3.4164078649987376e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.91it/s]                                               {'loss': 2.2004, 'grad_norm': 13.187504768371582, 'learning_rate': 1.7082039324993688e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.91it/s]                                               {'train_runtime': 2.9168, 'train_samples_per_second': 387.407, 'train_steps_per_second': 25.713, 'train_loss': 2.2363278516133627, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.91it/s]100%|██████████| 75/75 [00:02<00:00, 25.72it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(975, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(900, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(733, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(881, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1097, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(859, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(994, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(822, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(885, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(554, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1061, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(509, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 64.15it/s]  3%|▎         | 14/471 [00:00<00:07, 58.15it/s]  4%|▍         | 20/471 [00:00<00:08, 56.31it/s]  6%|▌         | 26/471 [00:00<00:07, 55.68it/s]  7%|▋         | 32/471 [00:00<00:08, 54.07it/s]  8%|▊         | 38/471 [00:00<00:08, 52.45it/s]  9%|▉         | 44/471 [00:00<00:08, 51.39it/s] 11%|█         | 50/471 [00:00<00:08, 51.69it/s] 12%|█▏        | 56/471 [00:01<00:07, 52.43it/s] 13%|█▎        | 62/471 [00:01<00:07, 51.27it/s] 14%|█▍        | 68/471 [00:01<00:07, 52.53it/s] 16%|█▌        | 74/471 [00:01<00:07, 53.06it/s] 17%|█▋        | 80/471 [00:01<00:07, 53.44it/s] 18%|█▊        | 86/471 [00:01<00:07, 53.21it/s] 20%|█▉        | 92/471 [00:01<00:07, 53.68it/s] 21%|██        | 98/471 [00:01<00:06, 53.39it/s] 22%|██▏       | 104/471 [00:01<00:06, 53.82it/s] 23%|██▎       | 110/471 [00:02<00:06, 53.76it/s] 25%|██▍       | 116/471 [00:02<00:06, 52.74it/s] 26%|██▌       | 122/471 [00:02<00:06, 53.59it/s] 27%|██▋       | 128/471 [00:02<00:06, 53.68it/s] 28%|██▊       | 134/471 [00:02<00:06, 53.13it/s] 30%|██▉       | 140/471 [00:02<00:06, 51.24it/s] 31%|███       | 146/471 [00:02<00:06, 50.64it/s] 32%|███▏      | 152/471 [00:02<00:06, 51.29it/s] 34%|███▎      | 158/471 [00:02<00:06, 51.82it/s] 35%|███▍      | 164/471 [00:03<00:05, 52.28it/s] 36%|███▌      | 170/471 [00:03<00:05, 51.83it/s] 37%|███▋      | 176/471 [00:03<00:05, 50.64it/s] 39%|███▊      | 182/471 [00:03<00:06, 44.18it/s] 40%|███▉      | 188/471 [00:03<00:06, 46.73it/s] 41%|████      | 194/471 [00:03<00:05, 46.40it/s] 42%|████▏     | 200/471 [00:03<00:05, 48.46it/s] 44%|████▎     | 206/471 [00:03<00:05, 50.32it/s] 45%|████▌     | 212/471 [00:04<00:05, 51.23it/s] 46%|████▋     | 218/471 [00:04<00:04, 51.86it/s] 48%|████▊     | 224/471 [00:04<00:04, 51.60it/s] 49%|████▉     | 230/471 [00:04<00:04, 51.14it/s] 50%|█████     | 236/471 [00:04<00:04, 50.32it/s] 51%|█████▏    | 242/471 [00:04<00:04, 48.87it/s] 53%|█████▎    | 248/471 [00:04<00:04, 50.49it/s] 54%|█████▍    | 254/471 [00:04<00:04, 50.07it/s] 55%|█████▌    | 260/471 [00:05<00:04, 50.97it/s] 56%|█████▋    | 266/471 [00:05<00:04, 50.59it/s] 58%|█████▊    | 272/471 [00:05<00:03, 50.88it/s] 59%|█████▉    | 278/471 [00:05<00:03, 49.95it/s] 60%|██████    | 284/471 [00:05<00:03, 50.34it/s] 62%|██████▏   | 290/471 [00:05<00:03, 50.34it/s] 63%|██████▎   | 296/471 [00:05<00:03, 51.22it/s] 64%|██████▍   | 302/471 [00:05<00:03, 52.07it/s] 65%|██████▌   | 308/471 [00:05<00:03, 52.42it/s] 67%|██████▋   | 314/471 [00:06<00:03, 52.10it/s] 68%|██████▊   | 320/471 [00:06<00:02, 52.81it/s] 69%|██████▉   | 326/471 [00:06<00:02, 52.94it/s] 70%|███████   | 332/471 [00:06<00:02, 51.19it/s] 72%|███████▏  | 338/471 [00:06<00:02, 51.93it/s] 73%|███████▎  | 344/471 [00:06<00:02, 52.02it/s] 74%|███████▍  | 350/471 [00:06<00:02, 52.35it/s] 76%|███████▌  | 356/471 [00:06<00:02, 52.35it/s] 77%|███████▋  | 362/471 [00:07<00:02, 52.71it/s] 78%|███████▊  | 368/471 [00:07<00:01, 52.94it/s] 79%|███████▉  | 374/471 [00:07<00:01, 53.03it/s] 81%|████████  | 380/471 [00:07<00:01, 52.98it/s] 82%|████████▏ | 386/471 [00:07<00:01, 52.81it/s] 83%|████████▎ | 392/471 [00:07<00:01, 52.99it/s] 85%|████████▍ | 398/471 [00:07<00:01, 53.26it/s] 86%|████████▌ | 404/471 [00:07<00:01, 51.93it/s] 87%|████████▋ | 410/471 [00:07<00:01, 52.58it/s] 88%|████████▊ | 416/471 [00:08<00:01, 52.68it/s] 90%|████████▉ | 422/471 [00:08<00:00, 53.04it/s] 91%|█████████ | 428/471 [00:08<00:00, 53.05it/s] 92%|█████████▏| 434/471 [00:08<00:00, 53.21it/s] 93%|█████████▎| 440/471 [00:08<00:00, 52.20it/s] 95%|█████████▍| 446/471 [00:08<00:00, 52.79it/s] 96%|█████████▌| 452/471 [00:08<00:00, 52.86it/s] 97%|█████████▋| 458/471 [00:08<00:00, 52.71it/s] 99%|█████████▊| 464/471 [00:08<00:00, 51.58it/s]100%|█████████▉| 470/471 [00:09<00:00, 52.44it/s]100%|██████████| 471/471 [00:09<00:00, 51.95it/s]
{'eval_loss': 2.2708282470703125, 'eval_model_preparation_time': 0.0031, 'eval_acc': 0.39723844928305896, 'eval_runtime': 9.0884, 'eval_samples_per_second': 828.747, 'eval_steps_per_second': 51.824}
ROUND:19
CLIENT:7
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3807, 'grad_norm': 4.158153057098389, 'learning_rate': 0.00012613495840300737, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 19.35it/s]                                              {'loss': 2.3194, 'grad_norm': 4.462508201599121, 'learning_rate': 0.00012445315895763393, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 22.69it/s]  4%|▍         | 3/75 [00:00<00:03, 23.78it/s]                                              {'loss': 2.0896, 'grad_norm': 5.846254825592041, 'learning_rate': 0.00012277135951226052, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.78it/s]                                              {'loss': 2.2823, 'grad_norm': 3.7990550994873047, 'learning_rate': 0.00012108956006688707, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 23.78it/s]                                              {'loss': 2.3158, 'grad_norm': 3.710024833679199, 'learning_rate': 0.00011940776062151364, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.78it/s]  8%|▊         | 6/75 [00:00<00:02, 23.74it/s]                                              {'loss': 2.318, 'grad_norm': 5.274846076965332, 'learning_rate': 0.00011772596117614021, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.74it/s]                                              {'loss': 2.216, 'grad_norm': 4.072000980377197, 'learning_rate': 0.00011604416173076679, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.74it/s]                                              {'loss': 2.0789, 'grad_norm': 3.4920499324798584, 'learning_rate': 0.00011436236228539334, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.74it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.56it/s]                                              {'loss': 2.4311, 'grad_norm': 4.5297064781188965, 'learning_rate': 0.00011268056284001991, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.56it/s]                                              {'loss': 2.26, 'grad_norm': 3.714111566543579, 'learning_rate': 0.00011099876339464648, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.56it/s]                                               {'loss': 2.1604, 'grad_norm': 3.6099233627319336, 'learning_rate': 0.00010931696394927306, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.56it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.30it/s]                                               {'loss': 2.3849, 'grad_norm': 7.08633279800415, 'learning_rate': 0.00010763516450389963, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.30it/s]                                               {'loss': 2.2113, 'grad_norm': 4.721251964569092, 'learning_rate': 0.00010595336505852618, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.30it/s]                                               {'loss': 2.0026, 'grad_norm': 2.981306552886963, 'learning_rate': 0.00010427156561315275, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.30it/s]                                               {'loss': 2.5089, 'grad_norm': 12.817168235778809, 'learning_rate': 0.00010258976616777933, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.30it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.82it/s]                                               {'loss': 2.1936, 'grad_norm': 4.3182454109191895, 'learning_rate': 0.0001009079667224059, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.82it/s]                                               {'loss': 2.3158, 'grad_norm': 4.4062395095825195, 'learning_rate': 9.922616727703246e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.82it/s]                                               {'loss': 2.1797, 'grad_norm': 3.8007967472076416, 'learning_rate': 9.754436783165903e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.82it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.14it/s]                                               {'loss': 2.5521, 'grad_norm': 4.229117393493652, 'learning_rate': 9.58625683862856e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.14it/s]                                               {'loss': 2.1526, 'grad_norm': 4.472933769226074, 'learning_rate': 9.418076894091217e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.14it/s]                                               {'loss': 2.0582, 'grad_norm': 3.8658061027526855, 'learning_rate': 9.249896949553873e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.14it/s] 29%|██▉       | 22/75 [00:00<00:02, 24.93it/s]                                               {'loss': 2.0176, 'grad_norm': 3.374356746673584, 'learning_rate': 9.081717005016531e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.93it/s]                                               {'loss': 2.0529, 'grad_norm': 3.924146890640259, 'learning_rate': 8.913537060479187e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.93it/s]                                               {'loss': 2.2744, 'grad_norm': 4.844844341278076, 'learning_rate': 8.745357115941844e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.93it/s] 33%|███▎      | 25/75 [00:01<00:02, 24.79it/s]                                               {'loss': 2.4412, 'grad_norm': 4.071472644805908, 'learning_rate': 8.577177171404502e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.79it/s]                                               {'loss': 2.2853, 'grad_norm': 3.7335681915283203, 'learning_rate': 8.408997226867158e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.79it/s]                                               {'loss': 2.1741, 'grad_norm': 3.5746750831604004, 'learning_rate': 8.240817282329814e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.79it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.41it/s]                                               {'loss': 2.0216, 'grad_norm': 3.614990472793579, 'learning_rate': 8.072637337792471e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.41it/s]                                               {'loss': 2.0348, 'grad_norm': 3.886411428451538, 'learning_rate': 7.904457393255129e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.41it/s]                                               {'loss': 2.4969, 'grad_norm': 7.878509998321533, 'learning_rate': 7.736277448717785e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.41it/s] 41%|████▏     | 31/75 [00:01<00:01, 25.25it/s]                                               {'loss': 2.1425, 'grad_norm': 3.548525810241699, 'learning_rate': 7.568097504180442e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.25it/s]                                               {'loss': 2.2693, 'grad_norm': 3.5116398334503174, 'learning_rate': 7.399917559643099e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.25it/s]                                               {'loss': 2.0816, 'grad_norm': 4.263000965118408, 'learning_rate': 7.231737615105756e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.25it/s] 45%|████▌     | 34/75 [00:01<00:01, 25.70it/s]                                               {'loss': 2.1034, 'grad_norm': 3.9909610748291016, 'learning_rate': 7.063557670568413e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.70it/s]                                               {'loss': 1.8454, 'grad_norm': 3.2345571517944336, 'learning_rate': 6.89537772603107e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.70it/s]                                               {'loss': 2.0897, 'grad_norm': 3.8089804649353027, 'learning_rate': 6.727197781493727e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.70it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.63it/s]                                               {'loss': 2.3255, 'grad_norm': 3.3834218978881836, 'learning_rate': 6.559017836956383e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.63it/s]                                               {'loss': 2.1738, 'grad_norm': 3.851470947265625, 'learning_rate': 6.39083789241904e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.63it/s]                                               {'loss': 2.1844, 'grad_norm': 4.255930423736572, 'learning_rate': 6.222657947881697e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.63it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.64it/s]                                               {'loss': 2.072, 'grad_norm': 4.452151775360107, 'learning_rate': 6.054478003344353e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.64it/s]                                               {'loss': 2.2339, 'grad_norm': 4.11206579208374, 'learning_rate': 5.886298058807011e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.64it/s]                                               {'loss': 2.1712, 'grad_norm': 3.494332790374756, 'learning_rate': 5.718118114269667e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.64it/s] 57%|█████▋    | 43/75 [00:01<00:01, 25.57it/s]                                               {'loss': 2.2159, 'grad_norm': 3.842988967895508, 'learning_rate': 5.549938169732324e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.57it/s]                                               {'loss': 2.376, 'grad_norm': 4.947388172149658, 'learning_rate': 5.381758225194982e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.57it/s]                                               {'loss': 2.1507, 'grad_norm': 13.281740188598633, 'learning_rate': 5.213578280657638e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.57it/s]                                               {'loss': 2.2179, 'grad_norm': 3.7376134395599365, 'learning_rate': 5.045398336120295e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.57it/s] 63%|██████▎   | 47/75 [00:01<00:01, 27.32it/s]                                               {'loss': 2.2673, 'grad_norm': 4.442843437194824, 'learning_rate': 4.877218391582951e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 27.32it/s]                                               {'loss': 2.2559, 'grad_norm': 3.7007570266723633, 'learning_rate': 4.709038447045609e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 27.32it/s]                                               {'loss': 2.0292, 'grad_norm': 3.7864253520965576, 'learning_rate': 4.5408585025082655e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 27.32it/s] 67%|██████▋   | 50/75 [00:01<00:00, 26.10it/s]                                               {'loss': 2.1499, 'grad_norm': 2.9525039196014404, 'learning_rate': 4.372678557970922e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.10it/s]                                               {'loss': 2.0549, 'grad_norm': 3.5419745445251465, 'learning_rate': 4.204498613433579e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 26.10it/s]                                               {'loss': 2.1327, 'grad_norm': 3.252718925476074, 'learning_rate': 4.036318668896236e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 26.10it/s] 71%|███████   | 53/75 [00:02<00:00, 23.08it/s]                                               {'loss': 2.1687, 'grad_norm': 4.118744373321533, 'learning_rate': 3.8681387243588925e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 23.08it/s]                                               {'loss': 2.2739, 'grad_norm': 4.045091152191162, 'learning_rate': 3.699958779821549e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 23.08it/s]                                               {'loss': 2.1188, 'grad_norm': 3.7488369941711426, 'learning_rate': 3.531778835284207e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 23.08it/s] 75%|███████▍  | 56/75 [00:02<00:00, 23.16it/s]                                               {'loss': 2.1312, 'grad_norm': 5.014135360717773, 'learning_rate': 3.3635988907468635e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 23.16it/s]                                               {'loss': 2.2736, 'grad_norm': 3.7225396633148193, 'learning_rate': 3.19541894620952e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 23.16it/s]                                               {'loss': 2.0789, 'grad_norm': 2.5841751098632812, 'learning_rate': 3.0272390016721766e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 23.16it/s] 79%|███████▊  | 59/75 [00:02<00:00, 24.04it/s]                                               {'loss': 2.179, 'grad_norm': 3.809819459915161, 'learning_rate': 2.8590590571348334e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.04it/s]                                               {'loss': 2.0241, 'grad_norm': 11.435458183288574, 'learning_rate': 2.690879112597491e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.04it/s]                                               {'loss': 2.0083, 'grad_norm': 3.7471165657043457, 'learning_rate': 2.5226991680601476e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.04it/s] 83%|████████▎ | 62/75 [00:02<00:00, 22.60it/s]                                               {'loss': 2.3697, 'grad_norm': 4.493386745452881, 'learning_rate': 2.3545192235228044e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 22.60it/s]                                               {'loss': 2.1828, 'grad_norm': 4.23785400390625, 'learning_rate': 2.186339278985461e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 22.60it/s]                                               {'loss': 2.1348, 'grad_norm': 4.334741592407227, 'learning_rate': 2.018159334448118e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 22.60it/s] 87%|████████▋ | 65/75 [00:02<00:00, 23.57it/s]                                               {'loss': 2.0647, 'grad_norm': 2.596985340118408, 'learning_rate': 1.8499793899107746e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 23.57it/s]                                               {'loss': 2.114, 'grad_norm': 3.4674177169799805, 'learning_rate': 1.6817994453734317e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 23.57it/s]                                               {'loss': 2.0864, 'grad_norm': 3.428565740585327, 'learning_rate': 1.5136195008360883e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 23.57it/s] 91%|█████████ | 68/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.1281, 'grad_norm': 4.017183303833008, 'learning_rate': 1.3454395562987454e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.33it/s]                                               {'loss': 1.9953, 'grad_norm': 5.222980976104736, 'learning_rate': 1.1772596117614022e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.3562, 'grad_norm': 5.146017551422119, 'learning_rate': 1.009079667224059e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.33it/s] 95%|█████████▍| 71/75 [00:02<00:00, 24.32it/s]                                               {'loss': 2.2281, 'grad_norm': 3.5518710613250732, 'learning_rate': 8.408997226867159e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.32it/s]                                               {'loss': 2.2376, 'grad_norm': 4.714362144470215, 'learning_rate': 6.727197781493727e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.32it/s]                                               {'loss': 2.2847, 'grad_norm': 4.238533020019531, 'learning_rate': 5.045398336120295e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.32it/s] 99%|█████████▊| 74/75 [00:02<00:00, 24.23it/s]                                               {'loss': 2.041, 'grad_norm': 3.876250743865967, 'learning_rate': 3.3635988907468635e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.23it/s]                                               {'loss': 2.5455, 'grad_norm': 13.70196533203125, 'learning_rate': 1.6817994453734318e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.23it/s]                                               {'train_runtime': 3.1187, 'train_samples_per_second': 362.327, 'train_steps_per_second': 24.048, 'train_loss': 2.19705579439799, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.23it/s]100%|██████████| 75/75 [00:03<00:00, 24.05it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:22
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.1498, 'grad_norm': 4.413435935974121, 'learning_rate': 0.00012613495840300737, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 14.86it/s]  3%|▎         | 2/75 [00:00<00:04, 18.00it/s]                                              {'loss': 2.1777, 'grad_norm': 4.108053207397461, 'learning_rate': 0.00012445315895763393, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:04, 18.00it/s]                                              {'loss': 2.1019, 'grad_norm': 3.878802537918091, 'learning_rate': 0.00012277135951226052, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 18.00it/s]                                              {'loss': 2.0358, 'grad_norm': 4.644155502319336, 'learning_rate': 0.00012108956006688707, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 18.00it/s]  7%|▋         | 5/75 [00:00<00:03, 21.87it/s]                                              {'loss': 1.956, 'grad_norm': 3.545103073120117, 'learning_rate': 0.00011940776062151364, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 21.87it/s]                                              {'loss': 2.3272, 'grad_norm': 4.949899196624756, 'learning_rate': 0.00011772596117614021, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 21.87it/s]                                              {'loss': 2.1704, 'grad_norm': 5.4982476234436035, 'learning_rate': 0.00011604416173076679, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 21.87it/s] 11%|█         | 8/75 [00:00<00:02, 23.47it/s]                                              {'loss': 1.922, 'grad_norm': 3.70507550239563, 'learning_rate': 0.00011436236228539334, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.47it/s]                                              {'loss': 2.0095, 'grad_norm': 3.6913864612579346, 'learning_rate': 0.00011268056284001991, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.47it/s]                                              {'loss': 2.1589, 'grad_norm': 3.5027518272399902, 'learning_rate': 0.00011099876339464648, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.47it/s] 15%|█▍        | 11/75 [00:00<00:02, 24.32it/s]                                               {'loss': 2.2682, 'grad_norm': 3.8082242012023926, 'learning_rate': 0.00010931696394927306, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.32it/s]                                               {'loss': 1.931, 'grad_norm': 3.682448387145996, 'learning_rate': 0.00010763516450389963, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.32it/s]                                               {'loss': 2.1023, 'grad_norm': 4.700563430786133, 'learning_rate': 0.00010595336505852618, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.32it/s] 19%|█▊        | 14/75 [00:00<00:02, 22.82it/s]                                               {'loss': 2.0665, 'grad_norm': 3.6797778606414795, 'learning_rate': 0.00010427156561315275, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 22.82it/s]                                               {'loss': 1.8195, 'grad_norm': 8.345973014831543, 'learning_rate': 0.00010258976616777933, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 22.82it/s]                                               {'loss': 1.9494, 'grad_norm': 3.1601650714874268, 'learning_rate': 0.0001009079667224059, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 22.82it/s]                                               {'loss': 2.2131, 'grad_norm': 4.139486789703369, 'learning_rate': 9.922616727703246e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 22.82it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.43it/s]                                               {'loss': 2.1035, 'grad_norm': 4.3046135902404785, 'learning_rate': 9.754436783165903e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.43it/s]                                               {'loss': 1.9942, 'grad_norm': 3.938011407852173, 'learning_rate': 9.58625683862856e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.43it/s]                                               {'loss': 2.1333, 'grad_norm': 4.587738990783691, 'learning_rate': 9.418076894091217e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.43it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.26it/s]                                               {'loss': 2.3367, 'grad_norm': 4.341776371002197, 'learning_rate': 9.249896949553873e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.26it/s]                                               {'loss': 2.0934, 'grad_norm': 4.2564849853515625, 'learning_rate': 9.081717005016531e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.26it/s]                                               {'loss': 2.1324, 'grad_norm': 4.030738830566406, 'learning_rate': 8.913537060479187e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.26it/s] 32%|███▏      | 24/75 [00:01<00:02, 24.22it/s]                                               {'loss': 2.0307, 'grad_norm': 4.263148307800293, 'learning_rate': 8.745357115941844e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 24.22it/s]                                               {'loss': 1.9436, 'grad_norm': 3.458533763885498, 'learning_rate': 8.577177171404502e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.22it/s]                                               {'loss': 1.8445, 'grad_norm': 4.398730278015137, 'learning_rate': 8.408997226867158e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.22it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.27it/s]                                               {'loss': 2.1619, 'grad_norm': 3.568445920944214, 'learning_rate': 8.240817282329814e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.27it/s]                                               {'loss': 2.0553, 'grad_norm': 3.299248456954956, 'learning_rate': 8.072637337792471e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.27it/s]                                               {'loss': 2.0966, 'grad_norm': 3.9619762897491455, 'learning_rate': 7.904457393255129e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.27it/s] 40%|████      | 30/75 [00:01<00:01, 25.21it/s]                                               {'loss': 1.5356, 'grad_norm': 7.8023223876953125, 'learning_rate': 7.736277448717785e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.21it/s]                                               {'loss': 1.9103, 'grad_norm': 3.5657873153686523, 'learning_rate': 7.568097504180442e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.21it/s]                                               {'loss': 2.0114, 'grad_norm': 4.135372638702393, 'learning_rate': 7.399917559643099e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.21it/s] 44%|████▍     | 33/75 [00:01<00:01, 24.40it/s]                                               {'loss': 2.1358, 'grad_norm': 3.9836716651916504, 'learning_rate': 7.231737615105756e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.40it/s]                                               {'loss': 2.0582, 'grad_norm': 4.403466701507568, 'learning_rate': 7.063557670568413e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.40it/s]                                               {'loss': 2.2426, 'grad_norm': 4.751720905303955, 'learning_rate': 6.89537772603107e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.40it/s] 48%|████▊     | 36/75 [00:01<00:01, 24.50it/s]                                               {'loss': 1.945, 'grad_norm': 3.9953527450561523, 'learning_rate': 6.727197781493727e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.50it/s]                                               {'loss': 2.0733, 'grad_norm': 4.0959625244140625, 'learning_rate': 6.559017836956383e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.50it/s]                                               {'loss': 2.1807, 'grad_norm': 3.844879388809204, 'learning_rate': 6.39083789241904e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.50it/s] 52%|█████▏    | 39/75 [00:01<00:01, 24.49it/s]                                               {'loss': 2.0353, 'grad_norm': 3.572563409805298, 'learning_rate': 6.222657947881697e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.49it/s]                                               {'loss': 2.0775, 'grad_norm': 4.455626964569092, 'learning_rate': 6.054478003344353e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.49it/s]                                               {'loss': 2.0693, 'grad_norm': 3.9767940044403076, 'learning_rate': 5.886298058807011e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.49it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.55it/s]                                               {'loss': 1.9854, 'grad_norm': 3.814690351486206, 'learning_rate': 5.718118114269667e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.55it/s]                                               {'loss': 2.1025, 'grad_norm': 3.816824436187744, 'learning_rate': 5.549938169732324e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.55it/s]                                               {'loss': 2.1297, 'grad_norm': 3.317246437072754, 'learning_rate': 5.381758225194982e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.55it/s]                                               {'loss': 1.9961, 'grad_norm': 13.000374794006348, 'learning_rate': 5.213578280657638e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.55it/s] 61%|██████▏   | 46/75 [00:01<00:01, 26.01it/s]                                               {'loss': 1.9552, 'grad_norm': 4.6877217292785645, 'learning_rate': 5.045398336120295e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.01it/s]                                               {'loss': 2.0912, 'grad_norm': 3.673340082168579, 'learning_rate': 4.877218391582951e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.01it/s]                                               {'loss': 2.0013, 'grad_norm': 4.0076165199279785, 'learning_rate': 4.709038447045609e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.01it/s] 65%|██████▌   | 49/75 [00:01<00:01, 25.62it/s]                                               {'loss': 1.9627, 'grad_norm': 3.7467119693756104, 'learning_rate': 4.5408585025082655e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.245, 'grad_norm': 5.918320178985596, 'learning_rate': 4.372678557970922e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.62it/s]                                               {'loss': 2.1514, 'grad_norm': 3.446282386779785, 'learning_rate': 4.204498613433579e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.62it/s] 69%|██████▉   | 52/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.0497, 'grad_norm': 3.516709327697754, 'learning_rate': 4.036318668896236e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.1461, 'grad_norm': 3.5537683963775635, 'learning_rate': 3.8681387243588925e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.63it/s]                                               {'loss': 1.9901, 'grad_norm': 3.114126205444336, 'learning_rate': 3.699958779821549e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.63it/s] 73%|███████▎  | 55/75 [00:02<00:00, 25.50it/s]                                               {'loss': 2.1053, 'grad_norm': 3.8195385932922363, 'learning_rate': 3.531778835284207e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.50it/s]                                               {'loss': 1.9325, 'grad_norm': 3.635841131210327, 'learning_rate': 3.3635988907468635e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.50it/s]                                               {'loss': 2.0186, 'grad_norm': 4.934141159057617, 'learning_rate': 3.19541894620952e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.50it/s] 77%|███████▋  | 58/75 [00:02<00:00, 25.53it/s]                                               {'loss': 2.0219, 'grad_norm': 3.4076499938964844, 'learning_rate': 3.0272390016721766e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.53it/s]                                               {'loss': 2.264, 'grad_norm': 4.114248752593994, 'learning_rate': 2.8590590571348334e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.53it/s]                                               {'loss': 2.1962, 'grad_norm': 13.09938907623291, 'learning_rate': 2.690879112597491e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.53it/s] 81%|████████▏ | 61/75 [00:02<00:00, 26.36it/s]                                               {'loss': 2.2078, 'grad_norm': 3.495539903640747, 'learning_rate': 2.5226991680601476e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.36it/s]                                               {'loss': 1.9388, 'grad_norm': 3.1776206493377686, 'learning_rate': 2.3545192235228044e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.36it/s]                                               {'loss': 2.1468, 'grad_norm': 3.838066816329956, 'learning_rate': 2.186339278985461e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.36it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.18it/s]                                               {'loss': 1.9682, 'grad_norm': 5.216790676116943, 'learning_rate': 2.018159334448118e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.18it/s]                                               {'loss': 1.9405, 'grad_norm': 3.786497116088867, 'learning_rate': 1.8499793899107746e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.18it/s]                                               {'loss': 1.9458, 'grad_norm': 4.0498738288879395, 'learning_rate': 1.6817994453734317e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.18it/s] 89%|████████▉ | 67/75 [00:02<00:00, 25.35it/s]                                               {'loss': 2.1384, 'grad_norm': 5.420022487640381, 'learning_rate': 1.5136195008360883e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.35it/s]                                               {'loss': 2.1797, 'grad_norm': 4.120866775512695, 'learning_rate': 1.3454395562987454e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.35it/s]                                               {'loss': 1.9821, 'grad_norm': 3.2963430881500244, 'learning_rate': 1.1772596117614022e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.35it/s] 93%|█████████▎| 70/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.1529, 'grad_norm': 4.562826156616211, 'learning_rate': 1.009079667224059e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.46it/s]                                               {'loss': 1.9881, 'grad_norm': 5.998873233795166, 'learning_rate': 8.408997226867159e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.0652, 'grad_norm': 3.889185905456543, 'learning_rate': 6.727197781493727e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.46it/s] 97%|█████████▋| 73/75 [00:02<00:00, 25.35it/s]                                               {'loss': 1.9982, 'grad_norm': 3.402338981628418, 'learning_rate': 5.045398336120295e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.35it/s]                                               {'loss': 2.1103, 'grad_norm': 3.3792314529418945, 'learning_rate': 3.3635988907468635e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.35it/s]                                               {'loss': 2.3279, 'grad_norm': 7.712345123291016, 'learning_rate': 1.6817994453734318e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.35it/s]                                               {'train_runtime': 3.1214, 'train_samples_per_second': 362.021, 'train_steps_per_second': 24.028, 'train_loss': 2.066606510480245, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.35it/s]100%|██████████| 75/75 [00:03<00:00, 24.03it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:41
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2818, 'grad_norm': 4.162084579467773, 'learning_rate': 0.00012613495840300737, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 21.63it/s]                                              {'loss': 2.192, 'grad_norm': 4.146425724029541, 'learning_rate': 0.00012445315895763393, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.90it/s]  4%|▍         | 3/75 [00:00<00:03, 23.74it/s]                                              {'loss': 2.382, 'grad_norm': 6.076650142669678, 'learning_rate': 0.00012277135951226052, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.74it/s]                                              {'loss': 2.1912, 'grad_norm': 3.988487958908081, 'learning_rate': 0.00012108956006688707, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 23.74it/s]                                              {'loss': 2.2422, 'grad_norm': 3.3373847007751465, 'learning_rate': 0.00011940776062151364, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.74it/s]  8%|▊         | 6/75 [00:00<00:02, 24.50it/s]                                              {'loss': 2.37, 'grad_norm': 4.608010292053223, 'learning_rate': 0.00011772596117614021, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.50it/s]                                              {'loss': 2.2678, 'grad_norm': 3.5566468238830566, 'learning_rate': 0.00011604416173076679, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.50it/s]                                              {'loss': 2.3136, 'grad_norm': 2.789472818374634, 'learning_rate': 0.00011436236228539334, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.50it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.43it/s]                                              {'loss': 2.2451, 'grad_norm': 4.00105619430542, 'learning_rate': 0.00011268056284001991, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.43it/s]                                              {'loss': 2.4299, 'grad_norm': 3.5607171058654785, 'learning_rate': 0.00011099876339464648, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.43it/s]                                               {'loss': 2.2782, 'grad_norm': 4.402071475982666, 'learning_rate': 0.00010931696394927306, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.43it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.81it/s]                                               {'loss': 2.2726, 'grad_norm': 4.3532891273498535, 'learning_rate': 0.00010763516450389963, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.81it/s]                                               {'loss': 2.245, 'grad_norm': 5.717965602874756, 'learning_rate': 0.00010595336505852618, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.81it/s]                                               {'loss': 2.47, 'grad_norm': 4.1023969650268555, 'learning_rate': 0.00010427156561315275, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.81it/s]                                               {'loss': 2.2689, 'grad_norm': 11.781855583190918, 'learning_rate': 0.00010258976616777933, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.81it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.92it/s]                                               {'loss': 2.2902, 'grad_norm': 3.473203182220459, 'learning_rate': 0.0001009079667224059, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.92it/s]                                               {'loss': 2.3566, 'grad_norm': 4.856609344482422, 'learning_rate': 9.922616727703246e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.92it/s]                                               {'loss': 2.428, 'grad_norm': 3.619311571121216, 'learning_rate': 9.754436783165903e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.92it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.63it/s]                                               {'loss': 2.1982, 'grad_norm': 3.491147994995117, 'learning_rate': 9.58625683862856e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.63it/s]                                               {'loss': 2.1292, 'grad_norm': 3.274752140045166, 'learning_rate': 9.418076894091217e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.63it/s]                                               {'loss': 2.2824, 'grad_norm': 4.325015068054199, 'learning_rate': 9.249896949553873e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.63it/s] 29%|██▉       | 22/75 [00:00<00:01, 26.66it/s]                                               {'loss': 2.3354, 'grad_norm': 4.359314441680908, 'learning_rate': 9.081717005016531e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:01, 26.66it/s]                                               {'loss': 2.2533, 'grad_norm': 3.811746120452881, 'learning_rate': 8.913537060479187e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 26.66it/s]                                               {'loss': 2.0673, 'grad_norm': 3.244323253631592, 'learning_rate': 8.745357115941844e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 26.66it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.91it/s]                                               {'loss': 2.2768, 'grad_norm': 3.6687633991241455, 'learning_rate': 8.577177171404502e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.91it/s]                                               {'loss': 2.2038, 'grad_norm': 2.883002758026123, 'learning_rate': 8.408997226867158e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.91it/s]                                               {'loss': 2.0388, 'grad_norm': 3.5338668823242188, 'learning_rate': 8.240817282329814e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.91it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.99it/s]                                               {'loss': 2.3544, 'grad_norm': 3.6491780281066895, 'learning_rate': 8.072637337792471e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.99it/s]                                               {'loss': 2.3747, 'grad_norm': 3.5862462520599365, 'learning_rate': 7.904457393255129e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.99it/s]                                               {'loss': 2.1185, 'grad_norm': 5.569722652435303, 'learning_rate': 7.736277448717785e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.99it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.11it/s]                                               {'loss': 2.2861, 'grad_norm': 3.3544106483459473, 'learning_rate': 7.568097504180442e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.11it/s]                                               {'loss': 2.1669, 'grad_norm': 5.052139759063721, 'learning_rate': 7.399917559643099e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.11it/s]                                               {'loss': 2.2503, 'grad_norm': 4.288965225219727, 'learning_rate': 7.231737615105756e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.11it/s] 45%|████▌     | 34/75 [00:01<00:01, 23.85it/s]                                               {'loss': 2.1963, 'grad_norm': 3.692265510559082, 'learning_rate': 7.063557670568413e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 23.85it/s]                                               {'loss': 2.1731, 'grad_norm': 3.8223087787628174, 'learning_rate': 6.89537772603107e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 23.85it/s]                                               {'loss': 2.1569, 'grad_norm': 4.029440402984619, 'learning_rate': 6.727197781493727e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 23.85it/s] 49%|████▉     | 37/75 [00:01<00:01, 23.59it/s]                                               {'loss': 2.2655, 'grad_norm': 4.494090557098389, 'learning_rate': 6.559017836956383e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 23.59it/s]                                               {'loss': 2.1734, 'grad_norm': 4.544981479644775, 'learning_rate': 6.39083789241904e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 23.59it/s]                                               {'loss': 2.2249, 'grad_norm': 4.110462665557861, 'learning_rate': 6.222657947881697e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.59it/s] 53%|█████▎    | 40/75 [00:01<00:01, 23.42it/s]                                               {'loss': 2.0838, 'grad_norm': 3.7015304565429688, 'learning_rate': 6.054478003344353e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.42it/s]                                               {'loss': 2.2957, 'grad_norm': 3.618384838104248, 'learning_rate': 5.886298058807011e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 23.42it/s]                                               {'loss': 2.2221, 'grad_norm': 4.673288822174072, 'learning_rate': 5.718118114269667e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 23.42it/s] 57%|█████▋    | 43/75 [00:01<00:01, 23.86it/s]                                               {'loss': 2.1784, 'grad_norm': 3.819344997406006, 'learning_rate': 5.549938169732324e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 23.86it/s]                                               {'loss': 2.289, 'grad_norm': 3.8250880241394043, 'learning_rate': 5.381758225194982e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 23.86it/s]                                               {'loss': 2.1733, 'grad_norm': 8.093816757202148, 'learning_rate': 5.213578280657638e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 23.86it/s]                                               {'loss': 2.3728, 'grad_norm': 4.681980609893799, 'learning_rate': 5.045398336120295e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 23.86it/s] 63%|██████▎   | 47/75 [00:01<00:01, 25.73it/s]                                               {'loss': 2.2489, 'grad_norm': 4.083240509033203, 'learning_rate': 4.877218391582951e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.73it/s]                                               {'loss': 2.1775, 'grad_norm': 4.638580322265625, 'learning_rate': 4.709038447045609e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.73it/s]                                               {'loss': 2.2514, 'grad_norm': 4.258547306060791, 'learning_rate': 4.5408585025082655e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.73it/s] 67%|██████▋   | 50/75 [00:01<00:00, 25.38it/s]                                               {'loss': 2.2668, 'grad_norm': 4.095426559448242, 'learning_rate': 4.372678557970922e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.38it/s]                                               {'loss': 2.2372, 'grad_norm': 3.9679720401763916, 'learning_rate': 4.204498613433579e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.38it/s]                                               {'loss': 2.1682, 'grad_norm': 3.1258697509765625, 'learning_rate': 4.036318668896236e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.38it/s] 71%|███████   | 53/75 [00:02<00:00, 25.42it/s]                                               {'loss': 2.0105, 'grad_norm': 3.704197406768799, 'learning_rate': 3.8681387243588925e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.42it/s]                                               {'loss': 2.1824, 'grad_norm': 3.416652202606201, 'learning_rate': 3.699958779821549e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.42it/s]                                               {'loss': 2.1985, 'grad_norm': 2.836808919906616, 'learning_rate': 3.531778835284207e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.42it/s] 75%|███████▍  | 56/75 [00:02<00:00, 24.92it/s]                                               {'loss': 2.2391, 'grad_norm': 3.54654598236084, 'learning_rate': 3.3635988907468635e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.92it/s]                                               {'loss': 2.3606, 'grad_norm': 4.466737270355225, 'learning_rate': 3.19541894620952e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.92it/s]                                               {'loss': 2.113, 'grad_norm': 3.476500988006592, 'learning_rate': 3.0272390016721766e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.92it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.2227, 'grad_norm': 5.264904499053955, 'learning_rate': 2.8590590571348334e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.26it/s]                                               {'loss': 1.8839, 'grad_norm': 9.338333129882812, 'learning_rate': 2.690879112597491e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.1337, 'grad_norm': 3.225923776626587, 'learning_rate': 2.5226991680601476e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.1747, 'grad_norm': 3.1740310192108154, 'learning_rate': 2.3545192235228044e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.26it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.39it/s]                                               {'loss': 2.3118, 'grad_norm': 4.293944358825684, 'learning_rate': 2.186339278985461e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.39it/s]                                               {'loss': 2.1575, 'grad_norm': 4.761473178863525, 'learning_rate': 2.018159334448118e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.39it/s]                                               {'loss': 2.1827, 'grad_norm': 3.9137821197509766, 'learning_rate': 1.8499793899107746e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.39it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.3058, 'grad_norm': 3.8767449855804443, 'learning_rate': 1.6817994453734317e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.1271, 'grad_norm': 4.258369445800781, 'learning_rate': 1.5136195008360883e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.3415, 'grad_norm': 5.276653289794922, 'learning_rate': 1.3454395562987454e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.45it/s] 92%|█████████▏| 69/75 [00:02<00:00, 25.76it/s]                                               {'loss': 2.3314, 'grad_norm': 4.104344367980957, 'learning_rate': 1.1772596117614022e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.76it/s]                                               {'loss': 2.1502, 'grad_norm': 3.4220898151397705, 'learning_rate': 1.009079667224059e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.76it/s]                                               {'loss': 1.9564, 'grad_norm': 4.116523742675781, 'learning_rate': 8.408997226867159e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.76it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.35it/s]                                               {'loss': 2.423, 'grad_norm': 6.271426677703857, 'learning_rate': 6.727197781493727e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.35it/s]                                               {'loss': 2.203, 'grad_norm': 3.029940366744995, 'learning_rate': 5.045398336120295e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.35it/s]                                               {'loss': 2.0762, 'grad_norm': 3.6891579627990723, 'learning_rate': 3.3635988907468635e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.35it/s]100%|██████████| 75/75 [00:02<00:00, 25.79it/s]                                               {'loss': 2.1661, 'grad_norm': 6.2548394203186035, 'learning_rate': 1.6817994453734318e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.79it/s]                                               {'train_runtime': 3.0689, 'train_samples_per_second': 368.205, 'train_steps_per_second': 24.438, 'train_loss': 2.230237487157186, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.79it/s]100%|██████████| 75/75 [00:03<00:00, 24.44it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:29
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2209, 'grad_norm': 4.806591987609863, 'learning_rate': 0.00012613495840300737, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:05, 13.66it/s]  3%|▎         | 2/75 [00:00<00:04, 17.44it/s]                                              {'loss': 2.2508, 'grad_norm': 4.094516277313232, 'learning_rate': 0.00012445315895763393, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:04, 17.44it/s]                                              {'loss': 2.3772, 'grad_norm': 4.849045276641846, 'learning_rate': 0.00012277135951226052, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 17.44it/s]                                              {'loss': 2.1637, 'grad_norm': 4.766115665435791, 'learning_rate': 0.00012108956006688707, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:04, 17.44it/s]  7%|▋         | 5/75 [00:00<00:03, 21.84it/s]                                              {'loss': 2.2477, 'grad_norm': 5.041707515716553, 'learning_rate': 0.00011940776062151364, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 21.84it/s]                                              {'loss': 2.3616, 'grad_norm': 5.121113300323486, 'learning_rate': 0.00011772596117614021, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 21.84it/s]                                              {'loss': 2.1789, 'grad_norm': 3.6744754314422607, 'learning_rate': 0.00011604416173076679, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 21.84it/s] 11%|█         | 8/75 [00:00<00:02, 23.29it/s]                                              {'loss': 2.3326, 'grad_norm': 3.5558390617370605, 'learning_rate': 0.00011436236228539334, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.29it/s]                                              {'loss': 2.1434, 'grad_norm': 3.6930394172668457, 'learning_rate': 0.00011268056284001991, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.29it/s]                                              {'loss': 2.218, 'grad_norm': 3.396646738052368, 'learning_rate': 0.00011099876339464648, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.29it/s] 15%|█▍        | 11/75 [00:00<00:02, 24.12it/s]                                               {'loss': 2.1808, 'grad_norm': 4.5241265296936035, 'learning_rate': 0.00010931696394927306, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.12it/s]                                               {'loss': 2.1831, 'grad_norm': 4.398669719696045, 'learning_rate': 0.00010763516450389963, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.12it/s]                                               {'loss': 2.3272, 'grad_norm': 4.428971767425537, 'learning_rate': 0.00010595336505852618, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.12it/s] 19%|█▊        | 14/75 [00:00<00:02, 24.49it/s]                                               {'loss': 2.4339, 'grad_norm': 3.999675989151001, 'learning_rate': 0.00010427156561315275, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.49it/s]                                               {'loss': 2.4081, 'grad_norm': 9.878457069396973, 'learning_rate': 0.00010258976616777933, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.49it/s]                                               {'loss': 2.2758, 'grad_norm': 4.485312461853027, 'learning_rate': 0.0001009079667224059, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 24.49it/s] 23%|██▎       | 17/75 [00:00<00:02, 24.77it/s]                                               {'loss': 2.1821, 'grad_norm': 3.9804675579071045, 'learning_rate': 9.922616727703246e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.77it/s]                                               {'loss': 2.2345, 'grad_norm': 4.015035152435303, 'learning_rate': 9.754436783165903e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.77it/s]                                               {'loss': 2.1577, 'grad_norm': 4.821695804595947, 'learning_rate': 9.58625683862856e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.77it/s] 27%|██▋       | 20/75 [00:00<00:02, 24.30it/s]                                               {'loss': 2.2241, 'grad_norm': 3.1501400470733643, 'learning_rate': 9.418076894091217e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.30it/s]                                               {'loss': 2.3411, 'grad_norm': 3.1561319828033447, 'learning_rate': 9.249896949553873e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.30it/s]                                               {'loss': 2.4508, 'grad_norm': 4.24420166015625, 'learning_rate': 9.081717005016531e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.30it/s] 31%|███       | 23/75 [00:00<00:02, 24.78it/s]                                               {'loss': 2.2984, 'grad_norm': 4.871368408203125, 'learning_rate': 8.913537060479187e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.78it/s]                                               {'loss': 2.3277, 'grad_norm': 4.624554634094238, 'learning_rate': 8.745357115941844e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.78it/s]                                               {'loss': 2.2783, 'grad_norm': 4.293708324432373, 'learning_rate': 8.577177171404502e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.78it/s] 35%|███▍      | 26/75 [00:01<00:02, 24.21it/s]                                               {'loss': 2.2213, 'grad_norm': 3.6011478900909424, 'learning_rate': 8.408997226867158e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.21it/s]                                               {'loss': 2.1106, 'grad_norm': 3.3850557804107666, 'learning_rate': 8.240817282329814e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.21it/s]                                               {'loss': 2.1002, 'grad_norm': 3.9947149753570557, 'learning_rate': 8.072637337792471e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.21it/s] 39%|███▊      | 29/75 [00:01<00:01, 24.68it/s]                                               {'loss': 2.0864, 'grad_norm': 3.612277030944824, 'learning_rate': 7.904457393255129e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.68it/s]                                               {'loss': 2.3819, 'grad_norm': 8.688048362731934, 'learning_rate': 7.736277448717785e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.68it/s]                                               {'loss': 2.124, 'grad_norm': 3.0282506942749023, 'learning_rate': 7.568097504180442e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.68it/s] 43%|████▎     | 32/75 [00:01<00:01, 24.99it/s]                                               {'loss': 2.1561, 'grad_norm': 4.0444464683532715, 'learning_rate': 7.399917559643099e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.99it/s]                                               {'loss': 2.1792, 'grad_norm': 4.146373271942139, 'learning_rate': 7.231737615105756e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.99it/s]                                               {'loss': 2.1479, 'grad_norm': 3.3836398124694824, 'learning_rate': 7.063557670568413e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.99it/s] 47%|████▋     | 35/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.2046, 'grad_norm': 3.3999221324920654, 'learning_rate': 6.89537772603107e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.3148, 'grad_norm': 3.6143689155578613, 'learning_rate': 6.727197781493727e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.2251, 'grad_norm': 4.947490215301514, 'learning_rate': 6.559017836956383e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.24it/s] 51%|█████     | 38/75 [00:01<00:01, 25.28it/s]                                               {'loss': 2.218, 'grad_norm': 5.272400856018066, 'learning_rate': 6.39083789241904e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.28it/s]                                               {'loss': 2.5581, 'grad_norm': 5.7181878089904785, 'learning_rate': 6.222657947881697e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.28it/s]                                               {'loss': 2.1648, 'grad_norm': 4.010426998138428, 'learning_rate': 6.054478003344353e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.28it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.38it/s]                                               {'loss': 2.073, 'grad_norm': 2.465409517288208, 'learning_rate': 5.886298058807011e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.38it/s]                                               {'loss': 2.1513, 'grad_norm': 4.724255084991455, 'learning_rate': 5.718118114269667e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.38it/s]                                               {'loss': 2.28, 'grad_norm': 4.825087547302246, 'learning_rate': 5.549938169732324e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.38it/s] 59%|█████▊    | 44/75 [00:01<00:01, 24.79it/s]                                               {'loss': 2.3175, 'grad_norm': 3.4043290615081787, 'learning_rate': 5.381758225194982e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.79it/s]                                               {'loss': 2.3009, 'grad_norm': 10.390287399291992, 'learning_rate': 5.213578280657638e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.79it/s]                                               {'loss': 2.1062, 'grad_norm': 3.1843910217285156, 'learning_rate': 5.045398336120295e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.79it/s] 63%|██████▎   | 47/75 [00:01<00:01, 22.49it/s]                                               {'loss': 2.3541, 'grad_norm': 3.623699903488159, 'learning_rate': 4.877218391582951e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 22.49it/s]                                               {'loss': 2.3364, 'grad_norm': 3.9268829822540283, 'learning_rate': 4.709038447045609e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 22.49it/s]                                               {'loss': 2.2347, 'grad_norm': 4.382050037384033, 'learning_rate': 4.5408585025082655e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 22.49it/s] 67%|██████▋   | 50/75 [00:02<00:01, 21.59it/s]                                               {'loss': 2.0397, 'grad_norm': 3.4772472381591797, 'learning_rate': 4.372678557970922e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 21.59it/s]                                               {'loss': 2.0759, 'grad_norm': 4.355464935302734, 'learning_rate': 4.204498613433579e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 21.59it/s]                                               {'loss': 2.0835, 'grad_norm': 3.4304592609405518, 'learning_rate': 4.036318668896236e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 21.59it/s] 71%|███████   | 53/75 [00:02<00:00, 22.62it/s]                                               {'loss': 2.0766, 'grad_norm': 3.4635238647460938, 'learning_rate': 3.8681387243588925e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 22.62it/s]                                               {'loss': 2.2736, 'grad_norm': 5.3365678787231445, 'learning_rate': 3.699958779821549e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 22.62it/s]                                               {'loss': 2.1885, 'grad_norm': 4.184593677520752, 'learning_rate': 3.531778835284207e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 22.62it/s] 75%|███████▍  | 56/75 [00:02<00:00, 22.81it/s]                                               {'loss': 2.2432, 'grad_norm': 4.11412239074707, 'learning_rate': 3.3635988907468635e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 22.81it/s]                                               {'loss': 2.1507, 'grad_norm': 3.4646406173706055, 'learning_rate': 3.19541894620952e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 22.81it/s]                                               {'loss': 2.2301, 'grad_norm': 3.3522253036499023, 'learning_rate': 3.0272390016721766e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 22.81it/s] 79%|███████▊  | 59/75 [00:02<00:00, 23.60it/s]                                               {'loss': 2.3378, 'grad_norm': 5.2091593742370605, 'learning_rate': 2.8590590571348334e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 23.60it/s]                                               {'loss': 2.1602, 'grad_norm': 8.512920379638672, 'learning_rate': 2.690879112597491e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 23.60it/s]                                               {'loss': 2.2674, 'grad_norm': 4.396317958831787, 'learning_rate': 2.5226991680601476e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 23.60it/s]                                               {'loss': 2.1179, 'grad_norm': 3.686339855194092, 'learning_rate': 2.3545192235228044e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 23.60it/s] 84%|████████▍ | 63/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.1152, 'grad_norm': 3.159177780151367, 'learning_rate': 2.186339278985461e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.2201, 'grad_norm': 4.188658237457275, 'learning_rate': 2.018159334448118e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.49it/s]                                               {'loss': 2.3133, 'grad_norm': 4.080529689788818, 'learning_rate': 1.8499793899107746e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.49it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.2254, 'grad_norm': 3.6277620792388916, 'learning_rate': 1.6817994453734317e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.1008, 'grad_norm': 3.66137433052063, 'learning_rate': 1.5136195008360883e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.2164, 'grad_norm': 4.249579906463623, 'learning_rate': 1.3454395562987454e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.45it/s] 92%|█████████▏| 69/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.2348, 'grad_norm': 4.1848626136779785, 'learning_rate': 1.1772596117614022e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.291, 'grad_norm': 3.3892745971679688, 'learning_rate': 1.009079667224059e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.2036, 'grad_norm': 4.0823869705200195, 'learning_rate': 8.408997226867159e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.33it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.13it/s]                                               {'loss': 2.289, 'grad_norm': 4.859358310699463, 'learning_rate': 6.727197781493727e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.13it/s]                                               {'loss': 2.0332, 'grad_norm': 4.210253715515137, 'learning_rate': 5.045398336120295e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 25.13it/s]                                               {'loss': 2.0881, 'grad_norm': 3.163083076477051, 'learning_rate': 3.3635988907468635e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 25.13it/s]100%|██████████| 75/75 [00:03<00:00, 26.09it/s]                                               {'loss': 2.2363, 'grad_norm': 17.077598571777344, 'learning_rate': 1.6817994453734318e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 26.09it/s]                                               {'train_runtime': 3.1827, 'train_samples_per_second': 355.044, 'train_steps_per_second': 23.565, 'train_loss': 2.226105295817057, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 26.09it/s]100%|██████████| 75/75 [00:03<00:00, 23.57it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:48
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3636, 'grad_norm': 4.985178470611572, 'learning_rate': 0.00012613495840300737, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.93it/s]                                              {'loss': 2.3033, 'grad_norm': 5.087338924407959, 'learning_rate': 0.00012445315895763393, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 25.00it/s]  4%|▍         | 3/75 [00:00<00:02, 24.66it/s]                                              {'loss': 2.4581, 'grad_norm': 3.7903642654418945, 'learning_rate': 0.00012277135951226052, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.66it/s]                                              {'loss': 2.1079, 'grad_norm': 3.6052086353302, 'learning_rate': 0.00012108956006688707, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.66it/s]                                              {'loss': 2.2804, 'grad_norm': 3.681122303009033, 'learning_rate': 0.00011940776062151364, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.66it/s]  8%|▊         | 6/75 [00:00<00:02, 24.38it/s]                                              {'loss': 2.2274, 'grad_norm': 4.23329496383667, 'learning_rate': 0.00011772596117614021, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.38it/s]                                              {'loss': 2.5093, 'grad_norm': 4.6254401206970215, 'learning_rate': 0.00011604416173076679, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.38it/s]                                              {'loss': 2.1946, 'grad_norm': 3.929967164993286, 'learning_rate': 0.00011436236228539334, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.38it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.30it/s]                                              {'loss': 2.2564, 'grad_norm': 4.945509910583496, 'learning_rate': 0.00011268056284001991, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.30it/s]                                              {'loss': 2.2385, 'grad_norm': 3.7064857482910156, 'learning_rate': 0.00011099876339464648, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.30it/s]                                               {'loss': 2.369, 'grad_norm': 3.4876463413238525, 'learning_rate': 0.00010931696394927306, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.30it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.07it/s]                                               {'loss': 2.2462, 'grad_norm': 3.722569704055786, 'learning_rate': 0.00010763516450389963, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.07it/s]                                               {'loss': 2.688, 'grad_norm': 6.32558012008667, 'learning_rate': 0.00010595336505852618, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.07it/s]                                               {'loss': 2.0863, 'grad_norm': 3.730389356613159, 'learning_rate': 0.00010427156561315275, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.07it/s]                                               {'loss': 1.8452, 'grad_norm': 9.027241706848145, 'learning_rate': 0.00010258976616777933, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.07it/s] 21%|██▏       | 16/75 [00:00<00:02, 25.29it/s]                                               {'loss': 2.4153, 'grad_norm': 3.7083053588867188, 'learning_rate': 0.0001009079667224059, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.29it/s]                                               {'loss': 2.2855, 'grad_norm': 3.8250505924224854, 'learning_rate': 9.922616727703246e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.29it/s]                                               {'loss': 2.6833, 'grad_norm': 5.460050106048584, 'learning_rate': 9.754436783165903e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.29it/s] 25%|██▌       | 19/75 [00:00<00:02, 24.79it/s]                                               {'loss': 2.2079, 'grad_norm': 4.06611967086792, 'learning_rate': 9.58625683862856e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.79it/s]                                               {'loss': 2.1859, 'grad_norm': 4.363134384155273, 'learning_rate': 9.418076894091217e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.79it/s]                                               {'loss': 2.1864, 'grad_norm': 4.045055866241455, 'learning_rate': 9.249896949553873e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.79it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.15it/s]                                               {'loss': 2.0643, 'grad_norm': 4.051315784454346, 'learning_rate': 9.081717005016531e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.15it/s]                                               {'loss': 2.1057, 'grad_norm': 3.50185227394104, 'learning_rate': 8.913537060479187e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.15it/s]                                               {'loss': 2.2132, 'grad_norm': 3.3750522136688232, 'learning_rate': 8.745357115941844e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 25.15it/s] 33%|███▎      | 25/75 [00:01<00:02, 24.96it/s]                                               {'loss': 2.1392, 'grad_norm': 4.418373107910156, 'learning_rate': 8.577177171404502e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.96it/s]                                               {'loss': 2.1504, 'grad_norm': 4.814078330993652, 'learning_rate': 8.408997226867158e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.96it/s]                                               {'loss': 2.2895, 'grad_norm': 5.522563457489014, 'learning_rate': 8.240817282329814e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.96it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.10it/s]                                               {'loss': 2.2207, 'grad_norm': 4.004858493804932, 'learning_rate': 8.072637337792471e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.10it/s]                                               {'loss': 2.2345, 'grad_norm': 4.722685813903809, 'learning_rate': 7.904457393255129e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.10it/s]                                               {'loss': 1.9658, 'grad_norm': 7.338901519775391, 'learning_rate': 7.736277448717785e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.10it/s]                                               {'loss': 2.249, 'grad_norm': 4.188440799713135, 'learning_rate': 7.568097504180442e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.10it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.75it/s]                                               {'loss': 2.168, 'grad_norm': 3.5704338550567627, 'learning_rate': 7.399917559643099e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.75it/s]                                               {'loss': 2.3266, 'grad_norm': 4.159718036651611, 'learning_rate': 7.231737615105756e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.75it/s]                                               {'loss': 2.2121, 'grad_norm': 3.9325687885284424, 'learning_rate': 7.063557670568413e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.75it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.34it/s]                                               {'loss': 2.1719, 'grad_norm': 3.4665892124176025, 'learning_rate': 6.89537772603107e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.34it/s]                                               {'loss': 2.2, 'grad_norm': 4.970634460449219, 'learning_rate': 6.727197781493727e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.34it/s]                                               {'loss': 2.1997, 'grad_norm': 3.984833002090454, 'learning_rate': 6.559017836956383e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.34it/s] 51%|█████     | 38/75 [00:01<00:01, 26.25it/s]                                               {'loss': 2.1839, 'grad_norm': 4.475820541381836, 'learning_rate': 6.39083789241904e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.25it/s]                                               {'loss': 2.3323, 'grad_norm': 4.755190372467041, 'learning_rate': 6.222657947881697e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.25it/s]                                               {'loss': 2.3411, 'grad_norm': 4.163032054901123, 'learning_rate': 6.054478003344353e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 26.25it/s] 55%|█████▍    | 41/75 [00:01<00:01, 26.31it/s]                                               {'loss': 2.1013, 'grad_norm': 3.476815700531006, 'learning_rate': 5.886298058807011e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 26.31it/s]                                               {'loss': 2.051, 'grad_norm': 6.3101983070373535, 'learning_rate': 5.718118114269667e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 26.31it/s]                                               {'loss': 2.327, 'grad_norm': 5.225748538970947, 'learning_rate': 5.549938169732324e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 26.31it/s] 59%|█████▊    | 44/75 [00:01<00:01, 26.11it/s]                                               {'loss': 2.3886, 'grad_norm': 3.509843349456787, 'learning_rate': 5.381758225194982e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 26.11it/s]                                               {'loss': 2.3777, 'grad_norm': 12.214921951293945, 'learning_rate': 5.213578280657638e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 26.11it/s]                                               {'loss': 2.0553, 'grad_norm': 3.412201166152954, 'learning_rate': 5.045398336120295e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.11it/s]                                               {'loss': 2.4206, 'grad_norm': 3.7753748893737793, 'learning_rate': 4.877218391582951e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.11it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.18it/s]                                               {'loss': 2.2141, 'grad_norm': 5.529378414154053, 'learning_rate': 4.709038447045609e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 27.18it/s]                                               {'loss': 2.1181, 'grad_norm': 5.915977954864502, 'learning_rate': 4.5408585025082655e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 27.18it/s]                                               {'loss': 2.0815, 'grad_norm': 2.4949302673339844, 'learning_rate': 4.372678557970922e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 27.18it/s] 68%|██████▊   | 51/75 [00:01<00:00, 25.88it/s]                                               {'loss': 2.1296, 'grad_norm': 4.1370463371276855, 'learning_rate': 4.204498613433579e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 25.88it/s]                                               {'loss': 2.4734, 'grad_norm': 4.027223110198975, 'learning_rate': 4.036318668896236e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.88it/s]                                               {'loss': 2.4497, 'grad_norm': 5.692533016204834, 'learning_rate': 3.8681387243588925e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.88it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.88it/s]                                               {'loss': 2.3241, 'grad_norm': 5.215112686157227, 'learning_rate': 3.699958779821549e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.88it/s]                                               {'loss': 2.2498, 'grad_norm': 4.782345294952393, 'learning_rate': 3.531778835284207e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.88it/s]                                               {'loss': 2.2425, 'grad_norm': 2.9638352394104004, 'learning_rate': 3.3635988907468635e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.88it/s] 76%|███████▌  | 57/75 [00:02<00:00, 25.08it/s]                                               {'loss': 1.9468, 'grad_norm': 4.576582431793213, 'learning_rate': 3.19541894620952e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.08it/s]                                               {'loss': 2.1226, 'grad_norm': 4.163718223571777, 'learning_rate': 3.0272390016721766e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.08it/s]                                               {'loss': 2.2141, 'grad_norm': 3.8260746002197266, 'learning_rate': 2.8590590571348334e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.08it/s] 80%|████████  | 60/75 [00:02<00:00, 25.59it/s]                                               {'loss': 2.3961, 'grad_norm': 7.192927360534668, 'learning_rate': 2.690879112597491e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.59it/s]                                               {'loss': 2.1998, 'grad_norm': 3.7045624256134033, 'learning_rate': 2.5226991680601476e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.59it/s]                                               {'loss': 2.3318, 'grad_norm': 3.951723098754883, 'learning_rate': 2.3545192235228044e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.59it/s] 84%|████████▍ | 63/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.2392, 'grad_norm': 4.4739274978637695, 'learning_rate': 2.186339278985461e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.14it/s]                                               {'loss': 1.9642, 'grad_norm': 4.393672466278076, 'learning_rate': 2.018159334448118e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.0456, 'grad_norm': 3.5812087059020996, 'learning_rate': 1.8499793899107746e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.14it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.31it/s]                                               {'loss': 2.3244, 'grad_norm': 4.160289287567139, 'learning_rate': 1.6817994453734317e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.31it/s]                                               {'loss': 2.2224, 'grad_norm': 4.101923942565918, 'learning_rate': 1.5136195008360883e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.31it/s]                                               {'loss': 2.0408, 'grad_norm': 4.9294867515563965, 'learning_rate': 1.3454395562987454e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.31it/s] 92%|█████████▏| 69/75 [00:02<00:00, 24.39it/s]                                               {'loss': 2.2505, 'grad_norm': 4.8906707763671875, 'learning_rate': 1.1772596117614022e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.39it/s]                                               {'loss': 2.2343, 'grad_norm': 4.42789363861084, 'learning_rate': 1.009079667224059e-05, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.39it/s]                                               {'loss': 2.324, 'grad_norm': 4.443211555480957, 'learning_rate': 8.408997226867159e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.39it/s] 96%|█████████▌| 72/75 [00:02<00:00, 24.27it/s]                                               {'loss': 2.3178, 'grad_norm': 4.287505626678467, 'learning_rate': 6.727197781493727e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.27it/s]                                               {'loss': 2.1945, 'grad_norm': 3.5577759742736816, 'learning_rate': 5.045398336120295e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.27it/s]                                               {'loss': 2.0971, 'grad_norm': 3.1491143703460693, 'learning_rate': 3.3635988907468635e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.27it/s]                                               {'loss': 2.2021, 'grad_norm': 13.274826049804688, 'learning_rate': 1.6817994453734318e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.27it/s]                                               {'train_runtime': 3.0719, 'train_samples_per_second': 367.848, 'train_steps_per_second': 24.415, 'train_loss': 2.234389368693034, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.27it/s]100%|██████████| 75/75 [00:03<00:00, 24.42it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(955, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(882, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(731, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(846, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1076, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(841, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(984, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(805, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(855, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(541, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1035, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(511, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 64.81it/s]  3%|▎         | 14/471 [00:00<00:08, 55.91it/s]  4%|▍         | 20/471 [00:00<00:08, 52.90it/s]  6%|▌         | 26/471 [00:00<00:08, 52.89it/s]  7%|▋         | 32/471 [00:00<00:08, 53.02it/s]  8%|▊         | 38/471 [00:00<00:08, 52.18it/s]  9%|▉         | 44/471 [00:00<00:08, 52.49it/s] 11%|█         | 50/471 [00:00<00:08, 51.42it/s] 12%|█▏        | 56/471 [00:01<00:08, 51.86it/s] 13%|█▎        | 62/471 [00:01<00:07, 52.54it/s] 14%|█▍        | 68/471 [00:01<00:07, 51.57it/s] 16%|█▌        | 74/471 [00:01<00:07, 50.66it/s] 17%|█▋        | 80/471 [00:01<00:07, 50.35it/s] 18%|█▊        | 86/471 [00:01<00:07, 51.51it/s] 20%|█▉        | 92/471 [00:01<00:07, 50.71it/s] 21%|██        | 98/471 [00:01<00:07, 51.33it/s] 22%|██▏       | 104/471 [00:02<00:07, 51.39it/s] 23%|██▎       | 110/471 [00:02<00:07, 50.33it/s] 25%|██▍       | 116/471 [00:02<00:06, 51.34it/s] 26%|██▌       | 122/471 [00:02<00:06, 51.79it/s] 27%|██▋       | 128/471 [00:02<00:06, 52.15it/s] 28%|██▊       | 134/471 [00:02<00:06, 51.67it/s] 30%|██▉       | 140/471 [00:02<00:06, 52.37it/s] 31%|███       | 146/471 [00:02<00:06, 52.60it/s] 32%|███▏      | 152/471 [00:02<00:06, 52.82it/s] 34%|███▎      | 158/471 [00:03<00:06, 49.84it/s] 35%|███▍      | 164/471 [00:03<00:06, 48.34it/s] 36%|███▌      | 170/471 [00:03<00:06, 49.39it/s] 37%|███▋      | 176/471 [00:03<00:05, 49.50it/s] 39%|███▊      | 182/471 [00:03<00:05, 51.14it/s] 40%|███▉      | 188/471 [00:03<00:05, 51.25it/s] 41%|████      | 194/471 [00:03<00:05, 50.23it/s] 42%|████▏     | 200/471 [00:03<00:05, 50.09it/s] 44%|████▎     | 206/471 [00:04<00:05, 50.81it/s] 45%|████▌     | 212/471 [00:04<00:05, 51.26it/s] 46%|████▋     | 218/471 [00:04<00:04, 51.59it/s] 48%|████▊     | 224/471 [00:04<00:04, 51.75it/s] 49%|████▉     | 230/471 [00:04<00:04, 52.15it/s] 50%|█████     | 236/471 [00:04<00:04, 52.53it/s] 51%|█████▏    | 242/471 [00:04<00:04, 52.50it/s] 53%|█████▎    | 248/471 [00:04<00:04, 52.10it/s] 54%|█████▍    | 254/471 [00:04<00:04, 52.39it/s] 55%|█████▌    | 260/471 [00:05<00:04, 52.71it/s] 56%|█████▋    | 266/471 [00:05<00:03, 52.79it/s] 58%|█████▊    | 272/471 [00:05<00:03, 52.96it/s] 59%|█████▉    | 278/471 [00:05<00:03, 52.43it/s] 60%|██████    | 284/471 [00:05<00:03, 52.36it/s] 62%|██████▏   | 290/471 [00:05<00:03, 52.14it/s] 63%|██████▎   | 296/471 [00:05<00:03, 52.02it/s] 64%|██████▍   | 302/471 [00:05<00:03, 52.17it/s] 65%|██████▌   | 308/471 [00:05<00:03, 52.40it/s] 67%|██████▋   | 314/471 [00:06<00:03, 51.49it/s] 68%|██████▊   | 320/471 [00:06<00:02, 51.81it/s] 69%|██████▉   | 326/471 [00:06<00:02, 51.87it/s] 70%|███████   | 332/471 [00:06<00:02, 51.73it/s] 72%|███████▏  | 338/471 [00:06<00:02, 52.02it/s] 73%|███████▎  | 344/471 [00:06<00:02, 51.49it/s] 74%|███████▍  | 350/471 [00:06<00:02, 50.54it/s] 76%|███████▌  | 356/471 [00:06<00:02, 51.54it/s] 77%|███████▋  | 362/471 [00:06<00:02, 51.98it/s] 78%|███████▊  | 368/471 [00:07<00:01, 52.41it/s] 79%|███████▉  | 374/471 [00:07<00:01, 52.50it/s] 81%|████████  | 380/471 [00:07<00:01, 52.65it/s] 82%|████████▏ | 386/471 [00:07<00:01, 52.93it/s] 83%|████████▎ | 392/471 [00:07<00:01, 52.56it/s] 85%|████████▍ | 398/471 [00:07<00:01, 52.60it/s] 86%|████████▌ | 404/471 [00:07<00:01, 52.91it/s] 87%|████████▋ | 410/471 [00:07<00:01, 52.10it/s] 88%|████████▊ | 416/471 [00:08<00:01, 51.90it/s] 90%|████████▉ | 422/471 [00:08<00:00, 51.92it/s] 91%|█████████ | 428/471 [00:08<00:00, 50.56it/s] 92%|█████████▏| 434/471 [00:08<00:00, 48.88it/s] 93%|█████████▎| 440/471 [00:08<00:00, 49.70it/s] 95%|█████████▍| 446/471 [00:08<00:00, 50.74it/s] 96%|█████████▌| 452/471 [00:08<00:00, 50.16it/s] 97%|█████████▋| 458/471 [00:08<00:00, 51.14it/s] 99%|█████████▊| 464/471 [00:08<00:00, 51.65it/s]100%|█████████▉| 470/471 [00:09<00:00, 51.50it/s]100%|██████████| 471/471 [00:09<00:00, 51.71it/s]
{'eval_loss': 2.2687056064605713, 'eval_model_preparation_time': 0.0031, 'eval_acc': 0.39723844928305896, 'eval_runtime': 9.1308, 'eval_samples_per_second': 824.899, 'eval_steps_per_second': 51.584}
ROUND:20
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2018, 'grad_norm': 4.55196475982666, 'learning_rate': 0.00012426406871192852, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.73it/s]                                              {'loss': 2.2301, 'grad_norm': 4.5249176025390625, 'learning_rate': 0.00012260721446243615, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.57it/s]  4%|▍         | 3/75 [00:00<00:02, 24.20it/s]                                              {'loss': 2.13, 'grad_norm': 4.3858466148376465, 'learning_rate': 0.00012095036021294377, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.20it/s]                                              {'loss': 2.2468, 'grad_norm': 3.6996543407440186, 'learning_rate': 0.00011929350596345138, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.20it/s]                                              {'loss': 2.1535, 'grad_norm': 4.210781574249268, 'learning_rate': 0.000117636651713959, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.20it/s]  8%|▊         | 6/75 [00:00<00:02, 23.94it/s]                                              {'loss': 2.3135, 'grad_norm': 3.8563663959503174, 'learning_rate': 0.00011597979746446662, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.94it/s]                                              {'loss': 2.3229, 'grad_norm': 4.04209041595459, 'learning_rate': 0.00011432294321497425, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.94it/s]                                              {'loss': 2.2357, 'grad_norm': 4.235507011413574, 'learning_rate': 0.00011266608896548185, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.94it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.53it/s]                                              {'loss': 2.3184, 'grad_norm': 3.587392568588257, 'learning_rate': 0.00011100923471598948, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.53it/s]                                              {'loss': 2.3644, 'grad_norm': 4.281716823577881, 'learning_rate': 0.0001093523804664971, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.53it/s]                                               {'loss': 2.2273, 'grad_norm': 4.178094387054443, 'learning_rate': 0.00010769552621700472, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.53it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.17it/s]                                               {'loss': 2.1964, 'grad_norm': 4.687856197357178, 'learning_rate': 0.00010603867196751234, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.17it/s]                                               {'loss': 2.3356, 'grad_norm': 4.158770561218262, 'learning_rate': 0.00010438181771801995, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.17it/s]                                               {'loss': 2.1358, 'grad_norm': 3.95229172706604, 'learning_rate': 0.00010272496346852758, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.17it/s] 20%|██        | 15/75 [00:00<00:02, 26.46it/s]                                               {'loss': 2.3344, 'grad_norm': 13.871278762817383, 'learning_rate': 0.0001010681092190352, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 26.46it/s]                                               {'loss': 2.0605, 'grad_norm': 2.8350186347961426, 'learning_rate': 9.941125496954282e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.46it/s]                                               {'loss': 2.3435, 'grad_norm': 4.255655288696289, 'learning_rate': 9.775440072005043e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.46it/s] 24%|██▍       | 18/75 [00:00<00:02, 26.21it/s]                                               {'loss': 2.2511, 'grad_norm': 3.4006972312927246, 'learning_rate': 9.609754647055805e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.21it/s]                                               {'loss': 2.2403, 'grad_norm': 4.446169376373291, 'learning_rate': 9.444069222106568e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.21it/s]                                               {'loss': 2.3271, 'grad_norm': 3.863590717315674, 'learning_rate': 9.27838379715733e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.21it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.58it/s]                                               {'loss': 2.0526, 'grad_norm': 3.2967190742492676, 'learning_rate': 9.112698372208091e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.58it/s]                                               {'loss': 2.0436, 'grad_norm': 3.9809207916259766, 'learning_rate': 8.947012947258853e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.58it/s]                                               {'loss': 2.2253, 'grad_norm': 4.460230350494385, 'learning_rate': 8.781327522309615e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.58it/s] 32%|███▏      | 24/75 [00:00<00:02, 25.29it/s]                                               {'loss': 2.2125, 'grad_norm': 3.2369561195373535, 'learning_rate': 8.615642097360377e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 25.29it/s]                                               {'loss': 2.2285, 'grad_norm': 3.5830838680267334, 'learning_rate': 8.44995667241114e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.29it/s]                                               {'loss': 2.0836, 'grad_norm': 5.9717583656311035, 'learning_rate': 8.2842712474619e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.29it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.39it/s]                                               {'loss': 2.1859, 'grad_norm': 3.9309186935424805, 'learning_rate': 8.118585822512663e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.39it/s]                                               {'loss': 2.2291, 'grad_norm': 4.448387622833252, 'learning_rate': 7.952900397563425e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.39it/s]                                               {'loss': 2.2943, 'grad_norm': 6.2021989822387695, 'learning_rate': 7.787214972614187e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.39it/s] 40%|████      | 30/75 [00:01<00:01, 25.59it/s]                                               {'loss': 2.0083, 'grad_norm': 12.867032051086426, 'learning_rate': 7.621529547664948e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.59it/s]                                               {'loss': 2.0903, 'grad_norm': 3.4773595333099365, 'learning_rate': 7.45584412271571e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.59it/s]                                               {'loss': 2.2486, 'grad_norm': 5.660372257232666, 'learning_rate': 7.290158697766473e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.59it/s] 44%|████▍     | 33/75 [00:01<00:01, 23.65it/s]                                               {'loss': 2.1856, 'grad_norm': 4.808892250061035, 'learning_rate': 7.124473272817235e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 23.65it/s]                                               {'loss': 2.0451, 'grad_norm': 2.903130531311035, 'learning_rate': 6.958787847867997e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 23.65it/s]                                               {'loss': 2.1846, 'grad_norm': 5.754716873168945, 'learning_rate': 6.793102422918758e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 23.65it/s] 48%|████▊     | 36/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.1065, 'grad_norm': 4.235044479370117, 'learning_rate': 6.62741699796952e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.0274, 'grad_norm': 3.7175846099853516, 'learning_rate': 6.461731573020283e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.1556, 'grad_norm': 3.2714555263519287, 'learning_rate': 6.296046148071046e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.56it/s] 52%|█████▏    | 39/75 [00:01<00:01, 24.20it/s]                                               {'loss': 2.0917, 'grad_norm': 4.644831657409668, 'learning_rate': 6.130360723121807e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.20it/s]                                               {'loss': 2.2259, 'grad_norm': 3.8300182819366455, 'learning_rate': 5.964675298172569e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.20it/s]                                               {'loss': 2.0777, 'grad_norm': 4.246355056762695, 'learning_rate': 5.798989873223331e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.20it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.19it/s]                                               {'loss': 2.1681, 'grad_norm': 4.556352615356445, 'learning_rate': 5.633304448274093e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.19it/s]                                               {'loss': 2.1564, 'grad_norm': 3.720858573913574, 'learning_rate': 5.467619023324855e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.19it/s]                                               {'loss': 2.2572, 'grad_norm': 3.284458637237549, 'learning_rate': 5.301933598375617e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.19it/s] 60%|██████    | 45/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.6875, 'grad_norm': 15.253076553344727, 'learning_rate': 5.136248173426379e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.2687, 'grad_norm': 3.686539649963379, 'learning_rate': 4.970562748477141e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.4049, 'grad_norm': 4.987480640411377, 'learning_rate': 4.8048773235279026e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.60it/s] 64%|██████▍   | 48/75 [00:01<00:01, 25.10it/s]                                               {'loss': 2.1952, 'grad_norm': 3.7849957942962646, 'learning_rate': 4.639191898578665e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.10it/s]                                               {'loss': 2.2242, 'grad_norm': 3.799384593963623, 'learning_rate': 4.4735064736294265e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.10it/s]                                               {'loss': 2.2513, 'grad_norm': 5.2052507400512695, 'learning_rate': 4.307821048680189e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.10it/s] 68%|██████▊   | 51/75 [00:02<00:00, 25.04it/s]                                               {'loss': 2.0746, 'grad_norm': 3.014092206954956, 'learning_rate': 4.14213562373095e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.04it/s]                                               {'loss': 2.086, 'grad_norm': 3.4189701080322266, 'learning_rate': 3.9764501987817126e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.04it/s]                                               {'loss': 2.0146, 'grad_norm': 3.736260175704956, 'learning_rate': 3.810764773832474e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.04it/s] 72%|███████▏  | 54/75 [00:02<00:00, 24.94it/s]                                               {'loss': 2.0959, 'grad_norm': 4.778591156005859, 'learning_rate': 3.6450793488832364e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.94it/s]                                               {'loss': 2.2769, 'grad_norm': 4.59504508972168, 'learning_rate': 3.479393923933999e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.94it/s]                                               {'loss': 2.1203, 'grad_norm': 4.23601770401001, 'learning_rate': 3.31370849898476e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.94it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.0542, 'grad_norm': 4.006442546844482, 'learning_rate': 3.148023074035523e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.1578, 'grad_norm': 4.206173419952393, 'learning_rate': 2.9823376490862844e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.96it/s]                                               {'loss': 2.0083, 'grad_norm': 2.9641430377960205, 'learning_rate': 2.8166522241370464e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.96it/s] 80%|████████  | 60/75 [00:02<00:00, 25.02it/s]                                               {'loss': 1.8893, 'grad_norm': 8.252667427062988, 'learning_rate': 2.6509667991878086e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.0615, 'grad_norm': 3.7411675453186035, 'learning_rate': 2.4852813742385705e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.1374, 'grad_norm': 3.050096035003662, 'learning_rate': 2.3195959492893325e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.02it/s] 84%|████████▍ | 63/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.1908, 'grad_norm': 5.177099227905273, 'learning_rate': 2.1539105243400944e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.171, 'grad_norm': 4.458566188812256, 'learning_rate': 1.9882250993908563e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.124, 'grad_norm': 4.6111578941345215, 'learning_rate': 1.8225396744416182e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.14it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.1664, 'grad_norm': 3.906015157699585, 'learning_rate': 1.65685424949238e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.1792, 'grad_norm': 3.1546690464019775, 'learning_rate': 1.4911688245431422e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.1885, 'grad_norm': 5.377077102661133, 'learning_rate': 1.3254833995939043e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.14it/s] 92%|█████████▏| 69/75 [00:02<00:00, 25.15it/s]                                               {'loss': 2.0778, 'grad_norm': 5.0227131843566895, 'learning_rate': 1.1597979746446662e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.15it/s]                                               {'loss': 2.0164, 'grad_norm': 3.495492935180664, 'learning_rate': 9.941125496954281e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.15it/s]                                               {'loss': 2.0974, 'grad_norm': 4.64587926864624, 'learning_rate': 8.2842712474619e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.15it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.22it/s]                                               {'loss': 2.2184, 'grad_norm': 3.921105146408081, 'learning_rate': 6.6274169979695215e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.22it/s]                                               {'loss': 2.2508, 'grad_norm': 3.3006699085235596, 'learning_rate': 4.970562748477141e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.22it/s]                                               {'loss': 2.2011, 'grad_norm': 3.9762959480285645, 'learning_rate': 3.3137084989847608e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.22it/s]                                               {'loss': 1.7381, 'grad_norm': 9.626605987548828, 'learning_rate': 1.6568542494923804e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.22it/s]                                               {'train_runtime': 3.0965, 'train_samples_per_second': 364.928, 'train_steps_per_second': 24.221, 'train_loss': 2.17576229095459, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.22it/s]100%|██████████| 75/75 [00:03<00:00, 24.22it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:1
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.1468, 'grad_norm': 4.089162349700928, 'learning_rate': 0.00012426406871192852, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.63it/s]                                              {'loss': 2.2842, 'grad_norm': 4.547130107879639, 'learning_rate': 0.00012260721446243615, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.55it/s]  4%|▍         | 3/75 [00:00<00:02, 24.33it/s]                                              {'loss': 2.3893, 'grad_norm': 4.299284934997559, 'learning_rate': 0.00012095036021294377, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.33it/s]                                              {'loss': 2.4952, 'grad_norm': 5.305490970611572, 'learning_rate': 0.00011929350596345138, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.33it/s]                                              {'loss': 2.3016, 'grad_norm': 4.036509990692139, 'learning_rate': 0.000117636651713959, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.33it/s]  8%|▊         | 6/75 [00:00<00:02, 24.96it/s]                                              {'loss': 2.1778, 'grad_norm': 3.947105646133423, 'learning_rate': 0.00011597979746446662, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.96it/s]                                              {'loss': 2.34, 'grad_norm': 5.09438943862915, 'learning_rate': 0.00011432294321497425, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.96it/s]                                              {'loss': 2.2172, 'grad_norm': 3.9861600399017334, 'learning_rate': 0.00011266608896548185, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.96it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.07it/s]                                              {'loss': 2.5234, 'grad_norm': 4.633272171020508, 'learning_rate': 0.00011100923471598948, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.07it/s]                                              {'loss': 2.5302, 'grad_norm': 4.448750019073486, 'learning_rate': 0.0001093523804664971, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.07it/s]                                               {'loss': 2.1431, 'grad_norm': 4.270920276641846, 'learning_rate': 0.00010769552621700472, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.07it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.23it/s]                                               {'loss': 2.2201, 'grad_norm': 3.952350378036499, 'learning_rate': 0.00010603867196751234, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.23it/s]                                               {'loss': 2.3737, 'grad_norm': 4.285837650299072, 'learning_rate': 0.00010438181771801995, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.23it/s]                                               {'loss': 2.3969, 'grad_norm': 4.699733734130859, 'learning_rate': 0.00010272496346852758, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.23it/s]                                               {'loss': 2.314, 'grad_norm': 9.613441467285156, 'learning_rate': 0.0001010681092190352, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.23it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.56it/s]                                               {'loss': 2.2625, 'grad_norm': 3.2003579139709473, 'learning_rate': 9.941125496954282e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.56it/s]                                               {'loss': 2.3554, 'grad_norm': 4.466390132904053, 'learning_rate': 9.775440072005043e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.56it/s]                                               {'loss': 2.2556, 'grad_norm': 4.668471813201904, 'learning_rate': 9.609754647055805e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.56it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.82it/s]                                               {'loss': 2.2116, 'grad_norm': 4.022794723510742, 'learning_rate': 9.444069222106568e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.82it/s]                                               {'loss': 2.0327, 'grad_norm': 3.996927261352539, 'learning_rate': 9.27838379715733e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.82it/s]                                               {'loss': 2.2548, 'grad_norm': 3.771470308303833, 'learning_rate': 9.112698372208091e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.82it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.70it/s]                                               {'loss': 2.2942, 'grad_norm': 3.573850631713867, 'learning_rate': 8.947012947258853e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.70it/s]                                               {'loss': 2.4907, 'grad_norm': 4.945742130279541, 'learning_rate': 8.781327522309615e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.70it/s]                                               {'loss': 2.163, 'grad_norm': 4.131736755371094, 'learning_rate': 8.615642097360377e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.70it/s] 33%|███▎      | 25/75 [00:01<00:02, 23.62it/s]                                               {'loss': 2.1424, 'grad_norm': 3.9306600093841553, 'learning_rate': 8.44995667241114e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.62it/s]                                               {'loss': 2.2772, 'grad_norm': 3.996722459793091, 'learning_rate': 8.2842712474619e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.62it/s]                                               {'loss': 2.202, 'grad_norm': 3.9988205432891846, 'learning_rate': 8.118585822512663e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 23.62it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.29it/s]                                               {'loss': 2.278, 'grad_norm': 3.7267167568206787, 'learning_rate': 7.952900397563425e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.29it/s]                                               {'loss': 2.2325, 'grad_norm': 4.752471923828125, 'learning_rate': 7.787214972614187e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.29it/s]                                               {'loss': 2.357, 'grad_norm': 9.122804641723633, 'learning_rate': 7.621529547664948e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.29it/s]                                               {'loss': 2.1791, 'grad_norm': 3.3252038955688477, 'learning_rate': 7.45584412271571e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.29it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.16it/s]                                               {'loss': 2.3018, 'grad_norm': 4.491366863250732, 'learning_rate': 7.290158697766473e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.16it/s]                                               {'loss': 2.153, 'grad_norm': 4.26248025894165, 'learning_rate': 7.124473272817235e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.16it/s]                                               {'loss': 2.2166, 'grad_norm': 4.415922164916992, 'learning_rate': 6.958787847867997e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.16it/s] 47%|████▋     | 35/75 [00:01<00:01, 25.95it/s]                                               {'loss': 2.1286, 'grad_norm': 4.225666046142578, 'learning_rate': 6.793102422918758e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.95it/s]                                               {'loss': 2.265, 'grad_norm': 4.712521553039551, 'learning_rate': 6.62741699796952e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.95it/s]                                               {'loss': 2.2394, 'grad_norm': 4.791680812835693, 'learning_rate': 6.461731573020283e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.95it/s] 51%|█████     | 38/75 [00:01<00:01, 25.86it/s]                                               {'loss': 2.2486, 'grad_norm': 3.4625730514526367, 'learning_rate': 6.296046148071046e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.86it/s]                                               {'loss': 2.1524, 'grad_norm': 4.933698654174805, 'learning_rate': 6.130360723121807e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.86it/s]                                               {'loss': 2.1713, 'grad_norm': 3.3090522289276123, 'learning_rate': 5.964675298172569e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.86it/s] 55%|█████▍    | 41/75 [00:01<00:01, 24.93it/s]                                               {'loss': 2.2285, 'grad_norm': 3.819058895111084, 'learning_rate': 5.798989873223331e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.93it/s]                                               {'loss': 2.272, 'grad_norm': 3.9562525749206543, 'learning_rate': 5.633304448274093e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.93it/s]                                               {'loss': 2.3141, 'grad_norm': 4.109442710876465, 'learning_rate': 5.467619023324855e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.93it/s] 59%|█████▊    | 44/75 [00:01<00:01, 24.70it/s]                                               {'loss': 2.2416, 'grad_norm': 3.4273922443389893, 'learning_rate': 5.301933598375617e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.70it/s]                                               {'loss': 2.1538, 'grad_norm': 7.985604763031006, 'learning_rate': 5.136248173426379e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.70it/s]                                               {'loss': 2.3117, 'grad_norm': 3.7646267414093018, 'learning_rate': 4.970562748477141e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.70it/s] 63%|██████▎   | 47/75 [00:01<00:01, 25.94it/s]                                               {'loss': 2.3862, 'grad_norm': 3.9896349906921387, 'learning_rate': 4.8048773235279026e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.94it/s]                                               {'loss': 2.3919, 'grad_norm': 4.739621639251709, 'learning_rate': 4.639191898578665e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.94it/s]                                               {'loss': 2.3808, 'grad_norm': 4.25851583480835, 'learning_rate': 4.4735064736294265e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.94it/s] 67%|██████▋   | 50/75 [00:01<00:00, 25.98it/s]                                               {'loss': 2.2339, 'grad_norm': 4.050170421600342, 'learning_rate': 4.307821048680189e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.98it/s]                                               {'loss': 2.0585, 'grad_norm': 3.614875316619873, 'learning_rate': 4.14213562373095e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.98it/s]                                               {'loss': 2.1608, 'grad_norm': 4.650147914886475, 'learning_rate': 3.9764501987817126e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.98it/s] 71%|███████   | 53/75 [00:02<00:00, 25.66it/s]                                               {'loss': 2.0877, 'grad_norm': 3.6916918754577637, 'learning_rate': 3.810764773832474e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.66it/s]                                               {'loss': 2.2834, 'grad_norm': 4.001403331756592, 'learning_rate': 3.6450793488832364e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.66it/s]                                               {'loss': 1.9522, 'grad_norm': 3.0281014442443848, 'learning_rate': 3.479393923933999e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.66it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.36it/s]                                               {'loss': 2.032, 'grad_norm': 4.197286128997803, 'learning_rate': 3.31370849898476e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.36it/s]                                               {'loss': 1.961, 'grad_norm': 4.172191143035889, 'learning_rate': 3.148023074035523e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.36it/s]                                               {'loss': 2.2979, 'grad_norm': 3.864020824432373, 'learning_rate': 2.9823376490862844e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.36it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.32it/s]                                               {'loss': 2.2953, 'grad_norm': 4.317982196807861, 'learning_rate': 2.8166522241370464e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.32it/s]                                               {'loss': 2.0167, 'grad_norm': 11.823585510253906, 'learning_rate': 2.6509667991878086e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.32it/s]                                               {'loss': 2.3095, 'grad_norm': 4.108768939971924, 'learning_rate': 2.4852813742385705e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.32it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.00it/s]                                               {'loss': 2.2682, 'grad_norm': 3.997997999191284, 'learning_rate': 2.3195959492893325e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.00it/s]                                               {'loss': 2.2563, 'grad_norm': 3.610753297805786, 'learning_rate': 2.1539105243400944e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.00it/s]                                               {'loss': 2.2389, 'grad_norm': 3.260328769683838, 'learning_rate': 1.9882250993908563e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.00it/s] 87%|████████▋ | 65/75 [00:02<00:00, 25.74it/s]                                               {'loss': 2.1765, 'grad_norm': 3.1153411865234375, 'learning_rate': 1.8225396744416182e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.74it/s]                                               {'loss': 2.1068, 'grad_norm': 4.216113567352295, 'learning_rate': 1.65685424949238e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.74it/s]                                               {'loss': 2.2346, 'grad_norm': 4.014830112457275, 'learning_rate': 1.4911688245431422e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.74it/s] 91%|█████████ | 68/75 [00:02<00:00, 25.25it/s]                                               {'loss': 2.1181, 'grad_norm': 3.7194881439208984, 'learning_rate': 1.3254833995939043e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.25it/s]                                               {'loss': 1.9116, 'grad_norm': 2.9399945735931396, 'learning_rate': 1.1597979746446662e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.25it/s]                                               {'loss': 2.0551, 'grad_norm': 4.116531848907471, 'learning_rate': 9.941125496954281e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.25it/s] 95%|█████████▍| 71/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.192, 'grad_norm': 3.886956214904785, 'learning_rate': 8.2842712474619e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.276, 'grad_norm': 3.804495334625244, 'learning_rate': 6.6274169979695215e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.23it/s]                                               {'loss': 2.2966, 'grad_norm': 4.718252658843994, 'learning_rate': 4.970562748477141e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.23it/s] 99%|█████████▊| 74/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.0653, 'grad_norm': 3.813952684402466, 'learning_rate': 3.3137084989847608e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.5549, 'grad_norm': 13.042367935180664, 'learning_rate': 1.6568542494923804e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.16it/s]                                               {'train_runtime': 3.108, 'train_samples_per_second': 363.573, 'train_steps_per_second': 24.131, 'train_loss': 2.237857027053833, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.16it/s]100%|██████████| 75/75 [00:03<00:00, 24.13it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:20
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4039, 'grad_norm': 4.80094575881958, 'learning_rate': 0.00012426406871192852, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.72it/s]                                              {'loss': 2.2704, 'grad_norm': 3.90918231010437, 'learning_rate': 0.00012260721446243615, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 25.38it/s]  4%|▍         | 3/75 [00:00<00:02, 26.02it/s]                                              {'loss': 2.3977, 'grad_norm': 4.039531230926514, 'learning_rate': 0.00012095036021294377, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 26.02it/s]                                              {'loss': 2.2219, 'grad_norm': 5.136666774749756, 'learning_rate': 0.00011929350596345138, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 26.02it/s]                                              {'loss': 2.3924, 'grad_norm': 3.9018237590789795, 'learning_rate': 0.000117636651713959, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 26.02it/s]  8%|▊         | 6/75 [00:00<00:02, 25.20it/s]                                              {'loss': 2.4208, 'grad_norm': 3.459545373916626, 'learning_rate': 0.00011597979746446662, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.20it/s]                                              {'loss': 2.3684, 'grad_norm': 3.911844491958618, 'learning_rate': 0.00011432294321497425, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.20it/s]                                              {'loss': 2.4296, 'grad_norm': 4.343414783477783, 'learning_rate': 0.00011266608896548185, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.20it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.25it/s]                                              {'loss': 2.1079, 'grad_norm': 3.453643798828125, 'learning_rate': 0.00011100923471598948, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.25it/s]                                              {'loss': 2.3413, 'grad_norm': 4.898995399475098, 'learning_rate': 0.0001093523804664971, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.25it/s]                                               {'loss': 2.5913, 'grad_norm': 4.353026390075684, 'learning_rate': 0.00010769552621700472, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.25it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.32it/s]                                               {'loss': 2.3352, 'grad_norm': 4.331086158752441, 'learning_rate': 0.00010603867196751234, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.32it/s]                                               {'loss': 2.0428, 'grad_norm': 5.39572811126709, 'learning_rate': 0.00010438181771801995, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.32it/s]                                               {'loss': 2.0637, 'grad_norm': 3.141735076904297, 'learning_rate': 0.00010272496346852758, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.32it/s] 20%|██        | 15/75 [00:00<00:02, 25.64it/s]                                               {'loss': 2.5864, 'grad_norm': 10.842740058898926, 'learning_rate': 0.0001010681092190352, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.64it/s]                                               {'loss': 2.1784, 'grad_norm': 3.797729969024658, 'learning_rate': 9.941125496954282e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.64it/s]                                               {'loss': 2.2765, 'grad_norm': 4.828541278839111, 'learning_rate': 9.775440072005043e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.64it/s] 24%|██▍       | 18/75 [00:00<00:02, 24.48it/s]                                               {'loss': 2.0766, 'grad_norm': 3.9793996810913086, 'learning_rate': 9.609754647055805e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.48it/s]                                               {'loss': 2.4433, 'grad_norm': 4.0776047706604, 'learning_rate': 9.444069222106568e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.48it/s]                                               {'loss': 2.3524, 'grad_norm': 3.730283260345459, 'learning_rate': 9.27838379715733e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.48it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.20it/s]                                               {'loss': 2.2878, 'grad_norm': 3.884532928466797, 'learning_rate': 9.112698372208091e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.20it/s]                                               {'loss': 2.2789, 'grad_norm': 4.003237247467041, 'learning_rate': 8.947012947258853e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.20it/s]                                               {'loss': 2.1458, 'grad_norm': 4.136482238769531, 'learning_rate': 8.781327522309615e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.20it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.35it/s]                                               {'loss': 2.2026, 'grad_norm': 4.21779727935791, 'learning_rate': 8.615642097360377e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.35it/s]                                               {'loss': 2.104, 'grad_norm': 4.845249652862549, 'learning_rate': 8.44995667241114e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.35it/s]                                               {'loss': 2.4399, 'grad_norm': 3.1125922203063965, 'learning_rate': 8.2842712474619e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.35it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.15it/s]                                               {'loss': 2.1816, 'grad_norm': 3.720508575439453, 'learning_rate': 8.118585822512663e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.15it/s]                                               {'loss': 2.3997, 'grad_norm': 3.82283616065979, 'learning_rate': 7.952900397563425e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.15it/s]                                               {'loss': 2.1769, 'grad_norm': 3.3311948776245117, 'learning_rate': 7.787214972614187e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.15it/s]                                               {'loss': 2.7139, 'grad_norm': 14.792529106140137, 'learning_rate': 7.621529547664948e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.15it/s] 41%|████▏     | 31/75 [00:01<00:01, 25.84it/s]                                               {'loss': 2.2114, 'grad_norm': 4.385410785675049, 'learning_rate': 7.45584412271571e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.84it/s]                                               {'loss': 2.3564, 'grad_norm': 4.716713905334473, 'learning_rate': 7.290158697766473e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.84it/s]                                               {'loss': 2.1374, 'grad_norm': 3.400803327560425, 'learning_rate': 7.124473272817235e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.84it/s] 45%|████▌     | 34/75 [00:01<00:01, 25.19it/s]                                               {'loss': 2.5043, 'grad_norm': 4.18659782409668, 'learning_rate': 6.958787847867997e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.19it/s]                                               {'loss': 2.0563, 'grad_norm': 3.67694354057312, 'learning_rate': 6.793102422918758e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.19it/s]                                               {'loss': 2.2268, 'grad_norm': 3.3551268577575684, 'learning_rate': 6.62741699796952e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.19it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.18it/s]                                               {'loss': 2.4065, 'grad_norm': 4.8900861740112305, 'learning_rate': 6.461731573020283e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.18it/s]                                               {'loss': 2.2292, 'grad_norm': 3.2158849239349365, 'learning_rate': 6.296046148071046e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.18it/s]                                               {'loss': 2.0768, 'grad_norm': 3.6036441326141357, 'learning_rate': 6.130360723121807e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.18it/s] 53%|█████▎    | 40/75 [00:01<00:01, 24.91it/s]                                               {'loss': 2.4727, 'grad_norm': 4.133918285369873, 'learning_rate': 5.964675298172569e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.91it/s]                                               {'loss': 2.0795, 'grad_norm': 4.402346611022949, 'learning_rate': 5.798989873223331e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.91it/s]                                               {'loss': 2.3962, 'grad_norm': 4.925174236297607, 'learning_rate': 5.633304448274093e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.91it/s] 57%|█████▋    | 43/75 [00:01<00:01, 24.64it/s]                                               {'loss': 2.1547, 'grad_norm': 3.405543565750122, 'learning_rate': 5.467619023324855e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.64it/s]                                               {'loss': 2.1106, 'grad_norm': 3.7616376876831055, 'learning_rate': 5.301933598375617e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.64it/s]                                               {'loss': 2.3922, 'grad_norm': 14.379426956176758, 'learning_rate': 5.136248173426379e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.64it/s] 61%|██████▏   | 46/75 [00:01<00:01, 25.33it/s]                                               {'loss': 2.2249, 'grad_norm': 4.686007976531982, 'learning_rate': 4.970562748477141e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.33it/s]                                               {'loss': 2.1926, 'grad_norm': 4.2863054275512695, 'learning_rate': 4.8048773235279026e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.33it/s]                                               {'loss': 2.4245, 'grad_norm': 4.406250953674316, 'learning_rate': 4.639191898578665e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.33it/s] 65%|██████▌   | 49/75 [00:01<00:01, 24.89it/s]                                               {'loss': 2.3277, 'grad_norm': 3.871803045272827, 'learning_rate': 4.4735064736294265e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 24.89it/s]                                               {'loss': 2.4112, 'grad_norm': 6.504454135894775, 'learning_rate': 4.307821048680189e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 24.89it/s]                                               {'loss': 2.0341, 'grad_norm': 3.6279361248016357, 'learning_rate': 4.14213562373095e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 24.89it/s] 69%|██████▉   | 52/75 [00:02<00:00, 24.55it/s]                                               {'loss': 2.1695, 'grad_norm': 3.631406545639038, 'learning_rate': 3.9764501987817126e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.55it/s]                                               {'loss': 2.1973, 'grad_norm': 4.810009002685547, 'learning_rate': 3.810764773832474e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.55it/s]                                               {'loss': 2.31, 'grad_norm': 3.300022840499878, 'learning_rate': 3.6450793488832364e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.55it/s] 73%|███████▎  | 55/75 [00:02<00:00, 24.43it/s]                                               {'loss': 2.1537, 'grad_norm': 3.800379991531372, 'learning_rate': 3.479393923933999e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.43it/s]                                               {'loss': 2.247, 'grad_norm': 3.8404488563537598, 'learning_rate': 3.31370849898476e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.43it/s]                                               {'loss': 2.3222, 'grad_norm': 4.2631306648254395, 'learning_rate': 3.148023074035523e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.43it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.38it/s]                                               {'loss': 2.1756, 'grad_norm': 4.844054698944092, 'learning_rate': 2.9823376490862844e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.38it/s]                                               {'loss': 2.1162, 'grad_norm': 4.0551323890686035, 'learning_rate': 2.8166522241370464e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.38it/s]                                               {'loss': 2.6149, 'grad_norm': 13.445489883422852, 'learning_rate': 2.6509667991878086e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.38it/s]                                               {'loss': 2.3208, 'grad_norm': 3.3508243560791016, 'learning_rate': 2.4852813742385705e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.38it/s] 83%|████████▎ | 62/75 [00:02<00:00, 25.96it/s]                                               {'loss': 2.3057, 'grad_norm': 4.589022159576416, 'learning_rate': 2.3195959492893325e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.96it/s]                                               {'loss': 1.9722, 'grad_norm': 3.953814744949341, 'learning_rate': 2.1539105243400944e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.96it/s]                                               {'loss': 2.2156, 'grad_norm': 3.937195062637329, 'learning_rate': 1.9882250993908563e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.96it/s] 87%|████████▋ | 65/75 [00:02<00:00, 25.61it/s]                                               {'loss': 1.8768, 'grad_norm': 3.6108784675598145, 'learning_rate': 1.8225396744416182e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.61it/s]                                               {'loss': 2.1383, 'grad_norm': 3.7639102935791016, 'learning_rate': 1.65685424949238e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.61it/s]                                               {'loss': 2.3829, 'grad_norm': 4.06861686706543, 'learning_rate': 1.4911688245431422e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.61it/s] 91%|█████████ | 68/75 [00:02<00:00, 24.91it/s]                                               {'loss': 2.1333, 'grad_norm': 4.267446517944336, 'learning_rate': 1.3254833995939043e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.91it/s]                                               {'loss': 2.2561, 'grad_norm': 4.138587951660156, 'learning_rate': 1.1597979746446662e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.91it/s]                                               {'loss': 2.3486, 'grad_norm': 3.254134178161621, 'learning_rate': 9.941125496954281e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.91it/s] 95%|█████████▍| 71/75 [00:02<00:00, 24.71it/s]                                               {'loss': 2.238, 'grad_norm': 4.033281326293945, 'learning_rate': 8.2842712474619e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.71it/s]                                               {'loss': 2.1929, 'grad_norm': 3.806445360183716, 'learning_rate': 6.6274169979695215e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.71it/s]                                               {'loss': 2.3066, 'grad_norm': 4.582650661468506, 'learning_rate': 4.970562748477141e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.71it/s] 99%|█████████▊| 74/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.2986, 'grad_norm': 3.788921594619751, 'learning_rate': 3.3137084989847608e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.3415, 'grad_norm': 17.762109756469727, 'learning_rate': 1.6568542494923804e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.16it/s]                                               {'train_runtime': 3.0875, 'train_samples_per_second': 365.993, 'train_steps_per_second': 24.292, 'train_loss': 2.2705605379740397, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.16it/s]100%|██████████| 75/75 [00:03<00:00, 24.29it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:15
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.1739, 'grad_norm': 3.179311752319336, 'learning_rate': 0.00012426406871192852, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 22.35it/s]                                              {'loss': 2.2566, 'grad_norm': 4.5750203132629395, 'learning_rate': 0.00012260721446243615, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.79it/s]  4%|▍         | 3/75 [00:00<00:02, 24.53it/s]                                              {'loss': 2.0794, 'grad_norm': 3.1183485984802246, 'learning_rate': 0.00012095036021294377, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.53it/s]                                              {'loss': 2.3652, 'grad_norm': 4.2717437744140625, 'learning_rate': 0.00011929350596345138, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.53it/s]                                              {'loss': 2.1492, 'grad_norm': 6.310450077056885, 'learning_rate': 0.000117636651713959, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.53it/s]  8%|▊         | 6/75 [00:00<00:02, 25.45it/s]                                              {'loss': 2.3871, 'grad_norm': 3.7375259399414062, 'learning_rate': 0.00011597979746446662, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.45it/s]                                              {'loss': 2.3409, 'grad_norm': 4.406360149383545, 'learning_rate': 0.00011432294321497425, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.45it/s]                                              {'loss': 2.3249, 'grad_norm': 4.917600154876709, 'learning_rate': 0.00011266608896548185, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.45it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.43it/s]                                              {'loss': 2.3263, 'grad_norm': 3.7365710735321045, 'learning_rate': 0.00011100923471598948, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.43it/s]                                              {'loss': 2.2706, 'grad_norm': 4.38142728805542, 'learning_rate': 0.0001093523804664971, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.43it/s]                                               {'loss': 2.3253, 'grad_norm': 3.5910820960998535, 'learning_rate': 0.00010769552621700472, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.43it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.54it/s]                                               {'loss': 2.3624, 'grad_norm': 4.694015026092529, 'learning_rate': 0.00010603867196751234, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.54it/s]                                               {'loss': 2.0964, 'grad_norm': 3.226947069168091, 'learning_rate': 0.00010438181771801995, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.54it/s]                                               {'loss': 2.0081, 'grad_norm': 4.0759100914001465, 'learning_rate': 0.00010272496346852758, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.54it/s]                                               {'loss': 2.6108, 'grad_norm': 9.283125877380371, 'learning_rate': 0.0001010681092190352, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.54it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.85it/s]                                               {'loss': 2.0923, 'grad_norm': 3.4839799404144287, 'learning_rate': 9.941125496954282e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.85it/s]                                               {'loss': 2.3752, 'grad_norm': 5.685778617858887, 'learning_rate': 9.775440072005043e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.85it/s]                                               {'loss': 2.1703, 'grad_norm': 3.8734054565429688, 'learning_rate': 9.609754647055805e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.85it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.54it/s]                                               {'loss': 2.2308, 'grad_norm': 4.2208251953125, 'learning_rate': 9.444069222106568e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.54it/s]                                               {'loss': 2.2234, 'grad_norm': 4.026309013366699, 'learning_rate': 9.27838379715733e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.54it/s]                                               {'loss': 2.0288, 'grad_norm': 3.1077356338500977, 'learning_rate': 9.112698372208091e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.54it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.78it/s]                                               {'loss': 2.2907, 'grad_norm': 4.063940048217773, 'learning_rate': 8.947012947258853e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.78it/s]                                               {'loss': 1.9406, 'grad_norm': 3.810633897781372, 'learning_rate': 8.781327522309615e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.78it/s]                                               {'loss': 2.3723, 'grad_norm': 4.317162990570068, 'learning_rate': 8.615642097360377e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.78it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.10it/s]                                               {'loss': 2.4775, 'grad_norm': 4.491972923278809, 'learning_rate': 8.44995667241114e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.10it/s]                                               {'loss': 2.376, 'grad_norm': 4.41903829574585, 'learning_rate': 8.2842712474619e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.10it/s]                                               {'loss': 2.3605, 'grad_norm': 3.5364387035369873, 'learning_rate': 8.118585822512663e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.10it/s] 37%|███▋      | 28/75 [00:01<00:01, 23.64it/s]                                               {'loss': 2.1516, 'grad_norm': 4.466905117034912, 'learning_rate': 7.952900397563425e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 23.64it/s]                                               {'loss': 2.0571, 'grad_norm': 4.206155300140381, 'learning_rate': 7.787214972614187e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 23.64it/s]                                               {'loss': 2.7154, 'grad_norm': 15.7589750289917, 'learning_rate': 7.621529547664948e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.64it/s]                                               {'loss': 2.3265, 'grad_norm': 3.5126852989196777, 'learning_rate': 7.45584412271571e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 23.64it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.00it/s]                                               {'loss': 2.0817, 'grad_norm': 5.215481758117676, 'learning_rate': 7.290158697766473e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.00it/s]                                               {'loss': 2.2111, 'grad_norm': 5.289865970611572, 'learning_rate': 7.124473272817235e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.00it/s]                                               {'loss': 2.1326, 'grad_norm': 3.787463903427124, 'learning_rate': 6.958787847867997e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.00it/s] 47%|████▋     | 35/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.0895, 'grad_norm': 3.934798240661621, 'learning_rate': 6.793102422918758e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.2031, 'grad_norm': 3.4251558780670166, 'learning_rate': 6.62741699796952e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.3332, 'grad_norm': 4.096796035766602, 'learning_rate': 6.461731573020283e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.57it/s] 51%|█████     | 38/75 [00:01<00:01, 24.24it/s]                                               {'loss': 2.2645, 'grad_norm': 3.183460235595703, 'learning_rate': 6.296046148071046e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.24it/s]                                               {'loss': 2.2287, 'grad_norm': 3.432086229324341, 'learning_rate': 6.130360723121807e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.24it/s]                                               {'loss': 2.0467, 'grad_norm': 3.7233729362487793, 'learning_rate': 5.964675298172569e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.24it/s] 55%|█████▍    | 41/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.1871, 'grad_norm': 4.625992774963379, 'learning_rate': 5.798989873223331e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.1377, 'grad_norm': 3.7810370922088623, 'learning_rate': 5.633304448274093e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.0391, 'grad_norm': 2.9619128704071045, 'learning_rate': 5.467619023324855e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.56it/s] 59%|█████▊    | 44/75 [00:01<00:01, 24.83it/s]                                               {'loss': 2.3202, 'grad_norm': 5.695099353790283, 'learning_rate': 5.301933598375617e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.83it/s]                                               {'loss': 2.1928, 'grad_norm': 7.709656715393066, 'learning_rate': 5.136248173426379e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.83it/s]                                               {'loss': 2.1688, 'grad_norm': 4.996444225311279, 'learning_rate': 4.970562748477141e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.83it/s]                                               {'loss': 2.0173, 'grad_norm': 3.2063024044036865, 'learning_rate': 4.8048773235279026e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 24.83it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.05it/s]                                               {'loss': 2.014, 'grad_norm': 3.761080265045166, 'learning_rate': 4.639191898578665e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.05it/s]                                               {'loss': 2.2199, 'grad_norm': 3.698413848876953, 'learning_rate': 4.4735064736294265e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.05it/s]                                               {'loss': 2.1577, 'grad_norm': 4.378711223602295, 'learning_rate': 4.307821048680189e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.05it/s] 68%|██████▊   | 51/75 [00:02<00:00, 25.65it/s]                                               {'loss': 2.3703, 'grad_norm': 3.754533052444458, 'learning_rate': 4.14213562373095e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.65it/s]                                               {'loss': 1.9896, 'grad_norm': 2.576253890991211, 'learning_rate': 3.9764501987817126e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.65it/s]                                               {'loss': 2.2742, 'grad_norm': 3.8832671642303467, 'learning_rate': 3.810764773832474e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.65it/s] 72%|███████▏  | 54/75 [00:02<00:00, 24.77it/s]                                               {'loss': 2.2597, 'grad_norm': 4.35368013381958, 'learning_rate': 3.6450793488832364e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.77it/s]                                               {'loss': 2.1601, 'grad_norm': 3.5224688053131104, 'learning_rate': 3.479393923933999e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.77it/s]                                               {'loss': 2.1106, 'grad_norm': 3.4454386234283447, 'learning_rate': 3.31370849898476e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.77it/s] 76%|███████▌  | 57/75 [00:02<00:00, 23.64it/s]                                               {'loss': 2.2566, 'grad_norm': 3.889514207839966, 'learning_rate': 3.148023074035523e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 23.64it/s]                                               {'loss': 2.1835, 'grad_norm': 4.081698417663574, 'learning_rate': 2.9823376490862844e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 23.64it/s]                                               {'loss': 2.4416, 'grad_norm': 4.851709365844727, 'learning_rate': 2.8166522241370464e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 23.64it/s]                                               {'loss': 2.5217, 'grad_norm': 12.27396297454834, 'learning_rate': 2.6509667991878086e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 23.64it/s] 81%|████████▏ | 61/75 [00:02<00:00, 25.70it/s]                                               {'loss': 2.0348, 'grad_norm': 3.6703848838806152, 'learning_rate': 2.4852813742385705e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.70it/s]                                               {'loss': 2.2859, 'grad_norm': 4.151124954223633, 'learning_rate': 2.3195959492893325e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.70it/s]                                               {'loss': 2.1475, 'grad_norm': 4.327176094055176, 'learning_rate': 2.1539105243400944e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.70it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.84it/s]                                               {'loss': 2.0991, 'grad_norm': 3.025786876678467, 'learning_rate': 1.9882250993908563e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.84it/s]                                               {'loss': 2.2219, 'grad_norm': 5.225205898284912, 'learning_rate': 1.8225396744416182e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.84it/s]                                               {'loss': 2.0191, 'grad_norm': 2.988511085510254, 'learning_rate': 1.65685424949238e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.84it/s] 89%|████████▉ | 67/75 [00:02<00:00, 25.40it/s]                                               {'loss': 2.0906, 'grad_norm': 4.169830799102783, 'learning_rate': 1.4911688245431422e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.40it/s]                                               {'loss': 2.2949, 'grad_norm': 4.160754203796387, 'learning_rate': 1.3254833995939043e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.40it/s]                                               {'loss': 2.0685, 'grad_norm': 3.2290754318237305, 'learning_rate': 1.1597979746446662e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.40it/s] 93%|█████████▎| 70/75 [00:02<00:00, 25.12it/s]                                               {'loss': 2.4157, 'grad_norm': 3.8134043216705322, 'learning_rate': 9.941125496954281e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.12it/s]                                               {'loss': 1.9281, 'grad_norm': 3.888068675994873, 'learning_rate': 8.2842712474619e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.12it/s]                                               {'loss': 2.3471, 'grad_norm': 3.8417623043060303, 'learning_rate': 6.6274169979695215e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.12it/s] 97%|█████████▋| 73/75 [00:02<00:00, 25.41it/s]                                               {'loss': 2.1575, 'grad_norm': 4.470005989074707, 'learning_rate': 4.970562748477141e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.41it/s]                                               {'loss': 2.3689, 'grad_norm': 4.7652907371521, 'learning_rate': 3.3137084989847608e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.41it/s]                                               {'loss': 1.71, 'grad_norm': 12.05007553100586, 'learning_rate': 1.6568542494923804e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.41it/s]                                               {'train_runtime': 3.0543, 'train_samples_per_second': 369.97, 'train_steps_per_second': 24.556, 'train_loss': 2.214265909194946, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.41it/s]100%|██████████| 75/75 [00:03<00:00, 24.56it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:11
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2858, 'grad_norm': 4.168070316314697, 'learning_rate': 0.00012426406871192852, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 18.03it/s]                                              {'loss': 2.258, 'grad_norm': 5.01920747756958, 'learning_rate': 0.00012260721446243615, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 20.95it/s]  4%|▍         | 3/75 [00:00<00:03, 22.40it/s]                                              {'loss': 2.2799, 'grad_norm': 3.461933135986328, 'learning_rate': 0.00012095036021294377, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 22.40it/s]                                              {'loss': 2.194, 'grad_norm': 4.074948310852051, 'learning_rate': 0.00011929350596345138, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 22.40it/s]                                              {'loss': 2.4817, 'grad_norm': 4.689697742462158, 'learning_rate': 0.000117636651713959, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.40it/s]  8%|▊         | 6/75 [00:00<00:02, 23.78it/s]                                              {'loss': 2.1917, 'grad_norm': 2.7110860347747803, 'learning_rate': 0.00011597979746446662, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.78it/s]                                              {'loss': 2.443, 'grad_norm': 3.832183599472046, 'learning_rate': 0.00011432294321497425, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.78it/s]                                              {'loss': 2.4881, 'grad_norm': 4.532564640045166, 'learning_rate': 0.00011266608896548185, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.78it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.38it/s]                                              {'loss': 2.2688, 'grad_norm': 4.548593044281006, 'learning_rate': 0.00011100923471598948, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.38it/s]                                              {'loss': 2.1675, 'grad_norm': 3.8635082244873047, 'learning_rate': 0.0001093523804664971, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.38it/s]                                               {'loss': 2.3145, 'grad_norm': 6.518005847930908, 'learning_rate': 0.00010769552621700472, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.38it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.19it/s]                                               {'loss': 2.2241, 'grad_norm': 5.824738025665283, 'learning_rate': 0.00010603867196751234, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.19it/s]                                               {'loss': 2.3084, 'grad_norm': 3.953768491744995, 'learning_rate': 0.00010438181771801995, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.19it/s]                                               {'loss': 2.367, 'grad_norm': 5.9929680824279785, 'learning_rate': 0.00010272496346852758, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.19it/s]                                               {'loss': 2.3103, 'grad_norm': 11.427643775939941, 'learning_rate': 0.0001010681092190352, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.19it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.53it/s]                                               {'loss': 2.4353, 'grad_norm': 4.175060749053955, 'learning_rate': 9.941125496954282e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.53it/s]                                               {'loss': 2.3436, 'grad_norm': 5.524069309234619, 'learning_rate': 9.775440072005043e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.53it/s]                                               {'loss': 2.1618, 'grad_norm': 3.2132604122161865, 'learning_rate': 9.609754647055805e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.53it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.46it/s]                                               {'loss': 2.3146, 'grad_norm': 3.4579315185546875, 'learning_rate': 9.444069222106568e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.46it/s]                                               {'loss': 2.1935, 'grad_norm': 4.056605815887451, 'learning_rate': 9.27838379715733e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.46it/s]                                               {'loss': 2.1286, 'grad_norm': 3.567681074142456, 'learning_rate': 9.112698372208091e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.46it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.92it/s]                                               {'loss': 2.4076, 'grad_norm': 4.1559739112854, 'learning_rate': 8.947012947258853e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.92it/s]                                               {'loss': 2.1054, 'grad_norm': 4.492182731628418, 'learning_rate': 8.781327522309615e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.92it/s]                                               {'loss': 2.1674, 'grad_norm': 3.119929552078247, 'learning_rate': 8.615642097360377e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.92it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.87it/s]                                               {'loss': 2.2624, 'grad_norm': 3.9861857891082764, 'learning_rate': 8.44995667241114e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.87it/s]                                               {'loss': 2.2771, 'grad_norm': 3.3261663913726807, 'learning_rate': 8.2842712474619e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.87it/s]                                               {'loss': 2.4224, 'grad_norm': 5.44749116897583, 'learning_rate': 8.118585822512663e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.87it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.1688, 'grad_norm': 3.084336996078491, 'learning_rate': 7.952900397563425e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.3409, 'grad_norm': 4.69493293762207, 'learning_rate': 7.787214972614187e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.0604, 'grad_norm': 13.615208625793457, 'learning_rate': 7.621529547664948e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.2018, 'grad_norm': 5.496753215789795, 'learning_rate': 7.45584412271571e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.34it/s] 43%|████▎     | 32/75 [00:01<00:01, 27.22it/s]                                               {'loss': 2.3554, 'grad_norm': 3.4950222969055176, 'learning_rate': 7.290158697766473e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 27.22it/s]                                               {'loss': 2.2269, 'grad_norm': 4.754012107849121, 'learning_rate': 7.124473272817235e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 27.22it/s]                                               {'loss': 2.2469, 'grad_norm': 3.8412976264953613, 'learning_rate': 6.958787847867997e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 27.22it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.75it/s]                                               {'loss': 2.3811, 'grad_norm': 4.546671390533447, 'learning_rate': 6.793102422918758e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.75it/s]                                               {'loss': 2.1461, 'grad_norm': 4.348658084869385, 'learning_rate': 6.62741699796952e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.75it/s]                                               {'loss': 2.3582, 'grad_norm': 3.206605911254883, 'learning_rate': 6.461731573020283e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.75it/s] 51%|█████     | 38/75 [00:01<00:01, 26.07it/s]                                               {'loss': 2.2192, 'grad_norm': 3.8364830017089844, 'learning_rate': 6.296046148071046e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.07it/s]                                               {'loss': 2.0207, 'grad_norm': 2.586247205734253, 'learning_rate': 6.130360723121807e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.07it/s]                                               {'loss': 2.247, 'grad_norm': 3.76491641998291, 'learning_rate': 5.964675298172569e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 26.07it/s] 55%|█████▍    | 41/75 [00:01<00:01, 26.01it/s]                                               {'loss': 2.2304, 'grad_norm': 4.766946792602539, 'learning_rate': 5.798989873223331e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 26.01it/s]                                               {'loss': 2.4889, 'grad_norm': 4.669435977935791, 'learning_rate': 5.633304448274093e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 26.01it/s]                                               {'loss': 2.1921, 'grad_norm': 4.409511089324951, 'learning_rate': 5.467619023324855e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 26.01it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.83it/s]                                               {'loss': 2.1751, 'grad_norm': 3.28010630607605, 'learning_rate': 5.301933598375617e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.83it/s]                                               {'loss': 2.3112, 'grad_norm': 19.380001068115234, 'learning_rate': 5.136248173426379e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.83it/s]                                               {'loss': 2.4212, 'grad_norm': 4.9805006980896, 'learning_rate': 4.970562748477141e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.83it/s]                                               {'loss': 2.0464, 'grad_norm': 3.46698260307312, 'learning_rate': 4.8048773235279026e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.83it/s] 64%|██████▍   | 48/75 [00:01<00:00, 27.15it/s]                                               {'loss': 2.0363, 'grad_norm': 3.558565616607666, 'learning_rate': 4.639191898578665e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 27.15it/s]                                               {'loss': 2.2821, 'grad_norm': 3.305769681930542, 'learning_rate': 4.4735064736294265e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 27.15it/s]                                               {'loss': 2.3294, 'grad_norm': 4.427410125732422, 'learning_rate': 4.307821048680189e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 27.15it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.78it/s]                                               {'loss': 2.0454, 'grad_norm': 3.585645914077759, 'learning_rate': 4.14213562373095e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 26.78it/s]                                               {'loss': 2.293, 'grad_norm': 4.464451789855957, 'learning_rate': 3.9764501987817126e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:01<00:00, 26.78it/s]                                               {'loss': 2.2857, 'grad_norm': 5.0464019775390625, 'learning_rate': 3.810764773832474e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 26.78it/s] 72%|███████▏  | 54/75 [00:02<00:00, 26.35it/s]                                               {'loss': 2.1787, 'grad_norm': 3.673119306564331, 'learning_rate': 3.6450793488832364e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 26.35it/s]                                               {'loss': 2.0978, 'grad_norm': 3.3121213912963867, 'learning_rate': 3.479393923933999e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 26.35it/s]                                               {'loss': 2.2001, 'grad_norm': 3.9632253646850586, 'learning_rate': 3.31370849898476e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 26.35it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.43it/s]                                               {'loss': 2.1626, 'grad_norm': 4.3110127449035645, 'learning_rate': 3.148023074035523e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 26.43it/s]                                               {'loss': 2.5188, 'grad_norm': 4.122905254364014, 'learning_rate': 2.9823376490862844e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 26.43it/s]                                               {'loss': 2.0597, 'grad_norm': 3.8596715927124023, 'learning_rate': 2.8166522241370464e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 26.43it/s]                                               {'loss': 2.4751, 'grad_norm': 12.410968780517578, 'learning_rate': 2.6509667991878086e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 26.43it/s] 81%|████████▏ | 61/75 [00:02<00:00, 26.74it/s]                                               {'loss': 2.105, 'grad_norm': 3.1597771644592285, 'learning_rate': 2.4852813742385705e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.74it/s]                                               {'loss': 2.117, 'grad_norm': 4.231268405914307, 'learning_rate': 2.3195959492893325e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.74it/s]                                               {'loss': 2.2274, 'grad_norm': 3.6221210956573486, 'learning_rate': 2.1539105243400944e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.74it/s] 85%|████████▌ | 64/75 [00:02<00:00, 26.62it/s]                                               {'loss': 2.1209, 'grad_norm': 3.9932100772857666, 'learning_rate': 1.9882250993908563e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.62it/s]                                               {'loss': 2.2227, 'grad_norm': 4.432359218597412, 'learning_rate': 1.8225396744416182e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.62it/s]                                               {'loss': 2.4857, 'grad_norm': 5.052748203277588, 'learning_rate': 1.65685424949238e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.62it/s] 89%|████████▉ | 67/75 [00:02<00:00, 26.03it/s]                                               {'loss': 2.1748, 'grad_norm': 3.2143523693084717, 'learning_rate': 1.4911688245431422e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.03it/s]                                               {'loss': 2.2418, 'grad_norm': 3.4413864612579346, 'learning_rate': 1.3254833995939043e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 26.03it/s]                                               {'loss': 2.1472, 'grad_norm': 5.180641174316406, 'learning_rate': 1.1597979746446662e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 26.03it/s] 93%|█████████▎| 70/75 [00:02<00:00, 25.51it/s]                                               {'loss': 2.1543, 'grad_norm': 2.604670524597168, 'learning_rate': 9.941125496954281e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.51it/s]                                               {'loss': 2.2693, 'grad_norm': 3.9112257957458496, 'learning_rate': 8.2842712474619e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.51it/s]                                               {'loss': 2.2329, 'grad_norm': 3.8840537071228027, 'learning_rate': 6.6274169979695215e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.51it/s] 97%|█████████▋| 73/75 [00:02<00:00, 25.58it/s]                                               {'loss': 2.2214, 'grad_norm': 4.718518257141113, 'learning_rate': 4.970562748477141e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.58it/s]                                               {'loss': 2.1515, 'grad_norm': 4.076282978057861, 'learning_rate': 3.3137084989847608e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.58it/s]                                               {'loss': 2.1559, 'grad_norm': 7.855681896209717, 'learning_rate': 1.6568542494923804e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.58it/s]                                               {'train_runtime': 2.9704, 'train_samples_per_second': 380.423, 'train_steps_per_second': 25.249, 'train_loss': 2.2484511089324952, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.58it/s]100%|██████████| 75/75 [00:02<00:00, 25.25it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(944, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(869, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(716, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(842, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1071, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(824, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(961, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(780, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(843, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(533, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(999, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(496, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 60.37it/s]  3%|▎         | 14/471 [00:00<00:08, 56.75it/s]  4%|▍         | 20/471 [00:00<00:08, 55.04it/s]  6%|▌         | 26/471 [00:00<00:08, 53.75it/s]  7%|▋         | 32/471 [00:00<00:08, 52.88it/s]  8%|▊         | 38/471 [00:00<00:08, 52.01it/s]  9%|▉         | 44/471 [00:00<00:08, 52.60it/s] 11%|█         | 50/471 [00:00<00:07, 52.76it/s] 12%|█▏        | 56/471 [00:01<00:07, 53.09it/s] 13%|█▎        | 62/471 [00:01<00:07, 52.52it/s] 14%|█▍        | 68/471 [00:01<00:07, 52.21it/s] 16%|█▌        | 74/471 [00:01<00:07, 52.60it/s] 17%|█▋        | 80/471 [00:01<00:07, 52.84it/s] 18%|█▊        | 86/471 [00:01<00:07, 52.48it/s] 20%|█▉        | 92/471 [00:01<00:07, 52.12it/s] 21%|██        | 98/471 [00:01<00:07, 52.47it/s] 22%|██▏       | 104/471 [00:01<00:06, 52.80it/s] 23%|██▎       | 110/471 [00:02<00:06, 52.86it/s] 25%|██▍       | 116/471 [00:02<00:06, 52.36it/s] 26%|██▌       | 122/471 [00:02<00:06, 52.53it/s] 27%|██▋       | 128/471 [00:02<00:06, 52.68it/s] 28%|██▊       | 134/471 [00:02<00:06, 52.86it/s] 30%|██▉       | 140/471 [00:02<00:06, 51.05it/s] 31%|███       | 146/471 [00:02<00:06, 51.43it/s] 32%|███▏      | 152/471 [00:02<00:06, 52.23it/s] 34%|███▎      | 158/471 [00:03<00:06, 50.42it/s] 35%|███▍      | 164/471 [00:03<00:05, 51.18it/s] 36%|███▌      | 170/471 [00:03<00:05, 50.74it/s] 37%|███▋      | 176/471 [00:03<00:05, 50.63it/s] 39%|███▊      | 182/471 [00:03<00:05, 51.70it/s] 40%|███▉      | 188/471 [00:03<00:05, 52.22it/s] 41%|████      | 194/471 [00:03<00:05, 52.31it/s] 42%|████▏     | 200/471 [00:03<00:05, 51.78it/s] 44%|████▎     | 206/471 [00:03<00:05, 52.17it/s] 45%|████▌     | 212/471 [00:04<00:04, 52.59it/s] 46%|████▋     | 218/471 [00:04<00:04, 52.23it/s] 48%|████▊     | 224/471 [00:04<00:04, 51.87it/s] 49%|████▉     | 230/471 [00:04<00:04, 51.11it/s] 50%|█████     | 236/471 [00:04<00:04, 51.79it/s] 51%|█████▏    | 242/471 [00:04<00:04, 52.10it/s] 53%|█████▎    | 248/471 [00:04<00:04, 51.38it/s] 54%|█████▍    | 254/471 [00:04<00:04, 50.78it/s] 55%|█████▌    | 260/471 [00:04<00:04, 51.34it/s] 56%|█████▋    | 266/471 [00:05<00:04, 50.00it/s] 58%|█████▊    | 272/471 [00:05<00:03, 50.42it/s] 59%|█████▉    | 278/471 [00:05<00:03, 50.95it/s] 60%|██████    | 284/471 [00:05<00:03, 50.27it/s] 62%|██████▏   | 290/471 [00:05<00:03, 50.88it/s] 63%|██████▎   | 296/471 [00:05<00:03, 51.19it/s] 64%|██████▍   | 302/471 [00:05<00:03, 47.99it/s] 65%|██████▌   | 307/471 [00:05<00:03, 47.09it/s] 66%|██████▌   | 312/471 [00:06<00:03, 47.43it/s] 68%|██████▊   | 318/471 [00:06<00:03, 49.50it/s] 69%|██████▊   | 323/471 [00:06<00:03, 49.07it/s] 70%|██████▉   | 329/471 [00:06<00:02, 49.96it/s] 71%|███████   | 335/471 [00:06<00:02, 50.48it/s] 72%|███████▏  | 341/471 [00:06<00:02, 50.87it/s] 74%|███████▎  | 347/471 [00:06<00:02, 50.99it/s] 75%|███████▍  | 353/471 [00:06<00:02, 51.68it/s] 76%|███████▌  | 359/471 [00:06<00:02, 50.76it/s] 77%|███████▋  | 365/471 [00:07<00:02, 51.15it/s] 79%|███████▉  | 371/471 [00:07<00:01, 51.55it/s] 80%|████████  | 377/471 [00:07<00:01, 51.87it/s] 81%|████████▏ | 383/471 [00:07<00:01, 52.24it/s] 83%|████████▎ | 389/471 [00:07<00:01, 52.34it/s] 84%|████████▍ | 395/471 [00:07<00:01, 52.36it/s] 85%|████████▌ | 401/471 [00:07<00:01, 52.10it/s] 86%|████████▋ | 407/471 [00:07<00:01, 52.20it/s] 88%|████████▊ | 413/471 [00:08<00:01, 51.20it/s] 89%|████████▉ | 419/471 [00:08<00:01, 51.46it/s] 90%|█████████ | 425/471 [00:08<00:00, 51.72it/s] 92%|█████████▏| 431/471 [00:08<00:00, 52.06it/s] 93%|█████████▎| 437/471 [00:08<00:00, 51.56it/s] 94%|█████████▍| 443/471 [00:08<00:00, 51.65it/s] 95%|█████████▌| 449/471 [00:08<00:00, 51.30it/s] 97%|█████████▋| 455/471 [00:08<00:00, 50.25it/s] 98%|█████████▊| 461/471 [00:08<00:00, 50.53it/s] 99%|█████████▉| 467/471 [00:09<00:00, 51.11it/s]100%|██████████| 471/471 [00:09<00:00, 51.57it/s]
{'eval_loss': 2.2658982276916504, 'eval_model_preparation_time': 0.0029, 'eval_acc': 0.3986988847583643, 'eval_runtime': 9.1555, 'eval_samples_per_second': 822.675, 'eval_steps_per_second': 51.444}
ROUND:21
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4491, 'grad_norm': 3.874049425125122, 'learning_rate': 0.00012249209307789376, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 20.57it/s]                                              {'loss': 2.443, 'grad_norm': 4.778947353363037, 'learning_rate': 0.00012085886517018852, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.55it/s]  4%|▍         | 3/75 [00:00<00:02, 24.67it/s]                                              {'loss': 2.4025, 'grad_norm': 4.768155574798584, 'learning_rate': 0.00011922563726248326, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.67it/s]                                              {'loss': 2.2761, 'grad_norm': 4.511019706726074, 'learning_rate': 0.000117592409354778, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.67it/s]                                              {'loss': 2.4123, 'grad_norm': 3.785555124282837, 'learning_rate': 0.00011595918144707276, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.67it/s]  8%|▊         | 6/75 [00:00<00:02, 25.06it/s]                                              {'loss': 2.0699, 'grad_norm': 3.42598295211792, 'learning_rate': 0.00011432595353936752, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.06it/s]                                              {'loss': 2.4114, 'grad_norm': 3.8868727684020996, 'learning_rate': 0.00011269272563166226, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.06it/s]                                              {'loss': 2.1898, 'grad_norm': 3.6051361560821533, 'learning_rate': 0.000111059497723957, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.06it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.10it/s]                                              {'loss': 2.6124, 'grad_norm': 4.712224960327148, 'learning_rate': 0.00010942626981625176, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.10it/s]                                              {'loss': 2.2308, 'grad_norm': 4.21434211730957, 'learning_rate': 0.00010779304190854651, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.10it/s]                                               {'loss': 2.3391, 'grad_norm': 3.3205461502075195, 'learning_rate': 0.00010615981400084126, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.10it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.87it/s]                                               {'loss': 2.0347, 'grad_norm': 3.400972604751587, 'learning_rate': 0.00010452658609313601, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.87it/s]                                               {'loss': 2.3318, 'grad_norm': 4.530295372009277, 'learning_rate': 0.00010289335818543076, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.87it/s]                                               {'loss': 2.253, 'grad_norm': 4.8058905601501465, 'learning_rate': 0.00010126013027772551, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.87it/s] 20%|██        | 15/75 [00:00<00:02, 26.43it/s]                                               {'loss': 2.5247, 'grad_norm': 8.371106147766113, 'learning_rate': 9.962690237002026e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 26.43it/s]                                               {'loss': 2.4589, 'grad_norm': 4.5763115882873535, 'learning_rate': 9.799367446231501e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.43it/s]                                               {'loss': 2.2518, 'grad_norm': 4.760530471801758, 'learning_rate': 9.636044655460976e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.43it/s] 24%|██▍       | 18/75 [00:00<00:02, 26.24it/s]                                               {'loss': 2.2241, 'grad_norm': 4.508058071136475, 'learning_rate': 9.47272186469045e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.24it/s]                                               {'loss': 2.5604, 'grad_norm': 4.751701831817627, 'learning_rate': 9.309399073919926e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.24it/s]                                               {'loss': 2.3446, 'grad_norm': 3.0902585983276367, 'learning_rate': 9.146076283149401e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.24it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.84it/s]                                               {'loss': 2.3754, 'grad_norm': 4.909451484680176, 'learning_rate': 8.982753492378876e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.84it/s]                                               {'loss': 2.2696, 'grad_norm': 3.784254312515259, 'learning_rate': 8.81943070160835e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.84it/s]                                               {'loss': 2.271, 'grad_norm': 3.2465462684631348, 'learning_rate': 8.656107910837825e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.84it/s] 32%|███▏      | 24/75 [00:00<00:01, 25.51it/s]                                               {'loss': 2.0909, 'grad_norm': 4.274524211883545, 'learning_rate': 8.492785120067301e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.51it/s]                                               {'loss': 2.2453, 'grad_norm': 4.207894802093506, 'learning_rate': 8.329462329296777e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.51it/s]                                               {'loss': 2.3701, 'grad_norm': 4.035468101501465, 'learning_rate': 8.16613953852625e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.51it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.26it/s]                                               {'loss': 2.1848, 'grad_norm': 4.227171897888184, 'learning_rate': 8.002816747755725e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.26it/s]                                               {'loss': 2.131, 'grad_norm': 3.7223618030548096, 'learning_rate': 7.839493956985201e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.26it/s]                                               {'loss': 2.2658, 'grad_norm': 4.7288312911987305, 'learning_rate': 7.676171166214677e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.26it/s]                                               {'loss': 2.1588, 'grad_norm': 6.511167526245117, 'learning_rate': 7.51284837544415e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.26it/s] 41%|████▏     | 31/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.2514, 'grad_norm': 5.184845447540283, 'learning_rate': 7.349525584673625e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.4019, 'grad_norm': 4.170958518981934, 'learning_rate': 7.186202793903101e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.3154, 'grad_norm': 3.7462172508239746, 'learning_rate': 7.022880003132575e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.37it/s] 45%|████▌     | 34/75 [00:01<00:01, 24.74it/s]                                               {'loss': 2.4192, 'grad_norm': 4.760856628417969, 'learning_rate': 6.859557212362051e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.74it/s]                                               {'loss': 2.2677, 'grad_norm': 3.7372090816497803, 'learning_rate': 6.696234421591525e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.74it/s]                                               {'loss': 2.01, 'grad_norm': 3.557811737060547, 'learning_rate': 6.532911630821001e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.74it/s] 49%|████▉     | 37/75 [00:01<00:01, 23.68it/s]                                               {'loss': 2.2806, 'grad_norm': 3.712566614151001, 'learning_rate': 6.369588840050475e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 23.68it/s]                                               {'loss': 2.1818, 'grad_norm': 3.7441084384918213, 'learning_rate': 6.206266049279951e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 23.68it/s]                                               {'loss': 2.2176, 'grad_norm': 3.404904365539551, 'learning_rate': 6.042943258509426e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.68it/s] 53%|█████▎    | 40/75 [00:01<00:01, 24.05it/s]                                               {'loss': 2.1767, 'grad_norm': 4.159719944000244, 'learning_rate': 5.8796204677389e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.05it/s]                                               {'loss': 2.3032, 'grad_norm': 6.12420129776001, 'learning_rate': 5.716297676968376e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.05it/s]                                               {'loss': 2.1475, 'grad_norm': 3.094590187072754, 'learning_rate': 5.55297488619785e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.05it/s] 57%|█████▋    | 43/75 [00:01<00:01, 24.02it/s]                                               {'loss': 2.2292, 'grad_norm': 3.9975008964538574, 'learning_rate': 5.389652095427326e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.02it/s]                                               {'loss': 2.4352, 'grad_norm': 4.915194988250732, 'learning_rate': 5.226329304656801e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.02it/s]                                               {'loss': 2.1882, 'grad_norm': 9.68300724029541, 'learning_rate': 5.063006513886276e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.02it/s]                                               {'loss': 2.2744, 'grad_norm': 3.6773452758789062, 'learning_rate': 4.8996837231157507e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.02it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.02it/s]                                               {'loss': 2.3659, 'grad_norm': 4.447134494781494, 'learning_rate': 4.736360932345225e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.02it/s]                                               {'loss': 2.1778, 'grad_norm': 4.166191577911377, 'learning_rate': 4.5730381415747006e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.02it/s]                                               {'loss': 2.2338, 'grad_norm': 3.648263454437256, 'learning_rate': 4.409715350804175e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.02it/s] 67%|██████▋   | 50/75 [00:01<00:00, 25.13it/s]                                               {'loss': 1.9618, 'grad_norm': 3.1793696880340576, 'learning_rate': 4.2463925600336506e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.13it/s]                                               {'loss': 2.127, 'grad_norm': 4.805140018463135, 'learning_rate': 4.083069769263125e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.13it/s]                                               {'loss': 2.3186, 'grad_norm': 3.776726484298706, 'learning_rate': 3.9197469784926005e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.13it/s] 71%|███████   | 53/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.4825, 'grad_norm': 3.923074960708618, 'learning_rate': 3.756424187722075e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.3365, 'grad_norm': 4.370794773101807, 'learning_rate': 3.5931013969515505e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.2903, 'grad_norm': 3.8854424953460693, 'learning_rate': 3.4297786061810255e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.14it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.17it/s]                                               {'loss': 2.1673, 'grad_norm': 4.361788749694824, 'learning_rate': 3.2664558154105004e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.17it/s]                                               {'loss': 2.3441, 'grad_norm': 4.4802565574646, 'learning_rate': 3.1031330246399754e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.17it/s]                                               {'loss': 2.3265, 'grad_norm': 5.466161727905273, 'learning_rate': 2.93981023386945e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.17it/s] 79%|███████▊  | 59/75 [00:02<00:00, 24.56it/s]                                               {'loss': 2.1013, 'grad_norm': 5.832948684692383, 'learning_rate': 2.776487443098925e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.56it/s]                                               {'loss': 2.2549, 'grad_norm': 18.292102813720703, 'learning_rate': 2.6131646523284003e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.56it/s]                                               {'loss': 2.1598, 'grad_norm': 3.4101357460021973, 'learning_rate': 2.4498418615578753e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.56it/s] 83%|████████▎ | 62/75 [00:02<00:00, 24.90it/s]                                               {'loss': 2.3243, 'grad_norm': 4.752613544464111, 'learning_rate': 2.2865190707873503e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 24.90it/s]                                               {'loss': 2.2, 'grad_norm': 3.977468967437744, 'learning_rate': 2.1231962800168253e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 24.90it/s]                                               {'loss': 1.9798, 'grad_norm': 3.1459579467773438, 'learning_rate': 1.9598734892463003e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 24.90it/s] 87%|████████▋ | 65/75 [00:02<00:00, 24.73it/s]                                               {'loss': 2.3905, 'grad_norm': 4.065247058868408, 'learning_rate': 1.7965506984757752e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.73it/s]                                               {'loss': 2.2817, 'grad_norm': 3.572385787963867, 'learning_rate': 1.6332279077052502e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.73it/s]                                               {'loss': 2.1915, 'grad_norm': 3.684591293334961, 'learning_rate': 1.469905116934725e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.73it/s] 91%|█████████ | 68/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.2794, 'grad_norm': 4.186595916748047, 'learning_rate': 1.3065823261642002e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.1359, 'grad_norm': 3.9627835750579834, 'learning_rate': 1.1432595353936752e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.1688, 'grad_norm': 4.613610744476318, 'learning_rate': 9.799367446231501e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.99it/s] 95%|█████████▍| 71/75 [00:02<00:00, 25.10it/s]                                               {'loss': 2.3319, 'grad_norm': 5.313529014587402, 'learning_rate': 8.166139538526251e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.10it/s]                                               {'loss': 2.3108, 'grad_norm': 4.261631965637207, 'learning_rate': 6.532911630821001e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.10it/s]                                               {'loss': 2.3389, 'grad_norm': 4.108331680297852, 'learning_rate': 4.899683723115751e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.10it/s] 99%|█████████▊| 74/75 [00:02<00:00, 24.91it/s]                                               {'loss': 2.0445, 'grad_norm': 3.7165887355804443, 'learning_rate': 3.2664558154105004e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.91it/s]                                               {'loss': 2.27, 'grad_norm': 18.953325271606445, 'learning_rate': 1.6332279077052502e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.91it/s]                                               {'train_runtime': 3.1629, 'train_samples_per_second': 357.271, 'train_steps_per_second': 23.713, 'train_loss': 2.269052209854126, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.91it/s]100%|██████████| 75/75 [00:03<00:00, 23.71it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:39
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.0584, 'grad_norm': 3.8963890075683594, 'learning_rate': 0.00012249209307789376, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 22.98it/s]                                              {'loss': 2.3561, 'grad_norm': 3.2964653968811035, 'learning_rate': 0.00012085886517018852, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 24.31it/s]  4%|▍         | 3/75 [00:00<00:02, 24.70it/s]                                              {'loss': 2.2336, 'grad_norm': 3.7783217430114746, 'learning_rate': 0.00011922563726248326, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.70it/s]                                              {'loss': 2.085, 'grad_norm': 4.739419460296631, 'learning_rate': 0.000117592409354778, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.70it/s]                                              {'loss': 2.3415, 'grad_norm': 4.99594259262085, 'learning_rate': 0.00011595918144707276, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.70it/s]  8%|▊         | 6/75 [00:00<00:02, 25.37it/s]                                              {'loss': 2.4422, 'grad_norm': 3.979656934738159, 'learning_rate': 0.00011432595353936752, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.37it/s]                                              {'loss': 2.1468, 'grad_norm': 4.015134811401367, 'learning_rate': 0.00011269272563166226, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.37it/s]                                              {'loss': 2.4115, 'grad_norm': 4.79031229019165, 'learning_rate': 0.000111059497723957, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.37it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.29it/s]                                              {'loss': 2.4157, 'grad_norm': 4.028238296508789, 'learning_rate': 0.00010942626981625176, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.29it/s]                                              {'loss': 2.3789, 'grad_norm': 4.241818904876709, 'learning_rate': 0.00010779304190854651, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.29it/s]                                               {'loss': 2.1928, 'grad_norm': 3.6937575340270996, 'learning_rate': 0.00010615981400084126, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.29it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.78it/s]                                               {'loss': 2.3626, 'grad_norm': 4.346842288970947, 'learning_rate': 0.00010452658609313601, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.78it/s]                                               {'loss': 1.9573, 'grad_norm': 3.0641465187072754, 'learning_rate': 0.00010289335818543076, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.78it/s]                                               {'loss': 2.3267, 'grad_norm': 3.6755294799804688, 'learning_rate': 0.00010126013027772551, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.78it/s] 20%|██        | 15/75 [00:00<00:02, 25.38it/s]                                               {'loss': 2.3039, 'grad_norm': 11.599931716918945, 'learning_rate': 9.962690237002026e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.38it/s]                                               {'loss': 2.4282, 'grad_norm': 4.259527206420898, 'learning_rate': 9.799367446231501e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.38it/s]                                               {'loss': 1.9996, 'grad_norm': 3.2475690841674805, 'learning_rate': 9.636044655460976e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.38it/s] 24%|██▍       | 18/75 [00:00<00:02, 24.15it/s]                                               {'loss': 2.1717, 'grad_norm': 3.9546539783477783, 'learning_rate': 9.47272186469045e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.15it/s]                                               {'loss': 2.1018, 'grad_norm': 3.36980938911438, 'learning_rate': 9.309399073919926e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.15it/s]                                               {'loss': 2.2293, 'grad_norm': 4.437922954559326, 'learning_rate': 9.146076283149401e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.15it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.16it/s]                                               {'loss': 2.2517, 'grad_norm': 3.872511148452759, 'learning_rate': 8.982753492378876e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.16it/s]                                               {'loss': 2.1494, 'grad_norm': 3.72776460647583, 'learning_rate': 8.81943070160835e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.16it/s]                                               {'loss': 2.1037, 'grad_norm': 5.577014446258545, 'learning_rate': 8.656107910837825e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.16it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.47it/s]                                               {'loss': 2.085, 'grad_norm': 3.8956799507141113, 'learning_rate': 8.492785120067301e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.47it/s]                                               {'loss': 2.2212, 'grad_norm': 3.805863618850708, 'learning_rate': 8.329462329296777e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.47it/s]                                               {'loss': 2.2165, 'grad_norm': 3.5569024085998535, 'learning_rate': 8.16613953852625e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.47it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.77it/s]                                               {'loss': 2.2626, 'grad_norm': 3.936445713043213, 'learning_rate': 8.002816747755725e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.77it/s]                                               {'loss': 2.4487, 'grad_norm': 5.213587284088135, 'learning_rate': 7.839493956985201e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.77it/s]                                               {'loss': 2.2922, 'grad_norm': 3.318575382232666, 'learning_rate': 7.676171166214677e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.77it/s] 40%|████      | 30/75 [00:01<00:01, 26.07it/s]                                               {'loss': 2.4362, 'grad_norm': 20.011253356933594, 'learning_rate': 7.51284837544415e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 26.07it/s]                                               {'loss': 2.0663, 'grad_norm': 3.2208807468414307, 'learning_rate': 7.349525584673625e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.07it/s]                                               {'loss': 2.3064, 'grad_norm': 3.715640068054199, 'learning_rate': 7.186202793903101e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.07it/s] 44%|████▍     | 33/75 [00:01<00:01, 25.68it/s]                                               {'loss': 2.2034, 'grad_norm': 3.2557199001312256, 'learning_rate': 7.022880003132575e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.68it/s]                                               {'loss': 2.3986, 'grad_norm': 3.3806467056274414, 'learning_rate': 6.859557212362051e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.68it/s]                                               {'loss': 2.0572, 'grad_norm': 4.137283802032471, 'learning_rate': 6.696234421591525e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.68it/s] 48%|████▊     | 36/75 [00:01<00:01, 25.53it/s]                                               {'loss': 2.1161, 'grad_norm': 3.908475399017334, 'learning_rate': 6.532911630821001e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.53it/s]                                               {'loss': 2.2011, 'grad_norm': 4.298100471496582, 'learning_rate': 6.369588840050475e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.53it/s]                                               {'loss': 2.3489, 'grad_norm': 4.621621608734131, 'learning_rate': 6.206266049279951e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.53it/s] 52%|█████▏    | 39/75 [00:01<00:01, 24.88it/s]                                               {'loss': 2.0826, 'grad_norm': 3.597391366958618, 'learning_rate': 6.042943258509426e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.88it/s]                                               {'loss': 2.044, 'grad_norm': 4.798910617828369, 'learning_rate': 5.8796204677389e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.88it/s]                                               {'loss': 2.1781, 'grad_norm': 4.011391639709473, 'learning_rate': 5.716297676968376e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.88it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.61it/s]                                               {'loss': 2.2473, 'grad_norm': 3.3513991832733154, 'learning_rate': 5.55297488619785e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.61it/s]                                               {'loss': 2.2837, 'grad_norm': 4.186781406402588, 'learning_rate': 5.389652095427326e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.61it/s]                                               {'loss': 2.1803, 'grad_norm': 3.2583539485931396, 'learning_rate': 5.226329304656801e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.61it/s]                                               {'loss': 2.7296, 'grad_norm': 17.00662612915039, 'learning_rate': 5.063006513886276e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.61it/s] 61%|██████▏   | 46/75 [00:01<00:01, 26.14it/s]                                               {'loss': 2.052, 'grad_norm': 3.1278293132781982, 'learning_rate': 4.8996837231157507e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.14it/s]                                               {'loss': 2.2104, 'grad_norm': 3.564751625061035, 'learning_rate': 4.736360932345225e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.14it/s]                                               {'loss': 2.1803, 'grad_norm': 3.2473251819610596, 'learning_rate': 4.5730381415747006e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.14it/s] 65%|██████▌   | 49/75 [00:01<00:01, 25.80it/s]                                               {'loss': 2.3337, 'grad_norm': 3.999795913696289, 'learning_rate': 4.409715350804175e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.80it/s]                                               {'loss': 2.4308, 'grad_norm': 4.30241584777832, 'learning_rate': 4.2463925600336506e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.80it/s]                                               {'loss': 2.1381, 'grad_norm': 4.741439342498779, 'learning_rate': 4.083069769263125e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.80it/s] 69%|██████▉   | 52/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.1698, 'grad_norm': 3.7521767616271973, 'learning_rate': 3.9197469784926005e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.2514, 'grad_norm': 3.830669403076172, 'learning_rate': 3.756424187722075e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.1971, 'grad_norm': 4.737621784210205, 'learning_rate': 3.5931013969515505e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.33it/s] 73%|███████▎  | 55/75 [00:02<00:00, 24.82it/s]                                               {'loss': 2.0628, 'grad_norm': 3.7383580207824707, 'learning_rate': 3.4297786061810255e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.82it/s]                                               {'loss': 2.2228, 'grad_norm': 3.813699245452881, 'learning_rate': 3.2664558154105004e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.82it/s]                                               {'loss': 2.2447, 'grad_norm': 3.9821627140045166, 'learning_rate': 3.1031330246399754e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.82it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.91it/s]                                               {'loss': 2.2463, 'grad_norm': 3.8670098781585693, 'learning_rate': 2.93981023386945e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.91it/s]                                               {'loss': 1.9849, 'grad_norm': 3.545707941055298, 'learning_rate': 2.776487443098925e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.91it/s]                                               {'loss': 2.3928, 'grad_norm': 8.758180618286133, 'learning_rate': 2.6131646523284003e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.91it/s]                                               {'loss': 2.2554, 'grad_norm': 4.392359256744385, 'learning_rate': 2.4498418615578753e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.91it/s] 83%|████████▎ | 62/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.3862, 'grad_norm': 3.975015878677368, 'learning_rate': 2.2865190707873503e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.0623, 'grad_norm': 3.501722812652588, 'learning_rate': 2.1231962800168253e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.45it/s]                                               {'loss': 2.0088, 'grad_norm': 4.266143798828125, 'learning_rate': 1.9598734892463003e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.45it/s] 87%|████████▋ | 65/75 [00:02<00:00, 24.19it/s]                                               {'loss': 2.0954, 'grad_norm': 3.362550973892212, 'learning_rate': 1.7965506984757752e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.19it/s]                                               {'loss': 2.3837, 'grad_norm': 4.498528957366943, 'learning_rate': 1.6332279077052502e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.19it/s]                                               {'loss': 2.097, 'grad_norm': 3.919851541519165, 'learning_rate': 1.469905116934725e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.19it/s] 91%|█████████ | 68/75 [00:02<00:00, 24.51it/s]                                               {'loss': 2.1221, 'grad_norm': 3.873405694961548, 'learning_rate': 1.3065823261642002e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.51it/s]                                               {'loss': 2.153, 'grad_norm': 3.519765615463257, 'learning_rate': 1.1432595353936752e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.51it/s]                                               {'loss': 2.3597, 'grad_norm': 4.944087028503418, 'learning_rate': 9.799367446231501e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.51it/s] 95%|█████████▍| 71/75 [00:02<00:00, 24.22it/s]                                               {'loss': 1.9879, 'grad_norm': 3.521334409713745, 'learning_rate': 8.166139538526251e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.22it/s]                                               {'loss': 2.0437, 'grad_norm': 3.5873782634735107, 'learning_rate': 6.532911630821001e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.22it/s]                                               {'loss': 2.1474, 'grad_norm': 3.933983564376831, 'learning_rate': 4.899683723115751e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.22it/s] 99%|█████████▊| 74/75 [00:02<00:00, 24.62it/s]                                               {'loss': 2.3935, 'grad_norm': 4.254178047180176, 'learning_rate': 3.2664558154105004e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.62it/s]                                               {'loss': 1.7658, 'grad_norm': 7.122752666473389, 'learning_rate': 1.6332279077052502e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 24.62it/s]                                               {'train_runtime': 3.0919, 'train_samples_per_second': 365.476, 'train_steps_per_second': 24.257, 'train_loss': 2.2164549175898234, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.62it/s]100%|██████████| 75/75 [00:03<00:00, 24.26it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:9
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2398, 'grad_norm': 4.318165302276611, 'learning_rate': 0.00012249209307789376, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 21.03it/s]                                              {'loss': 2.3619, 'grad_norm': 3.653831958770752, 'learning_rate': 0.00012085886517018852, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 21.59it/s]  4%|▍         | 3/75 [00:00<00:03, 23.00it/s]                                              {'loss': 2.4412, 'grad_norm': 5.40608024597168, 'learning_rate': 0.00011922563726248326, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.00it/s]                                              {'loss': 2.4436, 'grad_norm': 4.493546962738037, 'learning_rate': 0.000117592409354778, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 23.00it/s]                                              {'loss': 2.0866, 'grad_norm': 3.4173707962036133, 'learning_rate': 0.00011595918144707276, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 23.00it/s]  8%|▊         | 6/75 [00:00<00:02, 24.20it/s]                                              {'loss': 2.3128, 'grad_norm': 4.106190204620361, 'learning_rate': 0.00011432595353936752, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.20it/s]                                              {'loss': 2.4619, 'grad_norm': 3.7673118114471436, 'learning_rate': 0.00011269272563166226, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.20it/s]                                              {'loss': 2.1997, 'grad_norm': 3.249734401702881, 'learning_rate': 0.000111059497723957, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.20it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.67it/s]                                              {'loss': 2.2065, 'grad_norm': 4.151508808135986, 'learning_rate': 0.00010942626981625176, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.67it/s]                                              {'loss': 2.2651, 'grad_norm': 3.583548069000244, 'learning_rate': 0.00010779304190854651, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.67it/s]                                               {'loss': 2.1487, 'grad_norm': 3.3706324100494385, 'learning_rate': 0.00010615981400084126, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.67it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.73it/s]                                               {'loss': 2.2792, 'grad_norm': 5.198954105377197, 'learning_rate': 0.00010452658609313601, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.73it/s]                                               {'loss': 2.4634, 'grad_norm': 4.362377166748047, 'learning_rate': 0.00010289335818543076, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.73it/s]                                               {'loss': 2.5073, 'grad_norm': 3.6774990558624268, 'learning_rate': 0.00010126013027772551, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.73it/s] 20%|██        | 15/75 [00:00<00:02, 25.74it/s]                                               {'loss': 2.62, 'grad_norm': 7.90254545211792, 'learning_rate': 9.962690237002026e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.74it/s]                                               {'loss': 2.211, 'grad_norm': 2.9337313175201416, 'learning_rate': 9.799367446231501e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.74it/s]                                               {'loss': 2.1666, 'grad_norm': 4.6777424812316895, 'learning_rate': 9.636044655460976e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.74it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.25it/s]                                               {'loss': 2.3659, 'grad_norm': 3.4525721073150635, 'learning_rate': 9.47272186469045e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.25it/s]                                               {'loss': 2.2563, 'grad_norm': 4.305241107940674, 'learning_rate': 9.309399073919926e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.25it/s]                                               {'loss': 2.2341, 'grad_norm': 5.030282974243164, 'learning_rate': 9.146076283149401e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.25it/s] 28%|██▊       | 21/75 [00:00<00:02, 24.87it/s]                                               {'loss': 2.223, 'grad_norm': 4.109737873077393, 'learning_rate': 8.982753492378876e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.87it/s]                                               {'loss': 2.3018, 'grad_norm': 4.108894348144531, 'learning_rate': 8.81943070160835e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.87it/s]                                               {'loss': 2.3082, 'grad_norm': 3.743677854537964, 'learning_rate': 8.656107910837825e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.87it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.83it/s]                                               {'loss': 2.1271, 'grad_norm': 3.470592975616455, 'learning_rate': 8.492785120067301e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.83it/s]                                               {'loss': 2.0479, 'grad_norm': 3.7366716861724854, 'learning_rate': 8.329462329296777e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.83it/s]                                               {'loss': 2.4857, 'grad_norm': 3.9444334506988525, 'learning_rate': 8.16613953852625e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.83it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.69it/s]                                               {'loss': 2.3518, 'grad_norm': 4.35511589050293, 'learning_rate': 8.002816747755725e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.69it/s]                                               {'loss': 2.2126, 'grad_norm': 4.066681385040283, 'learning_rate': 7.839493956985201e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.69it/s]                                               {'loss': 2.3422, 'grad_norm': 4.346507549285889, 'learning_rate': 7.676171166214677e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.69it/s]                                               {'loss': 2.5051, 'grad_norm': 11.774142265319824, 'learning_rate': 7.51284837544415e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.69it/s] 41%|████▏     | 31/75 [00:01<00:01, 25.58it/s]                                               {'loss': 2.113, 'grad_norm': 4.126550197601318, 'learning_rate': 7.349525584673625e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.58it/s]                                               {'loss': 2.3216, 'grad_norm': 3.5650522708892822, 'learning_rate': 7.186202793903101e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.58it/s]                                               {'loss': 2.5086, 'grad_norm': 4.417179584503174, 'learning_rate': 7.022880003132575e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.58it/s] 45%|████▌     | 34/75 [00:01<00:01, 25.51it/s]                                               {'loss': 2.2583, 'grad_norm': 3.7626750469207764, 'learning_rate': 6.859557212362051e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.51it/s]                                               {'loss': 2.0953, 'grad_norm': 2.8357250690460205, 'learning_rate': 6.696234421591525e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.51it/s]                                               {'loss': 2.3732, 'grad_norm': 3.6623034477233887, 'learning_rate': 6.532911630821001e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.51it/s] 49%|████▉     | 37/75 [00:01<00:01, 23.38it/s]                                               {'loss': 2.1615, 'grad_norm': 3.4188499450683594, 'learning_rate': 6.369588840050475e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 23.38it/s]                                               {'loss': 2.2766, 'grad_norm': 4.316882133483887, 'learning_rate': 6.206266049279951e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 23.38it/s]                                               {'loss': 2.2382, 'grad_norm': 4.08267879486084, 'learning_rate': 6.042943258509426e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.38it/s] 53%|█████▎    | 40/75 [00:01<00:01, 23.74it/s]                                               {'loss': 2.2785, 'grad_norm': 3.2625834941864014, 'learning_rate': 5.8796204677389e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.74it/s]                                               {'loss': 2.3472, 'grad_norm': 6.071168899536133, 'learning_rate': 5.716297676968376e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 23.74it/s]                                               {'loss': 2.0641, 'grad_norm': 2.8071823120117188, 'learning_rate': 5.55297488619785e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 23.74it/s] 57%|█████▋    | 43/75 [00:01<00:01, 24.02it/s]                                               {'loss': 2.307, 'grad_norm': 5.934363842010498, 'learning_rate': 5.389652095427326e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.02it/s]                                               {'loss': 2.1206, 'grad_norm': 3.5214552879333496, 'learning_rate': 5.226329304656801e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.02it/s]                                               {'loss': 2.3979, 'grad_norm': 10.602810859680176, 'learning_rate': 5.063006513886276e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.02it/s] 61%|██████▏   | 46/75 [00:01<00:01, 25.52it/s]                                               {'loss': 2.3314, 'grad_norm': 4.213444709777832, 'learning_rate': 4.8996837231157507e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.52it/s]                                               {'loss': 2.5742, 'grad_norm': 4.2974534034729, 'learning_rate': 4.736360932345225e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.52it/s]                                               {'loss': 2.2095, 'grad_norm': 4.226650714874268, 'learning_rate': 4.5730381415747006e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.52it/s] 65%|██████▌   | 49/75 [00:01<00:01, 25.57it/s]                                               {'loss': 2.1128, 'grad_norm': 4.155025482177734, 'learning_rate': 4.409715350804175e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.57it/s]                                               {'loss': 2.2569, 'grad_norm': 4.9647955894470215, 'learning_rate': 4.2463925600336506e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.57it/s]                                               {'loss': 2.4084, 'grad_norm': 4.738175868988037, 'learning_rate': 4.083069769263125e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.57it/s] 69%|██████▉   | 52/75 [00:02<00:00, 25.52it/s]                                               {'loss': 2.2375, 'grad_norm': 4.328708648681641, 'learning_rate': 3.9197469784926005e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.52it/s]                                               {'loss': 2.0819, 'grad_norm': 3.253969192504883, 'learning_rate': 3.756424187722075e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.52it/s]                                               {'loss': 2.1856, 'grad_norm': 3.121396064758301, 'learning_rate': 3.5931013969515505e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.52it/s] 73%|███████▎  | 55/75 [00:02<00:00, 24.59it/s]                                               {'loss': 2.0186, 'grad_norm': 3.533460855484009, 'learning_rate': 3.4297786061810255e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.59it/s]                                               {'loss': 2.0593, 'grad_norm': 3.2170023918151855, 'learning_rate': 3.2664558154105004e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.59it/s]                                               {'loss': 2.1043, 'grad_norm': 3.3374619483947754, 'learning_rate': 3.1031330246399754e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.59it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.41it/s]                                               {'loss': 2.2694, 'grad_norm': 3.1271467208862305, 'learning_rate': 2.93981023386945e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.41it/s]                                               {'loss': 2.1934, 'grad_norm': 3.646206855773926, 'learning_rate': 2.776487443098925e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.41it/s]                                               {'loss': 2.8766, 'grad_norm': 17.647104263305664, 'learning_rate': 2.6131646523284003e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.41it/s]                                               {'loss': 2.1618, 'grad_norm': 2.9156529903411865, 'learning_rate': 2.4498418615578753e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.41it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.29it/s]                                               {'loss': 2.2412, 'grad_norm': 3.3975560665130615, 'learning_rate': 2.2865190707873503e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.29it/s]                                               {'loss': 2.4825, 'grad_norm': 4.771855354309082, 'learning_rate': 2.1231962800168253e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.29it/s]                                               {'loss': 2.2137, 'grad_norm': 3.3979604244232178, 'learning_rate': 1.9598734892463003e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.29it/s] 87%|████████▋ | 65/75 [00:02<00:00, 24.72it/s]                                               {'loss': 2.1587, 'grad_norm': 3.779679298400879, 'learning_rate': 1.7965506984757752e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.72it/s]                                               {'loss': 2.2036, 'grad_norm': 6.310225963592529, 'learning_rate': 1.6332279077052502e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.72it/s]                                               {'loss': 2.3429, 'grad_norm': 3.689565420150757, 'learning_rate': 1.469905116934725e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.72it/s] 91%|█████████ | 68/75 [00:02<00:00, 24.01it/s]                                               {'loss': 2.2391, 'grad_norm': 5.009048938751221, 'learning_rate': 1.3065823261642002e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.01it/s]                                               {'loss': 2.2841, 'grad_norm': 4.184817790985107, 'learning_rate': 1.1432595353936752e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.01it/s]                                               {'loss': 2.1578, 'grad_norm': 3.7587318420410156, 'learning_rate': 9.799367446231501e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.01it/s] 95%|█████████▍| 71/75 [00:02<00:00, 24.33it/s]                                               {'loss': 1.9855, 'grad_norm': 3.4453542232513428, 'learning_rate': 8.166139538526251e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.1712, 'grad_norm': 3.400834560394287, 'learning_rate': 6.532911630821001e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.2738, 'grad_norm': 3.863765239715576, 'learning_rate': 4.899683723115751e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.33it/s] 99%|█████████▊| 74/75 [00:03<00:00, 22.65it/s]                                               {'loss': 2.2003, 'grad_norm': 4.862368583679199, 'learning_rate': 3.2664558154105004e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 22.65it/s]                                               {'loss': 1.5317, 'grad_norm': 7.257989406585693, 'learning_rate': 1.6332279077052502e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 22.65it/s]                                               {'train_runtime': 3.1929, 'train_samples_per_second': 353.913, 'train_steps_per_second': 23.49, 'train_loss': 2.2614112345377606, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 22.65it/s]100%|██████████| 75/75 [00:03<00:00, 23.52it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:47
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.25, 'grad_norm': 4.630809783935547, 'learning_rate': 0.00012249209307789376, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.61it/s]                                              {'loss': 2.4094, 'grad_norm': 3.8071603775024414, 'learning_rate': 0.00012085886517018852, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.94it/s]  4%|▍         | 3/75 [00:00<00:03, 23.78it/s]                                              {'loss': 2.4607, 'grad_norm': 4.287560939788818, 'learning_rate': 0.00011922563726248326, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.78it/s]                                              {'loss': 2.3448, 'grad_norm': 4.283799648284912, 'learning_rate': 0.000117592409354778, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 23.78it/s]                                              {'loss': 2.4466, 'grad_norm': 3.691829204559326, 'learning_rate': 0.00011595918144707276, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.78it/s]  8%|▊         | 6/75 [00:00<00:02, 23.90it/s]                                              {'loss': 2.2422, 'grad_norm': 4.225291728973389, 'learning_rate': 0.00011432595353936752, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.90it/s]                                              {'loss': 2.3616, 'grad_norm': 4.036447048187256, 'learning_rate': 0.00011269272563166226, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.90it/s]                                              {'loss': 2.214, 'grad_norm': 4.379092693328857, 'learning_rate': 0.000111059497723957, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.90it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.19it/s]                                              {'loss': 2.3362, 'grad_norm': 4.552657127380371, 'learning_rate': 0.00010942626981625176, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.19it/s]                                              {'loss': 2.4182, 'grad_norm': 3.444110870361328, 'learning_rate': 0.00010779304190854651, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.19it/s]                                               {'loss': 2.5146, 'grad_norm': 4.168174743652344, 'learning_rate': 0.00010615981400084126, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.19it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.47it/s]                                               {'loss': 2.1516, 'grad_norm': 4.037045478820801, 'learning_rate': 0.00010452658609313601, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.47it/s]                                               {'loss': 2.4399, 'grad_norm': 4.3185553550720215, 'learning_rate': 0.00010289335818543076, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.47it/s]                                               {'loss': 2.279, 'grad_norm': 3.8062567710876465, 'learning_rate': 0.00010126013027772551, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.47it/s] 20%|██        | 15/75 [00:00<00:02, 23.26it/s]                                               {'loss': 2.6778, 'grad_norm': 11.306177139282227, 'learning_rate': 9.962690237002026e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.26it/s]                                               {'loss': 2.3238, 'grad_norm': 4.302721977233887, 'learning_rate': 9.799367446231501e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 23.26it/s]                                               {'loss': 2.3964, 'grad_norm': 4.661022186279297, 'learning_rate': 9.636044655460976e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 23.26it/s] 24%|██▍       | 18/75 [00:00<00:02, 23.21it/s]                                               {'loss': 2.2882, 'grad_norm': 4.179481029510498, 'learning_rate': 9.47272186469045e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 23.21it/s]                                               {'loss': 2.1509, 'grad_norm': 3.8759639263153076, 'learning_rate': 9.309399073919926e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 23.21it/s]                                               {'loss': 2.4837, 'grad_norm': 5.828006744384766, 'learning_rate': 9.146076283149401e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 23.21it/s] 28%|██▊       | 21/75 [00:00<00:02, 23.64it/s]                                               {'loss': 2.218, 'grad_norm': 3.998696804046631, 'learning_rate': 8.982753492378876e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 23.64it/s]                                               {'loss': 2.2168, 'grad_norm': 4.333856582641602, 'learning_rate': 8.81943070160835e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 23.64it/s]                                               {'loss': 2.3823, 'grad_norm': 3.518094778060913, 'learning_rate': 8.656107910837825e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 23.64it/s] 32%|███▏      | 24/75 [00:01<00:02, 23.72it/s]                                               {'loss': 2.2329, 'grad_norm': 3.707244396209717, 'learning_rate': 8.492785120067301e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 23.72it/s]                                               {'loss': 2.4611, 'grad_norm': 4.382444858551025, 'learning_rate': 8.329462329296777e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.72it/s]                                               {'loss': 2.0921, 'grad_norm': 3.2764413356781006, 'learning_rate': 8.16613953852625e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.72it/s] 36%|███▌      | 27/75 [00:01<00:02, 21.83it/s]                                               {'loss': 2.2512, 'grad_norm': 4.800823211669922, 'learning_rate': 8.002816747755725e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 21.83it/s]                                               {'loss': 2.1873, 'grad_norm': 4.981449127197266, 'learning_rate': 7.839493956985201e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 21.83it/s]                                               {'loss': 2.3869, 'grad_norm': 3.8340704441070557, 'learning_rate': 7.676171166214677e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 21.83it/s] 40%|████      | 30/75 [00:01<00:01, 23.30it/s]                                               {'loss': 2.5224, 'grad_norm': 10.18516731262207, 'learning_rate': 7.51284837544415e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.30it/s]                                               {'loss': 2.0496, 'grad_norm': 3.6576364040374756, 'learning_rate': 7.349525584673625e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 23.30it/s]                                               {'loss': 2.1876, 'grad_norm': 4.031735897064209, 'learning_rate': 7.186202793903101e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 23.30it/s] 44%|████▍     | 33/75 [00:01<00:01, 23.31it/s]                                               {'loss': 2.3979, 'grad_norm': 3.519029140472412, 'learning_rate': 7.022880003132575e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 23.31it/s]                                               {'loss': 2.25, 'grad_norm': 5.71940279006958, 'learning_rate': 6.859557212362051e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 23.31it/s]                                               {'loss': 2.3589, 'grad_norm': 4.5937581062316895, 'learning_rate': 6.696234421591525e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 23.31it/s] 48%|████▊     | 36/75 [00:01<00:01, 23.52it/s]                                               {'loss': 2.5391, 'grad_norm': 4.8709001541137695, 'learning_rate': 6.532911630821001e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 23.52it/s]                                               {'loss': 2.1197, 'grad_norm': 3.523662567138672, 'learning_rate': 6.369588840050475e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 23.52it/s]                                               {'loss': 2.1104, 'grad_norm': 2.908808469772339, 'learning_rate': 6.206266049279951e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 23.52it/s] 52%|█████▏    | 39/75 [00:01<00:01, 23.20it/s]                                               {'loss': 2.1645, 'grad_norm': 3.879260301589966, 'learning_rate': 6.042943258509426e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.20it/s]                                               {'loss': 2.2324, 'grad_norm': 4.1283674240112305, 'learning_rate': 5.8796204677389e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.20it/s]                                               {'loss': 2.2792, 'grad_norm': 3.773383378982544, 'learning_rate': 5.716297676968376e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 23.20it/s] 56%|█████▌    | 42/75 [00:01<00:01, 23.83it/s]                                               {'loss': 2.4615, 'grad_norm': 3.140465021133423, 'learning_rate': 5.55297488619785e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 23.83it/s]                                               {'loss': 2.1037, 'grad_norm': 4.265197277069092, 'learning_rate': 5.389652095427326e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 23.83it/s]                                               {'loss': 2.4131, 'grad_norm': 5.782740592956543, 'learning_rate': 5.226329304656801e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 23.83it/s] 60%|██████    | 45/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.3609, 'grad_norm': 15.227299690246582, 'learning_rate': 5.063006513886276e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.2508, 'grad_norm': 3.2747528553009033, 'learning_rate': 4.8996837231157507e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.56it/s]                                               {'loss': 2.2544, 'grad_norm': 3.864650249481201, 'learning_rate': 4.736360932345225e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 24.56it/s] 64%|██████▍   | 48/75 [00:02<00:01, 22.76it/s]                                               {'loss': 2.1406, 'grad_norm': 3.774658679962158, 'learning_rate': 4.5730381415747006e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 22.76it/s]                                               {'loss': 2.2938, 'grad_norm': 4.571779251098633, 'learning_rate': 4.409715350804175e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 22.76it/s]                                               {'loss': 2.3545, 'grad_norm': 5.098635196685791, 'learning_rate': 4.2463925600336506e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 22.76it/s] 68%|██████▊   | 51/75 [00:02<00:01, 22.35it/s]                                               {'loss': 2.1619, 'grad_norm': 4.0268235206604, 'learning_rate': 4.083069769263125e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 22.35it/s]                                               {'loss': 2.4534, 'grad_norm': 3.8729302883148193, 'learning_rate': 3.9197469784926005e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 22.35it/s]                                               {'loss': 2.1088, 'grad_norm': 3.2789127826690674, 'learning_rate': 3.756424187722075e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 22.35it/s] 72%|███████▏  | 54/75 [00:02<00:00, 21.25it/s]                                               {'loss': 2.2932, 'grad_norm': 4.689682483673096, 'learning_rate': 3.5931013969515505e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 21.25it/s]                                               {'loss': 2.5025, 'grad_norm': 4.226590156555176, 'learning_rate': 3.4297786061810255e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 21.25it/s]                                               {'loss': 2.2599, 'grad_norm': 4.298151969909668, 'learning_rate': 3.2664558154105004e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 21.25it/s] 76%|███████▌  | 57/75 [00:02<00:00, 22.33it/s]                                               {'loss': 2.1607, 'grad_norm': 4.096433162689209, 'learning_rate': 3.1031330246399754e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 22.33it/s]                                               {'loss': 2.1354, 'grad_norm': 3.8971424102783203, 'learning_rate': 2.93981023386945e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 22.33it/s]                                               {'loss': 2.3216, 'grad_norm': 4.640312194824219, 'learning_rate': 2.776487443098925e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 22.33it/s]                                               {'loss': 1.8097, 'grad_norm': 12.987706184387207, 'learning_rate': 2.6131646523284003e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 22.33it/s] 81%|████████▏ | 61/75 [00:02<00:00, 24.37it/s]                                               {'loss': 2.407, 'grad_norm': 5.149558067321777, 'learning_rate': 2.4498418615578753e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.37it/s]                                               {'loss': 2.2157, 'grad_norm': 3.573176622390747, 'learning_rate': 2.2865190707873503e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 24.37it/s]                                               {'loss': 2.1576, 'grad_norm': 4.727207183837891, 'learning_rate': 2.1231962800168253e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 24.37it/s] 85%|████████▌ | 64/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.1736, 'grad_norm': 2.6415321826934814, 'learning_rate': 1.9598734892463003e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.1687, 'grad_norm': 3.652986764907837, 'learning_rate': 1.7965506984757752e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.0918, 'grad_norm': 2.9845573902130127, 'learning_rate': 1.6332279077052502e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.33it/s] 89%|████████▉ | 67/75 [00:02<00:00, 24.52it/s]                                               {'loss': 2.2982, 'grad_norm': 4.1646599769592285, 'learning_rate': 1.469905116934725e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.52it/s]                                               {'loss': 2.0844, 'grad_norm': 4.781774520874023, 'learning_rate': 1.3065823261642002e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.52it/s]                                               {'loss': 2.2325, 'grad_norm': 3.93251371383667, 'learning_rate': 1.1432595353936752e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.52it/s] 93%|█████████▎| 70/75 [00:02<00:00, 23.93it/s]                                               {'loss': 2.1183, 'grad_norm': 4.0982441902160645, 'learning_rate': 9.799367446231501e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 23.93it/s]                                               {'loss': 2.2927, 'grad_norm': 4.016873836517334, 'learning_rate': 8.166139538526251e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 23.93it/s]                                               {'loss': 2.451, 'grad_norm': 3.78633451461792, 'learning_rate': 6.532911630821001e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 23.93it/s] 97%|█████████▋| 73/75 [00:03<00:00, 24.23it/s]                                               {'loss': 2.2729, 'grad_norm': 4.1806793212890625, 'learning_rate': 4.899683723115751e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 24.23it/s]                                               {'loss': 2.2511, 'grad_norm': 3.8926618099212646, 'learning_rate': 3.2664558154105004e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.23it/s]                                               {'loss': 2.3465, 'grad_norm': 10.544415473937988, 'learning_rate': 1.6332279077052502e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.23it/s]                                               {'train_runtime': 3.2687, 'train_samples_per_second': 345.706, 'train_steps_per_second': 22.945, 'train_loss': 2.2830994288126627, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.23it/s]100%|██████████| 75/75 [00:03<00:00, 22.95it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3417, 'grad_norm': 5.443821907043457, 'learning_rate': 0.00012249209307789376, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 16.25it/s]  3%|▎         | 2/75 [00:00<00:03, 18.98it/s]                                              {'loss': 2.147, 'grad_norm': 3.9408836364746094, 'learning_rate': 0.00012085886517018852, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 18.98it/s]                                              {'loss': 2.2398, 'grad_norm': 3.2357728481292725, 'learning_rate': 0.00011922563726248326, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 18.98it/s]                                              {'loss': 2.2232, 'grad_norm': 3.7084920406341553, 'learning_rate': 0.000117592409354778, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 18.98it/s]  7%|▋         | 5/75 [00:00<00:03, 22.74it/s]                                              {'loss': 2.3381, 'grad_norm': 4.712430000305176, 'learning_rate': 0.00011595918144707276, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.74it/s]                                              {'loss': 2.1122, 'grad_norm': 5.190765380859375, 'learning_rate': 0.00011432595353936752, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 22.74it/s]                                              {'loss': 2.2545, 'grad_norm': 4.838453769683838, 'learning_rate': 0.00011269272563166226, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 22.74it/s] 11%|█         | 8/75 [00:00<00:02, 23.54it/s]                                              {'loss': 2.212, 'grad_norm': 3.5013177394866943, 'learning_rate': 0.000111059497723957, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.54it/s]                                              {'loss': 2.0472, 'grad_norm': 3.3229761123657227, 'learning_rate': 0.00010942626981625176, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.54it/s]                                              {'loss': 2.1373, 'grad_norm': 4.47111177444458, 'learning_rate': 0.00010779304190854651, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.54it/s] 15%|█▍        | 11/75 [00:00<00:02, 24.11it/s]                                               {'loss': 2.3706, 'grad_norm': 3.4906914234161377, 'learning_rate': 0.00010615981400084126, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.11it/s]                                               {'loss': 2.3066, 'grad_norm': 3.3649022579193115, 'learning_rate': 0.00010452658609313601, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.11it/s]                                               {'loss': 2.4206, 'grad_norm': 5.780459880828857, 'learning_rate': 0.00010289335818543076, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.11it/s] 19%|█▊        | 14/75 [00:00<00:02, 23.54it/s]                                               {'loss': 2.3401, 'grad_norm': 4.780303955078125, 'learning_rate': 0.00010126013027772551, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.54it/s]                                               {'loss': 2.7719, 'grad_norm': 19.996360778808594, 'learning_rate': 9.962690237002026e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.54it/s]                                               {'loss': 2.0763, 'grad_norm': 4.033660888671875, 'learning_rate': 9.799367446231501e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 23.54it/s]                                               {'loss': 2.4091, 'grad_norm': 4.427636623382568, 'learning_rate': 9.636044655460976e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 23.54it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.79it/s]                                               {'loss': 2.1339, 'grad_norm': 4.0144524574279785, 'learning_rate': 9.47272186469045e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.79it/s]                                               {'loss': 1.9685, 'grad_norm': 3.3619065284729004, 'learning_rate': 9.309399073919926e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.79it/s]                                               {'loss': 2.1969, 'grad_norm': 3.1764562129974365, 'learning_rate': 9.146076283149401e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.79it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.67it/s]                                               {'loss': 2.4174, 'grad_norm': 5.649180889129639, 'learning_rate': 8.982753492378876e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.67it/s]                                               {'loss': 2.3161, 'grad_norm': 4.5934295654296875, 'learning_rate': 8.81943070160835e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.67it/s]                                               {'loss': 2.2237, 'grad_norm': 3.5491647720336914, 'learning_rate': 8.656107910837825e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.67it/s] 32%|███▏      | 24/75 [00:00<00:01, 25.86it/s]                                               {'loss': 2.1259, 'grad_norm': 3.590341567993164, 'learning_rate': 8.492785120067301e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.86it/s]                                               {'loss': 2.2963, 'grad_norm': 3.8399581909179688, 'learning_rate': 8.329462329296777e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:01, 25.86it/s]                                               {'loss': 2.3128, 'grad_norm': 3.6694679260253906, 'learning_rate': 8.16613953852625e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.86it/s] 36%|███▌      | 27/75 [00:01<00:01, 25.14it/s]                                               {'loss': 2.0899, 'grad_norm': 3.0693345069885254, 'learning_rate': 8.002816747755725e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.14it/s]                                               {'loss': 1.9525, 'grad_norm': 4.036799907684326, 'learning_rate': 7.839493956985201e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.14it/s]                                               {'loss': 2.1311, 'grad_norm': 3.7749252319335938, 'learning_rate': 7.676171166214677e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.14it/s]                                               {'loss': 2.4815, 'grad_norm': 13.489187240600586, 'learning_rate': 7.51284837544415e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.14it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.95it/s]                                               {'loss': 2.2022, 'grad_norm': 3.3151516914367676, 'learning_rate': 7.349525584673625e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.95it/s]                                               {'loss': 2.3149, 'grad_norm': 4.138952255249023, 'learning_rate': 7.186202793903101e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.95it/s]                                               {'loss': 2.1888, 'grad_norm': 3.383509635925293, 'learning_rate': 7.022880003132575e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.95it/s] 45%|████▌     | 34/75 [00:01<00:01, 26.59it/s]                                               {'loss': 2.0077, 'grad_norm': 3.880983352661133, 'learning_rate': 6.859557212362051e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.59it/s]                                               {'loss': 2.1491, 'grad_norm': 4.234176158905029, 'learning_rate': 6.696234421591525e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.59it/s]                                               {'loss': 2.0666, 'grad_norm': 3.0989668369293213, 'learning_rate': 6.532911630821001e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.59it/s] 49%|████▉     | 37/75 [00:01<00:01, 26.26it/s]                                               {'loss': 2.0302, 'grad_norm': 4.738647937774658, 'learning_rate': 6.369588840050475e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.26it/s]                                               {'loss': 2.0571, 'grad_norm': 3.538884162902832, 'learning_rate': 6.206266049279951e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.26it/s]                                               {'loss': 2.2001, 'grad_norm': 4.389058589935303, 'learning_rate': 6.042943258509426e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.26it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.2597, 'grad_norm': 4.401700496673584, 'learning_rate': 5.8796204677389e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.3078, 'grad_norm': 4.988253593444824, 'learning_rate': 5.716297676968376e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.2397, 'grad_norm': 3.6046619415283203, 'learning_rate': 5.55297488619785e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.62it/s] 57%|█████▋    | 43/75 [00:01<00:01, 25.35it/s]                                               {'loss': 2.2814, 'grad_norm': 3.2831475734710693, 'learning_rate': 5.389652095427326e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.35it/s]                                               {'loss': 2.0733, 'grad_norm': 4.322916030883789, 'learning_rate': 5.226329304656801e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.35it/s]                                               {'loss': 2.5768, 'grad_norm': 11.5382661819458, 'learning_rate': 5.063006513886276e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.35it/s]                                               {'loss': 2.0001, 'grad_norm': 3.1385269165039062, 'learning_rate': 4.8996837231157507e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.35it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.74it/s]                                               {'loss': 2.2661, 'grad_norm': 4.032170295715332, 'learning_rate': 4.736360932345225e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.74it/s]                                               {'loss': 2.0042, 'grad_norm': 3.0257999897003174, 'learning_rate': 4.5730381415747006e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.74it/s]                                               {'loss': 2.1303, 'grad_norm': 3.5700488090515137, 'learning_rate': 4.409715350804175e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.74it/s] 67%|██████▋   | 50/75 [00:01<00:00, 26.40it/s]                                               {'loss': 2.1761, 'grad_norm': 3.7778680324554443, 'learning_rate': 4.2463925600336506e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.40it/s]                                               {'loss': 2.1048, 'grad_norm': 3.4215853214263916, 'learning_rate': 4.083069769263125e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 26.40it/s]                                               {'loss': 2.0308, 'grad_norm': 2.705676317214966, 'learning_rate': 3.9197469784926005e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 26.40it/s] 71%|███████   | 53/75 [00:02<00:00, 26.07it/s]                                               {'loss': 2.2189, 'grad_norm': 4.179660797119141, 'learning_rate': 3.756424187722075e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 26.07it/s]                                               {'loss': 2.3116, 'grad_norm': 6.322704792022705, 'learning_rate': 3.5931013969515505e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 26.07it/s]                                               {'loss': 2.2038, 'grad_norm': 3.745150089263916, 'learning_rate': 3.4297786061810255e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 26.07it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.83it/s]                                               {'loss': 2.2552, 'grad_norm': 3.8010008335113525, 'learning_rate': 3.2664558154105004e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.83it/s]                                               {'loss': 2.1163, 'grad_norm': 4.384283542633057, 'learning_rate': 3.1031330246399754e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.83it/s]                                               {'loss': 2.1507, 'grad_norm': 5.268373966217041, 'learning_rate': 2.93981023386945e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.83it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.76it/s]                                               {'loss': 2.0958, 'grad_norm': 4.924665451049805, 'learning_rate': 2.776487443098925e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.76it/s]                                               {'loss': 2.2317, 'grad_norm': 10.30618953704834, 'learning_rate': 2.6131646523284003e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.76it/s]                                               {'loss': 2.2647, 'grad_norm': 3.9153642654418945, 'learning_rate': 2.4498418615578753e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.76it/s]                                               {'loss': 2.239, 'grad_norm': 2.926297903060913, 'learning_rate': 2.2865190707873503e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.76it/s] 84%|████████▍ | 63/75 [00:02<00:00, 27.16it/s]                                               {'loss': 2.3106, 'grad_norm': 3.6891932487487793, 'learning_rate': 2.1231962800168253e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 27.16it/s]                                               {'loss': 2.0495, 'grad_norm': 4.000565528869629, 'learning_rate': 1.9598734892463003e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 27.16it/s]                                               {'loss': 2.1255, 'grad_norm': 4.05513334274292, 'learning_rate': 1.7965506984757752e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 27.16it/s] 88%|████████▊ | 66/75 [00:02<00:00, 24.21it/s]                                               {'loss': 1.9908, 'grad_norm': 2.861027717590332, 'learning_rate': 1.6332279077052502e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.21it/s]                                               {'loss': 2.2103, 'grad_norm': 3.937859058380127, 'learning_rate': 1.469905116934725e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.21it/s]                                               {'loss': 2.0834, 'grad_norm': 3.9869961738586426, 'learning_rate': 1.3065823261642002e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.21it/s] 92%|█████████▏| 69/75 [00:02<00:00, 22.62it/s]                                               {'loss': 2.2055, 'grad_norm': 4.72480583190918, 'learning_rate': 1.1432595353936752e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 22.62it/s]                                               {'loss': 2.1557, 'grad_norm': 3.316776990890503, 'learning_rate': 9.799367446231501e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 22.62it/s]                                               {'loss': 2.2461, 'grad_norm': 4.134573936462402, 'learning_rate': 8.166139538526251e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 22.62it/s] 96%|█████████▌| 72/75 [00:02<00:00, 21.61it/s]                                               {'loss': 2.0249, 'grad_norm': 4.324875831604004, 'learning_rate': 6.532911630821001e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 21.61it/s]                                               {'loss': 2.0654, 'grad_norm': 3.234515428543091, 'learning_rate': 4.899683723115751e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 21.61it/s]                                               {'loss': 2.1424, 'grad_norm': 3.3600382804870605, 'learning_rate': 3.2664558154105004e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 21.61it/s]100%|██████████| 75/75 [00:03<00:00, 21.90it/s]                                               {'loss': 2.0634, 'grad_norm': 8.957633018493652, 'learning_rate': 1.6332279077052502e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 21.90it/s]                                               {'train_runtime': 3.1779, 'train_samples_per_second': 355.584, 'train_steps_per_second': 23.601, 'train_loss': 2.1976515833536783, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 21.90it/s]100%|██████████| 75/75 [00:03<00:00, 23.60it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(939, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(871, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(747, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(852, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1084, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(829, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(943, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(786, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(823, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(532, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(982, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(489, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 64.10it/s]  3%|▎         | 14/471 [00:00<00:07, 57.77it/s]  4%|▍         | 20/471 [00:00<00:08, 55.16it/s]  6%|▌         | 26/471 [00:00<00:08, 53.66it/s]  7%|▋         | 32/471 [00:00<00:08, 53.28it/s]  8%|▊         | 38/471 [00:00<00:08, 51.88it/s]  9%|▉         | 44/471 [00:00<00:08, 52.86it/s] 11%|█         | 50/471 [00:00<00:07, 52.91it/s] 12%|█▏        | 56/471 [00:01<00:07, 53.21it/s] 13%|█▎        | 62/471 [00:01<00:07, 53.19it/s] 14%|█▍        | 68/471 [00:01<00:07, 53.08it/s] 16%|█▌        | 74/471 [00:01<00:07, 53.29it/s] 17%|█▋        | 80/471 [00:01<00:07, 53.15it/s] 18%|█▊        | 86/471 [00:01<00:07, 53.16it/s] 20%|█▉        | 92/471 [00:01<00:07, 53.36it/s] 21%|██        | 98/471 [00:01<00:07, 52.27it/s] 22%|██▏       | 104/471 [00:01<00:07, 51.59it/s] 23%|██▎       | 110/471 [00:02<00:07, 51.40it/s] 25%|██▍       | 116/471 [00:02<00:07, 49.75it/s] 26%|██▌       | 122/471 [00:02<00:06, 50.24it/s] 27%|██▋       | 128/471 [00:02<00:06, 50.80it/s] 28%|██▊       | 134/471 [00:02<00:06, 51.05it/s] 30%|██▉       | 140/471 [00:02<00:06, 49.67it/s] 31%|███       | 145/471 [00:02<00:06, 49.64it/s] 32%|███▏      | 151/471 [00:02<00:06, 50.25it/s] 33%|███▎      | 157/471 [00:03<00:06, 50.74it/s] 35%|███▍      | 163/471 [00:03<00:06, 50.75it/s] 36%|███▌      | 169/471 [00:03<00:05, 50.52it/s] 37%|███▋      | 175/471 [00:03<00:05, 51.11it/s] 38%|███▊      | 181/471 [00:03<00:05, 51.14it/s] 40%|███▉      | 187/471 [00:03<00:05, 51.45it/s] 41%|████      | 193/471 [00:03<00:05, 51.43it/s] 42%|████▏     | 199/471 [00:03<00:05, 51.37it/s] 44%|████▎     | 205/471 [00:03<00:05, 51.83it/s] 45%|████▍     | 211/471 [00:04<00:05, 49.84it/s] 46%|████▌     | 217/471 [00:04<00:05, 48.17it/s] 47%|████▋     | 222/471 [00:04<00:05, 48.25it/s] 48%|████▊     | 227/471 [00:04<00:05, 48.08it/s] 49%|████▉     | 233/471 [00:04<00:04, 49.85it/s] 51%|█████     | 239/471 [00:04<00:04, 50.11it/s] 52%|█████▏    | 245/471 [00:04<00:04, 49.94it/s] 53%|█████▎    | 250/471 [00:04<00:04, 45.44it/s] 54%|█████▍    | 256/471 [00:05<00:04, 46.81it/s] 55%|█████▌    | 261/471 [00:05<00:04, 47.10it/s] 57%|█████▋    | 267/471 [00:05<00:04, 49.07it/s] 58%|█████▊    | 273/471 [00:05<00:04, 49.29it/s] 59%|█████▉    | 278/471 [00:05<00:04, 48.00it/s] 60%|██████    | 284/471 [00:05<00:03, 49.45it/s] 62%|██████▏   | 290/471 [00:05<00:03, 50.11it/s] 63%|██████▎   | 296/471 [00:05<00:03, 49.73it/s] 64%|██████▍   | 301/471 [00:05<00:03, 48.24it/s] 65%|██████▌   | 307/471 [00:06<00:03, 49.64it/s] 66%|██████▋   | 313/471 [00:06<00:03, 50.25it/s] 68%|██████▊   | 319/471 [00:06<00:02, 51.20it/s] 69%|██████▉   | 325/471 [00:06<00:02, 51.16it/s] 70%|███████   | 331/471 [00:06<00:02, 51.13it/s] 72%|███████▏  | 337/471 [00:06<00:02, 49.97it/s] 73%|███████▎  | 343/471 [00:06<00:02, 51.00it/s] 74%|███████▍  | 349/471 [00:06<00:02, 51.21it/s] 75%|███████▌  | 355/471 [00:06<00:02, 50.25it/s] 77%|███████▋  | 361/471 [00:07<00:02, 50.07it/s] 78%|███████▊  | 367/471 [00:07<00:02, 49.85it/s] 79%|███████▉  | 373/471 [00:07<00:01, 50.06it/s] 80%|████████  | 379/471 [00:07<00:01, 50.32it/s] 82%|████████▏ | 385/471 [00:07<00:01, 50.37it/s] 83%|████████▎ | 391/471 [00:07<00:01, 50.73it/s] 84%|████████▍ | 397/471 [00:07<00:01, 51.54it/s] 86%|████████▌ | 403/471 [00:07<00:01, 50.43it/s] 87%|████████▋ | 409/471 [00:08<00:01, 50.07it/s] 88%|████████▊ | 415/471 [00:08<00:01, 51.09it/s] 89%|████████▉ | 421/471 [00:08<00:00, 51.17it/s] 91%|█████████ | 427/471 [00:08<00:00, 51.71it/s] 92%|█████████▏| 433/471 [00:08<00:00, 52.19it/s] 93%|█████████▎| 439/471 [00:08<00:00, 52.43it/s] 94%|█████████▍| 445/471 [00:08<00:00, 52.21it/s] 96%|█████████▌| 451/471 [00:08<00:00, 52.11it/s] 97%|█████████▋| 457/471 [00:08<00:00, 52.54it/s] 98%|█████████▊| 463/471 [00:09<00:00, 52.39it/s]100%|█████████▉| 469/471 [00:09<00:00, 51.91it/s]100%|██████████| 471/471 [00:09<00:00, 50.96it/s]
{'eval_loss': 2.2638208866119385, 'eval_model_preparation_time': 0.0031, 'eval_acc': 0.3983005841741901, 'eval_runtime': 9.2647, 'eval_samples_per_second': 812.975, 'eval_steps_per_second': 50.838}
ROUND:22
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.1999, 'grad_norm': 4.571669578552246, 'learning_rate': 0.00012080992435478315, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.24it/s]                                              {'loss': 2.2184, 'grad_norm': 4.4871697425842285, 'learning_rate': 0.00011919912536338604, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.87it/s]  4%|▍         | 3/75 [00:00<00:03, 23.46it/s]                                              {'loss': 2.1199, 'grad_norm': 4.384507656097412, 'learning_rate': 0.00011758832637198894, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.46it/s]                                              {'loss': 2.2447, 'grad_norm': 3.7221364974975586, 'learning_rate': 0.00011597752738059181, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 23.46it/s]                                              {'loss': 2.1488, 'grad_norm': 4.238213062286377, 'learning_rate': 0.00011436672838919471, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.46it/s]  8%|▊         | 6/75 [00:00<00:02, 23.22it/s]                                              {'loss': 2.3111, 'grad_norm': 3.810375928878784, 'learning_rate': 0.0001127559293977976, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.22it/s]                                              {'loss': 2.3186, 'grad_norm': 3.936858654022217, 'learning_rate': 0.0001111451304064005, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.22it/s]                                              {'loss': 2.2307, 'grad_norm': 4.177446365356445, 'learning_rate': 0.00010953433141500338, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.22it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.32it/s]                                              {'loss': 2.3153, 'grad_norm': 3.5437510013580322, 'learning_rate': 0.00010792353242360628, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.32it/s]                                              {'loss': 2.3592, 'grad_norm': 4.320982933044434, 'learning_rate': 0.00010631273343220917, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.32it/s]                                               {'loss': 2.2189, 'grad_norm': 4.121299743652344, 'learning_rate': 0.00010470193444081207, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.32it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.52it/s]                                               {'loss': 2.1925, 'grad_norm': 4.700991630554199, 'learning_rate': 0.00010309113544941496, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.52it/s]                                               {'loss': 2.3318, 'grad_norm': 4.177919864654541, 'learning_rate': 0.00010148033645801784, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.52it/s]                                               {'loss': 2.1326, 'grad_norm': 3.953411102294922, 'learning_rate': 9.986953746662073e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.52it/s]                                               {'loss': 2.332, 'grad_norm': 13.841748237609863, 'learning_rate': 9.825873847522363e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.52it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.72it/s]                                               {'loss': 2.0559, 'grad_norm': 2.822971820831299, 'learning_rate': 9.664793948382652e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.72it/s]                                               {'loss': 2.3392, 'grad_norm': 4.211101055145264, 'learning_rate': 9.503714049242941e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.72it/s]                                               {'loss': 2.2466, 'grad_norm': 3.366231679916382, 'learning_rate': 9.34263415010323e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.72it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.69it/s]                                               {'loss': 2.237, 'grad_norm': 4.439693927764893, 'learning_rate': 9.18155425096352e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.69it/s]                                               {'loss': 2.3219, 'grad_norm': 3.8107283115386963, 'learning_rate': 9.020474351823809e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.69it/s]                                               {'loss': 2.0519, 'grad_norm': 3.327449321746826, 'learning_rate': 8.859394452684097e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.69it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.45it/s]                                               {'loss': 2.0399, 'grad_norm': 3.9441921710968018, 'learning_rate': 8.698314553544386e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.45it/s]                                               {'loss': 2.222, 'grad_norm': 4.452320575714111, 'learning_rate': 8.537234654404676e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.45it/s]                                               {'loss': 2.2111, 'grad_norm': 3.2427260875701904, 'learning_rate': 8.376154755264965e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 25.45it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.35it/s]                                               {'loss': 2.228, 'grad_norm': 3.5847628116607666, 'learning_rate': 8.215074856125255e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.35it/s]                                               {'loss': 2.0798, 'grad_norm': 5.994335174560547, 'learning_rate': 8.053994956985543e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.35it/s]                                               {'loss': 2.1823, 'grad_norm': 3.906991720199585, 'learning_rate': 7.892915057845831e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.35it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.03it/s]                                               {'loss': 2.2274, 'grad_norm': 4.432795524597168, 'learning_rate': 7.731835158706122e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.03it/s]                                               {'loss': 2.2901, 'grad_norm': 6.2735090255737305, 'learning_rate': 7.57075525956641e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.03it/s]                                               {'loss': 2.01, 'grad_norm': 12.889385223388672, 'learning_rate': 7.409675360426699e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.03it/s] 41%|████▏     | 31/75 [00:01<00:01, 24.31it/s]                                               {'loss': 2.0882, 'grad_norm': 3.5252597332000732, 'learning_rate': 7.248595461286988e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.31it/s]                                               {'loss': 2.2467, 'grad_norm': 5.620344161987305, 'learning_rate': 7.087515562147278e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.31it/s]                                               {'loss': 2.1825, 'grad_norm': 4.807383060455322, 'learning_rate': 6.926435663007567e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.31it/s] 45%|████▌     | 34/75 [00:01<00:01, 23.49it/s]                                               {'loss': 2.0425, 'grad_norm': 2.8947365283966064, 'learning_rate': 6.765355763867857e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 23.49it/s]                                               {'loss': 2.1813, 'grad_norm': 5.722508907318115, 'learning_rate': 6.604275864728145e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 23.49it/s]                                               {'loss': 2.1042, 'grad_norm': 4.252843856811523, 'learning_rate': 6.443195965588435e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 23.49it/s] 49%|████▉     | 37/75 [00:01<00:01, 23.01it/s]                                               {'loss': 2.0256, 'grad_norm': 3.7297885417938232, 'learning_rate': 6.282116066448724e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 23.01it/s]                                               {'loss': 2.1553, 'grad_norm': 3.265882730484009, 'learning_rate': 6.121036167309014e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 23.01it/s]                                               {'loss': 2.087, 'grad_norm': 4.608684062957764, 'learning_rate': 5.959956268169302e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.01it/s] 53%|█████▎    | 40/75 [00:01<00:01, 23.48it/s]                                               {'loss': 2.2224, 'grad_norm': 3.839413642883301, 'learning_rate': 5.7988763690295906e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.48it/s]                                               {'loss': 2.0776, 'grad_norm': 4.190760612487793, 'learning_rate': 5.63779646988988e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 23.48it/s]                                               {'loss': 2.1657, 'grad_norm': 4.5542683601379395, 'learning_rate': 5.476716570750169e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 23.48it/s] 57%|█████▋    | 43/75 [00:01<00:01, 23.92it/s]                                               {'loss': 2.155, 'grad_norm': 3.7332746982574463, 'learning_rate': 5.315636671610458e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 23.92it/s]                                               {'loss': 2.2557, 'grad_norm': 3.264726400375366, 'learning_rate': 5.154556772470748e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 23.92it/s]                                               {'loss': 2.686, 'grad_norm': 15.195622444152832, 'learning_rate': 4.9934768733310366e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 23.92it/s]                                               {'loss': 2.2713, 'grad_norm': 3.674664258956909, 'learning_rate': 4.832396974191326e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 23.92it/s] 63%|██████▎   | 47/75 [00:01<00:01, 25.91it/s]                                               {'loss': 2.4011, 'grad_norm': 5.030976295471191, 'learning_rate': 4.671317075051615e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.91it/s]                                               {'loss': 2.1903, 'grad_norm': 3.7384495735168457, 'learning_rate': 4.510237175911904e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.91it/s]                                               {'loss': 2.2215, 'grad_norm': 3.8153183460235596, 'learning_rate': 4.349157276772193e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.91it/s] 67%|██████▋   | 50/75 [00:02<00:00, 25.98it/s]                                               {'loss': 2.2502, 'grad_norm': 5.236891269683838, 'learning_rate': 4.1880773776324826e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.98it/s]                                               {'loss': 2.0726, 'grad_norm': 3.04544734954834, 'learning_rate': 4.0269974784927714e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.98it/s]                                               {'loss': 2.0862, 'grad_norm': 3.3860297203063965, 'learning_rate': 3.865917579353061e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.98it/s] 71%|███████   | 53/75 [00:02<00:00, 26.00it/s]                                               {'loss': 2.0094, 'grad_norm': 3.7182741165161133, 'learning_rate': 3.7048376802133496e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 26.00it/s]                                               {'loss': 2.0951, 'grad_norm': 4.762156963348389, 'learning_rate': 3.543757781073639e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 26.00it/s]                                               {'loss': 2.2769, 'grad_norm': 4.609956741333008, 'learning_rate': 3.3826778819339286e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 26.00it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.66it/s]                                               {'loss': 2.1185, 'grad_norm': 4.223711013793945, 'learning_rate': 3.2215979827942174e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.66it/s]                                               {'loss': 2.0531, 'grad_norm': 3.909688949584961, 'learning_rate': 3.060518083654507e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.66it/s]                                               {'loss': 2.1553, 'grad_norm': 4.209767818450928, 'learning_rate': 2.8994381845147953e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.66it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.007, 'grad_norm': 2.9547014236450195, 'learning_rate': 2.7383582853750844e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.46it/s]                                               {'loss': 1.8887, 'grad_norm': 8.229940414428711, 'learning_rate': 2.577278386235374e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.0588, 'grad_norm': 3.7578561305999756, 'learning_rate': 2.416198487095663e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.46it/s]                                               {'loss': 2.1375, 'grad_norm': 3.01029896736145, 'learning_rate': 2.255118587955952e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.46it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.55it/s]                                               {'loss': 2.1926, 'grad_norm': 5.165315628051758, 'learning_rate': 2.0940386888162413e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.55it/s]                                               {'loss': 2.167, 'grad_norm': 4.405113220214844, 'learning_rate': 1.9329587896765304e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.55it/s]                                               {'loss': 2.1247, 'grad_norm': 4.621716022491455, 'learning_rate': 1.7718788905368195e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.55it/s] 88%|████████▊ | 66/75 [00:02<00:00, 24.71it/s]                                               {'loss': 2.1659, 'grad_norm': 3.8727076053619385, 'learning_rate': 1.6107989913971087e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.71it/s]                                               {'loss': 2.1774, 'grad_norm': 3.1453964710235596, 'learning_rate': 1.4497190922573976e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.71it/s]                                               {'loss': 2.1873, 'grad_norm': 5.382433891296387, 'learning_rate': 1.288639193117687e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.71it/s] 92%|█████████▏| 69/75 [00:02<00:00, 22.23it/s]                                               {'loss': 2.0768, 'grad_norm': 5.023509979248047, 'learning_rate': 1.127559293977976e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 22.23it/s]                                               {'loss': 2.016, 'grad_norm': 3.4901926517486572, 'learning_rate': 9.664793948382652e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 22.23it/s]                                               {'loss': 2.0948, 'grad_norm': 4.625758171081543, 'learning_rate': 8.053994956985543e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 22.23it/s] 96%|█████████▌| 72/75 [00:02<00:00, 21.73it/s]                                               {'loss': 2.2174, 'grad_norm': 3.928076982498169, 'learning_rate': 6.443195965588435e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 21.73it/s]                                               {'loss': 2.2501, 'grad_norm': 3.2945680618286133, 'learning_rate': 4.832396974191326e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 21.73it/s]                                               {'loss': 2.2001, 'grad_norm': 3.960357427597046, 'learning_rate': 3.2215979827942174e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 21.73it/s]                                               {'loss': 1.7338, 'grad_norm': 9.580577850341797, 'learning_rate': 1.6107989913971087e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 21.73it/s]                                               {'train_runtime': 3.1952, 'train_samples_per_second': 353.654, 'train_steps_per_second': 23.473, 'train_loss': 2.173201627731323, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 21.73it/s]100%|██████████| 75/75 [00:03<00:00, 23.47it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:36
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.235, 'grad_norm': 4.171235084533691, 'learning_rate': 0.00012080992435478315, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.58it/s]                                              {'loss': 2.2979, 'grad_norm': 3.3696768283843994, 'learning_rate': 0.00011919912536338604, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 25.14it/s]  4%|▍         | 3/75 [00:00<00:03, 23.63it/s]                                              {'loss': 2.2094, 'grad_norm': 3.559579372406006, 'learning_rate': 0.00011758832637198894, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.63it/s]                                              {'loss': 2.2251, 'grad_norm': 3.8960678577423096, 'learning_rate': 0.00011597752738059181, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 23.63it/s]                                              {'loss': 2.3168, 'grad_norm': 4.79500675201416, 'learning_rate': 0.00011436672838919471, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.63it/s]  8%|▊         | 6/75 [00:00<00:02, 23.82it/s]                                              {'loss': 2.1891, 'grad_norm': 3.6893844604492188, 'learning_rate': 0.0001127559293977976, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.82it/s]                                              {'loss': 2.296, 'grad_norm': 3.844242811203003, 'learning_rate': 0.0001111451304064005, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.82it/s]                                              {'loss': 2.2728, 'grad_norm': 4.649946212768555, 'learning_rate': 0.00010953433141500338, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.82it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.32it/s]                                              {'loss': 2.1793, 'grad_norm': 3.9883463382720947, 'learning_rate': 0.00010792353242360628, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.32it/s]                                              {'loss': 2.0658, 'grad_norm': 3.5783426761627197, 'learning_rate': 0.00010631273343220917, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.32it/s]                                               {'loss': 2.1496, 'grad_norm': 4.88926362991333, 'learning_rate': 0.00010470193444081207, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.32it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.10it/s]                                               {'loss': 1.9676, 'grad_norm': 2.9257938861846924, 'learning_rate': 0.00010309113544941496, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.10it/s]                                               {'loss': 2.0445, 'grad_norm': 2.7514569759368896, 'learning_rate': 0.00010148033645801784, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.10it/s]                                               {'loss': 2.2584, 'grad_norm': 3.036112070083618, 'learning_rate': 9.986953746662073e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.10it/s]                                               {'loss': 2.0549, 'grad_norm': 7.679412364959717, 'learning_rate': 9.825873847522363e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.10it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.38it/s]                                               {'loss': 2.3069, 'grad_norm': 4.1403489112854, 'learning_rate': 9.664793948382652e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.38it/s]                                               {'loss': 2.1227, 'grad_norm': 3.550208330154419, 'learning_rate': 9.503714049242941e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.38it/s]                                               {'loss': 2.0962, 'grad_norm': 4.513476848602295, 'learning_rate': 9.34263415010323e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.38it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.92it/s]                                               {'loss': 2.2239, 'grad_norm': 3.834099054336548, 'learning_rate': 9.18155425096352e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.92it/s]                                               {'loss': 2.1894, 'grad_norm': 4.880523204803467, 'learning_rate': 9.020474351823809e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.92it/s]                                               {'loss': 1.9415, 'grad_norm': 2.942979574203491, 'learning_rate': 8.859394452684097e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.92it/s] 29%|██▉       | 22/75 [00:00<00:02, 24.80it/s]                                               {'loss': 2.255, 'grad_norm': 4.204308986663818, 'learning_rate': 8.698314553544386e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.80it/s]                                               {'loss': 2.0639, 'grad_norm': 4.009954452514648, 'learning_rate': 8.537234654404676e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.80it/s]                                               {'loss': 2.1737, 'grad_norm': 4.137163162231445, 'learning_rate': 8.376154755264965e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.80it/s] 33%|███▎      | 25/75 [00:01<00:02, 24.70it/s]                                               {'loss': 2.1975, 'grad_norm': 3.2009329795837402, 'learning_rate': 8.215074856125255e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.70it/s]                                               {'loss': 2.0336, 'grad_norm': 3.566899299621582, 'learning_rate': 8.053994956985543e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.70it/s]                                               {'loss': 2.0288, 'grad_norm': 3.27901291847229, 'learning_rate': 7.892915057845831e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.70it/s] 37%|███▋      | 28/75 [00:01<00:01, 24.41it/s]                                               {'loss': 2.2483, 'grad_norm': 5.210984706878662, 'learning_rate': 7.731835158706122e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.41it/s]                                               {'loss': 2.2497, 'grad_norm': 3.567843437194824, 'learning_rate': 7.57075525956641e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.41it/s]                                               {'loss': 1.8408, 'grad_norm': 8.273999214172363, 'learning_rate': 7.409675360426699e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.41it/s] 41%|████▏     | 31/75 [00:01<00:01, 24.80it/s]                                               {'loss': 2.1228, 'grad_norm': 3.2144534587860107, 'learning_rate': 7.248595461286988e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.80it/s]                                               {'loss': 2.1696, 'grad_norm': 2.931922435760498, 'learning_rate': 7.087515562147278e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.80it/s]                                               {'loss': 2.0126, 'grad_norm': 3.3815901279449463, 'learning_rate': 6.926435663007567e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.80it/s] 45%|████▌     | 34/75 [00:01<00:01, 25.27it/s]                                               {'loss': 2.0945, 'grad_norm': 3.352177858352661, 'learning_rate': 6.765355763867857e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.27it/s]                                               {'loss': 2.0939, 'grad_norm': 3.7981953620910645, 'learning_rate': 6.604275864728145e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.27it/s]                                               {'loss': 1.8831, 'grad_norm': 3.6327531337738037, 'learning_rate': 6.443195965588435e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.27it/s] 49%|████▉     | 37/75 [00:01<00:01, 24.32it/s]                                               {'loss': 2.2752, 'grad_norm': 4.5300445556640625, 'learning_rate': 6.282116066448724e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.32it/s]                                               {'loss': 1.9997, 'grad_norm': 3.0041868686676025, 'learning_rate': 6.121036167309014e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.32it/s]                                               {'loss': 2.0632, 'grad_norm': 3.712245464324951, 'learning_rate': 5.959956268169302e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.32it/s] 53%|█████▎    | 40/75 [00:01<00:01, 23.90it/s]                                               {'loss': 2.2358, 'grad_norm': 3.5469162464141846, 'learning_rate': 5.7988763690295906e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.90it/s]                                               {'loss': 2.1012, 'grad_norm': 3.5095906257629395, 'learning_rate': 5.63779646988988e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 23.90it/s]                                               {'loss': 2.2583, 'grad_norm': 2.563178539276123, 'learning_rate': 5.476716570750169e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 23.90it/s] 57%|█████▋    | 43/75 [00:01<00:01, 24.26it/s]                                               {'loss': 2.1203, 'grad_norm': 5.62733268737793, 'learning_rate': 5.315636671610458e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.26it/s]                                               {'loss': 2.335, 'grad_norm': 4.193118572235107, 'learning_rate': 5.154556772470748e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.26it/s]                                               {'loss': 2.4184, 'grad_norm': 16.912050247192383, 'learning_rate': 4.9934768733310366e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.26it/s] 61%|██████▏   | 46/75 [00:01<00:01, 25.40it/s]                                               {'loss': 1.9987, 'grad_norm': 3.3544747829437256, 'learning_rate': 4.832396974191326e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.40it/s]                                               {'loss': 2.1412, 'grad_norm': 4.054441452026367, 'learning_rate': 4.671317075051615e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.40it/s]                                               {'loss': 1.9048, 'grad_norm': 2.9825448989868164, 'learning_rate': 4.510237175911904e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.40it/s] 65%|██████▌   | 49/75 [00:01<00:01, 25.21it/s]                                               {'loss': 2.0272, 'grad_norm': 3.294267177581787, 'learning_rate': 4.349157276772193e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.21it/s]                                               {'loss': 2.079, 'grad_norm': 3.4624199867248535, 'learning_rate': 4.1880773776324826e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.21it/s]                                               {'loss': 2.2439, 'grad_norm': 3.493175983428955, 'learning_rate': 4.0269974784927714e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.21it/s] 69%|██████▉   | 52/75 [00:02<00:00, 25.30it/s]                                               {'loss': 2.3054, 'grad_norm': 5.125227928161621, 'learning_rate': 3.865917579353061e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.30it/s]                                               {'loss': 2.079, 'grad_norm': 3.1763815879821777, 'learning_rate': 3.7048376802133496e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.30it/s]                                               {'loss': 2.3039, 'grad_norm': 3.219529151916504, 'learning_rate': 3.543757781073639e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.30it/s] 73%|███████▎  | 55/75 [00:02<00:00, 25.34it/s]                                               {'loss': 1.964, 'grad_norm': 3.1193184852600098, 'learning_rate': 3.3826778819339286e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.34it/s]                                               {'loss': 1.9971, 'grad_norm': 4.362400054931641, 'learning_rate': 3.2215979827942174e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.34it/s]                                               {'loss': 2.1305, 'grad_norm': 3.481956958770752, 'learning_rate': 3.060518083654507e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.34it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.3959, 'grad_norm': 3.3887016773223877, 'learning_rate': 2.8994381845147953e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.0658, 'grad_norm': 3.3858935832977295, 'learning_rate': 2.7383582853750844e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.53it/s]                                               {'loss': 1.865, 'grad_norm': 8.310467720031738, 'learning_rate': 2.577278386235374e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.53it/s]                                               {'loss': 1.8566, 'grad_norm': 3.2325599193573, 'learning_rate': 2.416198487095663e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.53it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.17it/s]                                               {'loss': 2.0746, 'grad_norm': 2.667559862136841, 'learning_rate': 2.255118587955952e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.17it/s]                                               {'loss': 2.0766, 'grad_norm': 3.6172826290130615, 'learning_rate': 2.0940386888162413e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.17it/s]                                               {'loss': 2.1207, 'grad_norm': 2.775294065475464, 'learning_rate': 1.9329587896765304e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.17it/s] 87%|████████▋ | 65/75 [00:02<00:00, 25.47it/s]                                               {'loss': 2.0887, 'grad_norm': 3.436378240585327, 'learning_rate': 1.7718788905368195e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.47it/s]                                               {'loss': 2.2165, 'grad_norm': 4.049341678619385, 'learning_rate': 1.6107989913971087e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.47it/s]                                               {'loss': 2.1971, 'grad_norm': 2.941577434539795, 'learning_rate': 1.4497190922573976e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.47it/s] 91%|█████████ | 68/75 [00:02<00:00, 25.51it/s]                                               {'loss': 2.4802, 'grad_norm': 5.307058811187744, 'learning_rate': 1.288639193117687e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.51it/s]                                               {'loss': 2.108, 'grad_norm': 3.32769775390625, 'learning_rate': 1.127559293977976e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.51it/s]                                               {'loss': 2.256, 'grad_norm': 4.660231113433838, 'learning_rate': 9.664793948382652e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.51it/s] 95%|█████████▍| 71/75 [00:02<00:00, 25.22it/s]                                               {'loss': 1.8848, 'grad_norm': 3.68786358833313, 'learning_rate': 8.053994956985543e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.22it/s]                                               {'loss': 2.136, 'grad_norm': 3.4285459518432617, 'learning_rate': 6.443195965588435e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.22it/s]                                               {'loss': 2.1139, 'grad_norm': 4.019224166870117, 'learning_rate': 4.832396974191326e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.22it/s] 99%|█████████▊| 74/75 [00:02<00:00, 25.07it/s]                                               {'loss': 2.0033, 'grad_norm': 3.7316901683807373, 'learning_rate': 3.2215979827942174e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.07it/s]                                               {'loss': 1.3587, 'grad_norm': 5.599349021911621, 'learning_rate': 1.6107989913971087e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.07it/s]                                               {'train_runtime': 3.1302, 'train_samples_per_second': 360.999, 'train_steps_per_second': 23.96, 'train_loss': 2.1252125612894694, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.07it/s]100%|██████████| 75/75 [00:03<00:00, 23.96it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3121, 'grad_norm': 4.8363752365112305, 'learning_rate': 0.00012080992435478315, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.62it/s]                                              {'loss': 2.2704, 'grad_norm': 4.292884826660156, 'learning_rate': 0.00011919912536338604, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.97it/s]  4%|▍         | 3/75 [00:00<00:02, 25.60it/s]                                              {'loss': 2.2042, 'grad_norm': 4.310954570770264, 'learning_rate': 0.00011758832637198894, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.60it/s]                                              {'loss': 2.1303, 'grad_norm': 4.312979221343994, 'learning_rate': 0.00011597752738059181, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.60it/s]                                              {'loss': 2.2866, 'grad_norm': 4.829655647277832, 'learning_rate': 0.00011436672838919471, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.60it/s]  8%|▊         | 6/75 [00:00<00:02, 24.97it/s]                                              {'loss': 2.4538, 'grad_norm': 4.432963848114014, 'learning_rate': 0.0001127559293977976, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.97it/s]                                              {'loss': 2.2315, 'grad_norm': 3.829772710800171, 'learning_rate': 0.0001111451304064005, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.97it/s]                                              {'loss': 2.2407, 'grad_norm': 4.107072353363037, 'learning_rate': 0.00010953433141500338, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.97it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.21it/s]                                              {'loss': 2.5494, 'grad_norm': 5.07197380065918, 'learning_rate': 0.00010792353242360628, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.21it/s]                                              {'loss': 2.4011, 'grad_norm': 4.702639102935791, 'learning_rate': 0.00010631273343220917, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.21it/s]                                               {'loss': 2.1171, 'grad_norm': 3.7233283519744873, 'learning_rate': 0.00010470193444081207, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.21it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.03it/s]                                               {'loss': 2.1074, 'grad_norm': 3.2531001567840576, 'learning_rate': 0.00010309113544941496, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.03it/s]                                               {'loss': 2.5673, 'grad_norm': 4.961208343505859, 'learning_rate': 0.00010148033645801784, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.03it/s]                                               {'loss': 2.2514, 'grad_norm': 4.477943420410156, 'learning_rate': 9.986953746662073e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.03it/s] 20%|██        | 15/75 [00:00<00:02, 25.91it/s]                                               {'loss': 2.3939, 'grad_norm': 19.26045036315918, 'learning_rate': 9.825873847522363e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.91it/s]                                               {'loss': 2.2519, 'grad_norm': 4.170962810516357, 'learning_rate': 9.664793948382652e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.91it/s]                                               {'loss': 2.1945, 'grad_norm': 3.880918025970459, 'learning_rate': 9.503714049242941e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.91it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.46it/s]                                               {'loss': 2.198, 'grad_norm': 4.579624176025391, 'learning_rate': 9.34263415010323e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.46it/s]                                               {'loss': 2.0463, 'grad_norm': 3.0371453762054443, 'learning_rate': 9.18155425096352e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.46it/s]                                               {'loss': 2.4669, 'grad_norm': 3.3847479820251465, 'learning_rate': 9.020474351823809e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.46it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.47it/s]                                               {'loss': 2.1879, 'grad_norm': 4.03767204284668, 'learning_rate': 8.859394452684097e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.47it/s]                                               {'loss': 2.3408, 'grad_norm': 4.042254447937012, 'learning_rate': 8.698314553544386e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.47it/s]                                               {'loss': 2.4548, 'grad_norm': 3.761428117752075, 'learning_rate': 8.537234654404676e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.47it/s] 32%|███▏      | 24/75 [00:00<00:02, 24.90it/s]                                               {'loss': 2.3548, 'grad_norm': 3.7569754123687744, 'learning_rate': 8.376154755264965e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.90it/s]                                               {'loss': 2.232, 'grad_norm': 4.056349754333496, 'learning_rate': 8.215074856125255e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:02, 24.90it/s]                                               {'loss': 2.1708, 'grad_norm': 3.6759767532348633, 'learning_rate': 8.053994956985543e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.90it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.1905, 'grad_norm': 3.1409127712249756, 'learning_rate': 7.892915057845831e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.4289, 'grad_norm': 5.031583309173584, 'learning_rate': 7.731835158706122e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.2192, 'grad_norm': 3.567582607269287, 'learning_rate': 7.57075525956641e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.57it/s]                                               {'loss': 1.9426, 'grad_norm': 12.915263175964355, 'learning_rate': 7.409675360426699e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.57it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.03it/s]                                               {'loss': 2.3346, 'grad_norm': 3.5263917446136475, 'learning_rate': 7.248595461286988e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.03it/s]                                               {'loss': 2.2978, 'grad_norm': 4.835763931274414, 'learning_rate': 7.087515562147278e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.03it/s]                                               {'loss': 2.3009, 'grad_norm': 5.327939510345459, 'learning_rate': 6.926435663007567e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.03it/s] 45%|████▌     | 34/75 [00:01<00:01, 24.24it/s]                                               {'loss': 2.3486, 'grad_norm': 4.98621940612793, 'learning_rate': 6.765355763867857e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.24it/s]                                               {'loss': 2.0471, 'grad_norm': 3.756295919418335, 'learning_rate': 6.604275864728145e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.24it/s]                                               {'loss': 2.0935, 'grad_norm': 2.780064344406128, 'learning_rate': 6.443195965588435e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.24it/s] 49%|████▉     | 37/75 [00:01<00:01, 24.11it/s]                                               {'loss': 2.018, 'grad_norm': 3.946683168411255, 'learning_rate': 6.282116066448724e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.11it/s]                                               {'loss': 2.1142, 'grad_norm': 3.52978515625, 'learning_rate': 6.121036167309014e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.11it/s]                                               {'loss': 2.2805, 'grad_norm': 5.03590726852417, 'learning_rate': 5.959956268169302e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.11it/s] 53%|█████▎    | 40/75 [00:01<00:01, 23.75it/s]                                               {'loss': 2.2723, 'grad_norm': 3.7794997692108154, 'learning_rate': 5.7988763690295906e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.75it/s]                                               {'loss': 2.2409, 'grad_norm': 5.568080425262451, 'learning_rate': 5.63779646988988e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 23.75it/s]                                               {'loss': 2.3054, 'grad_norm': 5.296850204467773, 'learning_rate': 5.476716570750169e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 23.75it/s] 57%|█████▋    | 43/75 [00:01<00:01, 23.66it/s]                                               {'loss': 2.1653, 'grad_norm': 4.007196426391602, 'learning_rate': 5.315636671610458e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 23.66it/s]                                               {'loss': 2.5507, 'grad_norm': 4.397497177124023, 'learning_rate': 5.154556772470748e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 23.66it/s]                                               {'loss': 1.4457, 'grad_norm': 6.023737907409668, 'learning_rate': 4.9934768733310366e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 23.66it/s]                                               {'loss': 2.1361, 'grad_norm': 3.7529101371765137, 'learning_rate': 4.832396974191326e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 23.66it/s] 63%|██████▎   | 47/75 [00:01<00:01, 25.81it/s]                                               {'loss': 2.1822, 'grad_norm': 4.025527477264404, 'learning_rate': 4.671317075051615e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.81it/s]                                               {'loss': 2.2829, 'grad_norm': 3.9628028869628906, 'learning_rate': 4.510237175911904e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.81it/s]                                               {'loss': 2.4055, 'grad_norm': 3.763737678527832, 'learning_rate': 4.349157276772193e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.81it/s] 67%|██████▋   | 50/75 [00:01<00:00, 25.90it/s]                                               {'loss': 2.1705, 'grad_norm': 4.325214862823486, 'learning_rate': 4.1880773776324826e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.90it/s]                                               {'loss': 2.4025, 'grad_norm': 4.240840435028076, 'learning_rate': 4.0269974784927714e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.90it/s]                                               {'loss': 2.1218, 'grad_norm': 4.1511616706848145, 'learning_rate': 3.865917579353061e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.90it/s] 71%|███████   | 53/75 [00:02<00:00, 25.28it/s]                                               {'loss': 2.2447, 'grad_norm': 4.307995796203613, 'learning_rate': 3.7048376802133496e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.28it/s]                                               {'loss': 2.2895, 'grad_norm': 4.548433780670166, 'learning_rate': 3.543757781073639e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.28it/s]                                               {'loss': 2.3024, 'grad_norm': 3.259573459625244, 'learning_rate': 3.3826778819339286e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.28it/s] 75%|███████▍  | 56/75 [00:02<00:00, 24.39it/s]                                               {'loss': 2.2357, 'grad_norm': 3.511458396911621, 'learning_rate': 3.2215979827942174e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.39it/s]                                               {'loss': 2.0998, 'grad_norm': 3.6730992794036865, 'learning_rate': 3.060518083654507e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.39it/s]                                               {'loss': 2.1044, 'grad_norm': 3.4406750202178955, 'learning_rate': 2.8994381845147953e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.39it/s] 79%|███████▊  | 59/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.3003, 'grad_norm': 3.7840893268585205, 'learning_rate': 2.7383582853750844e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.53it/s]                                               {'loss': 1.6746, 'grad_norm': 8.187173843383789, 'learning_rate': 2.577278386235374e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.1549, 'grad_norm': 3.391362428665161, 'learning_rate': 2.416198487095663e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.53it/s] 83%|████████▎ | 62/75 [00:02<00:00, 25.76it/s]                                               {'loss': 2.1742, 'grad_norm': 4.093000411987305, 'learning_rate': 2.255118587955952e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.76it/s]                                               {'loss': 2.1446, 'grad_norm': 3.270205020904541, 'learning_rate': 2.0940386888162413e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.76it/s]                                               {'loss': 2.2772, 'grad_norm': 3.567084550857544, 'learning_rate': 1.9329587896765304e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.76it/s] 87%|████████▋ | 65/75 [00:02<00:00, 25.39it/s]                                               {'loss': 2.2408, 'grad_norm': 4.442534923553467, 'learning_rate': 1.7718788905368195e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.39it/s]                                               {'loss': 2.3593, 'grad_norm': 4.3401198387146, 'learning_rate': 1.6107989913971087e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.39it/s]                                               {'loss': 2.2369, 'grad_norm': 3.546627998352051, 'learning_rate': 1.4497190922573976e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.39it/s] 91%|█████████ | 68/75 [00:02<00:00, 25.22it/s]                                               {'loss': 2.0714, 'grad_norm': 3.2257304191589355, 'learning_rate': 1.288639193117687e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.22it/s]                                               {'loss': 2.2112, 'grad_norm': 4.461977481842041, 'learning_rate': 1.127559293977976e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.22it/s]                                               {'loss': 2.4595, 'grad_norm': 4.896937847137451, 'learning_rate': 9.664793948382652e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.22it/s] 95%|█████████▍| 71/75 [00:02<00:00, 25.44it/s]                                               {'loss': 2.1893, 'grad_norm': 4.203448295593262, 'learning_rate': 8.053994956985543e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.44it/s]                                               {'loss': 2.2043, 'grad_norm': 3.6281306743621826, 'learning_rate': 6.443195965588435e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.44it/s]                                               {'loss': 2.0902, 'grad_norm': 4.463611125946045, 'learning_rate': 4.832396974191326e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.44it/s] 99%|█████████▊| 74/75 [00:02<00:00, 25.03it/s]                                               {'loss': 2.3751, 'grad_norm': 4.120386123657227, 'learning_rate': 3.2215979827942174e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.03it/s]                                               {'loss': 2.0967, 'grad_norm': 14.272445678710938, 'learning_rate': 1.6107989913971087e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.03it/s]                                               {'train_runtime': 3.1133, 'train_samples_per_second': 362.958, 'train_steps_per_second': 24.09, 'train_loss': 2.2300141525268553, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.03it/s]100%|██████████| 75/75 [00:03<00:00, 24.09it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:28
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2791, 'grad_norm': 3.9339256286621094, 'learning_rate': 0.00012080992435478315, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 24.12it/s]                                              {'loss': 2.2701, 'grad_norm': 4.106926918029785, 'learning_rate': 0.00011919912536338604, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.90it/s]  4%|▍         | 3/75 [00:00<00:02, 25.03it/s]                                              {'loss': 2.2715, 'grad_norm': 3.7618706226348877, 'learning_rate': 0.00011758832637198894, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.03it/s]                                              {'loss': 2.2501, 'grad_norm': 4.602178573608398, 'learning_rate': 0.00011597752738059181, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.03it/s]                                              {'loss': 2.1965, 'grad_norm': 4.30992317199707, 'learning_rate': 0.00011436672838919471, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.03it/s]  8%|▊         | 6/75 [00:00<00:02, 25.40it/s]                                              {'loss': 2.2092, 'grad_norm': 3.370636224746704, 'learning_rate': 0.0001127559293977976, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.40it/s]                                              {'loss': 2.1738, 'grad_norm': 4.988447666168213, 'learning_rate': 0.0001111451304064005, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.40it/s]                                              {'loss': 2.4561, 'grad_norm': 4.539676189422607, 'learning_rate': 0.00010953433141500338, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.40it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.74it/s]                                              {'loss': 2.1499, 'grad_norm': 4.0833821296691895, 'learning_rate': 0.00010792353242360628, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.74it/s]                                              {'loss': 1.9898, 'grad_norm': 3.1350831985473633, 'learning_rate': 0.00010631273343220917, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.74it/s]                                               {'loss': 2.272, 'grad_norm': 4.464844226837158, 'learning_rate': 0.00010470193444081207, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.74it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.64it/s]                                               {'loss': 2.3439, 'grad_norm': 3.7067532539367676, 'learning_rate': 0.00010309113544941496, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.64it/s]                                               {'loss': 2.1545, 'grad_norm': 4.715273857116699, 'learning_rate': 0.00010148033645801784, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.64it/s]                                               {'loss': 2.3451, 'grad_norm': 4.546326637268066, 'learning_rate': 9.986953746662073e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.64it/s] 20%|██        | 15/75 [00:00<00:02, 26.93it/s]                                               {'loss': 2.2679, 'grad_norm': 10.677508354187012, 'learning_rate': 9.825873847522363e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 26.93it/s]                                               {'loss': 2.0989, 'grad_norm': 5.189759254455566, 'learning_rate': 9.664793948382652e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.93it/s]                                               {'loss': 2.1399, 'grad_norm': 3.504894495010376, 'learning_rate': 9.503714049242941e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.93it/s] 24%|██▍       | 18/75 [00:00<00:02, 26.77it/s]                                               {'loss': 2.0065, 'grad_norm': 3.047214984893799, 'learning_rate': 9.34263415010323e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.77it/s]                                               {'loss': 2.2171, 'grad_norm': 4.1073479652404785, 'learning_rate': 9.18155425096352e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.77it/s]                                               {'loss': 2.1909, 'grad_norm': 4.566042900085449, 'learning_rate': 9.020474351823809e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.77it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.79it/s]                                               {'loss': 2.1774, 'grad_norm': 3.6715245246887207, 'learning_rate': 8.859394452684097e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.79it/s]                                               {'loss': 2.2663, 'grad_norm': 4.217836856842041, 'learning_rate': 8.698314553544386e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.79it/s]                                               {'loss': 2.2742, 'grad_norm': 5.472505569458008, 'learning_rate': 8.537234654404676e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.79it/s] 32%|███▏      | 24/75 [00:00<00:01, 25.75it/s]                                               {'loss': 2.1949, 'grad_norm': 3.623173713684082, 'learning_rate': 8.376154755264965e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.75it/s]                                               {'loss': 2.0762, 'grad_norm': 3.2903997898101807, 'learning_rate': 8.215074856125255e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.75it/s]                                               {'loss': 2.1546, 'grad_norm': 3.7754335403442383, 'learning_rate': 8.053994956985543e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.75it/s] 36%|███▌      | 27/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.2833, 'grad_norm': 5.090334415435791, 'learning_rate': 7.892915057845831e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.1658, 'grad_norm': 3.6754581928253174, 'learning_rate': 7.731835158706122e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.0765, 'grad_norm': 2.836893081665039, 'learning_rate': 7.57075525956641e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.60it/s] 40%|████      | 30/75 [00:01<00:01, 24.01it/s]                                               {'loss': 2.3591, 'grad_norm': 11.32331657409668, 'learning_rate': 7.409675360426699e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.01it/s]                                               {'loss': 2.2468, 'grad_norm': 4.463110446929932, 'learning_rate': 7.248595461286988e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.01it/s]                                               {'loss': 2.2828, 'grad_norm': 3.2777674198150635, 'learning_rate': 7.087515562147278e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.01it/s] 44%|████▍     | 33/75 [00:01<00:01, 23.32it/s]                                               {'loss': 2.3213, 'grad_norm': 3.990495204925537, 'learning_rate': 6.926435663007567e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 23.32it/s]                                               {'loss': 1.9667, 'grad_norm': 3.544903516769409, 'learning_rate': 6.765355763867857e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 23.32it/s]                                               {'loss': 2.1086, 'grad_norm': 3.3476436138153076, 'learning_rate': 6.604275864728145e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 23.32it/s] 48%|████▊     | 36/75 [00:01<00:01, 23.55it/s]                                               {'loss': 2.1675, 'grad_norm': 4.87494421005249, 'learning_rate': 6.443195965588435e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 23.55it/s]                                               {'loss': 2.1321, 'grad_norm': 3.218635082244873, 'learning_rate': 6.282116066448724e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 23.55it/s]                                               {'loss': 1.9711, 'grad_norm': 3.8574328422546387, 'learning_rate': 6.121036167309014e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 23.55it/s] 52%|█████▏    | 39/75 [00:01<00:01, 23.97it/s]                                               {'loss': 2.0944, 'grad_norm': 3.95462703704834, 'learning_rate': 5.959956268169302e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.97it/s]                                               {'loss': 2.2516, 'grad_norm': 3.931236743927002, 'learning_rate': 5.7988763690295906e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.97it/s]                                               {'loss': 2.3499, 'grad_norm': 3.4842095375061035, 'learning_rate': 5.63779646988988e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 23.97it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.24it/s]                                               {'loss': 2.0316, 'grad_norm': 4.544404983520508, 'learning_rate': 5.476716570750169e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.24it/s]                                               {'loss': 2.3408, 'grad_norm': 3.081082582473755, 'learning_rate': 5.315636671610458e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.24it/s]                                               {'loss': 2.2715, 'grad_norm': 3.6890807151794434, 'learning_rate': 5.154556772470748e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.24it/s] 60%|██████    | 45/75 [00:01<00:01, 25.40it/s]                                               {'loss': 2.0714, 'grad_norm': 7.0326666831970215, 'learning_rate': 4.9934768733310366e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.40it/s]                                               {'loss': 2.0994, 'grad_norm': 4.503876209259033, 'learning_rate': 4.832396974191326e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.40it/s]                                               {'loss': 2.3638, 'grad_norm': 2.986098289489746, 'learning_rate': 4.671317075051615e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.40it/s] 64%|██████▍   | 48/75 [00:01<00:01, 25.58it/s]                                               {'loss': 2.2374, 'grad_norm': 5.170099258422852, 'learning_rate': 4.510237175911904e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.58it/s]                                               {'loss': 1.9721, 'grad_norm': 4.768950462341309, 'learning_rate': 4.349157276772193e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.58it/s]                                               {'loss': 2.1886, 'grad_norm': 3.343388557434082, 'learning_rate': 4.1880773776324826e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.58it/s] 68%|██████▊   | 51/75 [00:02<00:00, 25.47it/s]                                               {'loss': 2.329, 'grad_norm': 3.879988193511963, 'learning_rate': 4.0269974784927714e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.47it/s]                                               {'loss': 2.4643, 'grad_norm': 4.881341457366943, 'learning_rate': 3.865917579353061e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.47it/s]                                               {'loss': 2.1685, 'grad_norm': 3.8248648643493652, 'learning_rate': 3.7048376802133496e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.47it/s] 72%|███████▏  | 54/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.1468, 'grad_norm': 4.3892669677734375, 'learning_rate': 3.543757781073639e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.0825, 'grad_norm': 3.076460361480713, 'learning_rate': 3.3826778819339286e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.33it/s]                                               {'loss': 2.2365, 'grad_norm': 3.6700944900512695, 'learning_rate': 3.2215979827942174e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.33it/s] 76%|███████▌  | 57/75 [00:02<00:00, 25.26it/s]                                               {'loss': 1.9832, 'grad_norm': 3.2627761363983154, 'learning_rate': 3.060518083654507e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.0139, 'grad_norm': 3.0438930988311768, 'learning_rate': 2.8994381845147953e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.1654, 'grad_norm': 4.04425573348999, 'learning_rate': 2.7383582853750844e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.26it/s] 80%|████████  | 60/75 [00:02<00:00, 25.13it/s]                                               {'loss': 1.7688, 'grad_norm': 6.170637130737305, 'learning_rate': 2.577278386235374e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.13it/s]                                               {'loss': 1.9403, 'grad_norm': 4.339017868041992, 'learning_rate': 2.416198487095663e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.13it/s]                                               {'loss': 2.0442, 'grad_norm': 4.359141826629639, 'learning_rate': 2.255118587955952e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.13it/s] 84%|████████▍ | 63/75 [00:02<00:00, 25.44it/s]                                               {'loss': 2.4352, 'grad_norm': 4.144143104553223, 'learning_rate': 2.0940386888162413e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.44it/s]                                               {'loss': 2.0278, 'grad_norm': 3.79622483253479, 'learning_rate': 1.9329587896765304e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.44it/s]                                               {'loss': 2.1291, 'grad_norm': 4.668354034423828, 'learning_rate': 1.7718788905368195e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.44it/s] 88%|████████▊ | 66/75 [00:02<00:00, 24.97it/s]                                               {'loss': 2.0789, 'grad_norm': 3.1368820667266846, 'learning_rate': 1.6107989913971087e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.97it/s]                                               {'loss': 2.1674, 'grad_norm': 4.629192352294922, 'learning_rate': 1.4497190922573976e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.97it/s]                                               {'loss': 2.121, 'grad_norm': 3.693124771118164, 'learning_rate': 1.288639193117687e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.97it/s] 92%|█████████▏| 69/75 [00:02<00:00, 24.58it/s]                                               {'loss': 2.304, 'grad_norm': 3.0947349071502686, 'learning_rate': 1.127559293977976e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.58it/s]                                               {'loss': 2.2654, 'grad_norm': 4.659165382385254, 'learning_rate': 9.664793948382652e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.58it/s]                                               {'loss': 2.3157, 'grad_norm': 3.190863609313965, 'learning_rate': 8.053994956985543e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.58it/s] 96%|█████████▌| 72/75 [00:02<00:00, 24.86it/s]                                               {'loss': 2.1246, 'grad_norm': 3.487372875213623, 'learning_rate': 6.443195965588435e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.86it/s]                                               {'loss': 1.9856, 'grad_norm': 4.330549716949463, 'learning_rate': 4.832396974191326e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.86it/s]                                               {'loss': 2.0278, 'grad_norm': 4.303250789642334, 'learning_rate': 3.2215979827942174e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.86it/s]100%|██████████| 75/75 [00:02<00:00, 26.01it/s]                                               {'loss': 1.7959, 'grad_norm': 6.609007835388184, 'learning_rate': 1.6107989913971087e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.01it/s]                                               {'train_runtime': 3.091, 'train_samples_per_second': 365.574, 'train_steps_per_second': 24.264, 'train_loss': 2.172026205062866, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 26.01it/s]100%|██████████| 75/75 [00:03<00:00, 24.26it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:43
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3337, 'grad_norm': 3.7218515872955322, 'learning_rate': 0.00012080992435478315, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 17.17it/s]                                              {'loss': 2.2055, 'grad_norm': 3.427248239517212, 'learning_rate': 0.00011919912536338604, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 20.23it/s]  4%|▍         | 3/75 [00:00<00:03, 20.68it/s]                                              {'loss': 2.274, 'grad_norm': 3.2521371841430664, 'learning_rate': 0.00011758832637198894, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 20.68it/s]                                              {'loss': 2.2083, 'grad_norm': 3.5996434688568115, 'learning_rate': 0.00011597752738059181, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 20.68it/s]                                              {'loss': 2.2475, 'grad_norm': 4.14915657043457, 'learning_rate': 0.00011436672838919471, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 20.68it/s]  8%|▊         | 6/75 [00:00<00:03, 21.06it/s]                                              {'loss': 2.0523, 'grad_norm': 3.8142175674438477, 'learning_rate': 0.0001127559293977976, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 21.06it/s]                                              {'loss': 2.397, 'grad_norm': 3.529496669769287, 'learning_rate': 0.0001111451304064005, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 21.06it/s]                                              {'loss': 2.3218, 'grad_norm': 4.059598445892334, 'learning_rate': 0.00010953433141500338, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 21.06it/s] 12%|█▏        | 9/75 [00:00<00:02, 22.94it/s]                                              {'loss': 2.276, 'grad_norm': 4.429042816162109, 'learning_rate': 0.00010792353242360628, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 22.94it/s]                                              {'loss': 2.4746, 'grad_norm': 3.859905242919922, 'learning_rate': 0.00010631273343220917, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 22.94it/s]                                               {'loss': 2.0793, 'grad_norm': 3.8418774604797363, 'learning_rate': 0.00010470193444081207, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 22.94it/s] 16%|█▌        | 12/75 [00:00<00:02, 23.87it/s]                                               {'loss': 2.3773, 'grad_norm': 4.441615581512451, 'learning_rate': 0.00010309113544941496, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 23.87it/s]                                               {'loss': 2.2456, 'grad_norm': 4.110139846801758, 'learning_rate': 0.00010148033645801784, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.87it/s]                                               {'loss': 2.5383, 'grad_norm': 6.516495704650879, 'learning_rate': 9.986953746662073e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.87it/s]                                               {'loss': 2.1294, 'grad_norm': 15.006263732910156, 'learning_rate': 9.825873847522363e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.87it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.57it/s]                                               {'loss': 2.1186, 'grad_norm': 4.308981895446777, 'learning_rate': 9.664793948382652e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.57it/s]                                               {'loss': 2.3604, 'grad_norm': 4.639937400817871, 'learning_rate': 9.503714049242941e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.57it/s]                                               {'loss': 2.4013, 'grad_norm': 4.008825778961182, 'learning_rate': 9.34263415010323e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.57it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.34it/s]                                               {'loss': 2.3146, 'grad_norm': 4.497251987457275, 'learning_rate': 9.18155425096352e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.34it/s]                                               {'loss': 2.2302, 'grad_norm': 3.686889171600342, 'learning_rate': 9.020474351823809e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.34it/s]                                               {'loss': 2.4907, 'grad_norm': 4.761348724365234, 'learning_rate': 8.859394452684097e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.34it/s] 29%|██▉       | 22/75 [00:00<00:02, 26.05it/s]                                               {'loss': 2.2931, 'grad_norm': 4.121064186096191, 'learning_rate': 8.698314553544386e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 26.05it/s]                                               {'loss': 2.1871, 'grad_norm': 2.9860217571258545, 'learning_rate': 8.537234654404676e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:01, 26.05it/s]                                               {'loss': 2.222, 'grad_norm': 2.958665609359741, 'learning_rate': 8.376154755264965e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 26.05it/s] 33%|███▎      | 25/75 [00:01<00:02, 24.46it/s]                                               {'loss': 1.9141, 'grad_norm': 3.4604365825653076, 'learning_rate': 8.215074856125255e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.46it/s]                                               {'loss': 2.1317, 'grad_norm': 3.304758310317993, 'learning_rate': 8.053994956985543e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.46it/s]                                               {'loss': 2.2755, 'grad_norm': 3.3852646350860596, 'learning_rate': 7.892915057845831e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.46it/s] 37%|███▋      | 28/75 [00:01<00:01, 23.85it/s]                                               {'loss': 2.021, 'grad_norm': 3.0179967880249023, 'learning_rate': 7.731835158706122e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 23.85it/s]                                               {'loss': 2.1723, 'grad_norm': 3.3128647804260254, 'learning_rate': 7.57075525956641e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 23.85it/s]                                               {'loss': 2.261, 'grad_norm': 20.32912254333496, 'learning_rate': 7.409675360426699e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.85it/s]                                               {'loss': 2.1303, 'grad_norm': 3.009906053543091, 'learning_rate': 7.248595461286988e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 23.85it/s] 43%|████▎     | 32/75 [00:01<00:01, 25.03it/s]                                               {'loss': 2.3861, 'grad_norm': 3.550539970397949, 'learning_rate': 7.087515562147278e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.03it/s]                                               {'loss': 2.3254, 'grad_norm': 5.040048599243164, 'learning_rate': 6.926435663007567e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.03it/s]                                               {'loss': 2.0768, 'grad_norm': 3.400351047515869, 'learning_rate': 6.765355763867857e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.03it/s] 47%|████▋     | 35/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.1282, 'grad_norm': 3.396181344985962, 'learning_rate': 6.604275864728145e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.415, 'grad_norm': 3.771890163421631, 'learning_rate': 6.443195965588435e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.2061, 'grad_norm': 4.002682685852051, 'learning_rate': 6.282116066448724e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.13it/s] 51%|█████     | 38/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.1836, 'grad_norm': 4.294719696044922, 'learning_rate': 6.121036167309014e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.2094, 'grad_norm': 4.131132125854492, 'learning_rate': 5.959956268169302e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.34it/s]                                               {'loss': 2.1603, 'grad_norm': 3.1531951427459717, 'learning_rate': 5.7988763690295906e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.34it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.28it/s]                                               {'loss': 2.391, 'grad_norm': 4.071737289428711, 'learning_rate': 5.63779646988988e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.28it/s]                                               {'loss': 2.3263, 'grad_norm': 5.040724754333496, 'learning_rate': 5.476716570750169e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.28it/s]                                               {'loss': 2.1582, 'grad_norm': 3.699308156967163, 'learning_rate': 5.315636671610458e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.28it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.29it/s]                                               {'loss': 2.1632, 'grad_norm': 3.4345500469207764, 'learning_rate': 5.154556772470748e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.29it/s]                                               {'loss': 1.9568, 'grad_norm': 8.832167625427246, 'learning_rate': 4.9934768733310366e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.29it/s]                                               {'loss': 2.2133, 'grad_norm': 3.707026958465576, 'learning_rate': 4.832396974191326e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.29it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.47it/s]                                               {'loss': 2.1418, 'grad_norm': 2.888767719268799, 'learning_rate': 4.671317075051615e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.47it/s]                                               {'loss': 2.1052, 'grad_norm': 3.709726333618164, 'learning_rate': 4.510237175911904e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.47it/s]                                               {'loss': 2.2195, 'grad_norm': 4.329527854919434, 'learning_rate': 4.349157276772193e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.47it/s] 67%|██████▋   | 50/75 [00:02<00:00, 25.38it/s]                                               {'loss': 2.132, 'grad_norm': 4.077300071716309, 'learning_rate': 4.1880773776324826e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.38it/s]                                               {'loss': 2.2966, 'grad_norm': 3.2844090461730957, 'learning_rate': 4.0269974784927714e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.38it/s]                                               {'loss': 2.0486, 'grad_norm': 2.7079601287841797, 'learning_rate': 3.865917579353061e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.38it/s] 71%|███████   | 53/75 [00:02<00:00, 25.08it/s]                                               {'loss': 2.3302, 'grad_norm': 3.3993263244628906, 'learning_rate': 3.7048376802133496e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.08it/s]                                               {'loss': 2.1159, 'grad_norm': 4.8924336433410645, 'learning_rate': 3.543757781073639e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.08it/s]                                               {'loss': 2.1876, 'grad_norm': 3.1532034873962402, 'learning_rate': 3.3826778819339286e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.08it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.06it/s]                                               {'loss': 2.2174, 'grad_norm': 4.084054470062256, 'learning_rate': 3.2215979827942174e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.06it/s]                                               {'loss': 2.1066, 'grad_norm': 3.451517343521118, 'learning_rate': 3.060518083654507e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.06it/s]                                               {'loss': 2.2774, 'grad_norm': 3.334404230117798, 'learning_rate': 2.8994381845147953e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.06it/s] 79%|███████▊  | 59/75 [00:02<00:00, 24.93it/s]                                               {'loss': 2.4114, 'grad_norm': 5.149776458740234, 'learning_rate': 2.7383582853750844e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.93it/s]                                               {'loss': 2.4625, 'grad_norm': 11.484786033630371, 'learning_rate': 2.577278386235374e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.93it/s]                                               {'loss': 2.2964, 'grad_norm': 4.20208215713501, 'learning_rate': 2.416198487095663e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.93it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.10it/s]                                               {'loss': 2.0993, 'grad_norm': 3.726398229598999, 'learning_rate': 2.255118587955952e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.10it/s]                                               {'loss': 2.5769, 'grad_norm': 4.291277885437012, 'learning_rate': 2.0940386888162413e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.10it/s]                                               {'loss': 2.3356, 'grad_norm': 3.723963975906372, 'learning_rate': 1.9329587896765304e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.10it/s] 87%|████████▋ | 65/75 [00:02<00:00, 25.18it/s]                                               {'loss': 2.2703, 'grad_norm': 4.625814437866211, 'learning_rate': 1.7718788905368195e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.18it/s]                                               {'loss': 2.041, 'grad_norm': 6.147265434265137, 'learning_rate': 1.6107989913971087e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.18it/s]                                               {'loss': 1.9403, 'grad_norm': 2.8290340900421143, 'learning_rate': 1.4497190922573976e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.18it/s] 91%|█████████ | 68/75 [00:02<00:00, 24.15it/s]                                               {'loss': 2.1151, 'grad_norm': 3.5691261291503906, 'learning_rate': 1.288639193117687e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.15it/s]                                               {'loss': 2.1181, 'grad_norm': 4.259798526763916, 'learning_rate': 1.127559293977976e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.15it/s]                                               {'loss': 2.1321, 'grad_norm': 3.9161877632141113, 'learning_rate': 9.664793948382652e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.15it/s] 95%|█████████▍| 71/75 [00:02<00:00, 24.22it/s]                                               {'loss': 2.1944, 'grad_norm': 3.8131823539733887, 'learning_rate': 8.053994956985543e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.22it/s]                                               {'loss': 2.2496, 'grad_norm': 3.6766719818115234, 'learning_rate': 6.443195965588435e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 24.22it/s]                                               {'loss': 2.141, 'grad_norm': 3.500262975692749, 'learning_rate': 4.832396974191326e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 24.22it/s] 99%|█████████▊| 74/75 [00:02<00:00, 24.15it/s]                                               {'loss': 2.2542, 'grad_norm': 3.807431936264038, 'learning_rate': 3.2215979827942174e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.15it/s]                                               {'loss': 2.8253, 'grad_norm': 13.809301376342773, 'learning_rate': 1.6107989913971087e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.15it/s]                                               {'train_runtime': 3.1158, 'train_samples_per_second': 362.662, 'train_steps_per_second': 24.071, 'train_loss': 2.23501980304718, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.15it/s]100%|██████████| 75/75 [00:03<00:00, 24.10it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(930, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(857, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(735, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(847, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1061, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(835, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(943, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(764, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(807, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(525, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(989, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(484, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 65.48it/s]  3%|▎         | 14/471 [00:00<00:07, 57.35it/s]  4%|▍         | 20/471 [00:00<00:08, 53.74it/s]  6%|▌         | 26/471 [00:00<00:08, 51.17it/s]  7%|▋         | 32/471 [00:00<00:08, 50.46it/s]  8%|▊         | 38/471 [00:00<00:08, 51.40it/s]  9%|▉         | 44/471 [00:00<00:08, 52.13it/s] 11%|█         | 50/471 [00:00<00:08, 51.45it/s] 12%|█▏        | 56/471 [00:01<00:08, 51.54it/s] 13%|█▎        | 62/471 [00:01<00:08, 49.76it/s] 14%|█▍        | 67/471 [00:01<00:08, 48.88it/s] 15%|█▌        | 72/471 [00:01<00:08, 47.76it/s] 17%|█▋        | 78/471 [00:01<00:07, 49.15it/s] 18%|█▊        | 84/471 [00:01<00:07, 50.51it/s] 19%|█▉        | 90/471 [00:01<00:07, 50.09it/s] 20%|██        | 96/471 [00:01<00:07, 50.87it/s] 22%|██▏       | 102/471 [00:01<00:07, 51.56it/s] 23%|██▎       | 108/471 [00:02<00:07, 48.83it/s] 24%|██▍       | 113/471 [00:02<00:07, 48.91it/s] 25%|██▌       | 118/471 [00:02<00:07, 48.24it/s] 26%|██▌       | 123/471 [00:02<00:07, 47.29it/s] 27%|██▋       | 129/471 [00:02<00:06, 49.09it/s] 29%|██▊       | 135/471 [00:02<00:06, 50.04it/s] 30%|██▉       | 141/471 [00:02<00:06, 51.08it/s] 31%|███       | 147/471 [00:02<00:06, 51.49it/s] 32%|███▏      | 153/471 [00:03<00:06, 51.04it/s] 34%|███▍      | 159/471 [00:03<00:05, 52.01it/s] 35%|███▌      | 165/471 [00:03<00:05, 51.80it/s] 36%|███▋      | 171/471 [00:03<00:05, 51.98it/s] 38%|███▊      | 177/471 [00:03<00:05, 52.14it/s] 39%|███▉      | 183/471 [00:03<00:05, 52.10it/s] 40%|████      | 189/471 [00:03<00:05, 52.30it/s] 41%|████▏     | 195/471 [00:03<00:05, 47.91it/s] 43%|████▎     | 201/471 [00:03<00:05, 48.91it/s] 44%|████▎     | 206/471 [00:04<00:05, 48.74it/s] 45%|████▌     | 212/471 [00:04<00:05, 50.43it/s] 46%|████▋     | 218/471 [00:04<00:04, 51.12it/s] 48%|████▊     | 224/471 [00:04<00:04, 51.64it/s] 49%|████▉     | 230/471 [00:04<00:04, 52.09it/s] 50%|█████     | 236/471 [00:04<00:04, 52.29it/s] 51%|█████▏    | 242/471 [00:04<00:04, 52.48it/s] 53%|█████▎    | 248/471 [00:04<00:04, 52.68it/s] 54%|█████▍    | 254/471 [00:04<00:04, 52.50it/s] 55%|█████▌    | 260/471 [00:05<00:04, 51.55it/s] 56%|█████▋    | 266/471 [00:05<00:03, 51.49it/s] 58%|█████▊    | 272/471 [00:05<00:03, 51.95it/s] 59%|█████▉    | 278/471 [00:05<00:03, 50.62it/s] 60%|██████    | 284/471 [00:05<00:03, 51.32it/s] 62%|██████▏   | 290/471 [00:05<00:03, 50.94it/s] 63%|██████▎   | 296/471 [00:05<00:03, 51.39it/s] 64%|██████▍   | 302/471 [00:05<00:03, 51.13it/s] 65%|██████▌   | 308/471 [00:06<00:03, 50.66it/s] 67%|██████▋   | 314/471 [00:06<00:03, 51.19it/s] 68%|██████▊   | 320/471 [00:06<00:02, 50.97it/s] 69%|██████▉   | 326/471 [00:06<00:02, 49.71it/s] 70%|███████   | 332/471 [00:06<00:02, 50.86it/s] 72%|███████▏  | 338/471 [00:06<00:02, 49.34it/s] 73%|███████▎  | 344/471 [00:06<00:02, 50.15it/s] 74%|███████▍  | 350/471 [00:06<00:02, 50.81it/s] 76%|███████▌  | 356/471 [00:06<00:02, 51.48it/s] 77%|███████▋  | 362/471 [00:07<00:02, 50.92it/s] 78%|███████▊  | 368/471 [00:07<00:02, 51.33it/s] 79%|███████▉  | 374/471 [00:07<00:01, 50.37it/s] 81%|████████  | 380/471 [00:07<00:01, 51.06it/s] 82%|████████▏ | 386/471 [00:07<00:01, 51.33it/s] 83%|████████▎ | 392/471 [00:07<00:01, 51.70it/s] 85%|████████▍ | 398/471 [00:07<00:01, 51.26it/s] 86%|████████▌ | 404/471 [00:07<00:01, 51.58it/s] 87%|████████▋ | 410/471 [00:08<00:01, 51.86it/s] 88%|████████▊ | 416/471 [00:08<00:01, 51.97it/s] 90%|████████▉ | 422/471 [00:08<00:00, 52.32it/s] 91%|█████████ | 428/471 [00:08<00:00, 52.37it/s] 92%|█████████▏| 434/471 [00:08<00:00, 52.34it/s] 93%|█████████▎| 440/471 [00:08<00:00, 52.37it/s] 95%|█████████▍| 446/471 [00:08<00:00, 52.55it/s] 96%|█████████▌| 452/471 [00:08<00:00, 52.24it/s] 97%|█████████▋| 458/471 [00:08<00:00, 52.19it/s] 99%|█████████▊| 464/471 [00:09<00:00, 50.61it/s]100%|█████████▉| 470/471 [00:09<00:00, 50.92it/s]100%|██████████| 471/471 [00:09<00:00, 51.09it/s]
{'eval_loss': 2.261770725250244, 'eval_model_preparation_time': 0.0029, 'eval_acc': 0.3980350504514073, 'eval_runtime': 9.2399, 'eval_samples_per_second': 815.158, 'eval_steps_per_second': 50.974}
ROUND:23
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3071, 'grad_norm': 4.8428778648376465, 'learning_rate': 0.00011920963587930234, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 16.53it/s]  3%|▎         | 2/75 [00:00<00:03, 19.88it/s]                                              {'loss': 2.2655, 'grad_norm': 4.266244411468506, 'learning_rate': 0.00011762017406757832, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 19.88it/s]                                              {'loss': 2.2001, 'grad_norm': 4.226320743560791, 'learning_rate': 0.00011603071225585428, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 19.88it/s]                                              {'loss': 2.1291, 'grad_norm': 4.373096466064453, 'learning_rate': 0.00011444125044413025, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 19.88it/s]  7%|▋         | 5/75 [00:00<00:03, 22.12it/s]                                              {'loss': 2.2825, 'grad_norm': 4.856396675109863, 'learning_rate': 0.00011285178863240621, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.12it/s]                                              {'loss': 2.4488, 'grad_norm': 4.495315074920654, 'learning_rate': 0.00011126232682068219, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 22.12it/s]                                              {'loss': 2.2292, 'grad_norm': 3.79929780960083, 'learning_rate': 0.00010967286500895816, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 22.12it/s] 11%|█         | 8/75 [00:00<00:02, 23.92it/s]                                              {'loss': 2.2399, 'grad_norm': 4.077273368835449, 'learning_rate': 0.00010808340319723412, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.92it/s]                                              {'loss': 2.55, 'grad_norm': 5.091970920562744, 'learning_rate': 0.00010649394138551008, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.92it/s]                                              {'loss': 2.3982, 'grad_norm': 4.694037437438965, 'learning_rate': 0.00010490447957378606, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.92it/s] 15%|█▍        | 11/75 [00:00<00:02, 24.80it/s]                                               {'loss': 2.1143, 'grad_norm': 3.695868492126465, 'learning_rate': 0.00010331501776206203, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.80it/s]                                               {'loss': 2.1047, 'grad_norm': 3.225102663040161, 'learning_rate': 0.000101725555950338, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.80it/s]                                               {'loss': 2.5635, 'grad_norm': 4.969371795654297, 'learning_rate': 0.00010013609413861397, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.80it/s] 19%|█▊        | 14/75 [00:00<00:02, 24.08it/s]                                               {'loss': 2.2494, 'grad_norm': 4.499203205108643, 'learning_rate': 9.854663232688993e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.08it/s]                                               {'loss': 2.3891, 'grad_norm': 19.256196975708008, 'learning_rate': 9.695717051516591e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.08it/s]                                               {'loss': 2.2516, 'grad_norm': 4.137110710144043, 'learning_rate': 9.536770870344188e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 24.08it/s] 23%|██▎       | 17/75 [00:00<00:02, 24.63it/s]                                               {'loss': 2.193, 'grad_norm': 3.876112699508667, 'learning_rate': 9.377824689171784e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.63it/s]                                               {'loss': 2.1961, 'grad_norm': 4.530632972717285, 'learning_rate': 9.21887850799938e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.63it/s]                                               {'loss': 2.0433, 'grad_norm': 3.016754150390625, 'learning_rate': 9.059932326826978e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.63it/s] 27%|██▋       | 20/75 [00:00<00:02, 24.66it/s]                                               {'loss': 2.4642, 'grad_norm': 3.3992562294006348, 'learning_rate': 8.900986145654575e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.66it/s]                                               {'loss': 2.1849, 'grad_norm': 3.9962944984436035, 'learning_rate': 8.742039964482171e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.66it/s]                                               {'loss': 2.341, 'grad_norm': 3.957608938217163, 'learning_rate': 8.583093783309768e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.66it/s] 31%|███       | 23/75 [00:00<00:02, 23.98it/s]                                               {'loss': 2.4545, 'grad_norm': 3.76289701461792, 'learning_rate': 8.424147602137365e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 23.98it/s]                                               {'loss': 2.3532, 'grad_norm': 3.7415199279785156, 'learning_rate': 8.265201420964963e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 23.98it/s]                                               {'loss': 2.2289, 'grad_norm': 4.062475204467773, 'learning_rate': 8.10625523979256e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.98it/s] 35%|███▍      | 26/75 [00:01<00:02, 24.45it/s]                                               {'loss': 2.1697, 'grad_norm': 3.6601078510284424, 'learning_rate': 7.947309058620156e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 24.45it/s]                                               {'loss': 2.188, 'grad_norm': 3.1298182010650635, 'learning_rate': 7.788362877447753e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.45it/s]                                               {'loss': 2.4262, 'grad_norm': 5.023100852966309, 'learning_rate': 7.62941669627535e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.45it/s] 39%|███▊      | 29/75 [00:01<00:01, 24.63it/s]                                               {'loss': 2.2174, 'grad_norm': 3.555053234100342, 'learning_rate': 7.470470515102947e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.63it/s]                                               {'loss': 1.9412, 'grad_norm': 12.880200386047363, 'learning_rate': 7.311524333930543e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.63it/s]                                               {'loss': 2.3346, 'grad_norm': 3.5103557109832764, 'learning_rate': 7.15257815275814e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.63it/s] 43%|████▎     | 32/75 [00:01<00:01, 25.93it/s]                                               {'loss': 2.2941, 'grad_norm': 4.821526527404785, 'learning_rate': 6.993631971585737e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.93it/s]                                               {'loss': 2.301, 'grad_norm': 5.2804484367370605, 'learning_rate': 6.834685790413334e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.93it/s]                                               {'loss': 2.3479, 'grad_norm': 4.947579383850098, 'learning_rate': 6.675739609240932e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.93it/s] 47%|████▋     | 35/75 [00:01<00:01, 25.71it/s]                                               {'loss': 2.0441, 'grad_norm': 3.774583578109741, 'learning_rate': 6.516793428068528e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.71it/s]                                               {'loss': 2.0924, 'grad_norm': 2.7685937881469727, 'learning_rate': 6.357847246896125e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.71it/s]                                               {'loss': 2.0178, 'grad_norm': 3.9469823837280273, 'learning_rate': 6.198901065723722e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.71it/s] 51%|█████     | 38/75 [00:01<00:01, 25.47it/s]                                               {'loss': 2.1105, 'grad_norm': 3.497514009475708, 'learning_rate': 6.039954884551319e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.47it/s]                                               {'loss': 2.2804, 'grad_norm': 5.045560359954834, 'learning_rate': 5.881008703378916e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.47it/s]                                               {'loss': 2.2697, 'grad_norm': 3.7373218536376953, 'learning_rate': 5.7220625222065124e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.47it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.2385, 'grad_norm': 5.531085014343262, 'learning_rate': 5.5631163410341095e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.3029, 'grad_norm': 5.269560813903809, 'learning_rate': 5.404170159861706e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.1628, 'grad_norm': 4.011134624481201, 'learning_rate': 5.245223978689303e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.24it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.5508, 'grad_norm': 4.3914055824279785, 'learning_rate': 5.0862777975169e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.37it/s]                                               {'loss': 1.4446, 'grad_norm': 6.007359027862549, 'learning_rate': 4.927331616344497e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.1336, 'grad_norm': 3.753652811050415, 'learning_rate': 4.768385435172094e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.37it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.39it/s]                                               {'loss': 2.18, 'grad_norm': 4.006908893585205, 'learning_rate': 4.60943925399969e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.39it/s]                                               {'loss': 2.2817, 'grad_norm': 3.853144884109497, 'learning_rate': 4.4504930728272874e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.39it/s]                                               {'loss': 2.4053, 'grad_norm': 3.7631099224090576, 'learning_rate': 4.291546891654884e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.39it/s] 67%|██████▋   | 50/75 [00:02<00:01, 24.72it/s]                                               {'loss': 2.1705, 'grad_norm': 4.336297988891602, 'learning_rate': 4.1326007104824816e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 24.72it/s]                                               {'loss': 2.4012, 'grad_norm': 4.211477279663086, 'learning_rate': 3.973654529310078e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 24.72it/s]                                               {'loss': 2.1211, 'grad_norm': 4.137050151824951, 'learning_rate': 3.814708348137675e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.72it/s] 71%|███████   | 53/75 [00:02<00:00, 24.49it/s]                                               {'loss': 2.2432, 'grad_norm': 4.30918025970459, 'learning_rate': 3.6557621669652716e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.49it/s]                                               {'loss': 2.2885, 'grad_norm': 4.522202491760254, 'learning_rate': 3.496815985792869e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.49it/s]                                               {'loss': 2.3011, 'grad_norm': 3.2591652870178223, 'learning_rate': 3.337869804620466e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.49it/s] 75%|███████▍  | 56/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.2349, 'grad_norm': 3.5294463634490967, 'learning_rate': 3.178923623448062e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.098, 'grad_norm': 3.659421443939209, 'learning_rate': 3.0199774422756594e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.1043, 'grad_norm': 3.4242990016937256, 'learning_rate': 2.8610312611032562e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.83it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.2992, 'grad_norm': 3.7589287757873535, 'learning_rate': 2.702085079930853e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.26it/s]                                               {'loss': 1.6728, 'grad_norm': 8.149493217468262, 'learning_rate': 2.54313889875845e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.1527, 'grad_norm': 3.3891303539276123, 'learning_rate': 2.384192717586047e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.1727, 'grad_norm': 4.054765224456787, 'learning_rate': 2.2252465364136437e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.26it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.82it/s]                                               {'loss': 2.143, 'grad_norm': 3.2688848972320557, 'learning_rate': 2.0663003552412408e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.82it/s]                                               {'loss': 2.2758, 'grad_norm': 3.5710716247558594, 'learning_rate': 1.9073541740688376e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.82it/s]                                               {'loss': 2.2406, 'grad_norm': 4.397364616394043, 'learning_rate': 1.7484079928964344e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.82it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.76it/s]                                               {'loss': 2.3588, 'grad_norm': 4.316433906555176, 'learning_rate': 1.589461811724031e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.76it/s]                                               {'loss': 2.2361, 'grad_norm': 3.543973922729492, 'learning_rate': 1.4305156305516281e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.76it/s]                                               {'loss': 2.0697, 'grad_norm': 3.222191333770752, 'learning_rate': 1.271569449379225e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.76it/s] 92%|█████████▏| 69/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.2116, 'grad_norm': 4.467675685882568, 'learning_rate': 1.1126232682068218e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.4594, 'grad_norm': 4.899524688720703, 'learning_rate': 9.536770870344188e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.26it/s]                                               {'loss': 2.1868, 'grad_norm': 4.195858478546143, 'learning_rate': 7.947309058620156e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.26it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.2024, 'grad_norm': 3.6283929347991943, 'learning_rate': 6.357847246896125e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.0887, 'grad_norm': 4.458388805389404, 'learning_rate': 4.768385435172094e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.14it/s]                                               {'loss': 2.374, 'grad_norm': 4.112726211547852, 'learning_rate': 3.1789236234480626e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.14it/s]100%|██████████| 75/75 [00:02<00:00, 25.93it/s]                                               {'loss': 2.1024, 'grad_norm': 14.382407188415527, 'learning_rate': 1.5894618117240313e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.93it/s]                                               {'train_runtime': 3.1206, 'train_samples_per_second': 362.111, 'train_steps_per_second': 24.034, 'train_loss': 2.2283590857187905, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.93it/s]100%|██████████| 75/75 [00:03<00:00, 24.04it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.1966, 'grad_norm': 4.568873882293701, 'learning_rate': 0.00011920963587930234, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 19.92it/s]                                              {'loss': 2.215, 'grad_norm': 4.445551872253418, 'learning_rate': 0.00011762017406757832, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 23.08it/s]  4%|▍         | 3/75 [00:00<00:03, 22.61it/s]                                              {'loss': 2.1155, 'grad_norm': 4.357388496398926, 'learning_rate': 0.00011603071225585428, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 22.61it/s]                                              {'loss': 2.2415, 'grad_norm': 3.7574777603149414, 'learning_rate': 0.00011444125044413025, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 22.61it/s]                                              {'loss': 2.1449, 'grad_norm': 4.23737907409668, 'learning_rate': 0.00011285178863240621, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.61it/s]  8%|▊         | 6/75 [00:00<00:02, 23.38it/s]                                              {'loss': 2.3106, 'grad_norm': 3.7632126808166504, 'learning_rate': 0.00011126232682068219, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.38it/s]                                              {'loss': 2.3162, 'grad_norm': 3.8484482765197754, 'learning_rate': 0.00010967286500895816, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.38it/s]                                              {'loss': 2.2277, 'grad_norm': 4.149094104766846, 'learning_rate': 0.00010808340319723412, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.38it/s] 12%|█▏        | 9/75 [00:00<00:02, 23.95it/s]                                              {'loss': 2.3121, 'grad_norm': 3.5505805015563965, 'learning_rate': 0.00010649394138551008, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.95it/s]                                              {'loss': 2.3571, 'grad_norm': 4.305237293243408, 'learning_rate': 0.00010490447957378606, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.95it/s]                                               {'loss': 2.2146, 'grad_norm': 4.133017539978027, 'learning_rate': 0.00010331501776206203, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 23.95it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.23it/s]                                               {'loss': 2.1907, 'grad_norm': 4.710526943206787, 'learning_rate': 0.000101725555950338, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.23it/s]                                               {'loss': 2.3289, 'grad_norm': 4.170456409454346, 'learning_rate': 0.00010013609413861397, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.23it/s]                                               {'loss': 2.1303, 'grad_norm': 3.97632098197937, 'learning_rate': 9.854663232688993e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.23it/s] 20%|██        | 15/75 [00:00<00:02, 25.81it/s]                                               {'loss': 2.3257, 'grad_norm': 13.794232368469238, 'learning_rate': 9.695717051516591e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.81it/s]                                               {'loss': 2.0531, 'grad_norm': 2.8193490505218506, 'learning_rate': 9.536770870344188e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.81it/s]                                               {'loss': 2.3375, 'grad_norm': 4.181488990783691, 'learning_rate': 9.377824689171784e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.81it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.28it/s]                                               {'loss': 2.2438, 'grad_norm': 3.360374689102173, 'learning_rate': 9.21887850799938e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.28it/s]                                               {'loss': 2.2339, 'grad_norm': 4.424097061157227, 'learning_rate': 9.059932326826978e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.28it/s]                                               {'loss': 2.3192, 'grad_norm': 3.81264591217041, 'learning_rate': 8.900986145654575e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.28it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.43it/s]                                               {'loss': 2.0504, 'grad_norm': 3.301499366760254, 'learning_rate': 8.742039964482171e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.43it/s]                                               {'loss': 2.0371, 'grad_norm': 3.886091947555542, 'learning_rate': 8.583093783309768e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.43it/s]                                               {'loss': 2.2215, 'grad_norm': 4.417205333709717, 'learning_rate': 8.424147602137365e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.43it/s] 32%|███▏      | 24/75 [00:00<00:01, 25.69it/s]                                               {'loss': 2.2098, 'grad_norm': 3.2483463287353516, 'learning_rate': 8.265201420964963e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.69it/s]                                               {'loss': 2.2278, 'grad_norm': 3.598426342010498, 'learning_rate': 8.10625523979256e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.69it/s]                                               {'loss': 2.0748, 'grad_norm': 5.983978271484375, 'learning_rate': 7.947309058620156e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.69it/s] 36%|███▌      | 27/75 [00:01<00:01, 25.17it/s]                                               {'loss': 2.1801, 'grad_norm': 3.899552822113037, 'learning_rate': 7.788362877447753e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.17it/s]                                               {'loss': 2.2251, 'grad_norm': 4.432063102722168, 'learning_rate': 7.62941669627535e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.17it/s]                                               {'loss': 2.2878, 'grad_norm': 6.289440631866455, 'learning_rate': 7.470470515102947e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.17it/s] 40%|████      | 30/75 [00:01<00:01, 26.41it/s]                                               {'loss': 2.0072, 'grad_norm': 12.953636169433594, 'learning_rate': 7.311524333930543e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 26.41it/s]                                               {'loss': 2.0862, 'grad_norm': 3.510693311691284, 'learning_rate': 7.15257815275814e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.41it/s]                                               {'loss': 2.2462, 'grad_norm': 5.6147565841674805, 'learning_rate': 6.993631971585737e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.41it/s] 44%|████▍     | 33/75 [00:01<00:01, 26.27it/s]                                               {'loss': 2.1816, 'grad_norm': 4.7970499992370605, 'learning_rate': 6.834685790413334e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.27it/s]                                               {'loss': 2.0413, 'grad_norm': 2.8816826343536377, 'learning_rate': 6.675739609240932e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.27it/s]                                               {'loss': 2.1806, 'grad_norm': 5.755896091461182, 'learning_rate': 6.516793428068528e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.27it/s] 48%|████▊     | 36/75 [00:01<00:01, 26.15it/s]                                               {'loss': 2.1004, 'grad_norm': 4.213276386260986, 'learning_rate': 6.357847246896125e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.15it/s]                                               {'loss': 2.0239, 'grad_norm': 3.7235281467437744, 'learning_rate': 6.198901065723722e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.15it/s]                                               {'loss': 2.1543, 'grad_norm': 3.2398602962493896, 'learning_rate': 6.039954884551319e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.15it/s] 52%|█████▏    | 39/75 [00:01<00:01, 25.65it/s]                                               {'loss': 2.0869, 'grad_norm': 4.603306293487549, 'learning_rate': 5.881008703378916e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.65it/s]                                               {'loss': 2.2207, 'grad_norm': 3.842031240463257, 'learning_rate': 5.7220625222065124e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.65it/s]                                               {'loss': 2.0766, 'grad_norm': 4.170713424682617, 'learning_rate': 5.5631163410341095e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.65it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.82it/s]                                               {'loss': 2.1625, 'grad_norm': 4.518084526062012, 'learning_rate': 5.404170159861706e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.82it/s]                                               {'loss': 2.153, 'grad_norm': 3.7479045391082764, 'learning_rate': 5.245223978689303e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.82it/s]                                               {'loss': 2.2535, 'grad_norm': 3.2456490993499756, 'learning_rate': 5.0862777975169e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.82it/s] 60%|██████    | 45/75 [00:01<00:01, 24.21it/s]                                               {'loss': 2.6851, 'grad_norm': 15.072922706604004, 'learning_rate': 4.927331616344497e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.21it/s]                                               {'loss': 2.2708, 'grad_norm': 3.6975460052490234, 'learning_rate': 4.768385435172094e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 24.21it/s]                                               {'loss': 2.4008, 'grad_norm': 4.9876179695129395, 'learning_rate': 4.60943925399969e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 24.21it/s] 64%|██████▍   | 48/75 [00:01<00:01, 24.25it/s]                                               {'loss': 2.1878, 'grad_norm': 3.7469899654388428, 'learning_rate': 4.4504930728272874e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 24.25it/s]                                               {'loss': 2.2199, 'grad_norm': 3.757854461669922, 'learning_rate': 4.291546891654884e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 24.25it/s]                                               {'loss': 2.2485, 'grad_norm': 5.230559825897217, 'learning_rate': 4.1326007104824816e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:01, 24.25it/s] 68%|██████▊   | 51/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.0713, 'grad_norm': 3.0370328426361084, 'learning_rate': 3.973654529310078e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.0849, 'grad_norm': 3.4021389484405518, 'learning_rate': 3.814708348137675e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.007, 'grad_norm': 3.687741994857788, 'learning_rate': 3.6557621669652716e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.83it/s] 72%|███████▏  | 54/75 [00:02<00:00, 24.50it/s]                                               {'loss': 2.0929, 'grad_norm': 4.781384468078613, 'learning_rate': 3.496815985792869e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.50it/s]                                               {'loss': 2.2778, 'grad_norm': 4.593976020812988, 'learning_rate': 3.337869804620466e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.50it/s]                                               {'loss': 2.1161, 'grad_norm': 4.211784839630127, 'learning_rate': 3.178923623448062e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.50it/s] 76%|███████▌  | 57/75 [00:02<00:00, 23.92it/s]                                               {'loss': 2.0517, 'grad_norm': 3.8846287727355957, 'learning_rate': 3.0199774422756594e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 23.92it/s]                                               {'loss': 2.1537, 'grad_norm': 4.199094295501709, 'learning_rate': 2.8610312611032562e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 23.92it/s]                                               {'loss': 2.0066, 'grad_norm': 2.962761402130127, 'learning_rate': 2.702085079930853e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 23.92it/s] 80%|████████  | 60/75 [00:02<00:00, 21.74it/s]                                               {'loss': 1.8877, 'grad_norm': 8.165760040283203, 'learning_rate': 2.54313889875845e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 21.74it/s]                                               {'loss': 2.0569, 'grad_norm': 3.7015037536621094, 'learning_rate': 2.384192717586047e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 21.74it/s]                                               {'loss': 2.1369, 'grad_norm': 3.011554002761841, 'learning_rate': 2.2252465364136437e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 21.74it/s] 84%|████████▍ | 63/75 [00:02<00:00, 20.74it/s]                                               {'loss': 2.1911, 'grad_norm': 5.180500507354736, 'learning_rate': 2.0663003552412408e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 20.74it/s]                                               {'loss': 2.1651, 'grad_norm': 4.426010608673096, 'learning_rate': 1.9073541740688376e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 20.74it/s]                                               {'loss': 2.1226, 'grad_norm': 4.5929646492004395, 'learning_rate': 1.7484079928964344e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 20.74it/s] 88%|████████▊ | 66/75 [00:02<00:00, 19.71it/s]                                               {'loss': 2.1657, 'grad_norm': 3.9121761322021484, 'learning_rate': 1.589461811724031e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 19.71it/s]                                               {'loss': 2.1762, 'grad_norm': 3.135737419128418, 'learning_rate': 1.4305156305516281e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 19.71it/s]                                               {'loss': 2.1857, 'grad_norm': 5.385634422302246, 'learning_rate': 1.271569449379225e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 19.71it/s] 92%|█████████▏| 69/75 [00:02<00:00, 20.39it/s]                                               {'loss': 2.0776, 'grad_norm': 5.037076473236084, 'learning_rate': 1.1126232682068218e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 20.39it/s]                                               {'loss': 2.0148, 'grad_norm': 3.4717416763305664, 'learning_rate': 9.536770870344188e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 20.39it/s]                                               {'loss': 2.0941, 'grad_norm': 4.66142463684082, 'learning_rate': 7.947309058620156e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 20.39it/s] 96%|█████████▌| 72/75 [00:03<00:00, 19.66it/s]                                               {'loss': 2.2171, 'grad_norm': 3.916511297225952, 'learning_rate': 6.357847246896125e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 19.66it/s]                                               {'loss': 2.2494, 'grad_norm': 3.2981579303741455, 'learning_rate': 4.768385435172094e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 19.66it/s] 99%|█████████▊| 74/75 [00:03<00:00, 18.96it/s]                                               {'loss': 2.1989, 'grad_norm': 3.9383063316345215, 'learning_rate': 3.1789236234480626e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 18.96it/s]                                               {'loss': 1.732, 'grad_norm': 9.545988082885742, 'learning_rate': 1.5894618117240313e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 18.96it/s]                                               {'train_runtime': 3.5637, 'train_samples_per_second': 317.09, 'train_steps_per_second': 21.046, 'train_loss': 2.171347246170044, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 18.96it/s]100%|██████████| 75/75 [00:03<00:00, 21.05it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:47
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2451, 'grad_norm': 4.646678447723389, 'learning_rate': 0.00011920963587930234, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 18.11it/s]                                              {'loss': 2.4068, 'grad_norm': 3.918424129486084, 'learning_rate': 0.00011762017406757832, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 20.78it/s]  4%|▍         | 3/75 [00:00<00:03, 21.46it/s]                                              {'loss': 2.4581, 'grad_norm': 4.294386863708496, 'learning_rate': 0.00011603071225585428, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 21.46it/s]                                              {'loss': 2.3438, 'grad_norm': 4.242404460906982, 'learning_rate': 0.00011444125044413025, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 21.46it/s]                                              {'loss': 2.4431, 'grad_norm': 3.6957221031188965, 'learning_rate': 0.00011285178863240621, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 21.46it/s]  8%|▊         | 6/75 [00:00<00:02, 23.16it/s]                                              {'loss': 2.2351, 'grad_norm': 4.181369304656982, 'learning_rate': 0.00011126232682068219, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.16it/s]                                              {'loss': 2.3569, 'grad_norm': 4.039069652557373, 'learning_rate': 0.00010967286500895816, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.16it/s]                                              {'loss': 2.2089, 'grad_norm': 4.386476516723633, 'learning_rate': 0.00010808340319723412, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.16it/s] 12%|█▏        | 9/75 [00:00<00:02, 22.80it/s]                                              {'loss': 2.3296, 'grad_norm': 4.502092361450195, 'learning_rate': 0.00010649394138551008, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 22.80it/s]                                              {'loss': 2.4154, 'grad_norm': 3.3755035400390625, 'learning_rate': 0.00010490447957378606, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 22.80it/s]                                               {'loss': 2.5095, 'grad_norm': 4.188928127288818, 'learning_rate': 0.00010331501776206203, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 22.80it/s] 16%|█▌        | 12/75 [00:00<00:02, 22.74it/s]                                               {'loss': 2.144, 'grad_norm': 3.9654290676116943, 'learning_rate': 0.000101725555950338, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 22.74it/s]                                               {'loss': 2.4346, 'grad_norm': 4.233139514923096, 'learning_rate': 0.00010013609413861397, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 22.74it/s]                                               {'loss': 2.275, 'grad_norm': 3.7317397594451904, 'learning_rate': 9.854663232688993e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 22.74it/s] 20%|██        | 15/75 [00:00<00:02, 24.12it/s]                                               {'loss': 2.679, 'grad_norm': 11.572832107543945, 'learning_rate': 9.695717051516591e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.12it/s]                                               {'loss': 2.3235, 'grad_norm': 4.3528361320495605, 'learning_rate': 9.536770870344188e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 24.12it/s]                                               {'loss': 2.3948, 'grad_norm': 4.665763854980469, 'learning_rate': 9.377824689171784e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.12it/s] 24%|██▍       | 18/75 [00:00<00:02, 24.09it/s]                                               {'loss': 2.2852, 'grad_norm': 4.177332401275635, 'learning_rate': 9.21887850799938e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.09it/s]                                               {'loss': 2.1479, 'grad_norm': 3.8813910484313965, 'learning_rate': 9.059932326826978e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.09it/s]                                               {'loss': 2.4777, 'grad_norm': 5.909139633178711, 'learning_rate': 8.900986145654575e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.09it/s] 28%|██▊       | 21/75 [00:00<00:02, 23.53it/s]                                               {'loss': 2.2133, 'grad_norm': 4.024948596954346, 'learning_rate': 8.742039964482171e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 23.53it/s]                                               {'loss': 2.2135, 'grad_norm': 4.314184665679932, 'learning_rate': 8.583093783309768e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 23.53it/s]                                               {'loss': 2.379, 'grad_norm': 3.4914138317108154, 'learning_rate': 8.424147602137365e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 23.53it/s] 32%|███▏      | 24/75 [00:01<00:02, 23.84it/s]                                               {'loss': 2.2326, 'grad_norm': 3.704120397567749, 'learning_rate': 8.265201420964963e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 23.84it/s]                                               {'loss': 2.4596, 'grad_norm': 4.3751726150512695, 'learning_rate': 8.10625523979256e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.84it/s]                                               {'loss': 2.0877, 'grad_norm': 3.254135847091675, 'learning_rate': 7.947309058620156e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.84it/s] 36%|███▌      | 27/75 [00:01<00:02, 23.80it/s]                                               {'loss': 2.2491, 'grad_norm': 4.814700126647949, 'learning_rate': 7.788362877447753e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 23.80it/s]                                               {'loss': 2.1849, 'grad_norm': 5.0206756591796875, 'learning_rate': 7.62941669627535e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 23.80it/s]                                               {'loss': 2.3857, 'grad_norm': 3.8395514488220215, 'learning_rate': 7.470470515102947e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 23.80it/s]                                               {'loss': 2.5228, 'grad_norm': 10.141496658325195, 'learning_rate': 7.311524333930543e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.80it/s] 41%|████▏     | 31/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.0461, 'grad_norm': 3.713979959487915, 'learning_rate': 7.15257815275814e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.1844, 'grad_norm': 3.9914472103118896, 'learning_rate': 6.993631971585737e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.13it/s]                                               {'loss': 2.3943, 'grad_norm': 3.5113308429718018, 'learning_rate': 6.834685790413334e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.13it/s] 45%|████▌     | 34/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.2469, 'grad_norm': 5.68991231918335, 'learning_rate': 6.675739609240932e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.3527, 'grad_norm': 4.606601715087891, 'learning_rate': 6.516793428068528e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.24it/s]                                               {'loss': 2.5371, 'grad_norm': 4.879255294799805, 'learning_rate': 6.357847246896125e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.24it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.53it/s]                                               {'loss': 2.1155, 'grad_norm': 3.5328900814056396, 'learning_rate': 6.198901065723722e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.53it/s]                                               {'loss': 2.1095, 'grad_norm': 2.9302289485931396, 'learning_rate': 6.039954884551319e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.53it/s]                                               {'loss': 2.1618, 'grad_norm': 3.8229928016662598, 'learning_rate': 5.881008703378916e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.53it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.53it/s]                                               {'loss': 2.2309, 'grad_norm': 4.085745334625244, 'learning_rate': 5.7220625222065124e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.53it/s]                                               {'loss': 2.2771, 'grad_norm': 3.730666399002075, 'learning_rate': 5.5631163410341095e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.53it/s]                                               {'loss': 2.4606, 'grad_norm': 3.1220929622650146, 'learning_rate': 5.404170159861706e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.53it/s] 57%|█████▋    | 43/75 [00:01<00:01, 25.27it/s]                                               {'loss': 2.1017, 'grad_norm': 4.281436920166016, 'learning_rate': 5.245223978689303e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.27it/s]                                               {'loss': 2.4132, 'grad_norm': 5.861202239990234, 'learning_rate': 5.0862777975169e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.27it/s]                                               {'loss': 2.3525, 'grad_norm': 15.294954299926758, 'learning_rate': 4.927331616344497e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.27it/s]                                               {'loss': 2.248, 'grad_norm': 3.271385431289673, 'learning_rate': 4.768385435172094e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.27it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.02it/s]                                               {'loss': 2.2538, 'grad_norm': 3.90088152885437, 'learning_rate': 4.60943925399969e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.02it/s]                                               {'loss': 2.1385, 'grad_norm': 3.7708585262298584, 'learning_rate': 4.4504930728272874e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.02it/s]                                               {'loss': 2.2917, 'grad_norm': 4.548811912536621, 'learning_rate': 4.291546891654884e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.02it/s] 67%|██████▋   | 50/75 [00:02<00:00, 25.07it/s]                                               {'loss': 2.3514, 'grad_norm': 5.102100849151611, 'learning_rate': 4.1326007104824816e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.07it/s]                                               {'loss': 2.1582, 'grad_norm': 4.001291751861572, 'learning_rate': 3.973654529310078e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.07it/s]                                               {'loss': 2.4502, 'grad_norm': 3.877042055130005, 'learning_rate': 3.814708348137675e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.07it/s] 71%|███████   | 53/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.1066, 'grad_norm': 3.2833938598632812, 'learning_rate': 3.6557621669652716e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.295, 'grad_norm': 4.681485176086426, 'learning_rate': 3.496815985792869e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.99it/s]                                               {'loss': 2.5006, 'grad_norm': 4.246362686157227, 'learning_rate': 3.337869804620466e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.99it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.2574, 'grad_norm': 4.2804436683654785, 'learning_rate': 3.178923623448062e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.1594, 'grad_norm': 4.0637922286987305, 'learning_rate': 3.0199774422756594e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.1331, 'grad_norm': 3.975738525390625, 'learning_rate': 2.8610312611032562e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.02it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.20it/s]                                               {'loss': 2.322, 'grad_norm': 4.670123100280762, 'learning_rate': 2.702085079930853e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.20it/s]                                               {'loss': 1.8095, 'grad_norm': 12.890789985656738, 'learning_rate': 2.54313889875845e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.20it/s]                                               {'loss': 2.4084, 'grad_norm': 5.095854759216309, 'learning_rate': 2.384192717586047e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.20it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.33it/s]                                               {'loss': 2.2153, 'grad_norm': 3.537015914916992, 'learning_rate': 2.2252465364136437e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.33it/s]                                               {'loss': 2.1581, 'grad_norm': 4.762147426605225, 'learning_rate': 2.0663003552412408e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.33it/s]                                               {'loss': 2.1719, 'grad_norm': 2.6655633449554443, 'learning_rate': 1.9073541740688376e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.33it/s] 87%|████████▋ | 65/75 [00:02<00:00, 25.93it/s]                                               {'loss': 2.1679, 'grad_norm': 3.638946294784546, 'learning_rate': 1.7484079928964344e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.93it/s]                                               {'loss': 2.0896, 'grad_norm': 3.0036497116088867, 'learning_rate': 1.589461811724031e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.93it/s]                                               {'loss': 2.2967, 'grad_norm': 4.2146148681640625, 'learning_rate': 1.4305156305516281e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.93it/s] 91%|█████████ | 68/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.0818, 'grad_norm': 4.718501091003418, 'learning_rate': 1.271569449379225e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.2309, 'grad_norm': 3.9465253353118896, 'learning_rate': 1.1126232682068218e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.63it/s]                                               {'loss': 2.1163, 'grad_norm': 4.082272052764893, 'learning_rate': 9.536770870344188e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.63it/s] 95%|█████████▍| 71/75 [00:02<00:00, 25.07it/s]                                               {'loss': 2.2922, 'grad_norm': 4.075228214263916, 'learning_rate': 7.947309058620156e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.07it/s]                                               {'loss': 2.4503, 'grad_norm': 3.753061294555664, 'learning_rate': 6.357847246896125e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.07it/s]                                               {'loss': 2.2723, 'grad_norm': 4.186890602111816, 'learning_rate': 4.768385435172094e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.07it/s] 99%|█████████▊| 74/75 [00:02<00:00, 24.82it/s]                                               {'loss': 2.251, 'grad_norm': 3.8290398120880127, 'learning_rate': 3.1789236234480626e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 24.82it/s]                                               {'loss': 2.3428, 'grad_norm': 10.69871711730957, 'learning_rate': 1.5894618117240313e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.82it/s]                                               {'train_runtime': 3.1199, 'train_samples_per_second': 362.187, 'train_steps_per_second': 24.039, 'train_loss': 2.280632298787435, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.82it/s]100%|██████████| 75/75 [00:03<00:00, 24.04it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:36
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2286, 'grad_norm': 4.130249500274658, 'learning_rate': 0.00011920963587930234, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 22.77it/s]                                              {'loss': 2.2949, 'grad_norm': 3.36071515083313, 'learning_rate': 0.00011762017406757832, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:02, 24.81it/s]  4%|▍         | 3/75 [00:00<00:02, 25.09it/s]                                              {'loss': 2.2042, 'grad_norm': 3.552133321762085, 'learning_rate': 0.00011603071225585428, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 25.09it/s]                                              {'loss': 2.2218, 'grad_norm': 3.892481565475464, 'learning_rate': 0.00011444125044413025, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 25.09it/s]                                              {'loss': 2.3153, 'grad_norm': 4.809611797332764, 'learning_rate': 0.00011285178863240621, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 25.09it/s]  8%|▊         | 6/75 [00:00<00:02, 24.49it/s]                                              {'loss': 2.1867, 'grad_norm': 3.68414044380188, 'learning_rate': 0.00011126232682068219, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.49it/s]                                              {'loss': 2.2932, 'grad_norm': 3.8044302463531494, 'learning_rate': 0.00010967286500895816, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.49it/s]                                              {'loss': 2.2699, 'grad_norm': 4.605050086975098, 'learning_rate': 0.00010808340319723412, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.49it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.48it/s]                                              {'loss': 2.1764, 'grad_norm': 3.9437923431396484, 'learning_rate': 0.00010649394138551008, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.48it/s]                                              {'loss': 2.0627, 'grad_norm': 3.6105709075927734, 'learning_rate': 0.00010490447957378606, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.48it/s]                                               {'loss': 2.1468, 'grad_norm': 4.875627040863037, 'learning_rate': 0.00010331501776206203, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.48it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.42it/s]                                               {'loss': 1.9644, 'grad_norm': 2.8912782669067383, 'learning_rate': 0.000101725555950338, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.42it/s]                                               {'loss': 2.0422, 'grad_norm': 2.757838010787964, 'learning_rate': 0.00010013609413861397, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.42it/s]                                               {'loss': 2.2567, 'grad_norm': 3.044801712036133, 'learning_rate': 9.854663232688993e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.42it/s]                                               {'loss': 2.0535, 'grad_norm': 7.629593372344971, 'learning_rate': 9.695717051516591e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.42it/s] 21%|██▏       | 16/75 [00:00<00:02, 27.27it/s]                                               {'loss': 2.3052, 'grad_norm': 4.14193868637085, 'learning_rate': 9.536770870344188e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 27.27it/s]                                               {'loss': 2.1196, 'grad_norm': 3.529928207397461, 'learning_rate': 9.377824689171784e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 27.27it/s]                                               {'loss': 2.0918, 'grad_norm': 4.47970724105835, 'learning_rate': 9.21887850799938e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 27.27it/s] 25%|██▌       | 19/75 [00:00<00:02, 26.04it/s]                                               {'loss': 2.2219, 'grad_norm': 3.8469932079315186, 'learning_rate': 9.059932326826978e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 26.04it/s]                                               {'loss': 2.1861, 'grad_norm': 4.867815971374512, 'learning_rate': 8.900986145654575e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 26.04it/s]                                               {'loss': 1.9388, 'grad_norm': 2.970238447189331, 'learning_rate': 8.742039964482171e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 26.04it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.68it/s]                                               {'loss': 2.2537, 'grad_norm': 4.210722923278809, 'learning_rate': 8.583093783309768e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.68it/s]                                               {'loss': 2.0624, 'grad_norm': 4.010446071624756, 'learning_rate': 8.424147602137365e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.68it/s]                                               {'loss': 2.1724, 'grad_norm': 4.106761932373047, 'learning_rate': 8.265201420964963e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.68it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.50it/s]                                               {'loss': 2.1947, 'grad_norm': 3.1737418174743652, 'learning_rate': 8.10625523979256e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.50it/s]                                               {'loss': 2.0321, 'grad_norm': 3.5724127292633057, 'learning_rate': 7.947309058620156e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.50it/s]                                               {'loss': 2.0255, 'grad_norm': 3.2902979850769043, 'learning_rate': 7.788362877447753e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.50it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.246, 'grad_norm': 5.247151851654053, 'learning_rate': 7.62941669627535e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.2483, 'grad_norm': 3.5789966583251953, 'learning_rate': 7.470470515102947e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.00it/s]                                               {'loss': 1.8434, 'grad_norm': 8.32752513885498, 'learning_rate': 7.311524333930543e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.1208, 'grad_norm': 3.2338874340057373, 'learning_rate': 7.15257815275814e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.00it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.86it/s]                                               {'loss': 2.1685, 'grad_norm': 2.9493322372436523, 'learning_rate': 6.993631971585737e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.86it/s]                                               {'loss': 2.0109, 'grad_norm': 3.4051642417907715, 'learning_rate': 6.834685790413334e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.86it/s]                                               {'loss': 2.0935, 'grad_norm': 3.3410446643829346, 'learning_rate': 6.675739609240932e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.86it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.59it/s]                                               {'loss': 2.0921, 'grad_norm': 3.801116466522217, 'learning_rate': 6.516793428068528e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.59it/s]                                               {'loss': 1.8809, 'grad_norm': 3.644409418106079, 'learning_rate': 6.357847246896125e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.59it/s]                                               {'loss': 2.2742, 'grad_norm': 4.5743937492370605, 'learning_rate': 6.198901065723722e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.59it/s] 51%|█████     | 38/75 [00:01<00:01, 26.57it/s]                                               {'loss': 1.9983, 'grad_norm': 3.0259366035461426, 'learning_rate': 6.039954884551319e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.57it/s]                                               {'loss': 2.0623, 'grad_norm': 3.7150073051452637, 'learning_rate': 5.881008703378916e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.57it/s]                                               {'loss': 2.2332, 'grad_norm': 3.530341863632202, 'learning_rate': 5.7220625222065124e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 26.57it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.57it/s]                                               {'loss': 2.1007, 'grad_norm': 3.4902138710021973, 'learning_rate': 5.5631163410341095e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.57it/s]                                               {'loss': 2.2588, 'grad_norm': 2.566331386566162, 'learning_rate': 5.404170159861706e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.57it/s]                                               {'loss': 2.1176, 'grad_norm': 5.583022117614746, 'learning_rate': 5.245223978689303e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.57it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.99it/s]                                               {'loss': 2.3346, 'grad_norm': 4.177850723266602, 'learning_rate': 5.0862777975169e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.99it/s]                                               {'loss': 2.4137, 'grad_norm': 17.012514114379883, 'learning_rate': 4.927331616344497e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.99it/s]                                               {'loss': 1.9966, 'grad_norm': 3.353557825088501, 'learning_rate': 4.768385435172094e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.99it/s]                                               {'loss': 2.14, 'grad_norm': 4.058574199676514, 'learning_rate': 4.60943925399969e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.99it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.35it/s]                                               {'loss': 1.9055, 'grad_norm': 2.9840056896209717, 'learning_rate': 4.4504930728272874e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.35it/s]                                               {'loss': 2.0268, 'grad_norm': 3.2958595752716064, 'learning_rate': 4.291546891654884e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.35it/s]                                               {'loss': 2.0787, 'grad_norm': 3.4700310230255127, 'learning_rate': 4.1326007104824816e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.35it/s] 68%|██████▊   | 51/75 [00:01<00:00, 25.36it/s]                                               {'loss': 2.2435, 'grad_norm': 3.4864838123321533, 'learning_rate': 3.973654529310078e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 25.36it/s]                                               {'loss': 2.3037, 'grad_norm': 5.1274309158325195, 'learning_rate': 3.814708348137675e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.36it/s]                                               {'loss': 2.0779, 'grad_norm': 3.2217085361480713, 'learning_rate': 3.6557621669652716e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.36it/s] 72%|███████▏  | 54/75 [00:02<00:00, 23.79it/s]                                               {'loss': 2.3036, 'grad_norm': 3.2311205863952637, 'learning_rate': 3.496815985792869e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 23.79it/s]                                               {'loss': 1.9616, 'grad_norm': 3.131723165512085, 'learning_rate': 3.337869804620466e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 23.79it/s]                                               {'loss': 1.9961, 'grad_norm': 4.363602638244629, 'learning_rate': 3.178923623448062e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 23.79it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.1297, 'grad_norm': 3.519552230834961, 'learning_rate': 3.0199774422756594e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.3958, 'grad_norm': 3.4007620811462402, 'learning_rate': 2.8610312611032562e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.33it/s]                                               {'loss': 2.065, 'grad_norm': 3.3729088306427, 'learning_rate': 2.702085079930853e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.33it/s]                                               {'loss': 1.8632, 'grad_norm': 8.232616424560547, 'learning_rate': 2.54313889875845e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.33it/s] 81%|████████▏ | 61/75 [00:02<00:00, 26.10it/s]                                               {'loss': 1.8567, 'grad_norm': 3.2153234481811523, 'learning_rate': 2.384192717586047e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 26.10it/s]                                               {'loss': 2.0743, 'grad_norm': 2.668729066848755, 'learning_rate': 2.2252465364136437e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.10it/s]                                               {'loss': 2.0761, 'grad_norm': 3.6040120124816895, 'learning_rate': 2.0663003552412408e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.10it/s] 85%|████████▌ | 64/75 [00:02<00:00, 26.02it/s]                                               {'loss': 2.1191, 'grad_norm': 2.773190975189209, 'learning_rate': 1.9073541740688376e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.02it/s]                                               {'loss': 2.0872, 'grad_norm': 3.398578405380249, 'learning_rate': 1.7484079928964344e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.02it/s]                                               {'loss': 2.215, 'grad_norm': 4.049526691436768, 'learning_rate': 1.589461811724031e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.02it/s] 89%|████████▉ | 67/75 [00:02<00:00, 25.87it/s]                                               {'loss': 2.1948, 'grad_norm': 2.948920249938965, 'learning_rate': 1.4305156305516281e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.87it/s]                                               {'loss': 2.4816, 'grad_norm': 5.285155296325684, 'learning_rate': 1.271569449379225e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.87it/s]                                               {'loss': 2.1082, 'grad_norm': 3.337723970413208, 'learning_rate': 1.1126232682068218e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.87it/s] 93%|█████████▎| 70/75 [00:02<00:00, 26.09it/s]                                               {'loss': 2.2555, 'grad_norm': 4.650084018707275, 'learning_rate': 9.536770870344188e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 26.09it/s]                                               {'loss': 1.8838, 'grad_norm': 3.6793487071990967, 'learning_rate': 7.947309058620156e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 26.09it/s]                                               {'loss': 2.1342, 'grad_norm': 3.4340665340423584, 'learning_rate': 6.357847246896125e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 26.09it/s] 97%|█████████▋| 73/75 [00:02<00:00, 25.83it/s]                                               {'loss': 2.1129, 'grad_norm': 3.9716176986694336, 'learning_rate': 4.768385435172094e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.83it/s]                                               {'loss': 2.0031, 'grad_norm': 3.7291157245635986, 'learning_rate': 3.1789236234480626e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.83it/s]                                               {'loss': 1.3591, 'grad_norm': 5.5826616287231445, 'learning_rate': 1.5894618117240313e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.83it/s]                                               {'train_runtime': 3.008, 'train_samples_per_second': 375.67, 'train_steps_per_second': 24.934, 'train_loss': 2.1235765186945597, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.83it/s]100%|██████████| 75/75 [00:03<00:00, 24.94it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:5
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.1714, 'grad_norm': 4.269374847412109, 'learning_rate': 0.00011920963587930234, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 16.39it/s]                                              {'loss': 2.2473, 'grad_norm': 4.294763565063477, 'learning_rate': 0.00011762017406757832, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 20.58it/s]  4%|▍         | 3/75 [00:00<00:03, 22.14it/s]                                              {'loss': 2.3087, 'grad_norm': 3.337768316268921, 'learning_rate': 0.00011603071225585428, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 22.14it/s]                                              {'loss': 2.4592, 'grad_norm': 4.146269798278809, 'learning_rate': 0.00011444125044413025, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 22.14it/s]                                              {'loss': 1.9749, 'grad_norm': 2.453024387359619, 'learning_rate': 0.00011285178863240621, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 22.14it/s]  8%|▊         | 6/75 [00:00<00:02, 23.12it/s]                                              {'loss': 2.2919, 'grad_norm': 5.056991100311279, 'learning_rate': 0.00011126232682068219, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.12it/s]                                              {'loss': 2.1425, 'grad_norm': 5.856858730316162, 'learning_rate': 0.00010967286500895816, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.12it/s]                                              {'loss': 2.1689, 'grad_norm': 3.1450788974761963, 'learning_rate': 0.00010808340319723412, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.12it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.03it/s]                                              {'loss': 2.1873, 'grad_norm': 4.120021820068359, 'learning_rate': 0.00010649394138551008, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.03it/s]                                              {'loss': 2.2804, 'grad_norm': 3.172426462173462, 'learning_rate': 0.00010490447957378606, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.03it/s]                                               {'loss': 2.0577, 'grad_norm': 6.191335201263428, 'learning_rate': 0.00010331501776206203, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.03it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.14it/s]                                               {'loss': 2.2086, 'grad_norm': 4.7385077476501465, 'learning_rate': 0.000101725555950338, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.14it/s]                                               {'loss': 2.2582, 'grad_norm': 3.89530611038208, 'learning_rate': 0.00010013609413861397, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.14it/s]                                               {'loss': 2.3027, 'grad_norm': 4.829751968383789, 'learning_rate': 9.854663232688993e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.14it/s]                                               {'loss': 2.9217, 'grad_norm': 13.747967720031738, 'learning_rate': 9.695717051516591e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.14it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.47it/s]                                               {'loss': 2.2577, 'grad_norm': 3.7552359104156494, 'learning_rate': 9.536770870344188e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.47it/s]                                               {'loss': 2.2594, 'grad_norm': 4.086385726928711, 'learning_rate': 9.377824689171784e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.47it/s]                                               {'loss': 2.1849, 'grad_norm': 3.0754642486572266, 'learning_rate': 9.21887850799938e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.47it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.84it/s]                                               {'loss': 2.1713, 'grad_norm': 3.4438095092773438, 'learning_rate': 9.059932326826978e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.84it/s]                                               {'loss': 2.1653, 'grad_norm': 3.407231330871582, 'learning_rate': 8.900986145654575e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.84it/s]                                               {'loss': 2.1356, 'grad_norm': 4.039463996887207, 'learning_rate': 8.742039964482171e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.84it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.11it/s]                                               {'loss': 2.1816, 'grad_norm': 4.190280914306641, 'learning_rate': 8.583093783309768e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.11it/s]                                               {'loss': 2.1315, 'grad_norm': 3.6102399826049805, 'learning_rate': 8.424147602137365e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.11it/s]                                               {'loss': 2.1608, 'grad_norm': 3.6378278732299805, 'learning_rate': 8.265201420964963e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 25.11it/s] 33%|███▎      | 25/75 [00:01<00:01, 25.20it/s]                                               {'loss': 2.219, 'grad_norm': 4.924593448638916, 'learning_rate': 8.10625523979256e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:01, 25.20it/s]                                               {'loss': 2.1929, 'grad_norm': 4.443597316741943, 'learning_rate': 7.947309058620156e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.20it/s]                                               {'loss': 2.1414, 'grad_norm': 3.808098793029785, 'learning_rate': 7.788362877447753e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.20it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.29it/s]                                               {'loss': 2.1039, 'grad_norm': 3.3500254154205322, 'learning_rate': 7.62941669627535e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.29it/s]                                               {'loss': 1.9694, 'grad_norm': 2.980483055114746, 'learning_rate': 7.470470515102947e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.29it/s]                                               {'loss': 2.5152, 'grad_norm': 12.195634841918945, 'learning_rate': 7.311524333930543e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.29it/s]                                               {'loss': 1.9732, 'grad_norm': 4.100691795349121, 'learning_rate': 7.15257815275814e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.29it/s] 43%|████▎     | 32/75 [00:01<00:01, 26.72it/s]                                               {'loss': 2.2215, 'grad_norm': 3.4672906398773193, 'learning_rate': 6.993631971585737e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.72it/s]                                               {'loss': 2.1648, 'grad_norm': 3.80965256690979, 'learning_rate': 6.834685790413334e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.72it/s]                                               {'loss': 2.2542, 'grad_norm': 3.8888015747070312, 'learning_rate': 6.675739609240932e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.72it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.80it/s]                                               {'loss': 1.8669, 'grad_norm': 4.6416521072387695, 'learning_rate': 6.516793428068528e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.80it/s]                                               {'loss': 2.3517, 'grad_norm': 4.9118170738220215, 'learning_rate': 6.357847246896125e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.80it/s]                                               {'loss': 2.0349, 'grad_norm': 3.2171249389648438, 'learning_rate': 6.198901065723722e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.80it/s] 51%|█████     | 38/75 [00:01<00:01, 26.06it/s]                                               {'loss': 2.1565, 'grad_norm': 5.457543849945068, 'learning_rate': 6.039954884551319e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.06it/s]                                               {'loss': 2.2625, 'grad_norm': 3.6660704612731934, 'learning_rate': 5.881008703378916e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.06it/s]                                               {'loss': 2.1181, 'grad_norm': 3.380666732788086, 'learning_rate': 5.7220625222065124e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 26.06it/s] 55%|█████▍    | 41/75 [00:01<00:01, 25.85it/s]                                               {'loss': 2.2132, 'grad_norm': 5.333047866821289, 'learning_rate': 5.5631163410341095e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.85it/s]                                               {'loss': 2.2537, 'grad_norm': 4.210075855255127, 'learning_rate': 5.404170159861706e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.85it/s]                                               {'loss': 2.2251, 'grad_norm': 3.8016297817230225, 'learning_rate': 5.245223978689303e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.85it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.41it/s]                                               {'loss': 1.9684, 'grad_norm': 3.3338518142700195, 'learning_rate': 5.0862777975169e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.41it/s]                                               {'loss': 2.2983, 'grad_norm': 8.176949501037598, 'learning_rate': 4.927331616344497e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.41it/s]                                               {'loss': 2.0027, 'grad_norm': 3.2839667797088623, 'learning_rate': 4.768385435172094e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.41it/s] 63%|██████▎   | 47/75 [00:01<00:01, 25.50it/s]                                               {'loss': 2.2485, 'grad_norm': 3.458587408065796, 'learning_rate': 4.60943925399969e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.50it/s]                                               {'loss': 2.1302, 'grad_norm': 2.789124011993408, 'learning_rate': 4.4504930728272874e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.50it/s]                                               {'loss': 2.076, 'grad_norm': 3.1845104694366455, 'learning_rate': 4.291546891654884e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:01, 25.50it/s] 67%|██████▋   | 50/75 [00:01<00:00, 25.16it/s]                                               {'loss': 2.1119, 'grad_norm': 3.7007899284362793, 'learning_rate': 4.1326007104824816e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 25.16it/s]                                               {'loss': 2.2694, 'grad_norm': 4.640264987945557, 'learning_rate': 3.973654529310078e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.16it/s]                                               {'loss': 2.0557, 'grad_norm': 2.6955904960632324, 'learning_rate': 3.814708348137675e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.16it/s] 71%|███████   | 53/75 [00:02<00:00, 25.42it/s]                                               {'loss': 2.5379, 'grad_norm': 4.277792930603027, 'learning_rate': 3.6557621669652716e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.42it/s]                                               {'loss': 2.1402, 'grad_norm': 3.7279562950134277, 'learning_rate': 3.496815985792869e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.42it/s]                                               {'loss': 2.202, 'grad_norm': 4.734910011291504, 'learning_rate': 3.337869804620466e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.42it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.42it/s]                                               {'loss': 2.0706, 'grad_norm': 3.109245538711548, 'learning_rate': 3.178923623448062e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.42it/s]                                               {'loss': 2.0813, 'grad_norm': 3.5722174644470215, 'learning_rate': 3.0199774422756594e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.42it/s]                                               {'loss': 1.8639, 'grad_norm': 5.650358200073242, 'learning_rate': 2.8610312611032562e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.42it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.42it/s]                                               {'loss': 2.2957, 'grad_norm': 2.999460220336914, 'learning_rate': 2.702085079930853e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.42it/s]                                               {'loss': 1.8609, 'grad_norm': 10.206665992736816, 'learning_rate': 2.54313889875845e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.42it/s]                                               {'loss': 2.1289, 'grad_norm': 3.4555435180664062, 'learning_rate': 2.384192717586047e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.42it/s]                                               {'loss': 2.3093, 'grad_norm': 4.166014671325684, 'learning_rate': 2.2252465364136437e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.42it/s] 84%|████████▍ | 63/75 [00:02<00:00, 26.83it/s]                                               {'loss': 2.2155, 'grad_norm': 4.077803134918213, 'learning_rate': 2.0663003552412408e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.83it/s]                                               {'loss': 2.3426, 'grad_norm': 3.189587116241455, 'learning_rate': 1.9073541740688376e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.83it/s]                                               {'loss': 2.0313, 'grad_norm': 3.9508674144744873, 'learning_rate': 1.7484079928964344e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.83it/s] 88%|████████▊ | 66/75 [00:02<00:00, 26.56it/s]                                               {'loss': 2.0671, 'grad_norm': 3.667543649673462, 'learning_rate': 1.589461811724031e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.56it/s]                                               {'loss': 2.019, 'grad_norm': 4.602665424346924, 'learning_rate': 1.4305156305516281e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.56it/s]                                               {'loss': 2.1672, 'grad_norm': 5.314828872680664, 'learning_rate': 1.271569449379225e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 26.56it/s] 92%|█████████▏| 69/75 [00:02<00:00, 26.03it/s]                                               {'loss': 2.1099, 'grad_norm': 3.360830783843994, 'learning_rate': 1.1126232682068218e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 26.03it/s]                                               {'loss': 2.0513, 'grad_norm': 3.2867355346679688, 'learning_rate': 9.536770870344188e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 26.03it/s]                                               {'loss': 2.0332, 'grad_norm': 3.7963883876800537, 'learning_rate': 7.947309058620156e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 26.03it/s] 96%|█████████▌| 72/75 [00:02<00:00, 25.22it/s]                                               {'loss': 2.3232, 'grad_norm': 3.2246205806732178, 'learning_rate': 6.357847246896125e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.22it/s]                                               {'loss': 2.0405, 'grad_norm': 3.30977463722229, 'learning_rate': 4.768385435172094e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.22it/s]                                               {'loss': 2.1762, 'grad_norm': 3.7577552795410156, 'learning_rate': 3.1789236234480626e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.22it/s]                                               {'loss': 2.5547, 'grad_norm': 8.955453872680664, 'learning_rate': 1.5894618117240313e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.22it/s]                                               {'train_runtime': 3.067, 'train_samples_per_second': 368.432, 'train_steps_per_second': 24.453, 'train_loss': 2.1820140568415325, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.22it/s]100%|██████████| 75/75 [00:03<00:00, 24.45it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(904, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(848, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(708, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(829, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1059, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(806, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(913, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(722, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(782, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(525, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(946, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(480, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 6/471 [00:00<00:08, 54.12it/s]  3%|▎         | 12/471 [00:00<00:08, 53.52it/s]  4%|▍         | 18/471 [00:00<00:08, 53.26it/s]  5%|▌         | 24/471 [00:00<00:08, 52.35it/s]  6%|▋         | 30/471 [00:00<00:08, 52.64it/s]  8%|▊         | 36/471 [00:00<00:08, 52.82it/s]  9%|▉         | 42/471 [00:00<00:08, 52.62it/s] 10%|█         | 48/471 [00:00<00:08, 52.59it/s] 11%|█▏        | 54/471 [00:01<00:07, 52.58it/s] 13%|█▎        | 60/471 [00:01<00:07, 51.75it/s] 14%|█▍        | 66/471 [00:01<00:08, 50.40it/s] 15%|█▌        | 72/471 [00:01<00:07, 51.63it/s] 17%|█▋        | 78/471 [00:01<00:07, 52.33it/s] 18%|█▊        | 84/471 [00:01<00:07, 52.21it/s] 19%|█▉        | 90/471 [00:01<00:07, 52.70it/s] 20%|██        | 96/471 [00:01<00:07, 53.00it/s] 22%|██▏       | 102/471 [00:01<00:06, 53.04it/s] 23%|██▎       | 108/471 [00:02<00:06, 53.25it/s] 24%|██▍       | 114/471 [00:02<00:06, 53.25it/s] 25%|██▌       | 120/471 [00:02<00:06, 52.30it/s] 27%|██▋       | 126/471 [00:02<00:06, 52.82it/s] 28%|██▊       | 132/471 [00:02<00:06, 52.37it/s] 29%|██▉       | 138/471 [00:02<00:06, 52.14it/s] 31%|███       | 144/471 [00:02<00:06, 52.60it/s] 32%|███▏      | 150/471 [00:02<00:06, 52.96it/s] 33%|███▎      | 156/471 [00:02<00:06, 51.94it/s] 34%|███▍      | 162/471 [00:03<00:05, 51.91it/s] 36%|███▌      | 168/471 [00:03<00:05, 52.41it/s] 37%|███▋      | 174/471 [00:03<00:05, 52.66it/s] 38%|███▊      | 180/471 [00:03<00:05, 52.79it/s] 39%|███▉      | 186/471 [00:03<00:05, 52.87it/s] 41%|████      | 192/471 [00:03<00:05, 53.00it/s] 42%|████▏     | 198/471 [00:03<00:05, 52.56it/s] 43%|████▎     | 204/471 [00:03<00:05, 52.52it/s] 45%|████▍     | 210/471 [00:04<00:05, 51.35it/s] 46%|████▌     | 216/471 [00:04<00:04, 52.07it/s] 47%|████▋     | 222/471 [00:04<00:04, 52.58it/s] 48%|████▊     | 228/471 [00:04<00:04, 52.23it/s] 50%|████▉     | 234/471 [00:04<00:04, 52.56it/s] 51%|█████     | 240/471 [00:04<00:04, 52.58it/s] 52%|█████▏    | 246/471 [00:04<00:04, 52.81it/s] 54%|█████▎    | 252/471 [00:04<00:04, 52.06it/s] 55%|█████▍    | 258/471 [00:04<00:04, 52.44it/s] 56%|█████▌    | 264/471 [00:05<00:03, 52.68it/s] 57%|█████▋    | 270/471 [00:05<00:03, 52.71it/s] 59%|█████▊    | 276/471 [00:05<00:03, 52.95it/s] 60%|█████▉    | 282/471 [00:05<00:03, 48.45it/s] 61%|██████    | 287/471 [00:05<00:03, 48.12it/s] 62%|██████▏   | 293/471 [00:05<00:03, 48.72it/s] 63%|██████▎   | 298/471 [00:05<00:03, 47.02it/s] 64%|██████▍   | 303/471 [00:05<00:03, 43.83it/s] 65%|██████▌   | 308/471 [00:05<00:03, 43.65it/s] 67%|██████▋   | 314/471 [00:06<00:03, 46.41it/s] 68%|██████▊   | 319/471 [00:06<00:03, 46.83it/s] 69%|██████▉   | 325/471 [00:06<00:03, 48.59it/s] 70%|███████   | 331/471 [00:06<00:02, 49.73it/s] 72%|███████▏  | 337/471 [00:06<00:02, 50.16it/s] 73%|███████▎  | 343/471 [00:06<00:02, 50.43it/s] 74%|███████▍  | 349/471 [00:06<00:02, 51.51it/s] 75%|███████▌  | 355/471 [00:06<00:02, 51.30it/s] 77%|███████▋  | 361/471 [00:07<00:02, 51.88it/s] 78%|███████▊  | 367/471 [00:07<00:02, 48.99it/s] 79%|███████▉  | 372/471 [00:07<00:02, 47.00it/s] 80%|████████  | 377/471 [00:07<00:01, 47.47it/s] 81%|████████  | 382/471 [00:07<00:01, 46.24it/s] 82%|████████▏ | 388/471 [00:07<00:01, 48.08it/s] 84%|████████▎ | 394/471 [00:07<00:01, 48.95it/s] 85%|████████▍ | 400/471 [00:07<00:01, 50.12it/s] 86%|████████▌ | 406/471 [00:07<00:01, 50.75it/s] 87%|████████▋ | 412/471 [00:08<00:01, 50.34it/s] 89%|████████▊ | 418/471 [00:08<00:01, 50.86it/s] 90%|█████████ | 424/471 [00:08<00:00, 50.62it/s] 91%|█████████▏| 430/471 [00:08<00:00, 49.78it/s] 92%|█████████▏| 435/471 [00:08<00:00, 49.10it/s] 93%|█████████▎| 440/471 [00:08<00:00, 48.03it/s] 95%|█████████▍| 446/471 [00:08<00:00, 49.54it/s] 96%|█████████▌| 451/471 [00:08<00:00, 48.86it/s] 97%|█████████▋| 457/471 [00:08<00:00, 49.22it/s] 98%|█████████▊| 463/471 [00:09<00:00, 50.53it/s]100%|█████████▉| 469/471 [00:09<00:00, 50.40it/s]100%|██████████| 471/471 [00:09<00:00, 50.91it/s]
{'eval_loss': 2.2599124908447266, 'eval_model_preparation_time': 0.0029, 'eval_acc': 0.3983005841741901, 'eval_runtime': 9.2802, 'eval_samples_per_second': 811.618, 'eval_steps_per_second': 50.753}
ROUND:24
CLIENT:41
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2736, 'grad_norm': 4.136441230773926, 'learning_rate': 0.00011768428681777857, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 15.11it/s]  3%|▎         | 2/75 [00:00<00:04, 16.37it/s]                                              {'loss': 2.1786, 'grad_norm': 4.062069416046143, 'learning_rate': 0.00011611516299354152, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:04, 16.37it/s]                                              {'loss': 2.3778, 'grad_norm': 6.22465705871582, 'learning_rate': 0.00011454603916930449, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 16.37it/s]                                              {'loss': 2.1798, 'grad_norm': 3.9393484592437744, 'learning_rate': 0.00011297691534506743, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:04, 16.37it/s]  7%|▋         | 5/75 [00:00<00:03, 20.39it/s]                                              {'loss': 2.2361, 'grad_norm': 3.3311893939971924, 'learning_rate': 0.00011140779152083038, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 20.39it/s]                                              {'loss': 2.3651, 'grad_norm': 4.684694290161133, 'learning_rate': 0.00010983866769659334, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 20.39it/s]                                              {'loss': 2.2575, 'grad_norm': 3.5116665363311768, 'learning_rate': 0.00010826954387235629, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 20.39it/s] 11%|█         | 8/75 [00:00<00:03, 21.51it/s]                                              {'loss': 2.3103, 'grad_norm': 2.6791298389434814, 'learning_rate': 0.00010670042004811923, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 21.51it/s]                                              {'loss': 2.2355, 'grad_norm': 4.084474086761475, 'learning_rate': 0.0001051312962238822, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 21.51it/s]                                              {'loss': 2.4255, 'grad_norm': 3.5330419540405273, 'learning_rate': 0.00010356217239964515, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 21.51it/s] 15%|█▍        | 11/75 [00:00<00:02, 22.20it/s]                                               {'loss': 2.269, 'grad_norm': 4.456853866577148, 'learning_rate': 0.0001019930485754081, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 22.20it/s]                                               {'loss': 2.2655, 'grad_norm': 4.258124351501465, 'learning_rate': 0.00010042392475117106, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 22.20it/s]                                               {'loss': 2.2338, 'grad_norm': 5.6547651290893555, 'learning_rate': 9.8854800926934e-05, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 22.20it/s] 19%|█▊        | 14/75 [00:00<00:02, 23.20it/s]                                               {'loss': 2.461, 'grad_norm': 4.0377397537231445, 'learning_rate': 9.728567710269695e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.20it/s]                                               {'loss': 2.2291, 'grad_norm': 11.633161544799805, 'learning_rate': 9.57165532784599e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.20it/s]                                               {'loss': 2.2804, 'grad_norm': 3.442293643951416, 'learning_rate': 9.414742945422287e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 23.20it/s] 23%|██▎       | 17/75 [00:00<00:02, 24.86it/s]                                               {'loss': 2.3518, 'grad_norm': 4.806565761566162, 'learning_rate': 9.25783056299858e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.86it/s]                                               {'loss': 2.4324, 'grad_norm': 3.63523006439209, 'learning_rate': 9.100918180574876e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.86it/s]                                               {'loss': 2.1992, 'grad_norm': 3.499913454055786, 'learning_rate': 8.944005798151172e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.86it/s] 27%|██▋       | 20/75 [00:00<00:02, 22.61it/s]                                               {'loss': 2.1238, 'grad_norm': 3.2929744720458984, 'learning_rate': 8.787093415727467e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 22.61it/s]                                               {'loss': 2.2702, 'grad_norm': 4.345953941345215, 'learning_rate': 8.630181033303761e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 22.61it/s]                                               {'loss': 2.3287, 'grad_norm': 4.372357368469238, 'learning_rate': 8.473268650880057e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 22.61it/s] 31%|███       | 23/75 [00:01<00:02, 22.31it/s]                                               {'loss': 2.2441, 'grad_norm': 3.829803943634033, 'learning_rate': 8.316356268456352e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 22.31it/s]                                               {'loss': 2.0649, 'grad_norm': 3.2311034202575684, 'learning_rate': 8.159443886032648e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 22.31it/s]                                               {'loss': 2.2687, 'grad_norm': 3.6153714656829834, 'learning_rate': 8.002531503608944e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 22.31it/s] 35%|███▍      | 26/75 [00:01<00:02, 22.62it/s]                                               {'loss': 2.1957, 'grad_norm': 2.85861873626709, 'learning_rate': 7.845619121185238e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 22.62it/s]                                               {'loss': 2.0313, 'grad_norm': 3.4353253841400146, 'learning_rate': 7.688706738761533e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 22.62it/s]                                               {'loss': 2.35, 'grad_norm': 3.634894609451294, 'learning_rate': 7.53179435633783e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 22.62it/s] 39%|███▊      | 29/75 [00:01<00:01, 23.44it/s]                                               {'loss': 2.3713, 'grad_norm': 3.579669237136841, 'learning_rate': 7.374881973914124e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 23.44it/s]                                               {'loss': 2.1018, 'grad_norm': 5.500856399536133, 'learning_rate': 7.217969591490418e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.44it/s]                                               {'loss': 2.2849, 'grad_norm': 3.3806188106536865, 'learning_rate': 7.061057209066715e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 23.44it/s]                                               {'loss': 2.1613, 'grad_norm': 5.143928050994873, 'learning_rate': 6.90414482664301e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 23.44it/s] 44%|████▍     | 33/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.2473, 'grad_norm': 4.399620532989502, 'learning_rate': 6.747232444219305e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.1888, 'grad_norm': 3.6358373165130615, 'learning_rate': 6.590320061795601e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.1736, 'grad_norm': 3.7934770584106445, 'learning_rate': 6.433407679371895e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.39it/s] 48%|████▊     | 36/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.1534, 'grad_norm': 3.9397528171539307, 'learning_rate': 6.27649529694819e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.2665, 'grad_norm': 4.444161891937256, 'learning_rate': 6.119582914524487e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.31it/s]                                               {'loss': 2.1748, 'grad_norm': 4.467979907989502, 'learning_rate': 5.962670532100782e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.31it/s] 52%|█████▏    | 39/75 [00:01<00:01, 24.94it/s]                                               {'loss': 2.2237, 'grad_norm': 4.13212776184082, 'learning_rate': 5.805758149677076e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.94it/s]                                               {'loss': 2.0779, 'grad_norm': 3.6415517330169678, 'learning_rate': 5.648845767253371e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.94it/s]                                               {'loss': 2.291, 'grad_norm': 3.6354382038116455, 'learning_rate': 5.491933384829667e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.94it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.89it/s]                                               {'loss': 2.2102, 'grad_norm': 4.604663848876953, 'learning_rate': 5.3350210024059615e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.89it/s]                                               {'loss': 2.1777, 'grad_norm': 3.7764477729797363, 'learning_rate': 5.178108619982257e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.89it/s]                                               {'loss': 2.2803, 'grad_norm': 3.757812261581421, 'learning_rate': 5.021196237558553e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.89it/s] 60%|██████    | 45/75 [00:01<00:01, 25.98it/s]                                               {'loss': 2.1747, 'grad_norm': 8.035204887390137, 'learning_rate': 4.8642838551348475e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.98it/s]                                               {'loss': 2.3683, 'grad_norm': 4.698592662811279, 'learning_rate': 4.707371472711143e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.98it/s]                                               {'loss': 2.2442, 'grad_norm': 4.069528579711914, 'learning_rate': 4.550459090287438e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.98it/s] 64%|██████▍   | 48/75 [00:01<00:01, 25.93it/s]                                               {'loss': 2.1839, 'grad_norm': 4.7191948890686035, 'learning_rate': 4.3935467078637335e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 25.93it/s]                                               {'loss': 2.2491, 'grad_norm': 4.292372226715088, 'learning_rate': 4.2366343254400287e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 25.93it/s]                                               {'loss': 2.2693, 'grad_norm': 4.167245388031006, 'learning_rate': 4.079721943016324e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.93it/s] 68%|██████▊   | 51/75 [00:02<00:00, 24.40it/s]                                               {'loss': 2.2371, 'grad_norm': 4.157426834106445, 'learning_rate': 3.922809560592619e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 24.40it/s]                                               {'loss': 2.1582, 'grad_norm': 3.0940487384796143, 'learning_rate': 3.765897178168915e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.40it/s]                                               {'loss': 2.0048, 'grad_norm': 3.5998072624206543, 'learning_rate': 3.608984795745209e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.40it/s] 72%|███████▏  | 54/75 [00:02<00:00, 24.46it/s]                                               {'loss': 2.1762, 'grad_norm': 3.3643040657043457, 'learning_rate': 3.452072413321505e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.46it/s]                                               {'loss': 2.1966, 'grad_norm': 2.865945339202881, 'learning_rate': 3.295160030897801e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.46it/s]                                               {'loss': 2.2367, 'grad_norm': 3.5294156074523926, 'learning_rate': 3.138247648474095e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.46it/s] 76%|███████▌  | 57/75 [00:02<00:00, 24.66it/s]                                               {'loss': 2.3598, 'grad_norm': 4.564591407775879, 'learning_rate': 2.981335266050391e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.66it/s]                                               {'loss': 2.1107, 'grad_norm': 3.4412553310394287, 'learning_rate': 2.8244228836266857e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.66it/s]                                               {'loss': 2.218, 'grad_norm': 5.320131778717041, 'learning_rate': 2.6675105012029808e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.66it/s]                                               {'loss': 1.8818, 'grad_norm': 9.283784866333008, 'learning_rate': 2.5105981187792766e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.66it/s] 81%|████████▏ | 61/75 [00:02<00:00, 25.74it/s]                                               {'loss': 2.1308, 'grad_norm': 3.188951253890991, 'learning_rate': 2.3536857363555717e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.74it/s]                                               {'loss': 2.1714, 'grad_norm': 3.196240186691284, 'learning_rate': 2.1967733539318668e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.74it/s]                                               {'loss': 2.3085, 'grad_norm': 4.272805213928223, 'learning_rate': 2.039860971508162e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.74it/s] 85%|████████▌ | 64/75 [00:02<00:00, 25.53it/s]                                               {'loss': 2.1523, 'grad_norm': 4.682560443878174, 'learning_rate': 1.8829485890844573e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.53it/s]                                               {'loss': 2.1817, 'grad_norm': 3.910731077194214, 'learning_rate': 1.7260362066607524e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.53it/s]                                               {'loss': 2.3027, 'grad_norm': 3.897317409515381, 'learning_rate': 1.5691238242370476e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.53it/s] 89%|████████▉ | 67/75 [00:02<00:00, 25.73it/s]                                               {'loss': 2.1239, 'grad_norm': 4.257541179656982, 'learning_rate': 1.4122114418133428e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.73it/s]                                               {'loss': 2.3447, 'grad_norm': 5.129253387451172, 'learning_rate': 1.2552990593896383e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.73it/s]                                               {'loss': 2.3326, 'grad_norm': 4.004299640655518, 'learning_rate': 1.0983866769659334e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.73it/s] 93%|█████████▎| 70/75 [00:02<00:00, 25.32it/s]                                               {'loss': 2.1499, 'grad_norm': 3.3715243339538574, 'learning_rate': 9.414742945422287e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.32it/s]                                               {'loss': 1.9512, 'grad_norm': 4.069545269012451, 'learning_rate': 7.845619121185238e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.32it/s]                                               {'loss': 2.4221, 'grad_norm': 6.54819393157959, 'learning_rate': 6.276495296948191e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.32it/s] 97%|█████████▋| 73/75 [00:03<00:00, 25.23it/s]                                               {'loss': 2.1989, 'grad_norm': 2.9660778045654297, 'learning_rate': 4.707371472711143e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 25.23it/s]                                               {'loss': 2.0734, 'grad_norm': 3.610619306564331, 'learning_rate': 3.1382476484740957e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 25.23it/s]                                               {'loss': 2.156, 'grad_norm': 6.065937042236328, 'learning_rate': 1.5691238242370478e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.23it/s]                                               {'train_runtime': 3.1791, 'train_samples_per_second': 355.444, 'train_steps_per_second': 23.591, 'train_loss': 2.225243663787842, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.23it/s]100%|██████████| 75/75 [00:03<00:00, 23.59it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:45
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]  1%|▏         | 1/75 [00:00<00:07,  9.86it/s]                                              {'loss': 2.241, 'grad_norm': 3.8861310482025146, 'learning_rate': 0.00011768428681777857, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:07,  9.86it/s]                                              {'loss': 2.2153, 'grad_norm': 4.675482273101807, 'learning_rate': 0.00011611516299354152, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:07,  9.86it/s]                                              {'loss': 2.3605, 'grad_norm': 5.109785079956055, 'learning_rate': 0.00011454603916930449, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:07,  9.86it/s]  5%|▌         | 4/75 [00:00<00:03, 19.28it/s]                                              {'loss': 2.1143, 'grad_norm': 4.033624172210693, 'learning_rate': 0.00011297691534506743, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 19.28it/s]                                              {'loss': 2.2838, 'grad_norm': 3.6302905082702637, 'learning_rate': 0.00011140779152083038, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 19.28it/s]  8%|▊         | 6/75 [00:00<00:03, 18.57it/s]                                              {'loss': 2.3756, 'grad_norm': 4.464787483215332, 'learning_rate': 0.00010983866769659334, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 18.57it/s]                                              {'loss': 2.4553, 'grad_norm': 3.5813584327697754, 'learning_rate': 0.00010826954387235629, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 18.57it/s]                                              {'loss': 2.1763, 'grad_norm': 4.222074508666992, 'learning_rate': 0.00010670042004811923, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 18.57it/s] 12%|█▏        | 9/75 [00:00<00:03, 20.72it/s]                                              {'loss': 2.4657, 'grad_norm': 5.161046028137207, 'learning_rate': 0.0001051312962238822, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 20.72it/s]                                              {'loss': 2.358, 'grad_norm': 4.559937000274658, 'learning_rate': 0.00010356217239964515, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 20.72it/s]                                               {'loss': 2.0741, 'grad_norm': 2.990586519241333, 'learning_rate': 0.0001019930485754081, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 20.72it/s] 16%|█▌        | 12/75 [00:00<00:02, 21.28it/s]                                               {'loss': 2.2339, 'grad_norm': 3.586341619491577, 'learning_rate': 0.00010042392475117106, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 21.28it/s]                                               {'loss': 2.3236, 'grad_norm': 3.766218423843384, 'learning_rate': 9.8854800926934e-05, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 21.28it/s]                                               {'loss': 2.242, 'grad_norm': 4.848346710205078, 'learning_rate': 9.728567710269695e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 21.28it/s] 20%|██        | 15/75 [00:00<00:02, 22.56it/s]                                               {'loss': 2.2852, 'grad_norm': 15.390890121459961, 'learning_rate': 9.57165532784599e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 22.56it/s]                                               {'loss': 2.3849, 'grad_norm': 4.27736759185791, 'learning_rate': 9.414742945422287e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 22.56it/s]                                               {'loss': 2.1861, 'grad_norm': 4.767261028289795, 'learning_rate': 9.25783056299858e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 22.56it/s] 24%|██▍       | 18/75 [00:00<00:02, 23.56it/s]                                               {'loss': 2.1469, 'grad_norm': 3.6086196899414062, 'learning_rate': 9.100918180574876e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 23.56it/s]                                               {'loss': 2.1721, 'grad_norm': 3.4067699909210205, 'learning_rate': 8.944005798151172e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 23.56it/s]                                               {'loss': 2.3683, 'grad_norm': 3.485416889190674, 'learning_rate': 8.787093415727467e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 23.56it/s] 28%|██▊       | 21/75 [00:00<00:02, 23.01it/s]                                               {'loss': 2.2652, 'grad_norm': 5.895878791809082, 'learning_rate': 8.630181033303761e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 23.01it/s]                                               {'loss': 2.0223, 'grad_norm': 5.392293930053711, 'learning_rate': 8.473268650880057e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 23.01it/s]                                               {'loss': 2.2138, 'grad_norm': 4.213184356689453, 'learning_rate': 8.316356268456352e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 23.01it/s] 32%|███▏      | 24/75 [00:01<00:02, 23.72it/s]                                               {'loss': 2.1096, 'grad_norm': 3.652383327484131, 'learning_rate': 8.159443886032648e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 23.72it/s]                                               {'loss': 2.2357, 'grad_norm': 3.244114637374878, 'learning_rate': 8.002531503608944e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.72it/s]                                               {'loss': 1.9947, 'grad_norm': 3.2549266815185547, 'learning_rate': 7.845619121185238e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.72it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.59it/s]                                               {'loss': 2.192, 'grad_norm': 3.3836183547973633, 'learning_rate': 7.688706738761533e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.59it/s]                                               {'loss': 2.6005, 'grad_norm': 5.592009544372559, 'learning_rate': 7.53179435633783e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.59it/s]                                               {'loss': 2.2964, 'grad_norm': 3.4041554927825928, 'learning_rate': 7.374881973914124e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.59it/s]                                               {'loss': 2.4259, 'grad_norm': 16.880067825317383, 'learning_rate': 7.217969591490418e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.59it/s] 41%|████▏     | 31/75 [00:01<00:01, 26.12it/s]                                               {'loss': 2.2967, 'grad_norm': 4.46110725402832, 'learning_rate': 7.061057209066715e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 26.12it/s]                                               {'loss': 2.2771, 'grad_norm': 5.46864652633667, 'learning_rate': 6.90414482664301e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 26.12it/s]                                               {'loss': 2.3061, 'grad_norm': 3.0147273540496826, 'learning_rate': 6.747232444219305e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.12it/s] 45%|████▌     | 34/75 [00:01<00:01, 25.48it/s]                                               {'loss': 2.2826, 'grad_norm': 4.165058135986328, 'learning_rate': 6.590320061795601e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.48it/s]                                               {'loss': 1.8981, 'grad_norm': 3.6822781562805176, 'learning_rate': 6.433407679371895e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.48it/s]                                               {'loss': 1.9935, 'grad_norm': 5.297290802001953, 'learning_rate': 6.27649529694819e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.48it/s] 49%|████▉     | 37/75 [00:01<00:01, 25.71it/s]                                               {'loss': 2.3704, 'grad_norm': 4.484448432922363, 'learning_rate': 6.119582914524487e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.71it/s]                                               {'loss': 2.2957, 'grad_norm': 4.198986053466797, 'learning_rate': 5.962670532100782e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.71it/s]                                               {'loss': 2.2121, 'grad_norm': 4.238469123840332, 'learning_rate': 5.805758149677076e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.71it/s] 53%|█████▎    | 40/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.2082, 'grad_norm': 4.361655235290527, 'learning_rate': 5.648845767253371e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.199, 'grad_norm': 3.043729543685913, 'learning_rate': 5.491933384829667e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.39it/s]                                               {'loss': 2.3109, 'grad_norm': 4.761599063873291, 'learning_rate': 5.3350210024059615e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.39it/s] 57%|█████▋    | 43/75 [00:01<00:01, 25.12it/s]                                               {'loss': 2.1597, 'grad_norm': 4.133545398712158, 'learning_rate': 5.178108619982257e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.12it/s]                                               {'loss': 2.2257, 'grad_norm': 6.163312911987305, 'learning_rate': 5.021196237558553e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.12it/s]                                               {'loss': 1.984, 'grad_norm': 8.148533821105957, 'learning_rate': 4.8642838551348475e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.12it/s]                                               {'loss': 2.2294, 'grad_norm': 3.913377046585083, 'learning_rate': 4.707371472711143e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.12it/s] 63%|██████▎   | 47/75 [00:01<00:01, 26.69it/s]                                               {'loss': 2.0991, 'grad_norm': 4.203311443328857, 'learning_rate': 4.550459090287438e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.69it/s]                                               {'loss': 2.2405, 'grad_norm': 4.839192867279053, 'learning_rate': 4.3935467078637335e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.69it/s]                                               {'loss': 2.3027, 'grad_norm': 3.15429425239563, 'learning_rate': 4.2366343254400287e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:00, 26.69it/s] 67%|██████▋   | 50/75 [00:02<00:00, 26.08it/s]                                               {'loss': 1.9568, 'grad_norm': 3.815518379211426, 'learning_rate': 4.079721943016324e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 26.08it/s]                                               {'loss': 2.1363, 'grad_norm': 4.176182270050049, 'learning_rate': 3.922809560592619e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 26.08it/s]                                               {'loss': 1.9716, 'grad_norm': 3.0652353763580322, 'learning_rate': 3.765897178168915e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 26.08it/s] 71%|███████   | 53/75 [00:02<00:00, 25.32it/s]                                               {'loss': 2.255, 'grad_norm': 3.7023727893829346, 'learning_rate': 3.608984795745209e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.32it/s]                                               {'loss': 2.1831, 'grad_norm': 4.692931175231934, 'learning_rate': 3.452072413321505e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.32it/s]                                               {'loss': 2.3782, 'grad_norm': 4.12794828414917, 'learning_rate': 3.295160030897801e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.32it/s] 75%|███████▍  | 56/75 [00:02<00:00, 25.62it/s]                                               {'loss': 2.2297, 'grad_norm': 3.917117118835449, 'learning_rate': 3.138247648474095e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.62it/s]                                               {'loss': 2.2956, 'grad_norm': 3.7836170196533203, 'learning_rate': 2.981335266050391e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.62it/s]                                               {'loss': 2.164, 'grad_norm': 3.2863643169403076, 'learning_rate': 2.8244228836266857e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.62it/s] 79%|███████▊  | 59/75 [00:02<00:00, 25.43it/s]                                               {'loss': 2.2387, 'grad_norm': 5.3277587890625, 'learning_rate': 2.6675105012029808e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.43it/s]                                               {'loss': 2.0987, 'grad_norm': 15.056560516357422, 'learning_rate': 2.5105981187792766e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.43it/s]                                               {'loss': 2.1173, 'grad_norm': 4.489664554595947, 'learning_rate': 2.3536857363555717e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.43it/s] 83%|████████▎ | 62/75 [00:02<00:00, 25.79it/s]                                               {'loss': 2.3034, 'grad_norm': 4.312598705291748, 'learning_rate': 2.1967733539318668e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 25.79it/s]                                               {'loss': 2.1986, 'grad_norm': 3.458242416381836, 'learning_rate': 2.039860971508162e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.79it/s]                                               {'loss': 2.2017, 'grad_norm': 3.7307724952697754, 'learning_rate': 1.8829485890844573e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.79it/s] 87%|████████▋ | 65/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.1539, 'grad_norm': 2.8676788806915283, 'learning_rate': 1.7260362066607524e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.1134, 'grad_norm': 3.391401529312134, 'learning_rate': 1.5691238242370476e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.3063, 'grad_norm': 4.615767002105713, 'learning_rate': 1.4122114418133428e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.02it/s] 91%|█████████ | 68/75 [00:02<00:00, 25.11it/s]                                               {'loss': 2.1487, 'grad_norm': 4.080471515655518, 'learning_rate': 1.2552990593896383e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.11it/s]                                               {'loss': 2.0619, 'grad_norm': 4.126469612121582, 'learning_rate': 1.0983866769659334e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.11it/s]                                               {'loss': 2.1037, 'grad_norm': 3.486581802368164, 'learning_rate': 9.414742945422287e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.11it/s] 95%|█████████▍| 71/75 [00:02<00:00, 25.18it/s]                                               {'loss': 2.1431, 'grad_norm': 4.50358772277832, 'learning_rate': 7.845619121185238e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.18it/s]                                               {'loss': 2.2445, 'grad_norm': 3.38920521736145, 'learning_rate': 6.276495296948191e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.18it/s]                                               {'loss': 2.0968, 'grad_norm': 3.1624553203582764, 'learning_rate': 4.707371472711143e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.18it/s] 99%|█████████▊| 74/75 [00:03<00:00, 25.11it/s]                                               {'loss': 2.3733, 'grad_norm': 3.2491650581359863, 'learning_rate': 3.1382476484740957e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 25.11it/s]                                               {'loss': 1.8193, 'grad_norm': 8.565573692321777, 'learning_rate': 1.5691238242370478e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.11it/s]                                               {'train_runtime': 3.2068, 'train_samples_per_second': 352.371, 'train_steps_per_second': 23.387, 'train_loss': 2.2133899180094403, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.11it/s]100%|██████████| 75/75 [00:03<00:00, 23.39it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:49
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.1998, 'grad_norm': 3.6865062713623047, 'learning_rate': 0.00011768428681777857, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:05, 14.15it/s]  3%|▎         | 2/75 [00:00<00:04, 17.47it/s]                                              {'loss': 2.3579, 'grad_norm': 3.437718391418457, 'learning_rate': 0.00011611516299354152, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:04, 17.47it/s]                                              {'loss': 2.1297, 'grad_norm': 3.9099936485290527, 'learning_rate': 0.00011454603916930449, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:04, 17.47it/s]                                              {'loss': 2.3816, 'grad_norm': 4.709171772003174, 'learning_rate': 0.00011297691534506743, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:04, 17.47it/s]  7%|▋         | 5/75 [00:00<00:03, 21.66it/s]                                              {'loss': 2.216, 'grad_norm': 3.5599477291107178, 'learning_rate': 0.00011140779152083038, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 21.66it/s]                                              {'loss': 2.0724, 'grad_norm': 4.169110298156738, 'learning_rate': 0.00010983866769659334, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 21.66it/s]                                              {'loss': 2.3672, 'grad_norm': 3.878356456756592, 'learning_rate': 0.00010826954387235629, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 21.66it/s] 11%|█         | 8/75 [00:00<00:02, 22.73it/s]                                              {'loss': 2.2895, 'grad_norm': 3.9592368602752686, 'learning_rate': 0.00010670042004811923, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 22.73it/s]                                              {'loss': 2.2782, 'grad_norm': 3.740032911300659, 'learning_rate': 0.0001051312962238822, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 22.73it/s]                                              {'loss': 2.5062, 'grad_norm': 4.667350769042969, 'learning_rate': 0.00010356217239964515, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 22.73it/s] 15%|█▍        | 11/75 [00:00<00:02, 22.83it/s]                                               {'loss': 2.2961, 'grad_norm': 4.333146572113037, 'learning_rate': 0.0001019930485754081, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 22.83it/s]                                               {'loss': 2.2396, 'grad_norm': 4.064399242401123, 'learning_rate': 0.00010042392475117106, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 22.83it/s]                                               {'loss': 2.2949, 'grad_norm': 4.370765686035156, 'learning_rate': 9.8854800926934e-05, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 22.83it/s] 19%|█▊        | 14/75 [00:00<00:02, 23.08it/s]                                               {'loss': 2.1396, 'grad_norm': 4.273075103759766, 'learning_rate': 9.728567710269695e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.08it/s]                                               {'loss': 2.405, 'grad_norm': 3.8400847911834717, 'learning_rate': 9.57165532784599e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.08it/s]                                               {'loss': 2.3531, 'grad_norm': 4.167652606964111, 'learning_rate': 9.414742945422287e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 23.08it/s] 23%|██▎       | 17/75 [00:00<00:02, 24.12it/s]                                               {'loss': 2.112, 'grad_norm': 3.276885747909546, 'learning_rate': 9.25783056299858e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.12it/s]                                               {'loss': 2.1122, 'grad_norm': 3.3245460987091064, 'learning_rate': 9.100918180574876e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.12it/s]                                               {'loss': 2.3886, 'grad_norm': 3.4533605575561523, 'learning_rate': 8.944005798151172e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.12it/s] 27%|██▋       | 20/75 [00:00<00:02, 23.35it/s]                                               {'loss': 2.3993, 'grad_norm': 3.8905630111694336, 'learning_rate': 8.787093415727467e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 23.35it/s]                                               {'loss': 2.3099, 'grad_norm': 4.57963228225708, 'learning_rate': 8.630181033303761e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 23.35it/s]                                               {'loss': 2.2729, 'grad_norm': 4.770308971405029, 'learning_rate': 8.473268650880057e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 23.35it/s] 31%|███       | 23/75 [00:00<00:02, 23.59it/s]                                               {'loss': 2.1096, 'grad_norm': 3.5123729705810547, 'learning_rate': 8.316356268456352e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 23.59it/s]                                               {'loss': 2.2649, 'grad_norm': 5.190657615661621, 'learning_rate': 8.159443886032648e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 23.59it/s]                                               {'loss': 2.2843, 'grad_norm': 3.7000389099121094, 'learning_rate': 8.002531503608944e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.59it/s] 35%|███▍      | 26/75 [00:01<00:02, 23.50it/s]                                               {'loss': 2.2973, 'grad_norm': 4.14793586730957, 'learning_rate': 7.845619121185238e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.50it/s]                                               {'loss': 2.2688, 'grad_norm': 4.216289043426514, 'learning_rate': 7.688706738761533e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 23.50it/s]                                               {'loss': 2.0756, 'grad_norm': 2.98846697807312, 'learning_rate': 7.53179435633783e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 23.50it/s] 39%|███▊      | 29/75 [00:01<00:01, 23.71it/s]                                               {'loss': 2.1113, 'grad_norm': 3.857574462890625, 'learning_rate': 7.374881973914124e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 23.71it/s]                                               {'loss': 2.3265, 'grad_norm': 3.2815542221069336, 'learning_rate': 7.217969591490418e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.71it/s]                                               {'loss': 2.2306, 'grad_norm': 4.2128095626831055, 'learning_rate': 7.061057209066715e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 23.71it/s] 43%|████▎     | 32/75 [00:01<00:01, 24.00it/s]                                               {'loss': 2.4573, 'grad_norm': 4.118927955627441, 'learning_rate': 6.90414482664301e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.00it/s]                                               {'loss': 2.062, 'grad_norm': 3.619203805923462, 'learning_rate': 6.747232444219305e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 24.00it/s]                                               {'loss': 2.1036, 'grad_norm': 4.02865743637085, 'learning_rate': 6.590320061795601e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 24.00it/s] 47%|████▋     | 35/75 [00:01<00:01, 24.30it/s]                                               {'loss': 2.0258, 'grad_norm': 2.9273602962493896, 'learning_rate': 6.433407679371895e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 24.30it/s]                                               {'loss': 2.2716, 'grad_norm': 3.5236551761627197, 'learning_rate': 6.27649529694819e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 24.30it/s]                                               {'loss': 2.1523, 'grad_norm': 2.723550796508789, 'learning_rate': 6.119582914524487e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 24.30it/s] 51%|█████     | 38/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.2358, 'grad_norm': 4.2845635414123535, 'learning_rate': 5.962670532100782e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.2886, 'grad_norm': 4.636002063751221, 'learning_rate': 5.805758149677076e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 24.57it/s]                                               {'loss': 2.2217, 'grad_norm': 3.7768895626068115, 'learning_rate': 5.648845767253371e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 24.57it/s] 55%|█████▍    | 41/75 [00:01<00:01, 23.29it/s]                                               {'loss': 2.1133, 'grad_norm': 4.172475814819336, 'learning_rate': 5.491933384829667e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 23.29it/s]                                               {'loss': 2.1245, 'grad_norm': 4.448280334472656, 'learning_rate': 5.3350210024059615e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 23.29it/s]                                               {'loss': 2.2466, 'grad_norm': 3.812196969985962, 'learning_rate': 5.178108619982257e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 23.29it/s] 59%|█████▊    | 44/75 [00:01<00:01, 23.65it/s]                                               {'loss': 2.3761, 'grad_norm': 3.9691994190216064, 'learning_rate': 5.021196237558553e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 23.65it/s]                                               {'loss': 2.2344, 'grad_norm': 4.380616664886475, 'learning_rate': 4.8642838551348475e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 23.65it/s]                                               {'loss': 2.1243, 'grad_norm': 3.7898128032684326, 'learning_rate': 4.707371472711143e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 23.65it/s] 63%|██████▎   | 47/75 [00:01<00:01, 24.17it/s]                                               {'loss': 2.3297, 'grad_norm': 4.8260064125061035, 'learning_rate': 4.550459090287438e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 24.17it/s]                                               {'loss': 2.2066, 'grad_norm': 3.997572183609009, 'learning_rate': 4.3935467078637335e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 24.17it/s]                                               {'loss': 2.1076, 'grad_norm': 3.7314999103546143, 'learning_rate': 4.2366343254400287e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 24.17it/s] 67%|██████▋   | 50/75 [00:02<00:01, 23.89it/s]                                               {'loss': 2.2154, 'grad_norm': 3.892655611038208, 'learning_rate': 4.079721943016324e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 23.89it/s]                                               {'loss': 2.359, 'grad_norm': 3.469125747680664, 'learning_rate': 3.922809560592619e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 23.89it/s]                                               {'loss': 2.0454, 'grad_norm': 4.563335418701172, 'learning_rate': 3.765897178168915e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 23.89it/s] 71%|███████   | 53/75 [00:02<00:00, 23.84it/s]                                               {'loss': 2.1515, 'grad_norm': 5.209028720855713, 'learning_rate': 3.608984795745209e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 23.84it/s]                                               {'loss': 2.0852, 'grad_norm': 4.3186798095703125, 'learning_rate': 3.452072413321505e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 23.84it/s]                                               {'loss': 2.3752, 'grad_norm': 4.849681377410889, 'learning_rate': 3.295160030897801e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 23.84it/s] 75%|███████▍  | 56/75 [00:02<00:00, 23.71it/s]                                               {'loss': 2.356, 'grad_norm': 4.289616107940674, 'learning_rate': 3.138247648474095e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 23.71it/s]                                               {'loss': 2.1327, 'grad_norm': 4.774651050567627, 'learning_rate': 2.981335266050391e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 23.71it/s]                                               {'loss': 2.323, 'grad_norm': 5.051916122436523, 'learning_rate': 2.8244228836266857e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 23.71it/s] 79%|███████▊  | 59/75 [00:02<00:00, 24.26it/s]                                               {'loss': 2.1119, 'grad_norm': 3.6994407176971436, 'learning_rate': 2.6675105012029808e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.26it/s]                                               {'loss': 2.2746, 'grad_norm': 4.040630340576172, 'learning_rate': 2.5105981187792766e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.26it/s]                                               {'loss': 2.2048, 'grad_norm': 3.8421592712402344, 'learning_rate': 2.3536857363555717e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.26it/s] 83%|████████▎ | 62/75 [00:02<00:00, 24.54it/s]                                               {'loss': 2.2009, 'grad_norm': 4.506776332855225, 'learning_rate': 2.1967733539318668e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 24.54it/s]                                               {'loss': 2.207, 'grad_norm': 3.6444578170776367, 'learning_rate': 2.039860971508162e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 24.54it/s]                                               {'loss': 2.22, 'grad_norm': 4.038195610046387, 'learning_rate': 1.8829485890844573e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 24.54it/s] 87%|████████▋ | 65/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.0937, 'grad_norm': 4.1905694007873535, 'learning_rate': 1.7260362066607524e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.136, 'grad_norm': 3.4594740867614746, 'learning_rate': 1.5691238242370476e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.83it/s]                                               {'loss': 2.0625, 'grad_norm': 4.232099533081055, 'learning_rate': 1.4122114418133428e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 24.83it/s] 91%|█████████ | 68/75 [00:02<00:00, 24.69it/s]                                               {'loss': 2.2928, 'grad_norm': 3.6587440967559814, 'learning_rate': 1.2552990593896383e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 24.69it/s]                                               {'loss': 2.2149, 'grad_norm': 4.911400318145752, 'learning_rate': 1.0983866769659334e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 24.69it/s]                                               {'loss': 2.1957, 'grad_norm': 4.651612758636475, 'learning_rate': 9.414742945422287e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 24.69it/s] 95%|█████████▍| 71/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.079, 'grad_norm': 3.2544562816619873, 'learning_rate': 7.845619121185238e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 24.53it/s]                                               {'loss': 2.0711, 'grad_norm': 4.041069030761719, 'learning_rate': 6.276495296948191e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 24.53it/s]                                               {'loss': 2.1193, 'grad_norm': 3.1759302616119385, 'learning_rate': 4.707371472711143e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 24.53it/s] 99%|█████████▊| 74/75 [00:03<00:00, 24.99it/s]                                               {'loss': 2.347, 'grad_norm': 5.916484832763672, 'learning_rate': 3.1382476484740957e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.99it/s]                                               {'loss': 2.3716, 'grad_norm': 4.2348313331604, 'learning_rate': 1.5691238242370478e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.99it/s]                                               {'train_runtime': 3.248, 'train_samples_per_second': 369.464, 'train_steps_per_second': 23.091, 'train_loss': 2.226909144719442, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.99it/s]100%|██████████| 75/75 [00:03<00:00, 23.09it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:9
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2392, 'grad_norm': 4.329324245452881, 'learning_rate': 0.00011768428681777857, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 16.85it/s]                                              {'loss': 2.3537, 'grad_norm': 3.629809856414795, 'learning_rate': 0.00011611516299354152, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 20.65it/s]  4%|▍         | 3/75 [00:00<00:03, 21.92it/s]                                              {'loss': 2.4289, 'grad_norm': 5.227612495422363, 'learning_rate': 0.00011454603916930449, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 21.92it/s]                                              {'loss': 2.4393, 'grad_norm': 4.48869514465332, 'learning_rate': 0.00011297691534506743, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 21.92it/s]                                              {'loss': 2.0816, 'grad_norm': 3.343332529067993, 'learning_rate': 0.00011140779152083038, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 21.92it/s]  8%|▊         | 6/75 [00:00<00:02, 23.07it/s]                                              {'loss': 2.3032, 'grad_norm': 4.057690143585205, 'learning_rate': 0.00010983866769659334, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 23.07it/s]                                              {'loss': 2.4548, 'grad_norm': 3.704432487487793, 'learning_rate': 0.00010826954387235629, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 23.07it/s]                                              {'loss': 2.1936, 'grad_norm': 3.234097957611084, 'learning_rate': 0.00010670042004811923, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.07it/s] 12%|█▏        | 9/75 [00:00<00:02, 22.30it/s]                                              {'loss': 2.1975, 'grad_norm': 4.177892208099365, 'learning_rate': 0.0001051312962238822, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 22.30it/s]                                              {'loss': 2.2566, 'grad_norm': 3.5777480602264404, 'learning_rate': 0.00010356217239964515, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 22.30it/s]                                               {'loss': 2.1408, 'grad_norm': 3.3820300102233887, 'learning_rate': 0.0001019930485754081, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 22.30it/s] 16%|█▌        | 12/75 [00:00<00:02, 22.91it/s]                                               {'loss': 2.2768, 'grad_norm': 5.013429641723633, 'learning_rate': 0.00010042392475117106, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 22.91it/s]                                               {'loss': 2.4581, 'grad_norm': 4.359335422515869, 'learning_rate': 9.8854800926934e-05, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 22.91it/s]                                               {'loss': 2.5054, 'grad_norm': 3.7483136653900146, 'learning_rate': 9.728567710269695e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 22.91it/s] 20%|██        | 15/75 [00:00<00:02, 24.21it/s]                                               {'loss': 2.6104, 'grad_norm': 7.869256973266602, 'learning_rate': 9.57165532784599e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.21it/s]                                               {'loss': 2.2056, 'grad_norm': 2.951228618621826, 'learning_rate': 9.414742945422287e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 24.21it/s]                                               {'loss': 2.1611, 'grad_norm': 4.706282138824463, 'learning_rate': 9.25783056299858e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.21it/s] 24%|██▍       | 18/75 [00:00<00:02, 22.98it/s]                                               {'loss': 2.3636, 'grad_norm': 3.4366114139556885, 'learning_rate': 9.100918180574876e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 22.98it/s]                                               {'loss': 2.2522, 'grad_norm': 4.293674468994141, 'learning_rate': 8.944005798151172e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 22.98it/s]                                               {'loss': 2.2304, 'grad_norm': 5.0650224685668945, 'learning_rate': 8.787093415727467e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 22.98it/s] 28%|██▊       | 21/75 [00:00<00:02, 23.22it/s]                                               {'loss': 2.2188, 'grad_norm': 4.061996936798096, 'learning_rate': 8.630181033303761e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 23.22it/s]                                               {'loss': 2.2941, 'grad_norm': 4.080868721008301, 'learning_rate': 8.473268650880057e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 23.22it/s]                                               {'loss': 2.3067, 'grad_norm': 3.6630008220672607, 'learning_rate': 8.316356268456352e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 23.22it/s] 32%|███▏      | 24/75 [00:01<00:02, 23.60it/s]                                               {'loss': 2.1242, 'grad_norm': 3.4508984088897705, 'learning_rate': 8.159443886032648e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 23.60it/s]                                               {'loss': 2.0441, 'grad_norm': 3.707327365875244, 'learning_rate': 8.002531503608944e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 23.60it/s]                                               {'loss': 2.4849, 'grad_norm': 3.976416826248169, 'learning_rate': 7.845619121185238e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 23.60it/s] 36%|███▌      | 27/75 [00:01<00:02, 22.70it/s]                                               {'loss': 2.3476, 'grad_norm': 4.419511795043945, 'learning_rate': 7.688706738761533e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 22.70it/s]                                               {'loss': 2.2056, 'grad_norm': 4.034735679626465, 'learning_rate': 7.53179435633783e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 22.70it/s]                                               {'loss': 2.3397, 'grad_norm': 4.285665512084961, 'learning_rate': 7.374881973914124e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 22.70it/s] 40%|████      | 30/75 [00:01<00:02, 21.51it/s]                                               {'loss': 2.4969, 'grad_norm': 12.004728317260742, 'learning_rate': 7.217969591490418e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:02, 21.51it/s]                                               {'loss': 2.1106, 'grad_norm': 4.0718464851379395, 'learning_rate': 7.061057209066715e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:02, 21.51it/s]                                               {'loss': 2.3177, 'grad_norm': 3.4726412296295166, 'learning_rate': 6.90414482664301e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 21.51it/s] 44%|████▍     | 33/75 [00:01<00:01, 21.38it/s]                                               {'loss': 2.5038, 'grad_norm': 4.413076400756836, 'learning_rate': 6.747232444219305e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 21.38it/s]                                               {'loss': 2.254, 'grad_norm': 3.723353147506714, 'learning_rate': 6.590320061795601e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 21.38it/s]                                               {'loss': 2.0917, 'grad_norm': 2.814424753189087, 'learning_rate': 6.433407679371895e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 21.38it/s] 48%|████▊     | 36/75 [00:01<00:01, 22.31it/s]                                               {'loss': 2.373, 'grad_norm': 3.647717237472534, 'learning_rate': 6.27649529694819e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 22.31it/s]                                               {'loss': 2.159, 'grad_norm': 3.433387041091919, 'learning_rate': 6.119582914524487e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 22.31it/s]                                               {'loss': 2.2746, 'grad_norm': 4.303090572357178, 'learning_rate': 5.962670532100782e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 22.31it/s] 52%|█████▏    | 39/75 [00:01<00:01, 20.23it/s]                                               {'loss': 2.2385, 'grad_norm': 4.001088619232178, 'learning_rate': 5.805758149677076e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 20.23it/s]                                               {'loss': 2.2779, 'grad_norm': 3.2359509468078613, 'learning_rate': 5.648845767253371e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 20.23it/s]                                               {'loss': 2.3463, 'grad_norm': 6.119336128234863, 'learning_rate': 5.491933384829667e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 20.23it/s] 56%|█████▌    | 42/75 [00:01<00:01, 20.15it/s]                                               {'loss': 2.0609, 'grad_norm': 2.798295259475708, 'learning_rate': 5.3350210024059615e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 20.15it/s]                                               {'loss': 2.3101, 'grad_norm': 5.953582763671875, 'learning_rate': 5.178108619982257e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 20.15it/s]                                               {'loss': 2.1175, 'grad_norm': 3.534369707107544, 'learning_rate': 5.021196237558553e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 20.15it/s] 60%|██████    | 45/75 [00:02<00:01, 19.42it/s]                                               {'loss': 2.3914, 'grad_norm': 10.392661094665527, 'learning_rate': 4.8642838551348475e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 19.42it/s]                                               {'loss': 2.3272, 'grad_norm': 4.199271202087402, 'learning_rate': 4.707371472711143e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 19.42it/s] 63%|██████▎   | 47/75 [00:02<00:01, 18.94it/s]                                               {'loss': 2.576, 'grad_norm': 4.214790344238281, 'learning_rate': 4.550459090287438e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 18.94it/s]                                               {'loss': 2.2095, 'grad_norm': 4.267355442047119, 'learning_rate': 4.3935467078637335e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 18.94it/s]                                               {'loss': 2.1131, 'grad_norm': 4.132292747497559, 'learning_rate': 4.2366343254400287e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 18.94it/s] 67%|██████▋   | 50/75 [00:02<00:01, 20.05it/s]                                               {'loss': 2.2531, 'grad_norm': 4.97597599029541, 'learning_rate': 4.079721943016324e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:01, 20.05it/s]                                               {'loss': 2.411, 'grad_norm': 4.6941633224487305, 'learning_rate': 3.922809560592619e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:01, 20.05it/s]                                               {'loss': 2.2372, 'grad_norm': 4.309780120849609, 'learning_rate': 3.765897178168915e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:01, 20.05it/s] 71%|███████   | 53/75 [00:02<00:01, 21.40it/s]                                               {'loss': 2.0783, 'grad_norm': 3.230653762817383, 'learning_rate': 3.608984795745209e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:01, 21.40it/s]                                               {'loss': 2.1805, 'grad_norm': 3.098820447921753, 'learning_rate': 3.452072413321505e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 21.40it/s]                                               {'loss': 2.016, 'grad_norm': 3.48423433303833, 'learning_rate': 3.295160030897801e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 21.40it/s] 75%|███████▍  | 56/75 [00:02<00:00, 21.94it/s]                                               {'loss': 2.0589, 'grad_norm': 3.190293073654175, 'learning_rate': 3.138247648474095e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 21.94it/s]                                               {'loss': 2.1013, 'grad_norm': 3.314955472946167, 'learning_rate': 2.981335266050391e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 21.94it/s]                                               {'loss': 2.2633, 'grad_norm': 3.105214834213257, 'learning_rate': 2.8244228836266857e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 21.94it/s] 79%|███████▊  | 59/75 [00:02<00:00, 23.30it/s]                                               {'loss': 2.1901, 'grad_norm': 3.6571972370147705, 'learning_rate': 2.6675105012029808e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 23.30it/s]                                               {'loss': 2.8734, 'grad_norm': 17.333406448364258, 'learning_rate': 2.5105981187792766e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 23.30it/s]                                               {'loss': 2.1609, 'grad_norm': 2.951143741607666, 'learning_rate': 2.3536857363555717e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 23.30it/s]                                               {'loss': 2.2395, 'grad_norm': 3.4055047035217285, 'learning_rate': 2.1967733539318668e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 23.30it/s] 84%|████████▍ | 63/75 [00:02<00:00, 25.41it/s]                                               {'loss': 2.4796, 'grad_norm': 4.769642353057861, 'learning_rate': 2.039860971508162e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 25.41it/s]                                               {'loss': 2.2108, 'grad_norm': 3.3113362789154053, 'learning_rate': 1.8829485890844573e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 25.41it/s]                                               {'loss': 2.1587, 'grad_norm': 3.791081666946411, 'learning_rate': 1.7260362066607524e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 25.41it/s] 88%|████████▊ | 66/75 [00:02<00:00, 25.11it/s]                                               {'loss': 2.2039, 'grad_norm': 6.400018692016602, 'learning_rate': 1.5691238242370476e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 25.11it/s]                                               {'loss': 2.3415, 'grad_norm': 3.564847469329834, 'learning_rate': 1.4122114418133428e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 25.11it/s]                                               {'loss': 2.2378, 'grad_norm': 4.961794853210449, 'learning_rate': 1.2552990593896383e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 25.11it/s] 92%|█████████▏| 69/75 [00:03<00:00, 23.36it/s]                                               {'loss': 2.2836, 'grad_norm': 4.078955173492432, 'learning_rate': 1.0983866769659334e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 23.36it/s]                                               {'loss': 2.1543, 'grad_norm': 3.707432270050049, 'learning_rate': 9.414742945422287e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 23.36it/s]                                               {'loss': 1.9839, 'grad_norm': 3.4442272186279297, 'learning_rate': 7.845619121185238e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 23.36it/s] 96%|█████████▌| 72/75 [00:03<00:00, 23.20it/s]                                               {'loss': 2.1704, 'grad_norm': 3.3602163791656494, 'learning_rate': 6.276495296948191e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 23.20it/s]                                               {'loss': 2.2713, 'grad_norm': 3.776121139526367, 'learning_rate': 4.707371472711143e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 23.20it/s]                                               {'loss': 2.1981, 'grad_norm': 4.879799842834473, 'learning_rate': 3.1382476484740957e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 23.20it/s]                                               {'loss': 1.5312, 'grad_norm': 7.1116719245910645, 'learning_rate': 1.5691238242370478e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 23.20it/s]                                               {'train_runtime': 3.4373, 'train_samples_per_second': 328.75, 'train_steps_per_second': 21.82, 'train_loss': 2.2580897649129232, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 23.20it/s]100%|██████████| 75/75 [00:03<00:00, 21.87it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:47
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2418, 'grad_norm': 4.653680324554443, 'learning_rate': 0.00011768428681777857, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 17.43it/s]  3%|▎         | 2/75 [00:00<00:03, 19.25it/s]                                              {'loss': 2.4056, 'grad_norm': 3.938131093978882, 'learning_rate': 0.00011611516299354152, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 19.25it/s]                                              {'loss': 2.455, 'grad_norm': 4.255831241607666, 'learning_rate': 0.00011454603916930449, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 19.25it/s]                                              {'loss': 2.3417, 'grad_norm': 4.268747329711914, 'learning_rate': 0.00011297691534506743, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 19.25it/s]  7%|▋         | 5/75 [00:00<00:03, 21.31it/s]                                              {'loss': 2.4399, 'grad_norm': 3.6737945079803467, 'learning_rate': 0.00011140779152083038, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:03, 21.31it/s]                                              {'loss': 2.2327, 'grad_norm': 4.168468475341797, 'learning_rate': 0.00010983866769659334, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 21.31it/s]                                              {'loss': 2.3553, 'grad_norm': 4.073232173919678, 'learning_rate': 0.00010826954387235629, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 21.31it/s] 11%|█         | 8/75 [00:00<00:02, 23.45it/s]                                              {'loss': 2.2032, 'grad_norm': 4.390419960021973, 'learning_rate': 0.00010670042004811923, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 23.45it/s]                                              {'loss': 2.3272, 'grad_norm': 4.492262840270996, 'learning_rate': 0.0001051312962238822, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 23.45it/s]                                              {'loss': 2.4138, 'grad_norm': 3.3957290649414062, 'learning_rate': 0.00010356217239964515, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 23.45it/s] 15%|█▍        | 11/75 [00:00<00:02, 24.58it/s]                                               {'loss': 2.5078, 'grad_norm': 4.210716247558594, 'learning_rate': 0.0001019930485754081, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.58it/s]                                               {'loss': 2.1391, 'grad_norm': 3.9321112632751465, 'learning_rate': 0.00010042392475117106, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.58it/s]                                               {'loss': 2.4312, 'grad_norm': 4.271115303039551, 'learning_rate': 9.8854800926934e-05, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.58it/s] 19%|█▊        | 14/75 [00:00<00:02, 24.77it/s]                                               {'loss': 2.2744, 'grad_norm': 3.7730941772460938, 'learning_rate': 9.728567710269695e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.77it/s]                                               {'loss': 2.6806, 'grad_norm': 11.527193069458008, 'learning_rate': 9.57165532784599e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 24.77it/s]                                               {'loss': 2.3231, 'grad_norm': 4.361390113830566, 'learning_rate': 9.414742945422287e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 24.77it/s] 23%|██▎       | 17/75 [00:00<00:02, 24.92it/s]                                               {'loss': 2.3933, 'grad_norm': 4.665724754333496, 'learning_rate': 9.25783056299858e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 24.92it/s]                                               {'loss': 2.2823, 'grad_norm': 4.139030933380127, 'learning_rate': 9.100918180574876e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 24.92it/s]                                               {'loss': 2.1453, 'grad_norm': 3.875684976577759, 'learning_rate': 8.944005798151172e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 24.92it/s] 27%|██▋       | 20/75 [00:00<00:02, 24.61it/s]                                               {'loss': 2.4747, 'grad_norm': 6.013556003570557, 'learning_rate': 8.787093415727467e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 24.61it/s]                                               {'loss': 2.2121, 'grad_norm': 3.983011245727539, 'learning_rate': 8.630181033303761e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 24.61it/s]                                               {'loss': 2.2126, 'grad_norm': 4.309444427490234, 'learning_rate': 8.473268650880057e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 24.61it/s] 31%|███       | 23/75 [00:00<00:02, 24.55it/s]                                               {'loss': 2.3782, 'grad_norm': 3.493516206741333, 'learning_rate': 8.316356268456352e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 24.55it/s]                                               {'loss': 2.2313, 'grad_norm': 3.7254889011383057, 'learning_rate': 8.159443886032648e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 24.55it/s]                                               {'loss': 2.458, 'grad_norm': 4.354283332824707, 'learning_rate': 8.002531503608944e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 24.55it/s] 35%|███▍      | 26/75 [00:01<00:01, 24.65it/s]                                               {'loss': 2.085, 'grad_norm': 3.2149264812469482, 'learning_rate': 7.845619121185238e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 24.65it/s]                                               {'loss': 2.2462, 'grad_norm': 4.803745269775391, 'learning_rate': 7.688706738761533e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.65it/s]                                               {'loss': 2.1824, 'grad_norm': 5.044381141662598, 'learning_rate': 7.53179435633783e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.65it/s] 39%|███▊      | 29/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.3845, 'grad_norm': 3.826381206512451, 'learning_rate': 7.374881973914124e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.524, 'grad_norm': 10.141737937927246, 'learning_rate': 7.217969591490418e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.0439, 'grad_norm': 3.7241249084472656, 'learning_rate': 7.061057209066715e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.37it/s]                                               {'loss': 2.1825, 'grad_norm': 3.973240375518799, 'learning_rate': 6.90414482664301e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.37it/s] 44%|████▍     | 33/75 [00:01<00:01, 27.04it/s]                                               {'loss': 2.3931, 'grad_norm': 3.4569947719573975, 'learning_rate': 6.747232444219305e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 27.04it/s]                                               {'loss': 2.2464, 'grad_norm': 5.717804431915283, 'learning_rate': 6.590320061795601e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 27.04it/s]                                               {'loss': 2.3495, 'grad_norm': 4.645843982696533, 'learning_rate': 6.433407679371895e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 27.04it/s] 48%|████▊     | 36/75 [00:01<00:01, 26.87it/s]                                               {'loss': 2.5354, 'grad_norm': 4.883058547973633, 'learning_rate': 6.27649529694819e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.87it/s]                                               {'loss': 2.1143, 'grad_norm': 3.546682834625244, 'learning_rate': 6.119582914524487e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.87it/s]                                               {'loss': 2.1083, 'grad_norm': 2.956414222717285, 'learning_rate': 5.962670532100782e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 26.87it/s] 52%|█████▏    | 39/75 [00:01<00:01, 26.26it/s]                                               {'loss': 2.1602, 'grad_norm': 3.853991985321045, 'learning_rate': 5.805758149677076e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 26.26it/s]                                               {'loss': 2.2275, 'grad_norm': 4.098126411437988, 'learning_rate': 5.648845767253371e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 26.26it/s]                                               {'loss': 2.2745, 'grad_norm': 3.688441753387451, 'learning_rate': 5.491933384829667e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 26.26it/s] 56%|█████▌    | 42/75 [00:01<00:01, 25.92it/s]                                               {'loss': 2.459, 'grad_norm': 3.0863423347473145, 'learning_rate': 5.3350210024059615e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.92it/s]                                               {'loss': 2.1005, 'grad_norm': 4.259922981262207, 'learning_rate': 5.178108619982257e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.92it/s]                                               {'loss': 2.4115, 'grad_norm': 5.850076675415039, 'learning_rate': 5.021196237558553e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.92it/s]                                               {'loss': 2.3476, 'grad_norm': 15.618549346923828, 'learning_rate': 4.8642838551348475e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.92it/s] 61%|██████▏   | 46/75 [00:01<00:01, 27.10it/s]                                               {'loss': 2.2476, 'grad_norm': 3.2484004497528076, 'learning_rate': 4.707371472711143e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 27.10it/s]                                               {'loss': 2.2539, 'grad_norm': 3.915229320526123, 'learning_rate': 4.550459090287438e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 27.10it/s]                                               {'loss': 2.1382, 'grad_norm': 3.745537757873535, 'learning_rate': 4.3935467078637335e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:00, 27.10it/s] 65%|██████▌   | 49/75 [00:01<00:00, 26.40it/s]                                               {'loss': 2.2892, 'grad_norm': 4.544307231903076, 'learning_rate': 4.2366343254400287e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.40it/s]                                               {'loss': 2.3497, 'grad_norm': 5.132036209106445, 'learning_rate': 4.079721943016324e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.40it/s]                                               {'loss': 2.1564, 'grad_norm': 3.957913398742676, 'learning_rate': 3.922809560592619e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 26.40it/s] 69%|██████▉   | 52/75 [00:02<00:00, 26.12it/s]                                               {'loss': 2.4481, 'grad_norm': 3.88079571723938, 'learning_rate': 3.765897178168915e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 26.12it/s]                                               {'loss': 2.1063, 'grad_norm': 3.312981128692627, 'learning_rate': 3.608984795745209e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 26.12it/s]                                               {'loss': 2.2936, 'grad_norm': 4.672125816345215, 'learning_rate': 3.452072413321505e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 26.12it/s] 73%|███████▎  | 55/75 [00:02<00:00, 25.65it/s]                                               {'loss': 2.498, 'grad_norm': 4.296135425567627, 'learning_rate': 3.295160030897801e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.65it/s]                                               {'loss': 2.2555, 'grad_norm': 4.312835693359375, 'learning_rate': 3.138247648474095e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.65it/s]                                               {'loss': 2.159, 'grad_norm': 4.039053916931152, 'learning_rate': 2.981335266050391e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.65it/s] 77%|███████▋  | 58/75 [00:02<00:00, 25.36it/s]                                               {'loss': 2.1306, 'grad_norm': 4.027350902557373, 'learning_rate': 2.8244228836266857e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.36it/s]                                               {'loss': 2.3215, 'grad_norm': 4.6368727684021, 'learning_rate': 2.6675105012029808e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.36it/s]                                               {'loss': 1.805, 'grad_norm': 12.599261283874512, 'learning_rate': 2.5105981187792766e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.36it/s]                                               {'loss': 2.4071, 'grad_norm': 5.100438117980957, 'learning_rate': 2.3536857363555717e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.36it/s] 83%|████████▎ | 62/75 [00:02<00:00, 26.95it/s]                                               {'loss': 2.2142, 'grad_norm': 3.5428192615509033, 'learning_rate': 2.1967733539318668e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 26.95it/s]                                               {'loss': 2.1568, 'grad_norm': 4.790749549865723, 'learning_rate': 2.039860971508162e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 26.95it/s]                                               {'loss': 2.1709, 'grad_norm': 2.6741297245025635, 'learning_rate': 1.8829485890844573e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.95it/s] 87%|████████▋ | 65/75 [00:02<00:00, 26.49it/s]                                               {'loss': 2.1672, 'grad_norm': 3.673790693283081, 'learning_rate': 1.7260362066607524e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.49it/s]                                               {'loss': 2.0882, 'grad_norm': 3.021712064743042, 'learning_rate': 1.5691238242370476e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.49it/s]                                               {'loss': 2.2958, 'grad_norm': 4.234258651733398, 'learning_rate': 1.4122114418133428e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.49it/s] 91%|█████████ | 68/75 [00:02<00:00, 25.86it/s]                                               {'loss': 2.08, 'grad_norm': 4.675353527069092, 'learning_rate': 1.2552990593896383e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 25.86it/s]                                               {'loss': 2.23, 'grad_norm': 3.9083290100097656, 'learning_rate': 1.0983866769659334e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 25.86it/s]                                               {'loss': 2.1151, 'grad_norm': 4.119847774505615, 'learning_rate': 9.414742945422287e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 25.86it/s] 95%|█████████▍| 71/75 [00:02<00:00, 25.30it/s]                                               {'loss': 2.2912, 'grad_norm': 4.085280895233154, 'learning_rate': 7.845619121185238e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.30it/s]                                               {'loss': 2.4495, 'grad_norm': 3.7409164905548096, 'learning_rate': 6.276495296948191e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.30it/s]                                               {'loss': 2.2713, 'grad_norm': 4.1024322509765625, 'learning_rate': 4.707371472711143e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.30it/s] 99%|█████████▊| 74/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.2512, 'grad_norm': 3.8047866821289062, 'learning_rate': 3.1382476484740957e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.02it/s]                                               {'loss': 2.3429, 'grad_norm': 10.706520080566406, 'learning_rate': 1.5691238242370478e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.02it/s]                                               {'train_runtime': 3.0725, 'train_samples_per_second': 367.776, 'train_steps_per_second': 24.41, 'train_loss': 2.2789406315485636, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.02it/s]100%|██████████| 75/75 [00:03<00:00, 24.41it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(911, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(849, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(703, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(841, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(1081, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(788, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(917, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(714, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(782, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(525, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(945, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
the shape of matrix is  torch.Size([768, 768])
the size and non-zero element of matrix are  589824 tensor(473, device='cuda:0')
the shape of U S V are  torch.Size([768, 32]) torch.Size([32]) torch.Size([768, 32])
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:349: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/471 [00:00<?, ?it/s]  1%|▏         | 7/471 [00:00<00:07, 61.21it/s]  3%|▎         | 14/471 [00:00<00:07, 57.21it/s]  4%|▍         | 20/471 [00:00<00:08, 56.14it/s]  6%|▌         | 26/471 [00:00<00:08, 54.89it/s]  7%|▋         | 32/471 [00:00<00:08, 54.43it/s]  8%|▊         | 38/471 [00:00<00:07, 54.45it/s]  9%|▉         | 44/471 [00:00<00:07, 53.42it/s] 11%|█         | 50/471 [00:00<00:07, 53.79it/s] 12%|█▏        | 56/471 [00:01<00:07, 53.73it/s] 13%|█▎        | 62/471 [00:01<00:07, 52.09it/s] 14%|█▍        | 68/471 [00:01<00:07, 52.70it/s] 16%|█▌        | 74/471 [00:01<00:07, 52.76it/s] 17%|█▋        | 80/471 [00:01<00:07, 53.29it/s] 18%|█▊        | 86/471 [00:01<00:07, 52.25it/s] 20%|█▉        | 92/471 [00:01<00:07, 51.94it/s] 21%|██        | 98/471 [00:01<00:07, 51.88it/s] 22%|██▏       | 104/471 [00:01<00:07, 50.31it/s] 23%|██▎       | 110/471 [00:02<00:06, 51.79it/s] 25%|██▍       | 116/471 [00:02<00:06, 52.42it/s] 26%|██▌       | 122/471 [00:02<00:06, 52.57it/s] 27%|██▋       | 128/471 [00:02<00:06, 52.28it/s] 28%|██▊       | 134/471 [00:02<00:06, 52.31it/s] 30%|██▉       | 140/471 [00:02<00:06, 51.65it/s] 31%|███       | 146/471 [00:02<00:06, 52.36it/s] 32%|███▏      | 152/471 [00:02<00:06, 52.94it/s] 34%|███▎      | 158/471 [00:02<00:06, 51.02it/s] 35%|███▍      | 164/471 [00:03<00:05, 51.57it/s] 36%|███▌      | 170/471 [00:03<00:05, 52.03it/s] 37%|███▋      | 176/471 [00:03<00:05, 52.58it/s] 39%|███▊      | 182/471 [00:03<00:05, 52.68it/s] 40%|███▉      | 188/471 [00:03<00:05, 50.98it/s] 41%|████      | 194/471 [00:03<00:05, 51.52it/s] 42%|████▏     | 200/471 [00:03<00:05, 51.54it/s] 44%|████▎     | 206/471 [00:03<00:05, 48.24it/s] 45%|████▌     | 212/471 [00:04<00:05, 49.25it/s] 46%|████▋     | 218/471 [00:04<00:05, 49.13it/s] 47%|████▋     | 223/471 [00:04<00:05, 48.95it/s] 49%|████▊     | 229/471 [00:04<00:04, 50.24it/s] 50%|████▉     | 235/471 [00:04<00:04, 51.19it/s] 51%|█████     | 241/471 [00:04<00:04, 50.94it/s] 52%|█████▏    | 247/471 [00:04<00:04, 51.72it/s] 54%|█████▎    | 253/471 [00:04<00:04, 49.84it/s] 55%|█████▍    | 259/471 [00:05<00:04, 48.48it/s] 56%|█████▌    | 264/471 [00:05<00:04, 47.86it/s] 57%|█████▋    | 270/471 [00:05<00:04, 49.34it/s] 59%|█████▊    | 276/471 [00:05<00:03, 49.73it/s] 60%|█████▉    | 282/471 [00:05<00:03, 50.16it/s] 61%|██████    | 288/471 [00:05<00:03, 47.75it/s] 62%|██████▏   | 294/471 [00:05<00:03, 48.99it/s] 64%|██████▎   | 300/471 [00:05<00:03, 49.93it/s] 65%|██████▍   | 306/471 [00:05<00:03, 50.60it/s] 66%|██████▌   | 312/471 [00:06<00:03, 50.64it/s] 68%|██████▊   | 318/471 [00:06<00:02, 51.65it/s] 69%|██████▉   | 324/471 [00:06<00:02, 52.37it/s] 70%|███████   | 330/471 [00:06<00:02, 52.07it/s] 71%|███████▏  | 336/471 [00:06<00:02, 52.46it/s] 73%|███████▎  | 342/471 [00:06<00:02, 51.79it/s] 74%|███████▍  | 348/471 [00:06<00:02, 46.62it/s] 75%|███████▍  | 353/471 [00:06<00:02, 44.41it/s] 76%|███████▌  | 358/471 [00:07<00:02, 43.88it/s] 77%|███████▋  | 363/471 [00:07<00:02, 42.09it/s] 78%|███████▊  | 368/471 [00:07<00:02, 39.89it/s] 79%|███████▉  | 373/471 [00:07<00:02, 38.84it/s] 80%|████████  | 377/471 [00:07<00:02, 37.87it/s] 81%|████████  | 382/471 [00:07<00:02, 40.10it/s] 82%|████████▏ | 387/471 [00:07<00:02, 35.50it/s] 83%|████████▎ | 392/471 [00:07<00:02, 35.64it/s] 84%|████████▍ | 396/471 [00:08<00:02, 33.00it/s] 85%|████████▌ | 401/471 [00:08<00:01, 35.62it/s] 86%|████████▌ | 405/471 [00:08<00:01, 33.53it/s] 87%|████████▋ | 410/471 [00:08<00:01, 36.68it/s] 88%|████████▊ | 416/471 [00:08<00:01, 41.89it/s] 90%|████████▉ | 422/471 [00:08<00:01, 45.18it/s] 91%|█████████ | 428/471 [00:08<00:00, 46.37it/s] 92%|█████████▏| 434/471 [00:08<00:00, 47.25it/s] 93%|█████████▎| 439/471 [00:09<00:00, 46.36it/s] 94%|█████████▍| 444/471 [00:09<00:00, 42.68it/s] 95%|█████████▌| 449/471 [00:09<00:00, 38.97it/s] 96%|█████████▌| 453/471 [00:09<00:00, 32.78it/s] 97%|█████████▋| 457/471 [00:09<00:00, 31.69it/s] 98%|█████████▊| 463/471 [00:09<00:00, 36.71it/s]100%|█████████▉| 469/471 [00:09<00:00, 40.81it/s]100%|██████████| 471/471 [00:09<00:00, 47.31it/s]
{'eval_loss': 2.2580578327178955, 'eval_model_preparation_time': 0.0029, 'eval_acc': 0.3996282527881041, 'eval_runtime': 9.9763, 'eval_samples_per_second': 754.989, 'eval_steps_per_second': 47.212}
ROUND:25
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.4353, 'grad_norm': 3.778064012527466, 'learning_rate': 0.0001162277660168379, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:04, 15.28it/s]  3%|▎         | 2/75 [00:00<00:06, 11.07it/s]                                              {'loss': 2.4379, 'grad_norm': 4.704282760620117, 'learning_rate': 0.00011467806246994674, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:06, 11.07it/s]                                              {'loss': 2.3922, 'grad_norm': 4.859429359436035, 'learning_rate': 0.00011312835892305556, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:06, 11.07it/s]  5%|▌         | 4/75 [00:00<00:05, 12.36it/s]                                              {'loss': 2.2642, 'grad_norm': 4.417364597320557, 'learning_rate': 0.00011157865537616438, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:05, 12.36it/s]                                              {'loss': 2.4075, 'grad_norm': 3.7729055881500244, 'learning_rate': 0.00011002895182927322, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:05, 12.36it/s]  8%|▊         | 6/75 [00:00<00:05, 12.89it/s]                                              {'loss': 2.0606, 'grad_norm': 3.353161096572876, 'learning_rate': 0.00010847924828238204, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:05, 12.89it/s]                                              {'loss': 2.4056, 'grad_norm': 3.852909564971924, 'learning_rate': 0.00010692954473549088, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:05, 12.89it/s]                                              {'loss': 2.1816, 'grad_norm': 3.573259115219116, 'learning_rate': 0.00010537984118859969, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:05, 12.89it/s] 12%|█▏        | 9/75 [00:00<00:04, 16.42it/s]                                              {'loss': 2.6107, 'grad_norm': 4.687777042388916, 'learning_rate': 0.00010383013764170852, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:04, 16.42it/s]                                              {'loss': 2.2233, 'grad_norm': 4.103154182434082, 'learning_rate': 0.00010228043409481736, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:03, 16.42it/s]                                               {'loss': 2.3344, 'grad_norm': 3.3090834617614746, 'learning_rate': 0.00010073073054792618, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:03, 16.42it/s] 16%|█▌        | 12/75 [00:00<00:03, 18.99it/s]                                               {'loss': 2.0224, 'grad_norm': 3.342170238494873, 'learning_rate': 9.918102700103502e-05, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:03, 18.99it/s]                                               {'loss': 2.3262, 'grad_norm': 4.466772079467773, 'learning_rate': 9.763132345414384e-05, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:03, 18.99it/s]                                               {'loss': 2.2334, 'grad_norm': 4.761512756347656, 'learning_rate': 9.608161990725266e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:03, 18.99it/s] 20%|██        | 15/75 [00:00<00:02, 20.89it/s]                                               {'loss': 2.5266, 'grad_norm': 8.18964958190918, 'learning_rate': 9.45319163603615e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 20.89it/s]                                               {'loss': 2.4583, 'grad_norm': 4.707974433898926, 'learning_rate': 9.298221281347032e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 20.89it/s]                                               {'loss': 2.2421, 'grad_norm': 4.63231086730957, 'learning_rate': 9.143250926657914e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 20.89it/s] 24%|██▍       | 18/75 [00:01<00:02, 20.05it/s]                                               {'loss': 2.223, 'grad_norm': 4.507530212402344, 'learning_rate': 8.988280571968798e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:01<00:02, 20.05it/s]                                               {'loss': 2.5564, 'grad_norm': 4.731062889099121, 'learning_rate': 8.83331021727968e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:01<00:02, 20.05it/s]                                               {'loss': 2.3399, 'grad_norm': 3.1447300910949707, 'learning_rate': 8.678339862590564e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:01<00:02, 20.05it/s] 28%|██▊       | 21/75 [00:01<00:02, 21.64it/s]                                               {'loss': 2.3674, 'grad_norm': 4.822866916656494, 'learning_rate': 8.523369507901446e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:01<00:02, 21.64it/s]                                               {'loss': 2.2583, 'grad_norm': 3.7345283031463623, 'learning_rate': 8.368399153212328e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:01<00:02, 21.64it/s]                                               {'loss': 2.2654, 'grad_norm': 3.2213354110717773, 'learning_rate': 8.213428798523212e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:01<00:02, 21.64it/s] 32%|███▏      | 24/75 [00:01<00:02, 21.41it/s]                                               {'loss': 2.0862, 'grad_norm': 4.326324939727783, 'learning_rate': 8.058458443834094e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:02, 21.41it/s]                                               {'loss': 2.2388, 'grad_norm': 4.179981708526611, 'learning_rate': 7.903488089144978e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:02, 21.41it/s]                                               {'loss': 2.3658, 'grad_norm': 3.96746826171875, 'learning_rate': 7.74851773445586e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:02, 21.41it/s] 36%|███▌      | 27/75 [00:01<00:02, 21.20it/s]                                               {'loss': 2.1803, 'grad_norm': 4.227585792541504, 'learning_rate': 7.593547379766742e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:02, 21.20it/s]                                               {'loss': 2.1353, 'grad_norm': 3.6372389793395996, 'learning_rate': 7.438577025077626e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:02, 21.20it/s]                                               {'loss': 2.2617, 'grad_norm': 4.68883752822876, 'learning_rate': 7.28360667038851e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:02, 21.20it/s] 40%|████      | 30/75 [00:01<00:01, 23.26it/s]                                               {'loss': 2.1508, 'grad_norm': 6.318301677703857, 'learning_rate': 7.12863631569939e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 23.26it/s]                                               {'loss': 2.2475, 'grad_norm': 5.110889434814453, 'learning_rate': 6.973665961010274e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 23.26it/s]                                               {'loss': 2.3986, 'grad_norm': 4.152812957763672, 'learning_rate': 6.818695606321156e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 23.26it/s] 44%|████▍     | 33/75 [00:01<00:01, 22.56it/s]                                               {'loss': 2.3101, 'grad_norm': 3.7043168544769287, 'learning_rate': 6.66372525163204e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 22.56it/s]                                               {'loss': 2.4213, 'grad_norm': 4.756319522857666, 'learning_rate': 6.508754896942924e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 22.56it/s]                                               {'loss': 2.2663, 'grad_norm': 3.743567943572998, 'learning_rate': 6.353784542253805e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 22.56it/s] 48%|████▊     | 36/75 [00:01<00:01, 22.76it/s]                                               {'loss': 2.0001, 'grad_norm': 3.5217998027801514, 'learning_rate': 6.198814187564688e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 22.76it/s]                                               {'loss': 2.2836, 'grad_norm': 3.778225898742676, 'learning_rate': 6.043843832875571e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 22.76it/s]                                               {'loss': 2.1773, 'grad_norm': 3.7406673431396484, 'learning_rate': 5.888873478186454e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 22.76it/s] 52%|█████▏    | 39/75 [00:01<00:01, 23.29it/s]                                               {'loss': 2.2095, 'grad_norm': 3.3679580688476562, 'learning_rate': 5.733903123497337e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 23.29it/s]                                               {'loss': 2.1674, 'grad_norm': 4.0905351638793945, 'learning_rate': 5.578932768808219e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 23.29it/s]                                               {'loss': 2.296, 'grad_norm': 6.219213962554932, 'learning_rate': 5.423962414119102e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:02<00:01, 23.29it/s] 56%|█████▌    | 42/75 [00:02<00:01, 22.98it/s]                                               {'loss': 2.1472, 'grad_norm': 3.1180925369262695, 'learning_rate': 5.2689920594299844e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:02<00:01, 22.98it/s]                                               {'loss': 2.2257, 'grad_norm': 3.9837939739227295, 'learning_rate': 5.114021704740868e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:02<00:01, 22.98it/s]                                               {'loss': 2.431, 'grad_norm': 4.923239707946777, 'learning_rate': 4.959051350051751e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:02<00:01, 22.98it/s]                                               {'loss': 2.1942, 'grad_norm': 9.523125648498535, 'learning_rate': 4.804080995362633e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:02<00:01, 22.98it/s] 61%|██████▏   | 46/75 [00:02<00:01, 25.36it/s]                                               {'loss': 2.2697, 'grad_norm': 3.687525510787964, 'learning_rate': 4.649110640673516e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:02<00:01, 25.36it/s]                                               {'loss': 2.3651, 'grad_norm': 4.47414493560791, 'learning_rate': 4.494140285984399e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:02<00:01, 25.36it/s]                                               {'loss': 2.1753, 'grad_norm': 4.139091491699219, 'learning_rate': 4.339169931295282e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:02<00:01, 25.36it/s] 65%|██████▌   | 49/75 [00:02<00:01, 25.54it/s]                                               {'loss': 2.2289, 'grad_norm': 3.604410409927368, 'learning_rate': 4.184199576606164e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 25.54it/s]                                               {'loss': 1.9591, 'grad_norm': 3.170240879058838, 'learning_rate': 4.029229221917047e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.54it/s]                                               {'loss': 2.1253, 'grad_norm': 4.785527229309082, 'learning_rate': 3.87425886722793e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.54it/s] 69%|██████▉   | 52/75 [00:02<00:00, 24.94it/s]                                               {'loss': 2.32, 'grad_norm': 3.8027849197387695, 'learning_rate': 3.719288512538813e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 24.94it/s]                                               {'loss': 2.4818, 'grad_norm': 3.90995192527771, 'learning_rate': 3.564318157849695e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 24.94it/s]                                               {'loss': 2.3323, 'grad_norm': 4.3083062171936035, 'learning_rate': 3.409347803160578e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 24.94it/s] 73%|███████▎  | 55/75 [00:02<00:00, 24.67it/s]                                               {'loss': 2.293, 'grad_norm': 3.841081380844116, 'learning_rate': 3.254377448471462e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 24.67it/s]                                               {'loss': 2.1662, 'grad_norm': 4.394155025482178, 'learning_rate': 3.099407093782344e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 24.67it/s]                                               {'loss': 2.3398, 'grad_norm': 4.483227729797363, 'learning_rate': 2.944436739093227e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 24.67it/s] 77%|███████▋  | 58/75 [00:02<00:00, 24.52it/s]                                               {'loss': 2.3194, 'grad_norm': 5.304539203643799, 'learning_rate': 2.7894663844041096e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 24.52it/s]                                               {'loss': 2.1049, 'grad_norm': 5.724545955657959, 'learning_rate': 2.6344960297149922e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 24.52it/s]                                               {'loss': 2.2584, 'grad_norm': 19.151123046875, 'learning_rate': 2.4795256750258755e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 24.52it/s] 81%|████████▏ | 61/75 [00:02<00:00, 24.78it/s]                                               {'loss': 2.159, 'grad_norm': 3.374037027359009, 'learning_rate': 2.324555320336758e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 24.78it/s]                                               {'loss': 2.322, 'grad_norm': 4.716724872589111, 'learning_rate': 2.169584965647641e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 24.78it/s]                                               {'loss': 2.1979, 'grad_norm': 3.958699941635132, 'learning_rate': 2.0146146109585236e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 24.78it/s] 85%|████████▌ | 64/75 [00:02<00:00, 24.86it/s]                                               {'loss': 1.9783, 'grad_norm': 3.1559994220733643, 'learning_rate': 1.8596442562694065e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 24.86it/s]                                               {'loss': 2.3873, 'grad_norm': 4.0609235763549805, 'learning_rate': 1.704673901580289e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 24.86it/s]                                               {'loss': 2.2811, 'grad_norm': 3.5463149547576904, 'learning_rate': 1.549703546891172e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 24.86it/s] 89%|████████▉ | 67/75 [00:03<00:00, 24.66it/s]                                               {'loss': 2.1895, 'grad_norm': 3.6828954219818115, 'learning_rate': 1.3947331922020548e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:03<00:00, 24.66it/s]                                               {'loss': 2.2808, 'grad_norm': 4.1607866287231445, 'learning_rate': 1.2397628375129377e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 24.66it/s]                                               {'loss': 2.128, 'grad_norm': 3.9035634994506836, 'learning_rate': 1.0847924828238205e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 24.66it/s] 93%|█████████▎| 70/75 [00:03<00:00, 24.88it/s]                                               {'loss': 2.1659, 'grad_norm': 4.593481540679932, 'learning_rate': 9.298221281347033e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 24.88it/s]                                               {'loss': 2.332, 'grad_norm': 5.300356388092041, 'learning_rate': 7.74851773445586e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 24.88it/s]                                               {'loss': 2.3181, 'grad_norm': 4.180377960205078, 'learning_rate': 6.198814187564689e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 24.88it/s] 97%|█████████▋| 73/75 [00:03<00:00, 24.51it/s]                                               {'loss': 2.3333, 'grad_norm': 4.1280083656311035, 'learning_rate': 4.649110640673516e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 24.51it/s]                                               {'loss': 2.0408, 'grad_norm': 3.7160065174102783, 'learning_rate': 3.0994070937823443e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 24.51it/s]                                               {'loss': 2.2562, 'grad_norm': 18.527429580688477, 'learning_rate': 1.5497035468911722e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.51it/s]                                               {'train_runtime': 3.5053, 'train_samples_per_second': 322.367, 'train_steps_per_second': 21.396, 'train_loss': 2.265020858446757, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 24.51it/s]100%|██████████| 75/75 [00:03<00:00, 21.42it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]  1%|▏         | 1/75 [00:00<00:10,  7.20it/s]                                              {'loss': 2.2578, 'grad_norm': 3.8190181255340576, 'learning_rate': 0.0001162277660168379, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:10,  7.20it/s]                                              {'loss': 2.4974, 'grad_norm': 5.532905578613281, 'learning_rate': 0.00011467806246994674, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:10,  7.20it/s]                                              {'loss': 2.1422, 'grad_norm': 2.6269471645355225, 'learning_rate': 0.00011312835892305556, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:10,  7.20it/s]  5%|▌         | 4/75 [00:00<00:04, 17.31it/s]                                              {'loss': 2.3997, 'grad_norm': 4.40062141418457, 'learning_rate': 0.00011157865537616438, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:04, 17.31it/s]                                              {'loss': 2.3015, 'grad_norm': 3.615809917449951, 'learning_rate': 0.00011002895182927322, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:04, 17.31it/s]                                              {'loss': 2.1441, 'grad_norm': 3.9503417015075684, 'learning_rate': 0.00010847924828238204, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:03, 17.31it/s]  9%|▉         | 7/75 [00:00<00:03, 20.33it/s]                                              {'loss': 2.2914, 'grad_norm': 3.0351860523223877, 'learning_rate': 0.00010692954473549088, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:03, 20.33it/s]                                              {'loss': 2.3439, 'grad_norm': 5.290655136108398, 'learning_rate': 0.00010537984118859969, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:03, 20.33it/s]                                              {'loss': 2.1334, 'grad_norm': 4.7768096923828125, 'learning_rate': 0.00010383013764170852, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:03, 20.33it/s] 13%|█▎        | 10/75 [00:00<00:02, 21.96it/s]                                               {'loss': 2.1812, 'grad_norm': 3.0137267112731934, 'learning_rate': 0.00010228043409481736, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 21.96it/s]                                               {'loss': 2.2564, 'grad_norm': 3.549187183380127, 'learning_rate': 0.00010073073054792618, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 21.96it/s]                                               {'loss': 2.1633, 'grad_norm': 3.4031848907470703, 'learning_rate': 9.918102700103502e-05, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 21.96it/s] 17%|█▋        | 13/75 [00:00<00:02, 23.33it/s]                                               {'loss': 2.1415, 'grad_norm': 3.3721377849578857, 'learning_rate': 9.763132345414384e-05, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 23.33it/s]                                               {'loss': 2.2831, 'grad_norm': 3.6456053256988525, 'learning_rate': 9.608161990725266e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 23.33it/s]                                               {'loss': 2.4494, 'grad_norm': 9.940252304077148, 'learning_rate': 9.45319163603615e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 23.33it/s]                                               {'loss': 2.3159, 'grad_norm': 3.7896177768707275, 'learning_rate': 9.298221281347032e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 23.33it/s] 23%|██▎       | 17/75 [00:00<00:02, 25.36it/s]                                               {'loss': 2.0488, 'grad_norm': 3.164571762084961, 'learning_rate': 9.143250926657914e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.36it/s]                                               {'loss': 2.1591, 'grad_norm': 3.547729015350342, 'learning_rate': 8.988280571968798e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.36it/s]                                               {'loss': 2.2311, 'grad_norm': 3.7193968296051025, 'learning_rate': 8.83331021727968e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.36it/s] 27%|██▋       | 20/75 [00:00<00:02, 25.50it/s]                                               {'loss': 2.1107, 'grad_norm': 3.4636480808258057, 'learning_rate': 8.678339862590564e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.50it/s]                                               {'loss': 2.244, 'grad_norm': 3.416609048843384, 'learning_rate': 8.523369507901446e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.50it/s]                                               {'loss': 2.2859, 'grad_norm': 3.8904831409454346, 'learning_rate': 8.368399153212328e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.50it/s] 31%|███       | 23/75 [00:00<00:02, 25.57it/s]                                               {'loss': 2.0777, 'grad_norm': 3.930825710296631, 'learning_rate': 8.213428798523212e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.57it/s]                                               {'loss': 2.2227, 'grad_norm': 3.637697219848633, 'learning_rate': 8.058458443834094e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:01<00:01, 25.57it/s]                                               {'loss': 2.2712, 'grad_norm': 4.034475803375244, 'learning_rate': 7.903488089144978e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:01<00:01, 25.57it/s] 35%|███▍      | 26/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.2353, 'grad_norm': 3.9419167041778564, 'learning_rate': 7.74851773445586e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.2495, 'grad_norm': 4.060908794403076, 'learning_rate': 7.593547379766742e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.62it/s]                                               {'loss': 2.3304, 'grad_norm': 3.9113986492156982, 'learning_rate': 7.438577025077626e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.62it/s] 39%|███▊      | 29/75 [00:01<00:01, 25.49it/s]                                               {'loss': 2.3098, 'grad_norm': 5.010018825531006, 'learning_rate': 7.28360667038851e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.49it/s]                                               {'loss': 2.5592, 'grad_norm': 9.362327575683594, 'learning_rate': 7.12863631569939e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.49it/s]                                               {'loss': 2.1703, 'grad_norm': 4.20443868637085, 'learning_rate': 6.973665961010274e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.49it/s]                                               {'loss': 2.3422, 'grad_norm': 3.395768404006958, 'learning_rate': 6.818695606321156e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 25.49it/s] 44%|████▍     | 33/75 [00:01<00:01, 26.80it/s]                                               {'loss': 1.9689, 'grad_norm': 3.2890512943267822, 'learning_rate': 6.66372525163204e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 26.80it/s]                                               {'loss': 2.1903, 'grad_norm': 3.844442844390869, 'learning_rate': 6.508754896942924e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 26.80it/s]                                               {'loss': 2.2281, 'grad_norm': 3.571895122528076, 'learning_rate': 6.353784542253805e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.80it/s] 48%|████▊     | 36/75 [00:01<00:01, 25.82it/s]                                               {'loss': 2.3931, 'grad_norm': 4.291928768157959, 'learning_rate': 6.198814187564688e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.82it/s]                                               {'loss': 2.1134, 'grad_norm': 2.7461071014404297, 'learning_rate': 6.043843832875571e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.82it/s]                                               {'loss': 2.1118, 'grad_norm': 3.1312832832336426, 'learning_rate': 5.888873478186454e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.82it/s] 52%|█████▏    | 39/75 [00:01<00:01, 25.02it/s]                                               {'loss': 2.4889, 'grad_norm': 4.637818336486816, 'learning_rate': 5.733903123497337e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.02it/s]                                               {'loss': 2.2126, 'grad_norm': 3.8323957920074463, 'learning_rate': 5.578932768808219e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.02it/s]                                               {'loss': 2.022, 'grad_norm': 3.496148109436035, 'learning_rate': 5.423962414119102e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.02it/s] 56%|█████▌    | 42/75 [00:01<00:01, 24.16it/s]                                               {'loss': 2.2318, 'grad_norm': 4.0205206871032715, 'learning_rate': 5.2689920594299844e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.16it/s]                                               {'loss': 2.0929, 'grad_norm': 3.859768867492676, 'learning_rate': 5.114021704740868e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.16it/s]                                               {'loss': 1.9886, 'grad_norm': 3.000063896179199, 'learning_rate': 4.959051350051751e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 24.16it/s]                                               {'loss': 2.4684, 'grad_norm': 15.504607200622559, 'learning_rate': 4.804080995362633e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 24.16it/s] 61%|██████▏   | 46/75 [00:01<00:01, 26.32it/s]                                               {'loss': 2.0922, 'grad_norm': 4.282392501831055, 'learning_rate': 4.649110640673516e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.32it/s]                                               {'loss': 2.1297, 'grad_norm': 3.8034045696258545, 'learning_rate': 4.494140285984399e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.32it/s]                                               {'loss': 2.2536, 'grad_norm': 4.834860801696777, 'learning_rate': 4.339169931295282e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.32it/s] 65%|██████▌   | 49/75 [00:02<00:01, 25.95it/s]                                               {'loss': 2.2125, 'grad_norm': 5.207570552825928, 'learning_rate': 4.184199576606164e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:02<00:01, 25.95it/s]                                               {'loss': 2.1143, 'grad_norm': 3.121359348297119, 'learning_rate': 4.029229221917047e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:02<00:00, 25.95it/s]                                               {'loss': 1.9762, 'grad_norm': 3.339700698852539, 'learning_rate': 3.87425886722793e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 25.95it/s] 69%|██████▉   | 52/75 [00:02<00:00, 25.85it/s]                                               {'loss': 2.1414, 'grad_norm': 3.4263784885406494, 'learning_rate': 3.719288512538813e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.85it/s]                                               {'loss': 2.2882, 'grad_norm': 3.477288007736206, 'learning_rate': 3.564318157849695e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.85it/s]                                               {'loss': 2.1611, 'grad_norm': 3.360874891281128, 'learning_rate': 3.409347803160578e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.85it/s] 73%|███████▎  | 55/75 [00:02<00:00, 25.79it/s]                                               {'loss': 2.0795, 'grad_norm': 2.9071457386016846, 'learning_rate': 3.254377448471462e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.79it/s]                                               {'loss': 2.132, 'grad_norm': 3.3529868125915527, 'learning_rate': 3.099407093782344e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.79it/s]                                               {'loss': 2.2408, 'grad_norm': 4.012999534606934, 'learning_rate': 2.944436739093227e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.79it/s] 77%|███████▋  | 58/75 [00:02<00:01, 16.59it/s]                                               {'loss': 2.4031, 'grad_norm': 3.7302916049957275, 'learning_rate': 2.7894663844041096e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:01, 16.59it/s]                                               {'loss': 2.1733, 'grad_norm': 3.4151859283447266, 'learning_rate': 2.6344960297149922e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 16.59it/s]                                               {'loss': 1.7233, 'grad_norm': 7.6340250968933105, 'learning_rate': 2.4795256750258755e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 16.59it/s] 81%|████████▏ | 61/75 [00:02<00:00, 18.90it/s]                                               {'loss': 2.3199, 'grad_norm': 3.815966844558716, 'learning_rate': 2.324555320336758e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 18.90it/s]                                               {'loss': 2.2795, 'grad_norm': 3.538276195526123, 'learning_rate': 2.169584965647641e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 18.90it/s]                                               {'loss': 2.0681, 'grad_norm': 3.2072181701660156, 'learning_rate': 2.0146146109585236e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 18.90it/s] 85%|████████▌ | 64/75 [00:02<00:00, 19.76it/s]                                               {'loss': 2.0363, 'grad_norm': 3.1504464149475098, 'learning_rate': 1.8596442562694065e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 19.76it/s]                                               {'loss': 2.1769, 'grad_norm': 4.398992538452148, 'learning_rate': 1.704673901580289e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 19.76it/s]                                               {'loss': 2.2181, 'grad_norm': 3.203657388687134, 'learning_rate': 1.549703546891172e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 19.76it/s] 89%|████████▉ | 67/75 [00:02<00:00, 20.32it/s]                                               {'loss': 2.3835, 'grad_norm': 4.808732986450195, 'learning_rate': 1.3947331922020548e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 20.32it/s]                                               {'loss': 2.0588, 'grad_norm': 3.1644320487976074, 'learning_rate': 1.2397628375129377e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:03<00:00, 20.32it/s]                                               {'loss': 1.9993, 'grad_norm': 3.014122247695923, 'learning_rate': 1.0847924828238205e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:03<00:00, 20.32it/s] 93%|█████████▎| 70/75 [00:03<00:00, 19.60it/s]                                               {'loss': 2.2184, 'grad_norm': 3.127389430999756, 'learning_rate': 9.298221281347033e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:03<00:00, 19.60it/s]                                               {'loss': 2.1607, 'grad_norm': 4.257294654846191, 'learning_rate': 7.74851773445586e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:03<00:00, 19.60it/s]                                               {'loss': 2.0431, 'grad_norm': 3.56433367729187, 'learning_rate': 6.198814187564689e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:03<00:00, 19.60it/s] 97%|█████████▋| 73/75 [00:03<00:00, 20.22it/s]                                               {'loss': 2.0601, 'grad_norm': 3.4207258224487305, 'learning_rate': 4.649110640673516e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:03<00:00, 20.22it/s]                                               {'loss': 2.2748, 'grad_norm': 2.949357032775879, 'learning_rate': 3.0994070937823443e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:03<00:00, 20.22it/s]                                               {'loss': 2.1313, 'grad_norm': 11.53180980682373, 'learning_rate': 1.5497035468911722e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 20.22it/s]                                               {'train_runtime': 3.4299, 'train_samples_per_second': 329.458, 'train_steps_per_second': 21.867, 'train_loss': 2.2028132597605388, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 20.22it/s]100%|██████████| 75/75 [00:03<00:00, 21.89it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:10
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.3102, 'grad_norm': 5.070274353027344, 'learning_rate': 0.0001162277660168379, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 20.57it/s]                                              {'loss': 2.2353, 'grad_norm': 4.308603286743164, 'learning_rate': 0.00011467806246994674, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 22.04it/s]  4%|▍         | 3/75 [00:00<00:03, 23.51it/s]                                              {'loss': 2.1223, 'grad_norm': 3.7575769424438477, 'learning_rate': 0.00011312835892305556, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:03, 23.51it/s]                                              {'loss': 2.1182, 'grad_norm': 4.025935649871826, 'learning_rate': 0.00011157865537616438, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:03, 23.51it/s]                                              {'loss': 2.3447, 'grad_norm': 3.697082757949829, 'learning_rate': 0.00011002895182927322, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 23.51it/s]  8%|▊         | 6/75 [00:00<00:02, 25.04it/s]                                              {'loss': 2.4507, 'grad_norm': 4.323853969573975, 'learning_rate': 0.00010847924828238204, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 25.04it/s]                                              {'loss': 2.4558, 'grad_norm': 4.389401435852051, 'learning_rate': 0.00010692954473549088, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 25.04it/s]                                              {'loss': 2.5534, 'grad_norm': 4.202746868133545, 'learning_rate': 0.00010537984118859969, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 25.04it/s] 12%|█▏        | 9/75 [00:00<00:02, 25.10it/s]                                              {'loss': 2.1881, 'grad_norm': 4.247563362121582, 'learning_rate': 0.00010383013764170852, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 25.10it/s]                                              {'loss': 2.1308, 'grad_norm': 4.116547107696533, 'learning_rate': 0.00010228043409481736, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 25.10it/s]                                               {'loss': 2.3273, 'grad_norm': 4.83939266204834, 'learning_rate': 0.00010073073054792618, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 25.10it/s] 16%|█▌        | 12/75 [00:00<00:02, 25.35it/s]                                               {'loss': 2.3917, 'grad_norm': 5.823189735412598, 'learning_rate': 9.918102700103502e-05, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 25.35it/s]                                               {'loss': 2.2114, 'grad_norm': 4.092507362365723, 'learning_rate': 9.763132345414384e-05, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 25.35it/s]                                               {'loss': 2.146, 'grad_norm': 3.706272840499878, 'learning_rate': 9.608161990725266e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 25.35it/s]                                               {'loss': 1.9682, 'grad_norm': 9.926405906677246, 'learning_rate': 9.45319163603615e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.35it/s] 21%|██▏       | 16/75 [00:00<00:02, 26.82it/s]                                               {'loss': 2.261, 'grad_norm': 3.778357982635498, 'learning_rate': 9.298221281347032e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 26.82it/s]                                               {'loss': 2.2465, 'grad_norm': 4.481486797332764, 'learning_rate': 9.143250926657914e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 26.82it/s]                                               {'loss': 2.2661, 'grad_norm': 3.811838388442993, 'learning_rate': 8.988280571968798e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 26.82it/s] 25%|██▌       | 19/75 [00:00<00:02, 25.74it/s]                                               {'loss': 2.1774, 'grad_norm': 3.325683832168579, 'learning_rate': 8.83331021727968e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.74it/s]                                               {'loss': 2.3578, 'grad_norm': 3.794729709625244, 'learning_rate': 8.678339862590564e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.74it/s]                                               {'loss': 2.1571, 'grad_norm': 4.221835613250732, 'learning_rate': 8.523369507901446e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.74it/s] 29%|██▉       | 22/75 [00:00<00:02, 25.67it/s]                                               {'loss': 2.2637, 'grad_norm': 3.327857732772827, 'learning_rate': 8.368399153212328e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.67it/s]                                               {'loss': 2.1917, 'grad_norm': 4.5884246826171875, 'learning_rate': 8.213428798523212e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.67it/s]                                               {'loss': 2.0314, 'grad_norm': 2.909165382385254, 'learning_rate': 8.058458443834094e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:01, 25.67it/s] 33%|███▎      | 25/75 [00:00<00:01, 25.81it/s]                                               {'loss': 2.2435, 'grad_norm': 3.790616512298584, 'learning_rate': 7.903488089144978e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.81it/s]                                               {'loss': 2.1954, 'grad_norm': 3.75264048576355, 'learning_rate': 7.74851773445586e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.81it/s]                                               {'loss': 2.4861, 'grad_norm': 4.929963111877441, 'learning_rate': 7.593547379766742e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 25.81it/s] 37%|███▋      | 28/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.225, 'grad_norm': 3.429546356201172, 'learning_rate': 7.438577025077626e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.2367, 'grad_norm': 4.104579448699951, 'learning_rate': 7.28360667038851e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.3323, 'grad_norm': 11.061941146850586, 'learning_rate': 7.12863631569939e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 25.60it/s]                                               {'loss': 2.2042, 'grad_norm': 3.7694640159606934, 'learning_rate': 6.973665961010274e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 25.60it/s] 43%|████▎     | 32/75 [00:01<00:01, 27.01it/s]                                               {'loss': 2.1814, 'grad_norm': 3.431612014770508, 'learning_rate': 6.818695606321156e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 27.01it/s]                                               {'loss': 2.3651, 'grad_norm': 4.538145542144775, 'learning_rate': 6.66372525163204e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 27.01it/s]                                               {'loss': 2.4418, 'grad_norm': 4.473653316497803, 'learning_rate': 6.508754896942924e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 27.01it/s] 47%|████▋     | 35/75 [00:01<00:01, 26.07it/s]                                               {'loss': 2.2362, 'grad_norm': 4.181347846984863, 'learning_rate': 6.353784542253805e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 26.07it/s]                                               {'loss': 2.4151, 'grad_norm': 4.122305393218994, 'learning_rate': 6.198814187564688e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 26.07it/s]                                               {'loss': 2.368, 'grad_norm': 6.528984546661377, 'learning_rate': 6.043843832875571e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 26.07it/s] 51%|█████     | 38/75 [00:01<00:01, 25.51it/s]                                               {'loss': 2.0003, 'grad_norm': 4.069209575653076, 'learning_rate': 5.888873478186454e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.51it/s]                                               {'loss': 2.0668, 'grad_norm': 3.3588335514068604, 'learning_rate': 5.733903123497337e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.51it/s]                                               {'loss': 2.2949, 'grad_norm': 3.9417405128479004, 'learning_rate': 5.578932768808219e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.51it/s] 55%|█████▍    | 41/75 [00:01<00:01, 24.92it/s]                                               {'loss': 2.1818, 'grad_norm': 5.382138252258301, 'learning_rate': 5.423962414119102e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 24.92it/s]                                               {'loss': 2.2434, 'grad_norm': 3.6408634185791016, 'learning_rate': 5.2689920594299844e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 24.92it/s]                                               {'loss': 2.1663, 'grad_norm': 3.6412813663482666, 'learning_rate': 5.114021704740868e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 24.92it/s] 59%|█████▊    | 44/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.0274, 'grad_norm': 3.64951229095459, 'learning_rate': 4.959051350051751e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.001, 'grad_norm': 10.083907127380371, 'learning_rate': 4.804080995362633e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.2762, 'grad_norm': 5.131377220153809, 'learning_rate': 4.649110640673516e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 25.00it/s]                                               {'loss': 2.0994, 'grad_norm': 4.403530597686768, 'learning_rate': 4.494140285984399e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 25.00it/s] 64%|██████▍   | 48/75 [00:01<00:01, 26.52it/s]                                               {'loss': 2.1234, 'grad_norm': 4.535976886749268, 'learning_rate': 4.339169931295282e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.52it/s]                                               {'loss': 2.29, 'grad_norm': 4.567953109741211, 'learning_rate': 4.184199576606164e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.52it/s]                                               {'loss': 2.4075, 'grad_norm': 3.977766752243042, 'learning_rate': 4.029229221917047e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.52it/s] 68%|██████▊   | 51/75 [00:01<00:00, 26.30it/s]                                               {'loss': 2.4152, 'grad_norm': 4.267088890075684, 'learning_rate': 3.87425886722793e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:01<00:00, 26.30it/s]                                               {'loss': 2.0849, 'grad_norm': 3.165727138519287, 'learning_rate': 3.719288512538813e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 26.30it/s]                                               {'loss': 2.2216, 'grad_norm': 4.8006486892700195, 'learning_rate': 3.564318157849695e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 26.30it/s] 72%|███████▏  | 54/75 [00:02<00:00, 26.31it/s]                                               {'loss': 1.9321, 'grad_norm': 3.7306289672851562, 'learning_rate': 3.409347803160578e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 26.31it/s]                                               {'loss': 2.1544, 'grad_norm': 3.6480801105499268, 'learning_rate': 3.254377448471462e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 26.31it/s]                                               {'loss': 2.2129, 'grad_norm': 4.035627365112305, 'learning_rate': 3.099407093782344e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 26.31it/s] 76%|███████▌  | 57/75 [00:02<00:00, 26.29it/s]                                               {'loss': 2.0939, 'grad_norm': 4.1095099449157715, 'learning_rate': 2.944436739093227e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 26.29it/s]                                               {'loss': 2.2485, 'grad_norm': 3.1783125400543213, 'learning_rate': 2.7894663844041096e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 26.29it/s]                                               {'loss': 2.259, 'grad_norm': 3.1776909828186035, 'learning_rate': 2.6344960297149922e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 26.29it/s]                                               {'loss': 2.4612, 'grad_norm': 19.526020050048828, 'learning_rate': 2.4795256750258755e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 26.29it/s] 81%|████████▏ | 61/75 [00:02<00:00, 27.36it/s]                                               {'loss': 2.2215, 'grad_norm': 3.8053324222564697, 'learning_rate': 2.324555320336758e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 27.36it/s]                                               {'loss': 2.1135, 'grad_norm': 3.8861398696899414, 'learning_rate': 2.169584965647641e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 27.36it/s]                                               {'loss': 2.1704, 'grad_norm': 3.818624973297119, 'learning_rate': 2.0146146109585236e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 27.36it/s] 85%|████████▌ | 64/75 [00:02<00:00, 26.90it/s]                                               {'loss': 2.092, 'grad_norm': 4.259589672088623, 'learning_rate': 1.8596442562694065e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 26.90it/s]                                               {'loss': 2.0331, 'grad_norm': 3.486888885498047, 'learning_rate': 1.704673901580289e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.90it/s]                                               {'loss': 2.2836, 'grad_norm': 3.2844178676605225, 'learning_rate': 1.549703546891172e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.90it/s] 89%|████████▉ | 67/75 [00:02<00:00, 26.27it/s]                                               {'loss': 2.1841, 'grad_norm': 3.639418601989746, 'learning_rate': 1.3947331922020548e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.27it/s]                                               {'loss': 2.0943, 'grad_norm': 5.722110748291016, 'learning_rate': 1.2397628375129377e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 26.27it/s]                                               {'loss': 2.2897, 'grad_norm': 3.62776517868042, 'learning_rate': 1.0847924828238205e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 26.27it/s] 93%|█████████▎| 70/75 [00:02<00:00, 26.19it/s]                                               {'loss': 2.3318, 'grad_norm': 3.4245097637176514, 'learning_rate': 9.298221281347033e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 26.19it/s]                                               {'loss': 2.3563, 'grad_norm': 4.392288684844971, 'learning_rate': 7.74851773445586e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 26.19it/s]                                               {'loss': 2.2387, 'grad_norm': 4.2489728927612305, 'learning_rate': 6.198814187564689e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 26.19it/s] 97%|█████████▋| 73/75 [00:02<00:00, 26.28it/s]                                               {'loss': 1.9192, 'grad_norm': 3.96549391746521, 'learning_rate': 4.649110640673516e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 26.28it/s]                                               {'loss': 2.2978, 'grad_norm': 4.713054180145264, 'learning_rate': 3.0994070937823443e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 26.28it/s]                                               {'loss': 2.5571, 'grad_norm': 9.172491073608398, 'learning_rate': 1.5497035468911722e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.28it/s]                                               {'train_runtime': 2.9815, 'train_samples_per_second': 379.001, 'train_steps_per_second': 25.155, 'train_loss': 2.2304342651367186, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 26.28it/s]100%|██████████| 75/75 [00:02<00:00, 25.16it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  447
the shape of loraA and loraB are  torch.Size([32, 768]) torch.Size([768, 32])
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:315: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/75 [00:00<?, ?it/s]                                      {'loss': 2.2372, 'grad_norm': 3.231822967529297, 'learning_rate': 0.0001162277660168379, 'epoch': 0.07}
  1%|▏         | 1/75 [00:00<00:03, 23.08it/s]                                              {'loss': 2.4658, 'grad_norm': 3.867725372314453, 'learning_rate': 0.00011467806246994674, 'epoch': 0.13}
  3%|▎         | 2/75 [00:00<00:03, 24.06it/s]  4%|▍         | 3/75 [00:00<00:02, 24.28it/s]                                              {'loss': 2.1718, 'grad_norm': 3.8000173568725586, 'learning_rate': 0.00011312835892305556, 'epoch': 0.2}
  4%|▍         | 3/75 [00:00<00:02, 24.28it/s]                                              {'loss': 2.0536, 'grad_norm': 4.5236287117004395, 'learning_rate': 0.00011157865537616438, 'epoch': 0.27}
  5%|▌         | 4/75 [00:00<00:02, 24.28it/s]                                              {'loss': 2.1843, 'grad_norm': 3.385741949081421, 'learning_rate': 0.00011002895182927322, 'epoch': 0.33}
  7%|▋         | 5/75 [00:00<00:02, 24.28it/s]  8%|▊         | 6/75 [00:00<00:02, 24.31it/s]                                              {'loss': 2.4554, 'grad_norm': 4.102043628692627, 'learning_rate': 0.00010847924828238204, 'epoch': 0.4}
  8%|▊         | 6/75 [00:00<00:02, 24.31it/s]                                              {'loss': 2.323, 'grad_norm': 3.4791259765625, 'learning_rate': 0.00010692954473549088, 'epoch': 0.47}
  9%|▉         | 7/75 [00:00<00:02, 24.31it/s]                                              {'loss': 2.1206, 'grad_norm': 3.666585922241211, 'learning_rate': 0.00010537984118859969, 'epoch': 0.53}
 11%|█         | 8/75 [00:00<00:02, 24.31it/s] 12%|█▏        | 9/75 [00:00<00:02, 24.64it/s]                                              {'loss': 2.2026, 'grad_norm': 3.327296733856201, 'learning_rate': 0.00010383013764170852, 'epoch': 0.6}
 12%|█▏        | 9/75 [00:00<00:02, 24.64it/s]                                              {'loss': 2.1637, 'grad_norm': 4.812324523925781, 'learning_rate': 0.00010228043409481736, 'epoch': 0.67}
 13%|█▎        | 10/75 [00:00<00:02, 24.64it/s]                                               {'loss': 2.3056, 'grad_norm': 3.9202170372009277, 'learning_rate': 0.00010073073054792618, 'epoch': 0.73}
 15%|█▍        | 11/75 [00:00<00:02, 24.64it/s] 16%|█▌        | 12/75 [00:00<00:02, 24.97it/s]                                               {'loss': 2.2707, 'grad_norm': 3.65208101272583, 'learning_rate': 9.918102700103502e-05, 'epoch': 0.8}
 16%|█▌        | 12/75 [00:00<00:02, 24.97it/s]                                               {'loss': 2.3521, 'grad_norm': 3.7320334911346436, 'learning_rate': 9.763132345414384e-05, 'epoch': 0.87}
 17%|█▋        | 13/75 [00:00<00:02, 24.97it/s]                                               {'loss': 2.2938, 'grad_norm': 4.464829921722412, 'learning_rate': 9.608161990725266e-05, 'epoch': 0.93}
 19%|█▊        | 14/75 [00:00<00:02, 24.97it/s] 20%|██        | 15/75 [00:00<00:02, 25.99it/s]                                               {'loss': 2.537, 'grad_norm': 10.88240909576416, 'learning_rate': 9.45319163603615e-05, 'epoch': 1.0}
 20%|██        | 15/75 [00:00<00:02, 25.99it/s]                                               {'loss': 2.2563, 'grad_norm': 3.9366695880889893, 'learning_rate': 9.298221281347032e-05, 'epoch': 1.07}
 21%|██▏       | 16/75 [00:00<00:02, 25.99it/s]                                               {'loss': 2.3682, 'grad_norm': 4.450294017791748, 'learning_rate': 9.143250926657914e-05, 'epoch': 1.13}
 23%|██▎       | 17/75 [00:00<00:02, 25.99it/s] 24%|██▍       | 18/75 [00:00<00:02, 25.69it/s]                                               {'loss': 2.3079, 'grad_norm': 4.834252834320068, 'learning_rate': 8.988280571968798e-05, 'epoch': 1.2}
 24%|██▍       | 18/75 [00:00<00:02, 25.69it/s]                                               {'loss': 2.2091, 'grad_norm': 3.1356897354125977, 'learning_rate': 8.83331021727968e-05, 'epoch': 1.27}
 25%|██▌       | 19/75 [00:00<00:02, 25.69it/s]                                               {'loss': 2.355, 'grad_norm': 3.689305067062378, 'learning_rate': 8.678339862590564e-05, 'epoch': 1.33}
 27%|██▋       | 20/75 [00:00<00:02, 25.69it/s] 28%|██▊       | 21/75 [00:00<00:02, 25.49it/s]                                               {'loss': 2.3706, 'grad_norm': 3.691666603088379, 'learning_rate': 8.523369507901446e-05, 'epoch': 1.4}
 28%|██▊       | 21/75 [00:00<00:02, 25.49it/s]                                               {'loss': 2.2327, 'grad_norm': 4.261940002441406, 'learning_rate': 8.368399153212328e-05, 'epoch': 1.47}
 29%|██▉       | 22/75 [00:00<00:02, 25.49it/s]                                               {'loss': 2.2864, 'grad_norm': 3.9508070945739746, 'learning_rate': 8.213428798523212e-05, 'epoch': 1.53}
 31%|███       | 23/75 [00:00<00:02, 25.49it/s] 32%|███▏      | 24/75 [00:00<00:02, 25.16it/s]                                               {'loss': 2.1279, 'grad_norm': 4.297810077667236, 'learning_rate': 8.058458443834094e-05, 'epoch': 1.6}
 32%|███▏      | 24/75 [00:00<00:02, 25.16it/s]                                               {'loss': 2.2498, 'grad_norm': 3.614997625350952, 'learning_rate': 7.903488089144978e-05, 'epoch': 1.67}
 33%|███▎      | 25/75 [00:00<00:01, 25.16it/s]                                               {'loss': 2.1159, 'grad_norm': 4.041125297546387, 'learning_rate': 7.74851773445586e-05, 'epoch': 1.73}
 35%|███▍      | 26/75 [00:01<00:01, 25.16it/s] 36%|███▌      | 27/75 [00:01<00:01, 24.97it/s]                                               {'loss': 2.1215, 'grad_norm': 4.127326011657715, 'learning_rate': 7.593547379766742e-05, 'epoch': 1.8}
 36%|███▌      | 27/75 [00:01<00:01, 24.97it/s]                                               {'loss': 2.1325, 'grad_norm': 3.1498210430145264, 'learning_rate': 7.438577025077626e-05, 'epoch': 1.87}
 37%|███▋      | 28/75 [00:01<00:01, 24.97it/s]                                               {'loss': 2.0305, 'grad_norm': 3.5917062759399414, 'learning_rate': 7.28360667038851e-05, 'epoch': 1.93}
 39%|███▊      | 29/75 [00:01<00:01, 24.97it/s] 40%|████      | 30/75 [00:01<00:01, 24.84it/s]                                               {'loss': 2.8477, 'grad_norm': 13.787885665893555, 'learning_rate': 7.12863631569939e-05, 'epoch': 2.0}
 40%|████      | 30/75 [00:01<00:01, 24.84it/s]                                               {'loss': 2.036, 'grad_norm': 3.226599931716919, 'learning_rate': 6.973665961010274e-05, 'epoch': 2.07}
 41%|████▏     | 31/75 [00:01<00:01, 24.84it/s]                                               {'loss': 2.3385, 'grad_norm': 3.5854148864746094, 'learning_rate': 6.818695606321156e-05, 'epoch': 2.13}
 43%|████▎     | 32/75 [00:01<00:01, 24.84it/s] 44%|████▍     | 33/75 [00:01<00:01, 25.14it/s]                                               {'loss': 2.3624, 'grad_norm': 3.7842063903808594, 'learning_rate': 6.66372525163204e-05, 'epoch': 2.2}
 44%|████▍     | 33/75 [00:01<00:01, 25.14it/s]                                               {'loss': 2.2431, 'grad_norm': 3.4499692916870117, 'learning_rate': 6.508754896942924e-05, 'epoch': 2.27}
 45%|████▌     | 34/75 [00:01<00:01, 25.14it/s]                                               {'loss': 1.9873, 'grad_norm': 2.848193883895874, 'learning_rate': 6.353784542253805e-05, 'epoch': 2.33}
 47%|████▋     | 35/75 [00:01<00:01, 25.14it/s] 48%|████▊     | 36/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.1586, 'grad_norm': 4.2997941970825195, 'learning_rate': 6.198814187564688e-05, 'epoch': 2.4}
 48%|████▊     | 36/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.2599, 'grad_norm': 3.2164089679718018, 'learning_rate': 6.043843832875571e-05, 'epoch': 2.47}
 49%|████▉     | 37/75 [00:01<00:01, 25.30it/s]                                               {'loss': 2.3249, 'grad_norm': 3.8813257217407227, 'learning_rate': 5.888873478186454e-05, 'epoch': 2.53}
 51%|█████     | 38/75 [00:01<00:01, 25.30it/s] 52%|█████▏    | 39/75 [00:01<00:01, 25.18it/s]                                               {'loss': 2.1694, 'grad_norm': 3.6142687797546387, 'learning_rate': 5.733903123497337e-05, 'epoch': 2.6}
 52%|█████▏    | 39/75 [00:01<00:01, 25.18it/s]                                               {'loss': 2.1574, 'grad_norm': 3.564486503601074, 'learning_rate': 5.578932768808219e-05, 'epoch': 2.67}
 53%|█████▎    | 40/75 [00:01<00:01, 25.18it/s]                                               {'loss': 2.1473, 'grad_norm': 3.114389419555664, 'learning_rate': 5.423962414119102e-05, 'epoch': 2.73}
 55%|█████▍    | 41/75 [00:01<00:01, 25.18it/s] 56%|█████▌    | 42/75 [00:01<00:01, 25.06it/s]                                               {'loss': 2.2357, 'grad_norm': 3.7520413398742676, 'learning_rate': 5.2689920594299844e-05, 'epoch': 2.8}
 56%|█████▌    | 42/75 [00:01<00:01, 25.06it/s]                                               {'loss': 1.9514, 'grad_norm': 4.200520992279053, 'learning_rate': 5.114021704740868e-05, 'epoch': 2.87}
 57%|█████▋    | 43/75 [00:01<00:01, 25.06it/s]                                               {'loss': 2.1826, 'grad_norm': 3.160595417022705, 'learning_rate': 4.959051350051751e-05, 'epoch': 2.93}
 59%|█████▊    | 44/75 [00:01<00:01, 25.06it/s]                                               {'loss': 2.7681, 'grad_norm': 14.41838264465332, 'learning_rate': 4.804080995362633e-05, 'epoch': 3.0}
 60%|██████    | 45/75 [00:01<00:01, 25.06it/s] 61%|██████▏   | 46/75 [00:01<00:01, 26.74it/s]                                               {'loss': 2.2645, 'grad_norm': 3.1727256774902344, 'learning_rate': 4.649110640673516e-05, 'epoch': 3.07}
 61%|██████▏   | 46/75 [00:01<00:01, 26.74it/s]                                               {'loss': 2.409, 'grad_norm': 4.139040946960449, 'learning_rate': 4.494140285984399e-05, 'epoch': 3.13}
 63%|██████▎   | 47/75 [00:01<00:01, 26.74it/s]                                               {'loss': 2.2481, 'grad_norm': 5.272216796875, 'learning_rate': 4.339169931295282e-05, 'epoch': 3.2}
 64%|██████▍   | 48/75 [00:01<00:01, 26.74it/s] 65%|██████▌   | 49/75 [00:01<00:00, 26.25it/s]                                               {'loss': 2.1496, 'grad_norm': 3.473262071609497, 'learning_rate': 4.184199576606164e-05, 'epoch': 3.27}
 65%|██████▌   | 49/75 [00:01<00:00, 26.25it/s]                                               {'loss': 2.0292, 'grad_norm': 2.983168363571167, 'learning_rate': 4.029229221917047e-05, 'epoch': 3.33}
 67%|██████▋   | 50/75 [00:01<00:00, 26.25it/s]                                               {'loss': 2.1441, 'grad_norm': 3.2753329277038574, 'learning_rate': 3.87425886722793e-05, 'epoch': 3.4}
 68%|██████▊   | 51/75 [00:02<00:00, 26.25it/s] 69%|██████▉   | 52/75 [00:02<00:00, 25.95it/s]                                               {'loss': 2.2018, 'grad_norm': 3.9264750480651855, 'learning_rate': 3.719288512538813e-05, 'epoch': 3.47}
 69%|██████▉   | 52/75 [00:02<00:00, 25.95it/s]                                               {'loss': 1.9362, 'grad_norm': 3.9637551307678223, 'learning_rate': 3.564318157849695e-05, 'epoch': 3.53}
 71%|███████   | 53/75 [00:02<00:00, 25.95it/s]                                               {'loss': 2.4922, 'grad_norm': 4.617176532745361, 'learning_rate': 3.409347803160578e-05, 'epoch': 3.6}
 72%|███████▏  | 54/75 [00:02<00:00, 25.95it/s] 73%|███████▎  | 55/75 [00:02<00:00, 25.89it/s]                                               {'loss': 2.0336, 'grad_norm': 3.5632336139678955, 'learning_rate': 3.254377448471462e-05, 'epoch': 3.67}
 73%|███████▎  | 55/75 [00:02<00:00, 25.89it/s]                                               {'loss': 2.1532, 'grad_norm': 3.552873134613037, 'learning_rate': 3.099407093782344e-05, 'epoch': 3.73}
 75%|███████▍  | 56/75 [00:02<00:00, 25.89it/s]                                               {'loss': 2.127, 'grad_norm': 4.009310245513916, 'learning_rate': 2.944436739093227e-05, 'epoch': 3.8}
 76%|███████▌  | 57/75 [00:02<00:00, 25.89it/s] 77%|███████▋  | 58/75 [00:02<00:00, 25.54it/s]                                               {'loss': 2.298, 'grad_norm': 2.694305419921875, 'learning_rate': 2.7894663844041096e-05, 'epoch': 3.87}
 77%|███████▋  | 58/75 [00:02<00:00, 25.54it/s]                                               {'loss': 2.088, 'grad_norm': 3.504513740539551, 'learning_rate': 2.6344960297149922e-05, 'epoch': 3.93}
 79%|███████▊  | 59/75 [00:02<00:00, 25.54it/s]                                               {'loss': 2.0769, 'grad_norm': 9.276765823364258, 'learning_rate': 2.4795256750258755e-05, 'epoch': 4.0}
 80%|████████  | 60/75 [00:02<00:00, 25.54it/s]                                               {'loss': 1.9972, 'grad_norm': 3.3339293003082275, 'learning_rate': 2.324555320336758e-05, 'epoch': 4.07}
 81%|████████▏ | 61/75 [00:02<00:00, 25.54it/s] 83%|████████▎ | 62/75 [00:02<00:00, 27.23it/s]                                               {'loss': 2.1168, 'grad_norm': 4.137547016143799, 'learning_rate': 2.169584965647641e-05, 'epoch': 4.13}
 83%|████████▎ | 62/75 [00:02<00:00, 27.23it/s]                                               {'loss': 2.2702, 'grad_norm': 3.79656982421875, 'learning_rate': 2.0146146109585236e-05, 'epoch': 4.2}
 84%|████████▍ | 63/75 [00:02<00:00, 27.23it/s]                                               {'loss': 2.0533, 'grad_norm': 2.968104839324951, 'learning_rate': 1.8596442562694065e-05, 'epoch': 4.27}
 85%|████████▌ | 64/75 [00:02<00:00, 27.23it/s] 87%|████████▋ | 65/75 [00:02<00:00, 26.67it/s]                                               {'loss': 2.2075, 'grad_norm': 3.958771228790283, 'learning_rate': 1.704673901580289e-05, 'epoch': 4.33}
 87%|████████▋ | 65/75 [00:02<00:00, 26.67it/s]                                               {'loss': 2.1353, 'grad_norm': 3.4603164196014404, 'learning_rate': 1.549703546891172e-05, 'epoch': 4.4}
 88%|████████▊ | 66/75 [00:02<00:00, 26.67it/s]                                               {'loss': 2.2319, 'grad_norm': 4.220460414886475, 'learning_rate': 1.3947331922020548e-05, 'epoch': 4.47}
 89%|████████▉ | 67/75 [00:02<00:00, 26.67it/s] 91%|█████████ | 68/75 [00:02<00:00, 26.13it/s]                                               {'loss': 2.2564, 'grad_norm': 3.8899807929992676, 'learning_rate': 1.2397628375129377e-05, 'epoch': 4.53}
 91%|█████████ | 68/75 [00:02<00:00, 26.13it/s]                                               {'loss': 2.3931, 'grad_norm': 4.133997917175293, 'learning_rate': 1.0847924828238205e-05, 'epoch': 4.6}
 92%|█████████▏| 69/75 [00:02<00:00, 26.13it/s]                                               {'loss': 2.0196, 'grad_norm': 2.9139280319213867, 'learning_rate': 9.298221281347033e-06, 'epoch': 4.67}
 93%|█████████▎| 70/75 [00:02<00:00, 26.13it/s] 95%|█████████▍| 71/75 [00:02<00:00, 25.54it/s]                                               {'loss': 2.0175, 'grad_norm': 3.208840847015381, 'learning_rate': 7.74851773445586e-06, 'epoch': 4.73}
 95%|█████████▍| 71/75 [00:02<00:00, 25.54it/s]                                               {'loss': 2.2425, 'grad_norm': 3.595585823059082, 'learning_rate': 6.198814187564689e-06, 'epoch': 4.8}
 96%|█████████▌| 72/75 [00:02<00:00, 25.54it/s]                                               {'loss': 2.3744, 'grad_norm': 3.8243277072906494, 'learning_rate': 4.649110640673516e-06, 'epoch': 4.87}
 97%|█████████▋| 73/75 [00:02<00:00, 25.54it/s] 99%|█████████▊| 74/75 [00:02<00:00, 25.83it/s]                                               {'loss': 2.2232, 'grad_norm': 3.588121175765991, 'learning_rate': 3.0994070937823443e-06, 'epoch': 4.93}
 99%|█████████▊| 74/75 [00:02<00:00, 25.83it/s]                                               {'loss': 1.8078, 'grad_norm': 10.198233604431152, 'learning_rate': 1.5497035468911722e-06, 'epoch': 5.0}
100%|██████████| 75/75 [00:02<00:00, 25.83it/s]                                               {'train_runtime': 3.0093, 'train_samples_per_second': 375.499, 'train_steps_per_second': 24.922, 'train_loss': 2.219598239262899, 'epoch': 5.0}
100%|██████████| 75/75 [00:03<00:00, 25.83it/s]100%|██████████| 75/75 [00:03<00:00, 24.92it/s]

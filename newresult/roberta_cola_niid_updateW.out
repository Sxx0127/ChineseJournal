nohup: ignoring input
/home/suxiaoxin/.conda/envs/sxx/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at ./data/glue/glue.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./model/models--roberta-base/ and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 589,824 || all params: 125,236,994 || trainable%: 0.4710
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): RobertaForSequenceClassification(
      (roberta): RobertaModel(
        (embeddings): RobertaEmbeddings(
          (word_embeddings): Embedding(50265, 768, padding_idx=1)
          (position_embeddings): Embedding(514, 768, padding_idx=1)
          (token_type_embeddings): Embedding(1, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): RobertaEncoder(
          (layer): ModuleList(
            (0-11): 12 x RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSdpaSelfAttention(
                  (query): lora.Linear(
                    (base_layer): Linear(in_features=768, out_features=768, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.05, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=768, out_features=16, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=16, out_features=768, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): lora.Linear(
                    (base_layer): Linear(in_features=768, out_features=768, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.05, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=768, out_features=16, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=16, out_features=768, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (classifier): RobertaClassificationHead(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (out_proj): Linear(in_features=768, out_features=2, bias=True)
      )
    )
  )
)
ROUND:0
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:01<00:41,  1.44s/it]                                              {'loss': 0.6388, 'grad_norm': 1.4005168676376343, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:01<00:41,  1.44s/it]  7%|▋         | 2/30 [00:01<00:21,  1.32it/s]                                              {'loss': 1.0447, 'grad_norm': 6.367481231689453, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:21,  1.32it/s] 10%|█         | 3/30 [00:01<00:14,  1.86it/s]                                              {'loss': 1.4145, 'grad_norm': 5.909910202026367, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:02<00:14,  1.86it/s] 13%|█▎        | 4/30 [00:02<00:11,  2.35it/s]                                              {'loss': 1.6263, 'grad_norm': 5.317267894744873, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:02<00:11,  2.35it/s] 17%|█▋        | 5/30 [00:02<00:08,  2.78it/s]                                              {'loss': 0.9472, 'grad_norm': 3.1480088233947754, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:02<00:08,  2.78it/s] 20%|██        | 6/30 [00:02<00:06,  3.66it/s]                                              {'loss': 1.0693, 'grad_norm': 5.5416388511657715, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:06,  3.66it/s] 23%|██▎       | 7/30 [00:02<00:06,  3.60it/s]                                              {'loss': 0.7133, 'grad_norm': 1.850606083869934, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:06,  3.60it/s] 27%|██▋       | 8/30 [00:03<00:06,  3.52it/s]                                              {'loss': 0.8409, 'grad_norm': 5.16675329208374, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:03<00:06,  3.52it/s] 30%|███       | 9/30 [00:03<00:05,  3.53it/s]                                              {'loss': 0.7365, 'grad_norm': 2.135141372680664, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:03<00:05,  3.53it/s] 33%|███▎      | 10/30 [00:03<00:05,  3.56it/s]                                               {'loss': 0.7032, 'grad_norm': 0.6126570701599121, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:05,  3.56it/s] 37%|███▋      | 11/30 [00:04<00:05,  3.62it/s]                                               {'loss': 0.716, 'grad_norm': 0.8783379793167114, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:04<00:05,  3.62it/s]                                               {'loss': 0.5712, 'grad_norm': 2.3020858764648438, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:04<00:04,  3.62it/s] 43%|████▎     | 13/30 [00:04<00:03,  4.33it/s]                                               {'loss': 0.8319, 'grad_norm': 1.6218489408493042, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:04<00:03,  4.33it/s] 47%|████▋     | 14/30 [00:04<00:03,  4.13it/s]                                               {'loss': 0.656, 'grad_norm': 1.144134759902954, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:03,  4.13it/s] 50%|█████     | 15/30 [00:04<00:03,  4.00it/s]                                               {'loss': 0.7155, 'grad_norm': 5.533949851989746, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:03,  4.00it/s] 53%|█████▎    | 16/30 [00:05<00:03,  3.90it/s]                                               {'loss': 0.7041, 'grad_norm': 1.0108829736709595, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:05<00:03,  3.90it/s] 57%|█████▋    | 17/30 [00:05<00:03,  3.83it/s]                                               {'loss': 0.6973, 'grad_norm': 1.9846556186676025, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:05<00:03,  3.83it/s]                                               {'loss': 0.7075, 'grad_norm': 1.420720100402832, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:03,  3.83it/s] 63%|██████▎   | 19/30 [00:05<00:02,  4.48it/s]                                               {'loss': 0.6819, 'grad_norm': 1.0436923503875732, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  4.48it/s] 67%|██████▋   | 20/30 [00:06<00:02,  4.28it/s]                                               {'loss': 0.6773, 'grad_norm': 0.6120210886001587, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:06<00:02,  4.28it/s] 70%|███████   | 21/30 [00:06<00:02,  4.15it/s]                                               {'loss': 0.7204, 'grad_norm': 0.9211769104003906, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:06<00:02,  4.15it/s] 73%|███████▎  | 22/30 [00:06<00:02,  3.98it/s]                                               {'loss': 0.7136, 'grad_norm': 1.3112200498580933, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:06<00:02,  3.98it/s] 77%|███████▋  | 23/30 [00:06<00:01,  3.88it/s]                                               {'loss': 0.69, 'grad_norm': 1.8725273609161377, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  3.88it/s]                                               {'loss': 0.6584, 'grad_norm': 1.2874832153320312, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  3.88it/s] 83%|████████▎ | 25/30 [00:07<00:01,  4.46it/s]                                               {'loss': 0.6622, 'grad_norm': 1.0944267511367798, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:07<00:01,  4.46it/s] 87%|████████▋ | 26/30 [00:07<00:00,  4.27it/s]                                               {'loss': 0.6514, 'grad_norm': 1.177304983139038, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:07<00:00,  4.27it/s] 90%|█████████ | 27/30 [00:07<00:00,  4.08it/s]                                               {'loss': 0.6678, 'grad_norm': 0.9974465370178223, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:07<00:00,  4.08it/s] 93%|█████████▎| 28/30 [00:07<00:00,  4.30it/s]                                               {'loss': 0.6799, 'grad_norm': 0.6460588574409485, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:08<00:00,  4.30it/s] 97%|█████████▋| 29/30 [00:08<00:00,  4.73it/s]                                               {'loss': 0.6138, 'grad_norm': 1.343395709991455, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:08<00:00,  4.73it/s]                                               {'loss': 0.6637, 'grad_norm': 1.2779037952423096, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.73it/s]                                               {'train_runtime': 8.2972, 'train_samples_per_second': 51.222, 'train_steps_per_second': 3.616, 'train_loss': 0.7804885586102803, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.73it/s]100%|██████████| 30/30 [00:08<00:00,  3.62it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:64
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.42it/s]                                              {'loss': 0.9727, 'grad_norm': 1.871934175491333, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.42it/s]  7%|▋         | 2/30 [00:00<00:06,  4.30it/s]                                              {'loss': 0.5338, 'grad_norm': 1.2208521366119385, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.30it/s] 10%|█         | 3/30 [00:00<00:05,  5.01it/s]                                              {'loss': 0.3484, 'grad_norm': 1.727776050567627, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  5.01it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.97it/s]                                              {'loss': 0.1922, 'grad_norm': 1.5756406784057617, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.97it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.49it/s]                                              {'loss': 0.1555, 'grad_norm': 0.5358352661132812, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.49it/s]                                              {'loss': 0.0228, 'grad_norm': 0.4462833106517792, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.49it/s] 23%|██▎       | 7/30 [00:01<00:03,  7.54it/s]                                              {'loss': 0.0153, 'grad_norm': 0.3370838761329651, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  7.54it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.36it/s]                                              {'loss': 0.0035, 'grad_norm': 0.09431260079145432, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.36it/s] 30%|███       | 9/30 [00:01<00:02,  7.10it/s]                                              {'loss': 0.0093, 'grad_norm': 0.4377182722091675, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.10it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.44it/s]                                               {'loss': 0.011, 'grad_norm': 0.4709473252296448, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.44it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.48it/s]                                               {'loss': 0.4549, 'grad_norm': 1.5269911289215088, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.48it/s]                                               {'loss': 0.0007, 'grad_norm': 0.027298256754875183, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.48it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.23it/s]                                               {'loss': 0.0006, 'grad_norm': 0.011512666940689087, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.23it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.96it/s]                                               {'loss': 0.4722, 'grad_norm': 2.5536413192749023, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.96it/s] 50%|█████     | 15/30 [00:02<00:01,  7.60it/s]                                               {'loss': 0.0009, 'grad_norm': 0.01757085509598255, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.60it/s]                                               {'loss': 0.0016, 'grad_norm': 0.04468359798192978, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.60it/s] 57%|█████▋    | 17/30 [00:02<00:01,  9.04it/s]                                               {'loss': 0.0023, 'grad_norm': 0.054326381534338, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  9.04it/s]                                               {'loss': 0.0019, 'grad_norm': 0.06037186458706856, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  9.04it/s] 63%|██████▎   | 19/30 [00:02<00:01, 11.00it/s]                                               {'loss': 0.4224, 'grad_norm': 1.783085823059082, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01, 11.00it/s]                                               {'loss': 0.0024, 'grad_norm': 0.07571293413639069, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 11.00it/s] 70%|███████   | 21/30 [00:02<00:00, 11.46it/s]                                               {'loss': 0.0043, 'grad_norm': 0.13900451362133026, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.46it/s]                                               {'loss': 0.007, 'grad_norm': 0.21014520525932312, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.46it/s] 77%|███████▋  | 23/30 [00:02<00:00, 11.79it/s]                                               {'loss': 0.0065, 'grad_norm': 0.193483367562294, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 11.79it/s]                                               {'loss': 0.0023, 'grad_norm': 0.06673794984817505, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 11.79it/s] 83%|████████▎ | 25/30 [00:02<00:00, 12.70it/s]                                               {'loss': 0.4436, 'grad_norm': 5.388617038726807, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.70it/s]                                               {'loss': 0.0054, 'grad_norm': 0.143761545419693, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 12.70it/s] 90%|█████████ | 27/30 [00:03<00:00, 12.40it/s]                                               {'loss': 0.0055, 'grad_norm': 0.12900234758853912, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 12.40it/s]                                               {'loss': 0.0044, 'grad_norm': 0.13673634827136993, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 12.40it/s] 97%|█████████▋| 29/30 [00:03<00:00, 11.81it/s]                                               {'loss': 0.0036, 'grad_norm': 0.09153585880994797, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 11.81it/s]                                               {'loss': 0.0033, 'grad_norm': 0.09267550706863403, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.81it/s]                                               {'train_runtime': 3.4135, 'train_samples_per_second': 124.505, 'train_steps_per_second': 8.789, 'train_loss': 0.1370069779048208, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.81it/s]100%|██████████| 30/30 [00:03<00:00,  8.81it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.67it/s]                                              {'loss': 0.59, 'grad_norm': 0.25399214029312134, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.67it/s]  7%|▋         | 2/30 [00:00<00:07,  3.60it/s]                                              {'loss': 0.6675, 'grad_norm': 0.6621816754341125, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.60it/s] 10%|█         | 3/30 [00:00<00:07,  3.58it/s]                                              {'loss': 0.7623, 'grad_norm': 0.5680248141288757, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.58it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.61it/s]                                              {'loss': 0.651, 'grad_norm': 1.1885638236999512, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.61it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.90it/s]                                              {'loss': 0.6632, 'grad_norm': 0.4213665723800659, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.90it/s]                                              {'loss': 0.7468, 'grad_norm': 1.7116076946258545, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.90it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.98it/s]                                              {'loss': 0.6151, 'grad_norm': 0.5424310564994812, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.98it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.94it/s]                                              {'loss': 0.7354, 'grad_norm': 0.9516140818595886, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.94it/s] 30%|███       | 9/30 [00:02<00:04,  4.86it/s]                                              {'loss': 0.6729, 'grad_norm': 0.4524596035480499, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.86it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.73it/s]                                               {'loss': 0.5883, 'grad_norm': 0.9332521557807922, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.73it/s] 37%|███▋      | 11/30 [00:02<00:03,  4.88it/s]                                               {'loss': 0.6355, 'grad_norm': 0.3359020948410034, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  4.88it/s] 40%|████      | 12/30 [00:02<00:03,  5.73it/s]                                               {'loss': 0.4847, 'grad_norm': 0.9527598023414612, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.73it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.27it/s]                                               {'loss': 0.5497, 'grad_norm': 1.1038286685943604, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.27it/s] 47%|████▋     | 14/30 [00:02<00:03,  5.08it/s]                                               {'loss': 0.5589, 'grad_norm': 0.56981360912323, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  5.08it/s] 50%|█████     | 15/30 [00:03<00:03,  4.79it/s]                                               {'loss': 0.8235, 'grad_norm': 2.836682081222534, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.79it/s] 53%|█████▎    | 16/30 [00:03<00:02,  4.80it/s]                                               {'loss': 0.6076, 'grad_norm': 0.8298165202140808, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  4.80it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.35it/s]                                               {'loss': 0.4804, 'grad_norm': 1.5088376998901367, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.35it/s]                                               {'loss': 0.7358, 'grad_norm': 1.1862049102783203, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.35it/s] 63%|██████▎   | 19/30 [00:03<00:01,  7.27it/s]                                               {'loss': 0.5274, 'grad_norm': 0.9665462374687195, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  7.27it/s]                                               {'loss': 0.6681, 'grad_norm': 0.8185516595840454, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.27it/s] 70%|███████   | 21/30 [00:03<00:01,  8.46it/s]                                               {'loss': 0.5942, 'grad_norm': 0.5948647260665894, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  8.46it/s]                                               {'loss': 0.545, 'grad_norm': 0.8182544708251953, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:00,  8.46it/s] 77%|███████▋  | 23/30 [00:04<00:00,  9.01it/s]                                               {'loss': 0.8632, 'grad_norm': 2.707021474838257, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:00,  9.01it/s]                                               {'loss': 0.4697, 'grad_norm': 1.7301082611083984, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  9.01it/s] 83%|████████▎ | 25/30 [00:04<00:00,  9.23it/s]                                               {'loss': 0.5562, 'grad_norm': 0.5269181132316589, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  9.23it/s] 87%|████████▋ | 26/30 [00:04<00:00,  9.18it/s]                                               {'loss': 0.6396, 'grad_norm': 0.901508629322052, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  9.18it/s] 90%|█████████ | 27/30 [00:04<00:00,  9.06it/s]                                               {'loss': 0.5227, 'grad_norm': 0.933239221572876, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  9.06it/s] 93%|█████████▎| 28/30 [00:04<00:00,  8.79it/s]                                               {'loss': 0.66, 'grad_norm': 1.0919649600982666, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  8.79it/s] 97%|█████████▋| 29/30 [00:04<00:00,  8.58it/s]                                               {'loss': 0.5645, 'grad_norm': 1.2086607217788696, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  8.58it/s]                                               {'loss': 0.4147, 'grad_norm': 1.5233778953552246, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  8.58it/s]                                               {'train_runtime': 5.1535, 'train_samples_per_second': 82.468, 'train_steps_per_second': 5.821, 'train_loss': 0.6198077897230784, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  8.58it/s]100%|██████████| 30/30 [00:05<00:00,  5.83it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:80
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.6277, 'grad_norm': 0.7007676362991333, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 11.80it/s]  7%|▋         | 2/30 [00:00<00:02, 10.33it/s]                                              {'loss': 0.5536, 'grad_norm': 1.015824556350708, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 10.33it/s]                                              {'loss': 0.3641, 'grad_norm': 1.416274070739746, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.33it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.81it/s]                                              {'loss': 0.2786, 'grad_norm': 0.8449075222015381, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  9.81it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.44it/s]                                              {'loss': 0.2612, 'grad_norm': 1.018212080001831, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  9.44it/s]                                              {'loss': 0.8928, 'grad_norm': 3.7048964500427246, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02,  9.44it/s] 23%|██▎       | 7/30 [00:00<00:01, 11.55it/s]                                              {'loss': 0.2274, 'grad_norm': 2.389568328857422, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 11.55it/s]                                              {'loss': 0.8015, 'grad_norm': 4.00150203704834, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 11.55it/s] 30%|███       | 9/30 [00:00<00:01, 11.84it/s]                                              {'loss': 0.2178, 'grad_norm': 4.267513275146484, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 11.84it/s]                                              {'loss': 0.4352, 'grad_norm': 6.086329936981201, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.84it/s] 37%|███▋      | 11/30 [00:00<00:01, 11.84it/s]                                               {'loss': 0.0828, 'grad_norm': 3.9781007766723633, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 11.84it/s]                                               {'loss': 0.6602, 'grad_norm': 14.941079139709473, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 11.84it/s] 43%|████▎     | 13/30 [00:01<00:01, 12.01it/s]                                               {'loss': 0.3862, 'grad_norm': 1.2051881551742554, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 12.01it/s]                                               {'loss': 0.2161, 'grad_norm': 4.052003860473633, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 12.01it/s] 50%|█████     | 15/30 [00:01<00:01,  9.09it/s]                                               {'loss': 0.9072, 'grad_norm': 7.204794883728027, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  9.09it/s]                                               {'loss': 0.455, 'grad_norm': 6.618232727050781, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  9.09it/s] 57%|█████▋    | 17/30 [00:01<00:01,  8.13it/s]                                               {'loss': 0.1594, 'grad_norm': 2.712836742401123, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01,  8.13it/s] 60%|██████    | 18/30 [00:01<00:01,  8.08it/s]                                               {'loss': 0.6931, 'grad_norm': 2.087141513824463, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01,  8.08it/s] 63%|██████▎   | 19/30 [00:02<00:01,  7.25it/s]                                               {'loss': 0.1078, 'grad_norm': 2.0118799209594727, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.25it/s] 67%|██████▋   | 20/30 [00:02<00:01,  7.28it/s]                                               {'loss': 0.525, 'grad_norm': 1.7062792778015137, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  7.28it/s] 70%|███████   | 21/30 [00:02<00:01,  7.07it/s]                                               {'loss': 0.2763, 'grad_norm': 1.7016469240188599, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  7.07it/s] 73%|███████▎  | 22/30 [00:02<00:01,  6.88it/s]                                               {'loss': 0.5817, 'grad_norm': 1.7789480686187744, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  6.88it/s] 77%|███████▋  | 23/30 [00:02<00:01,  6.77it/s]                                               {'loss': 0.2396, 'grad_norm': 1.0779695510864258, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:01,  6.77it/s] 80%|████████  | 24/30 [00:02<00:00,  6.69it/s]                                               {'loss': 0.4799, 'grad_norm': 1.272999882698059, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  6.69it/s] 83%|████████▎ | 25/30 [00:03<00:00,  5.78it/s]                                               {'loss': 0.2148, 'grad_norm': 1.6460269689559937, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  5.78it/s] 87%|████████▋ | 26/30 [00:03<00:00,  5.80it/s]                                               {'loss': 0.3584, 'grad_norm': 0.7227937579154968, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  5.80it/s] 90%|█████████ | 27/30 [00:03<00:00,  5.54it/s]                                               {'loss': 0.3704, 'grad_norm': 1.1229984760284424, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  5.54it/s] 93%|█████████▎| 28/30 [00:03<00:00,  5.38it/s]                                               {'loss': 0.411, 'grad_norm': 2.4662933349609375, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  5.38it/s] 97%|█████████▋| 29/30 [00:03<00:00,  5.41it/s]                                               {'loss': 0.2289, 'grad_norm': 0.7166114449501038, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  5.41it/s]                                               {'loss': 0.5063, 'grad_norm': 4.354124546051025, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  5.41it/s]                                               {'train_runtime': 4.1894, 'train_samples_per_second': 101.446, 'train_steps_per_second': 7.161, 'train_loss': 0.41733457098404564, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.41it/s]100%|██████████| 30/30 [00:04<00:00,  7.13it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.64it/s]                                              {'loss': 0.6888, 'grad_norm': 0.8841153979301453, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.64it/s]  7%|▋         | 2/30 [00:00<00:03,  7.16it/s]                                              {'loss': 0.7561, 'grad_norm': 0.8362323045730591, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.16it/s] 10%|█         | 3/30 [00:00<00:03,  7.23it/s]                                              {'loss': 0.7057, 'grad_norm': 0.4058668613433838, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.23it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.38it/s]                                              {'loss': 0.6521, 'grad_norm': 0.9173121452331543, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.38it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.28it/s]                                              {'loss': 0.7248, 'grad_norm': 0.6597058773040771, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.28it/s]                                              {'loss': 0.7029, 'grad_norm': 0.5055615901947021, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.28it/s] 23%|██▎       | 7/30 [00:00<00:03,  7.61it/s]                                              {'loss': 0.6853, 'grad_norm': 0.3144643306732178, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:03,  7.61it/s] 27%|██▋       | 8/30 [00:01<00:03,  7.33it/s]                                              {'loss': 0.7102, 'grad_norm': 0.41733574867248535, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  7.33it/s] 30%|███       | 9/30 [00:01<00:02,  7.19it/s]                                              {'loss': 0.7267, 'grad_norm': 0.3539634644985199, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.19it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.75it/s]                                               {'loss': 0.6781, 'grad_norm': 0.195914626121521, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.75it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.75it/s]                                               {'loss': 0.6694, 'grad_norm': 0.5187716484069824, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.75it/s]                                               {'loss': 0.5155, 'grad_norm': 0.9829460978507996, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.75it/s] 43%|████▎     | 13/30 [00:01<00:01,  8.53it/s]                                               {'loss': 0.6774, 'grad_norm': 0.5203406810760498, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  8.53it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.26it/s]                                               {'loss': 0.6286, 'grad_norm': 0.3958926796913147, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.26it/s] 50%|█████     | 15/30 [00:01<00:01,  7.88it/s]                                               {'loss': 0.7465, 'grad_norm': 0.9490768313407898, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  7.88it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.49it/s]                                               {'loss': 0.668, 'grad_norm': 0.4988964796066284, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.49it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.98it/s]                                               {'loss': 0.6746, 'grad_norm': 0.46701744198799133, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.98it/s]                                               {'loss': 0.6553, 'grad_norm': 0.49170705676078796, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.98it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.19it/s]                                               {'loss': 0.7057, 'grad_norm': 0.6182738542556763, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  9.19it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.74it/s]                                               {'loss': 0.6934, 'grad_norm': 0.5554694533348083, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.74it/s] 70%|███████   | 21/30 [00:02<00:01,  8.35it/s]                                               {'loss': 0.676, 'grad_norm': 0.39144715666770935, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.35it/s] 73%|███████▎  | 22/30 [00:02<00:00,  8.01it/s]                                               {'loss': 0.6942, 'grad_norm': 0.5146713256835938, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  8.01it/s] 77%|███████▋  | 23/30 [00:02<00:00,  7.85it/s]                                               {'loss': 0.6587, 'grad_norm': 0.4391045868396759, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  7.85it/s]                                               {'loss': 0.6495, 'grad_norm': 0.46995019912719727, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  7.85it/s] 83%|████████▎ | 25/30 [00:03<00:00, 10.46it/s]                                               {'loss': 0.7057, 'grad_norm': 0.38282984495162964, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 10.46it/s]                                               {'loss': 0.6687, 'grad_norm': 0.36368563771247864, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 10.46it/s] 90%|█████████ | 27/30 [00:03<00:00,  8.49it/s]                                               {'loss': 0.6472, 'grad_norm': 0.478702574968338, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  8.49it/s] 93%|█████████▎| 28/30 [00:03<00:00,  7.37it/s]                                               {'loss': 0.6454, 'grad_norm': 0.20705491304397583, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  7.37it/s] 97%|█████████▋| 29/30 [00:03<00:00,  6.62it/s]                                               {'loss': 0.7093, 'grad_norm': 0.44281482696533203, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  6.62it/s]                                               {'loss': 0.6511, 'grad_norm': 0.4042463004589081, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  6.62it/s]                                               {'train_runtime': 3.925, 'train_samples_per_second': 108.279, 'train_steps_per_second': 7.643, 'train_loss': 0.679032051563263, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  6.62it/s]100%|██████████| 30/30 [00:03<00:00,  7.65it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.5429, 'grad_norm': 0.3323134481906891, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 10.59it/s]  7%|▋         | 2/30 [00:00<00:02, 12.28it/s]                                              {'loss': 0.7396, 'grad_norm': 0.6940767168998718, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 12.28it/s]                                              {'loss': 0.7259, 'grad_norm': 0.5771657824516296, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 12.28it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.52it/s]                                              {'loss': 0.6381, 'grad_norm': 0.5914069414138794, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.52it/s]                                              {'loss': 0.6791, 'grad_norm': 0.46004533767700195, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:01, 12.52it/s] 20%|██        | 6/30 [00:00<00:01, 12.71it/s]                                              {'loss': 0.7871, 'grad_norm': 1.5754685401916504, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 12.71it/s]                                              {'loss': 0.6424, 'grad_norm': 0.5827187895774841, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 12.71it/s] 27%|██▋       | 8/30 [00:00<00:01, 11.99it/s]                                              {'loss': 0.6931, 'grad_norm': 0.4361380338668823, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 11.99it/s]                                              {'loss': 0.6313, 'grad_norm': 0.38450488448143005, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 11.99it/s] 33%|███▎      | 10/30 [00:00<00:01, 11.81it/s]                                               {'loss': 0.6692, 'grad_norm': 0.22819282114505768, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.81it/s]                                               {'loss': 0.6681, 'grad_norm': 0.20492105185985565, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 11.81it/s] 40%|████      | 12/30 [00:01<00:01, 11.10it/s]                                               {'loss': 0.6355, 'grad_norm': 1.2692853212356567, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 11.10it/s]                                               {'loss': 0.6644, 'grad_norm': 0.8052757382392883, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 11.10it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.46it/s]                                               {'loss': 0.6718, 'grad_norm': 0.42716261744499207, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.46it/s] 50%|█████     | 15/30 [00:01<00:01,  8.23it/s]                                               {'loss': 0.6654, 'grad_norm': 0.8588471412658691, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  8.23it/s] 53%|█████▎    | 16/30 [00:01<00:01,  7.85it/s]                                               {'loss': 0.6002, 'grad_norm': 0.5135666131973267, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  7.85it/s] 57%|█████▋    | 17/30 [00:01<00:01,  7.59it/s]                                               {'loss': 0.6503, 'grad_norm': 0.38684868812561035, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01,  7.59it/s]                                               {'loss': 0.637, 'grad_norm': 0.5888269543647766, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01,  7.59it/s] 63%|██████▎   | 19/30 [00:02<00:01,  7.68it/s]                                               {'loss': 0.6334, 'grad_norm': 0.8219261765480042, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.68it/s] 67%|██████▋   | 20/30 [00:02<00:01,  7.48it/s]                                               {'loss': 0.6345, 'grad_norm': 0.3489810824394226, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  7.48it/s] 70%|███████   | 21/30 [00:02<00:01,  6.84it/s]                                               {'loss': 0.6299, 'grad_norm': 0.4063757061958313, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  6.84it/s] 73%|███████▎  | 22/30 [00:02<00:01,  6.47it/s]                                               {'loss': 0.6179, 'grad_norm': 0.5095772743225098, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  6.47it/s] 77%|███████▋  | 23/30 [00:02<00:01,  6.20it/s]                                               {'loss': 0.581, 'grad_norm': 0.480997771024704, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:01,  6.20it/s]                                               {'loss': 0.5314, 'grad_norm': 0.7003899812698364, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  6.20it/s] 83%|████████▎ | 25/30 [00:03<00:00,  6.52it/s]                                               {'loss': 0.7148, 'grad_norm': 0.7760977149009705, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  6.52it/s] 87%|████████▋ | 26/30 [00:03<00:00,  6.23it/s]                                               {'loss': 0.5833, 'grad_norm': 0.6849678158760071, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  6.23it/s] 90%|█████████ | 27/30 [00:03<00:00,  6.77it/s]                                               {'loss': 0.6109, 'grad_norm': 0.5320348739624023, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  6.77it/s] 93%|█████████▎| 28/30 [00:03<00:00,  6.05it/s]                                               {'loss': 0.5921, 'grad_norm': 0.45830807089805603, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  6.05it/s] 97%|█████████▋| 29/30 [00:03<00:00,  5.73it/s]                                               {'loss': 0.5245, 'grad_norm': 0.8357928991317749, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  5.73it/s]100%|██████████| 30/30 [00:03<00:00,  6.43it/s]                                               {'loss': 0.5786, 'grad_norm': 0.7819607257843018, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  6.43it/s]                                               {'train_runtime': 4.0907, 'train_samples_per_second': 103.895, 'train_steps_per_second': 7.334, 'train_loss': 0.6391143759091695, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.43it/s]100%|██████████| 30/30 [00:04<00:00,  7.34it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:43
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.26it/s]                                              {'loss': 0.5379, 'grad_norm': 0.4597685933113098, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.26it/s]  7%|▋         | 2/30 [00:00<00:04,  6.26it/s]                                              {'loss': 0.7718, 'grad_norm': 1.219869613647461, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.26it/s] 10%|█         | 3/30 [00:00<00:04,  6.23it/s]                                              {'loss': 0.7299, 'grad_norm': 0.5379438400268555, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.23it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.83it/s]                                              {'loss': 0.7469, 'grad_norm': 1.0939141511917114, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.83it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.07it/s]                                              {'loss': 0.6736, 'grad_norm': 0.3037467300891876, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.07it/s]                                              {'loss': 0.7669, 'grad_norm': 1.3577905893325806, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.07it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.11it/s]                                              {'loss': 0.6884, 'grad_norm': 0.4107373356819153, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.11it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.72it/s]                                              {'loss': 0.6863, 'grad_norm': 0.236302450299263, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.72it/s] 30%|███       | 9/30 [00:01<00:02,  7.40it/s]                                              {'loss': 0.662, 'grad_norm': 0.27035653591156006, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.40it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.63it/s]                                               {'loss': 0.6636, 'grad_norm': 0.28594592213630676, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.63it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.60it/s]                                               {'loss': 0.6748, 'grad_norm': 0.18145623803138733, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.60it/s]                                               {'loss': 0.6376, 'grad_norm': 1.3635226488113403, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.60it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.36it/s]                                               {'loss': 0.6503, 'grad_norm': 0.3772719204425812, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.36it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.11it/s]                                               {'loss': 0.6161, 'grad_norm': 0.40015411376953125, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.11it/s] 50%|█████     | 15/30 [00:02<00:02,  7.01it/s]                                               {'loss': 0.692, 'grad_norm': 0.7727009057998657, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.01it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.47it/s]                                               {'loss': 0.6458, 'grad_norm': 0.33605024218559265, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.47it/s] 57%|█████▋    | 17/30 [00:02<00:02,  5.85it/s]                                               {'loss': 0.6614, 'grad_norm': 0.29993003606796265, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  5.85it/s]                                               {'loss': 0.7145, 'grad_norm': 0.6329452991485596, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:02,  5.85it/s] 63%|██████▎   | 19/30 [00:02<00:01,  6.21it/s]                                               {'loss': 0.6859, 'grad_norm': 0.8080011606216431, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  6.21it/s] 67%|██████▋   | 20/30 [00:02<00:01,  5.93it/s]                                               {'loss': 0.6565, 'grad_norm': 0.1768110692501068, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.93it/s] 70%|███████   | 21/30 [00:03<00:01,  5.82it/s]                                               {'loss': 0.685, 'grad_norm': 0.26819634437561035, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.82it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.23it/s]                                               {'loss': 0.6575, 'grad_norm': 0.4043060839176178, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.23it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.49it/s]                                               {'loss': 0.6535, 'grad_norm': 0.3793719708919525, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.49it/s]                                               {'loss': 0.6583, 'grad_norm': 0.3663519024848938, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.49it/s] 83%|████████▎ | 25/30 [00:03<00:00,  7.27it/s]                                               {'loss': 0.7424, 'grad_norm': 0.7347246408462524, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  7.27it/s] 87%|████████▋ | 26/30 [00:03<00:00,  6.78it/s]                                               {'loss': 0.6313, 'grad_norm': 0.3131698668003082, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  6.78it/s] 90%|█████████ | 27/30 [00:04<00:00,  6.34it/s]                                               {'loss': 0.6505, 'grad_norm': 0.43941211700439453, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  6.34it/s] 93%|█████████▎| 28/30 [00:04<00:00,  5.94it/s]                                               {'loss': 0.6594, 'grad_norm': 0.33283117413520813, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  5.94it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.54it/s]                                               {'loss': 0.619, 'grad_norm': 0.5138232707977295, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.54it/s]100%|██████████| 30/30 [00:04<00:00,  6.26it/s]                                               {'loss': 0.5985, 'grad_norm': 0.40348848700523376, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.26it/s]                                               {'train_runtime': 4.6527, 'train_samples_per_second': 91.345, 'train_steps_per_second': 6.448, 'train_loss': 0.6705928206443786, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.26it/s]100%|██████████| 30/30 [00:04<00:00,  6.45it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.75it/s]                                              {'loss': 0.682, 'grad_norm': 0.6487892270088196, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.75it/s]                                              {'loss': 0.7365, 'grad_norm': 0.8148269057273865, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.75it/s] 10%|█         | 3/30 [00:00<00:02, 10.94it/s]                                              {'loss': 0.7168, 'grad_norm': 0.4599253237247467, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.94it/s]                                              {'loss': 0.6823, 'grad_norm': 0.5245240926742554, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.94it/s] 17%|█▋        | 5/30 [00:00<00:02, 12.02it/s]                                              {'loss': 0.701, 'grad_norm': 0.39455899596214294, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.02it/s]                                              {'loss': 0.7474, 'grad_norm': 1.4640787839889526, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 12.02it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.49it/s]                                              {'loss': 0.6218, 'grad_norm': 0.36953848600387573, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  9.49it/s]                                              {'loss': 0.6929, 'grad_norm': 0.3168694078922272, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  9.49it/s] 30%|███       | 9/30 [00:00<00:02,  9.98it/s]                                              {'loss': 0.7516, 'grad_norm': 0.4121895134449005, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:02,  9.98it/s]                                              {'loss': 0.6586, 'grad_norm': 0.17572355270385742, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:02,  9.98it/s] 37%|███▋      | 11/30 [00:01<00:01, 10.50it/s]                                               {'loss': 0.6775, 'grad_norm': 0.21055461466312408, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 10.50it/s]                                               {'loss': 0.4848, 'grad_norm': 1.1582809686660767, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 10.50it/s] 43%|████▎     | 13/30 [00:01<00:01, 11.91it/s]                                               {'loss': 0.6576, 'grad_norm': 0.9787609577178955, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 11.91it/s]                                               {'loss': 0.61, 'grad_norm': 1.306656837463379, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.91it/s] 50%|█████     | 15/30 [00:01<00:01, 11.82it/s]                                               {'loss': 0.7153, 'grad_norm': 1.8279019594192505, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 11.82it/s]                                               {'loss': 0.6314, 'grad_norm': 0.4232509136199951, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.82it/s] 57%|█████▋    | 17/30 [00:01<00:01, 11.73it/s]                                               {'loss': 0.6398, 'grad_norm': 0.488411545753479, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.73it/s]                                               {'loss': 0.6263, 'grad_norm': 0.5973438024520874, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 11.73it/s] 63%|██████▎   | 19/30 [00:01<00:00, 12.43it/s]                                               {'loss': 0.5714, 'grad_norm': 1.715735673904419, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 12.43it/s]                                               {'loss': 0.7247, 'grad_norm': 1.135233998298645, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 12.43it/s] 70%|███████   | 21/30 [00:01<00:00, 12.36it/s]                                               {'loss': 0.7104, 'grad_norm': 0.603685736656189, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 12.36it/s]                                               {'loss': 0.6184, 'grad_norm': 0.7819553017616272, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 12.36it/s] 77%|███████▋  | 23/30 [00:01<00:00, 12.48it/s]                                               {'loss': 0.7521, 'grad_norm': 1.5122252702713013, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 12.48it/s]                                               {'loss': 0.5679, 'grad_norm': 2.509436845779419, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 12.48it/s] 83%|████████▎ | 25/30 [00:02<00:00, 13.89it/s]                                               {'loss': 0.6922, 'grad_norm': 0.5544222593307495, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 13.89it/s]                                               {'loss': 0.595, 'grad_norm': 0.6705347895622253, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.89it/s] 90%|█████████ | 27/30 [00:02<00:00, 13.57it/s]                                               {'loss': 0.6718, 'grad_norm': 0.5806758403778076, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.57it/s]                                               {'loss': 0.7137, 'grad_norm': 0.9033098816871643, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.57it/s] 97%|█████████▋| 29/30 [00:02<00:00, 13.29it/s]                                               {'loss': 0.6112, 'grad_norm': 0.6382206678390503, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.29it/s]                                               {'loss': 0.607, 'grad_norm': 0.6388480067253113, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.29it/s]                                               {'train_runtime': 2.5456, 'train_samples_per_second': 166.953, 'train_steps_per_second': 11.785, 'train_loss': 0.6623150169849396, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.29it/s]100%|██████████| 30/30 [00:02<00:00, 11.79it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:7
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  7.64it/s]                                              {'loss': 0.6482, 'grad_norm': 0.48440036177635193, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  7.64it/s]                                              {'loss': 0.67, 'grad_norm': 0.734569251537323, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.64it/s] 10%|█         | 3/30 [00:00<00:02,  9.85it/s]                                              {'loss': 0.6706, 'grad_norm': 0.8394435048103333, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02,  9.85it/s]                                              {'loss': 0.6089, 'grad_norm': 0.8721417188644409, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  9.85it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.19it/s]                                              {'loss': 0.705, 'grad_norm': 1.4170769453048706, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.19it/s]                                              {'loss': 0.961, 'grad_norm': 1.9929624795913696, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.19it/s] 23%|██▎       | 7/30 [00:00<00:01, 13.44it/s]                                              {'loss': 0.5588, 'grad_norm': 0.6798190474510193, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 13.44it/s]                                              {'loss': 0.6662, 'grad_norm': 5.735291957855225, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.44it/s] 30%|███       | 9/30 [00:00<00:01, 13.21it/s]                                              {'loss': 0.7647, 'grad_norm': 1.6427652835845947, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.21it/s]                                              {'loss': 0.5653, 'grad_norm': 0.701953113079071, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.21it/s] 37%|███▋      | 11/30 [00:00<00:01, 13.20it/s]                                               {'loss': 0.4908, 'grad_norm': 1.2453519105911255, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.20it/s]                                               {'loss': 0.5102, 'grad_norm': 1.149135708808899, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 13.20it/s] 43%|████▎     | 13/30 [00:00<00:01, 14.69it/s]                                               {'loss': 0.7596, 'grad_norm': 0.9438140392303467, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 14.69it/s]                                               {'loss': 0.5697, 'grad_norm': 0.7509909272193909, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 14.69it/s] 50%|█████     | 15/30 [00:01<00:01, 14.20it/s]                                               {'loss': 0.6497, 'grad_norm': 1.0618948936462402, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 14.20it/s]                                               {'loss': 0.5963, 'grad_norm': 0.7650233507156372, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:00, 14.20it/s] 57%|█████▋    | 17/30 [00:01<00:00, 13.46it/s]                                               {'loss': 0.6025, 'grad_norm': 1.32370126247406, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.46it/s]                                               {'loss': 0.6839, 'grad_norm': 1.0355395078659058, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.46it/s] 63%|██████▎   | 19/30 [00:01<00:00, 14.78it/s]                                               {'loss': 0.4932, 'grad_norm': 1.595774531364441, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.78it/s]                                               {'loss': 0.6627, 'grad_norm': 0.5598867535591125, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.78it/s] 70%|███████   | 21/30 [00:01<00:00, 14.31it/s]                                               {'loss': 0.6292, 'grad_norm': 1.1532946825027466, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.31it/s]                                               {'loss': 0.6275, 'grad_norm': 0.712239682674408, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 14.31it/s] 77%|███████▋  | 23/30 [00:01<00:00, 13.74it/s]                                               {'loss': 0.5654, 'grad_norm': 0.8759034276008606, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.74it/s]                                               {'loss': 0.702, 'grad_norm': 0.9447341561317444, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 13.74it/s] 83%|████████▎ | 25/30 [00:01<00:00, 14.98it/s]                                               {'loss': 0.6196, 'grad_norm': 0.5370454788208008, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.98it/s]                                               {'loss': 0.5739, 'grad_norm': 0.6895399689674377, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 14.98it/s] 90%|█████████ | 27/30 [00:01<00:00, 14.17it/s]                                               {'loss': 0.5488, 'grad_norm': 1.306820273399353, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 14.17it/s]                                               {'loss': 0.5786, 'grad_norm': 0.805790364742279, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 14.17it/s] 97%|█████████▋| 29/30 [00:02<00:00, 13.27it/s]                                               {'loss': 0.5987, 'grad_norm': 0.88437819480896, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.27it/s]                                               {'loss': 0.6319, 'grad_norm': 1.2930991649627686, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.27it/s]                                               {'train_runtime': 2.2779, 'train_samples_per_second': 186.579, 'train_steps_per_second': 13.17, 'train_loss': 0.6304266969362895, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.27it/s]100%|██████████| 30/30 [00:02<00:00, 13.18it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.9842, 'grad_norm': 2.0161242485046387, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 11.27it/s]  7%|▋         | 2/30 [00:00<00:02, 12.68it/s]                                              {'loss': 0.5864, 'grad_norm': 1.085045337677002, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 12.68it/s]                                              {'loss': 0.42, 'grad_norm': 1.5830849409103394, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 12.68it/s] 13%|█▎        | 4/30 [00:00<00:02, 11.81it/s]                                              {'loss': 0.3752, 'grad_norm': 1.2272568941116333, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.81it/s]                                              {'loss': 0.1316, 'grad_norm': 1.176522970199585, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.81it/s] 20%|██        | 6/30 [00:00<00:01, 13.06it/s]                                              {'loss': 0.0376, 'grad_norm': 0.6250765323638916, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 13.06it/s]                                              {'loss': 0.0509, 'grad_norm': 0.7619750499725342, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 13.06it/s] 27%|██▋       | 8/30 [00:00<00:01, 12.89it/s]                                              {'loss': 0.0136, 'grad_norm': 0.35880786180496216, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 12.89it/s]                                              {'loss': 0.0118, 'grad_norm': 0.6542733311653137, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 12.89it/s] 33%|███▎      | 10/30 [00:00<00:01, 12.86it/s]                                               {'loss': 0.237, 'grad_norm': 1.6744003295898438, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 12.86it/s]                                               {'loss': 0.0016, 'grad_norm': 0.02368435636162758, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.86it/s] 40%|████      | 12/30 [00:00<00:01, 14.52it/s]                                               {'loss': 0.0023, 'grad_norm': 0.05467021092772484, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 14.52it/s]                                               {'loss': 0.0013, 'grad_norm': 0.017307355999946594, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 14.52it/s] 47%|████▋     | 14/30 [00:01<00:01, 13.32it/s]                                               {'loss': 0.0017, 'grad_norm': 0.03558754920959473, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.32it/s]                                               {'loss': 0.4562, 'grad_norm': 1.4138628244400024, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.32it/s] 53%|█████▎    | 16/30 [00:01<00:01, 11.78it/s]                                               {'loss': 0.0016, 'grad_norm': 0.02592976577579975, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.78it/s]                                               {'loss': 0.0031, 'grad_norm': 0.0501333512365818, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.78it/s] 60%|██████    | 18/30 [00:01<00:01, 11.27it/s]                                               {'loss': 0.0016, 'grad_norm': 0.030667103826999664, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 11.27it/s]                                               {'loss': 0.0019, 'grad_norm': 0.02694862335920334, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 11.27it/s] 67%|██████▋   | 20/30 [00:01<00:01,  8.35it/s]                                               {'loss': 0.0018, 'grad_norm': 0.027213377878069878, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:01,  8.35it/s] 70%|███████   | 21/30 [00:02<00:01,  7.84it/s]                                               {'loss': 0.0023, 'grad_norm': 0.032192692160606384, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  7.84it/s] 73%|███████▎  | 22/30 [00:02<00:01,  7.93it/s]                                               {'loss': 0.0027, 'grad_norm': 0.04815196245908737, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  7.93it/s]                                               {'loss': 0.4442, 'grad_norm': 2.3112761974334717, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  7.93it/s] 80%|████████  | 24/30 [00:02<00:00,  8.65it/s]                                               {'loss': 0.0034, 'grad_norm': 0.05784108117222786, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  8.65it/s] 83%|████████▎ | 25/30 [00:02<00:00,  7.90it/s]                                               {'loss': 0.0039, 'grad_norm': 0.052641939371824265, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00,  7.90it/s] 87%|████████▋ | 26/30 [00:02<00:00,  7.82it/s]                                               {'loss': 0.0038, 'grad_norm': 0.05567127838730812, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00,  7.82it/s] 90%|█████████ | 27/30 [00:02<00:00,  7.25it/s]                                               {'loss': 0.3172, 'grad_norm': 1.3147056102752686, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00,  7.25it/s] 93%|█████████▎| 28/30 [00:02<00:00,  7.04it/s]                                               {'loss': 0.0033, 'grad_norm': 0.04964581876993179, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00,  7.04it/s] 97%|█████████▋| 29/30 [00:03<00:00,  7.63it/s]                                               {'loss': 0.0029, 'grad_norm': 0.05199353024363518, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  7.63it/s]                                               {'loss': 0.0043, 'grad_norm': 0.07717021554708481, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  7.63it/s]                                               {'train_runtime': 3.2074, 'train_samples_per_second': 132.506, 'train_steps_per_second': 9.353, 'train_loss': 0.13698408410418778, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  7.63it/s]100%|██████████| 30/30 [00:03<00:00,  9.35it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
the shape of base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.weight is (768, 768)
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  8%|▊         | 5/66 [00:00<00:01, 37.48it/s] 14%|█▎        | 9/66 [00:00<00:01, 33.36it/s] 20%|█▉        | 13/66 [00:00<00:02, 23.62it/s] 24%|██▍       | 16/66 [00:00<00:02, 20.16it/s] 29%|██▉       | 19/66 [00:00<00:02, 18.68it/s] 32%|███▏      | 21/66 [00:01<00:02, 17.00it/s] 35%|███▍      | 23/66 [00:01<00:02, 15.86it/s] 39%|███▉      | 26/66 [00:01<00:02, 17.90it/s] 44%|████▍     | 29/66 [00:01<00:01, 20.05it/s] 48%|████▊     | 32/66 [00:01<00:01, 19.17it/s] 52%|█████▏    | 34/66 [00:01<00:01, 18.33it/s] 55%|█████▍    | 36/66 [00:01<00:01, 17.09it/s] 58%|█████▊    | 38/66 [00:01<00:01, 16.75it/s] 62%|██████▏   | 41/66 [00:02<00:01, 17.84it/s] 65%|██████▌   | 43/66 [00:02<00:01, 17.86it/s] 68%|██████▊   | 45/66 [00:02<00:01, 17.40it/s] 73%|███████▎  | 48/66 [00:02<00:00, 19.22it/s] 77%|███████▋  | 51/66 [00:02<00:00, 19.50it/s] 82%|████████▏ | 54/66 [00:02<00:00, 18.36it/s] 86%|████████▋ | 57/66 [00:02<00:00, 19.04it/s] 89%|████████▉ | 59/66 [00:03<00:00, 17.87it/s] 92%|█████████▏| 61/66 [00:03<00:00, 16.98it/s] 95%|█████████▌| 63/66 [00:03<00:00, 16.25it/s]100%|██████████| 66/66 [00:03<00:00, 18.92it/s]100%|██████████| 66/66 [00:03<00:00, 18.86it/s]
{'eval_loss': 0.6451835036277771, 'eval_model_preparation_time': 0.0065, 'eval_acc': 0.6701821668264621, 'eval_runtime': 3.5377, 'eval_samples_per_second': 294.823, 'eval_steps_per_second': 18.656}
ROUND:1
CLIENT:26
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:01<00:31,  1.08s/it]                                              {'loss': 0.822, 'grad_norm': 2.275845527648926, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:01<00:31,  1.08s/it]  7%|▋         | 2/30 [00:01<00:15,  1.79it/s]                                              {'loss': 0.548, 'grad_norm': 2.8223345279693604, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:15,  1.79it/s] 10%|█         | 3/30 [00:01<00:09,  2.84it/s]                                              {'loss': 0.6648, 'grad_norm': 2.4911043643951416, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:09,  2.84it/s]                                              {'loss': 1.2023, 'grad_norm': 5.380534648895264, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:09,  2.84it/s] 17%|█▋        | 5/30 [00:01<00:04,  5.01it/s]                                              {'loss': 0.6972, 'grad_norm': 4.99997615814209, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:04,  5.01it/s]                                              {'loss': 1.4818, 'grad_norm': 10.878933906555176, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.01it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.53it/s]                                              {'loss': 0.5743, 'grad_norm': 2.0425565242767334, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.53it/s]                                              {'loss': 0.7249, 'grad_norm': 1.9474385976791382, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.53it/s] 30%|███       | 9/30 [00:01<00:02,  7.53it/s]                                              {'loss': 0.6902, 'grad_norm': 1.4280813932418823, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.53it/s] 33%|███▎      | 10/30 [00:02<00:02,  7.30it/s]                                               {'loss': 0.6813, 'grad_norm': 1.2966711521148682, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:02,  7.30it/s] 37%|███▋      | 11/30 [00:02<00:02,  7.37it/s]                                               {'loss': 0.6767, 'grad_norm': 1.8257429599761963, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:02,  7.37it/s] 40%|████      | 12/30 [00:02<00:02,  7.46it/s]                                               {'loss': 0.6178, 'grad_norm': 2.279658794403076, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  7.46it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.81it/s]                                               {'loss': 0.6713, 'grad_norm': 1.0273395776748657, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.81it/s]                                               {'loss': 0.5664, 'grad_norm': 1.4856104850769043, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.81it/s] 50%|█████     | 15/30 [00:02<00:01,  8.15it/s]                                               {'loss': 0.9458, 'grad_norm': 3.8864541053771973, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.15it/s]                                               {'loss': 0.5665, 'grad_norm': 0.8225086331367493, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.15it/s] 57%|█████▋    | 17/30 [00:02<00:01,  9.51it/s]                                               {'loss': 0.5866, 'grad_norm': 0.7506380081176758, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  9.51it/s]                                               {'loss': 0.8042, 'grad_norm': 2.380688428878784, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  9.51it/s] 63%|██████▎   | 19/30 [00:03<00:01, 10.63it/s]                                               {'loss': 0.625, 'grad_norm': 1.1156139373779297, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01, 10.63it/s]                                               {'loss': 0.6821, 'grad_norm': 0.9049289226531982, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:00, 10.63it/s] 70%|███████   | 21/30 [00:03<00:00, 11.11it/s]                                               {'loss': 0.5175, 'grad_norm': 1.1498591899871826, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:00, 11.11it/s]                                               {'loss': 0.7001, 'grad_norm': 1.6348026990890503, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00, 11.11it/s] 77%|███████▋  | 23/30 [00:03<00:00, 11.30it/s]                                               {'loss': 0.7539, 'grad_norm': 1.685475468635559, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00, 11.30it/s]                                               {'loss': 0.5165, 'grad_norm': 1.4482513666152954, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00, 11.30it/s] 83%|████████▎ | 25/30 [00:03<00:00, 12.64it/s]                                               {'loss': 0.5726, 'grad_norm': 1.0526639223098755, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 12.64it/s]                                               {'loss': 0.6388, 'grad_norm': 0.6888982057571411, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 12.64it/s] 90%|█████████ | 27/30 [00:03<00:00, 12.77it/s]                                               {'loss': 0.6319, 'grad_norm': 0.8408635258674622, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 12.77it/s]                                               {'loss': 0.6436, 'grad_norm': 0.7649332284927368, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 12.77it/s] 97%|█████████▋| 29/30 [00:03<00:00, 12.87it/s]                                               {'loss': 0.7969, 'grad_norm': 2.067134141921997, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 12.87it/s]                                               {'loss': 0.3831, 'grad_norm': 2.2756452560424805, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.87it/s]                                               {'train_runtime': 3.8926, 'train_samples_per_second': 109.181, 'train_steps_per_second': 7.707, 'train_loss': 0.6994689007600149, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.87it/s]100%|██████████| 30/30 [00:03<00:00,  7.71it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:80
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.5856, 'grad_norm': 1.958051085472107, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 10.12it/s]  7%|▋         | 2/30 [00:00<00:02, 10.25it/s]                                              {'loss': 0.359, 'grad_norm': 1.5753265619277954, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 10.25it/s]                                              {'loss': 0.245, 'grad_norm': 1.911149263381958, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.25it/s] 13%|█▎        | 4/30 [00:00<00:02, 10.54it/s]                                              {'loss': 0.2366, 'grad_norm': 1.3813031911849976, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.54it/s]                                              {'loss': 0.2829, 'grad_norm': 2.929399013519287, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.54it/s] 20%|██        | 6/30 [00:00<00:01, 12.30it/s]                                              {'loss': 0.8146, 'grad_norm': 12.776453971862793, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 12.30it/s]                                              {'loss': 0.185, 'grad_norm': 1.5605342388153076, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 12.30it/s] 27%|██▋       | 8/30 [00:00<00:01, 11.36it/s]                                              {'loss': 0.793, 'grad_norm': 4.7245378494262695, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 11.36it/s]                                              {'loss': 0.2009, 'grad_norm': 1.1432596445083618, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 11.36it/s] 33%|███▎      | 10/30 [00:00<00:01, 10.84it/s]                                               {'loss': 0.4946, 'grad_norm': 3.5629916191101074, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 10.84it/s]                                               {'loss': 0.1161, 'grad_norm': 2.091881036758423, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 10.84it/s] 40%|████      | 12/30 [00:01<00:01, 10.81it/s]                                               {'loss': 0.7188, 'grad_norm': 8.284188270568848, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 10.81it/s]                                               {'loss': 0.2441, 'grad_norm': 1.8104757070541382, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 10.81it/s] 47%|████▋     | 14/30 [00:01<00:01, 10.73it/s]                                               {'loss': 0.2332, 'grad_norm': 1.8050950765609741, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 10.73it/s]                                               {'loss': 0.7901, 'grad_norm': 5.880457401275635, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 10.73it/s] 53%|█████▎    | 16/30 [00:01<00:01, 11.39it/s]                                               {'loss': 0.4094, 'grad_norm': 2.4321630001068115, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.39it/s]                                               {'loss': 0.1664, 'grad_norm': 2.0204713344573975, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.39it/s] 60%|██████    | 18/30 [00:01<00:00, 12.77it/s]                                               {'loss': 0.7002, 'grad_norm': 3.257863998413086, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 12.77it/s]                                               {'loss': 0.1671, 'grad_norm': 2.0874361991882324, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 12.77it/s] 67%|██████▋   | 20/30 [00:01<00:00, 12.37it/s]                                               {'loss': 0.4808, 'grad_norm': 2.3360002040863037, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 12.37it/s]                                               {'loss': 0.2743, 'grad_norm': 2.1580705642700195, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 12.37it/s] 73%|███████▎  | 22/30 [00:01<00:00, 11.54it/s]                                               {'loss': 0.5047, 'grad_norm': 2.5892865657806396, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 11.54it/s]                                               {'loss': 0.2711, 'grad_norm': 1.5942044258117676, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 11.54it/s] 80%|████████  | 24/30 [00:02<00:00, 10.71it/s]                                               {'loss': 0.4565, 'grad_norm': 3.7478249073028564, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 10.71it/s]                                               {'loss': 0.208, 'grad_norm': 1.3656888008117676, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 10.71it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.78it/s]                                               {'loss': 0.3589, 'grad_norm': 2.2263824939727783, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00,  9.78it/s]                                               {'loss': 0.3927, 'grad_norm': 2.293389081954956, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00,  9.78it/s] 93%|█████████▎| 28/30 [00:02<00:00,  9.88it/s]                                               {'loss': 0.3251, 'grad_norm': 2.4066672325134277, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00,  9.88it/s]                                               {'loss': 0.2287, 'grad_norm': 1.573592185974121, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00,  9.88it/s]100%|██████████| 30/30 [00:02<00:00, 10.69it/s]                                               {'loss': 0.5274, 'grad_norm': 15.399681091308594, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 10.69it/s]                                               {'train_runtime': 2.84, 'train_samples_per_second': 149.648, 'train_steps_per_second': 10.563, 'train_loss': 0.39235953564445175, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 10.69it/s]100%|██████████| 30/30 [00:02<00:00, 10.57it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  7.93it/s]                                              {'loss': 0.8197, 'grad_norm': 2.7217748165130615, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  7.93it/s]                                              {'loss': 0.5833, 'grad_norm': 1.240183711051941, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.93it/s] 10%|█         | 3/30 [00:00<00:02, 11.45it/s]                                              {'loss': 0.501, 'grad_norm': 1.291715145111084, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.45it/s]                                              {'loss': 0.6283, 'grad_norm': 3.1696078777313232, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.45it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.87it/s]                                              {'loss': 0.9155, 'grad_norm': 3.8944621086120605, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.87it/s]                                              {'loss': 1.0135, 'grad_norm': 4.921360969543457, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.87it/s] 23%|██▎       | 7/30 [00:00<00:01, 12.98it/s]                                              {'loss': 0.525, 'grad_norm': 3.666295289993286, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 12.98it/s]                                              {'loss': 0.6848, 'grad_norm': 1.8263102769851685, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 12.98it/s] 30%|███       | 9/30 [00:00<00:01, 12.72it/s]                                              {'loss': 0.6329, 'grad_norm': 2.404710531234741, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 12.72it/s]                                              {'loss': 0.6389, 'grad_norm': 3.089775562286377, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 12.72it/s] 37%|███▋      | 11/30 [00:00<00:01, 12.21it/s]                                               {'loss': 0.7044, 'grad_norm': 1.4145175218582153, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.21it/s]                                               {'loss': 0.5228, 'grad_norm': 2.154719591140747, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 12.21it/s] 43%|████▎     | 13/30 [00:01<00:01, 13.30it/s]                                               {'loss': 0.5915, 'grad_norm': 1.2671571969985962, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 13.30it/s]                                               {'loss': 0.5464, 'grad_norm': 1.25858473777771, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.30it/s] 50%|█████     | 15/30 [00:01<00:01, 12.82it/s]                                               {'loss': 0.7622, 'grad_norm': 2.544041156768799, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 12.82it/s]                                               {'loss': 0.7355, 'grad_norm': 2.133383274078369, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 12.82it/s] 57%|█████▋    | 17/30 [00:01<00:01, 12.85it/s]                                               {'loss': 0.577, 'grad_norm': 0.9067404270172119, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.85it/s]                                               {'loss': 0.7186, 'grad_norm': 2.8744149208068848, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 12.85it/s] 63%|██████▎   | 19/30 [00:01<00:00, 13.95it/s]                                               {'loss': 0.6682, 'grad_norm': 2.289426803588867, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.95it/s]                                               {'loss': 0.4671, 'grad_norm': 1.6512761116027832, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.95it/s] 70%|███████   | 21/30 [00:01<00:00, 13.26it/s]                                               {'loss': 0.7536, 'grad_norm': 2.5740537643432617, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.26it/s]                                               {'loss': 0.7973, 'grad_norm': 1.9764474630355835, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.26it/s] 77%|███████▋  | 23/30 [00:01<00:00, 12.63it/s]                                               {'loss': 0.6243, 'grad_norm': 1.4772814512252808, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 12.63it/s]                                               {'loss': 0.5725, 'grad_norm': 1.929238200187683, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 12.63it/s] 83%|████████▎ | 25/30 [00:01<00:00, 13.43it/s]                                               {'loss': 0.6568, 'grad_norm': 1.685340166091919, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 13.43it/s]                                               {'loss': 0.6935, 'grad_norm': 1.23966383934021, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.43it/s] 90%|█████████ | 27/30 [00:02<00:00, 12.32it/s]                                               {'loss': 0.5535, 'grad_norm': 0.9035372734069824, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 12.32it/s]                                               {'loss': 0.4784, 'grad_norm': 1.6043260097503662, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.32it/s] 97%|█████████▋| 29/30 [00:02<00:00, 12.37it/s]                                               {'loss': 0.8638, 'grad_norm': 4.429882526397705, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.37it/s]                                               {'loss': 0.666, 'grad_norm': 5.3705854415893555, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.37it/s]                                               {'train_runtime': 2.4125, 'train_samples_per_second': 176.168, 'train_steps_per_second': 12.435, 'train_loss': 0.6632085929314295, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.37it/s]100%|██████████| 30/30 [00:02<00:00, 12.44it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  9.05it/s]                                              {'loss': 0.5361, 'grad_norm': 0.8898445963859558, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  9.05it/s]                                              {'loss': 0.8149, 'grad_norm': 3.657768726348877, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  9.05it/s] 10%|█         | 3/30 [00:00<00:02, 11.58it/s]                                              {'loss': 0.7523, 'grad_norm': 2.272216558456421, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.58it/s]                                              {'loss': 0.7354, 'grad_norm': 2.754194974899292, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.58it/s] 17%|█▋        | 5/30 [00:00<00:02, 12.35it/s]                                              {'loss': 0.7146, 'grad_norm': 3.0939266681671143, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.35it/s]                                              {'loss': 0.8825, 'grad_norm': 3.6985886096954346, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 12.35it/s] 23%|██▎       | 7/30 [00:00<00:01, 14.20it/s]                                              {'loss': 0.6873, 'grad_norm': 2.1969330310821533, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.20it/s]                                              {'loss': 0.6499, 'grad_norm': 0.9359391927719116, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 14.20it/s] 30%|███       | 9/30 [00:00<00:01, 13.58it/s]                                              {'loss': 0.6905, 'grad_norm': 1.245125651359558, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.58it/s]                                              {'loss': 0.6489, 'grad_norm': 1.4953794479370117, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.58it/s] 37%|███▋      | 11/30 [00:00<00:01, 12.92it/s]                                               {'loss': 0.6474, 'grad_norm': 0.7602333426475525, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.92it/s]                                               {'loss': 0.636, 'grad_norm': 1.8931877613067627, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 12.92it/s] 43%|████▎     | 13/30 [00:00<00:01, 13.52it/s]                                               {'loss': 0.6348, 'grad_norm': 1.3678308725357056, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 13.52it/s]                                               {'loss': 0.6252, 'grad_norm': 1.5688563585281372, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.52it/s] 50%|█████     | 15/30 [00:01<00:01, 12.66it/s]                                               {'loss': 0.6941, 'grad_norm': 5.993531227111816, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 12.66it/s]                                               {'loss': 0.5447, 'grad_norm': 1.0512738227844238, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 12.66it/s] 57%|█████▋    | 17/30 [00:01<00:01, 12.81it/s]                                               {'loss': 0.5735, 'grad_norm': 1.9741230010986328, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.81it/s]                                               {'loss': 0.5884, 'grad_norm': 2.2951767444610596, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 12.81it/s] 63%|██████▎   | 19/30 [00:01<00:00, 12.95it/s]                                               {'loss': 0.5569, 'grad_norm': 2.076533317565918, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 12.95it/s]                                               {'loss': 0.5749, 'grad_norm': 1.2266815900802612, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 12.95it/s] 70%|███████   | 21/30 [00:01<00:00, 11.80it/s]                                               {'loss': 0.5474, 'grad_norm': 1.4328616857528687, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 11.80it/s]                                               {'loss': 0.5819, 'grad_norm': 10.245735168457031, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 11.80it/s] 77%|███████▋  | 23/30 [00:01<00:00, 11.34it/s]                                               {'loss': 0.5991, 'grad_norm': 2.519392490386963, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 11.34it/s]                                               {'loss': 0.4895, 'grad_norm': 2.3503143787384033, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 11.34it/s] 83%|████████▎ | 25/30 [00:02<00:00, 12.08it/s]                                               {'loss': 0.5449, 'grad_norm': 2.266361713409424, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.08it/s]                                               {'loss': 0.5075, 'grad_norm': 1.8376879692077637, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 12.08it/s] 90%|█████████ | 27/30 [00:02<00:00, 12.16it/s]                                               {'loss': 0.5384, 'grad_norm': 2.240600347518921, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 12.16it/s]                                               {'loss': 0.5656, 'grad_norm': 3.415053129196167, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.16it/s] 97%|█████████▋| 29/30 [00:02<00:00, 12.34it/s]                                               {'loss': 0.4541, 'grad_norm': 1.968603491783142, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.34it/s]                                               {'loss': 0.401, 'grad_norm': 4.078668594360352, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.34it/s]                                               {'train_runtime': 2.4701, 'train_samples_per_second': 172.059, 'train_steps_per_second': 12.145, 'train_loss': 0.6139223317305247, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.34it/s]100%|██████████| 30/30 [00:02<00:00, 12.15it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:77
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.33it/s]                                              {'loss': 0.8504, 'grad_norm': 3.3538930416107178, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.33it/s]                                              {'loss': 0.3987, 'grad_norm': 2.566176414489746, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.33it/s] 10%|█         | 3/30 [00:00<00:02, 11.45it/s]                                              {'loss': 0.6617, 'grad_norm': 3.8191118240356445, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.45it/s]                                              {'loss': 0.3882, 'grad_norm': 2.3316187858581543, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.45it/s] 17%|█▋        | 5/30 [00:00<00:02, 12.25it/s]                                              {'loss': 0.3316, 'grad_norm': 1.162330985069275, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.25it/s]                                              {'loss': 0.4022, 'grad_norm': 2.5007855892181396, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 12.25it/s] 23%|██▎       | 7/30 [00:00<00:01, 13.49it/s]                                              {'loss': 0.5633, 'grad_norm': 1.9968968629837036, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 13.49it/s]                                              {'loss': 0.443, 'grad_norm': 1.30241858959198, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.49it/s] 30%|███       | 9/30 [00:00<00:01, 12.32it/s]                                              {'loss': 0.4605, 'grad_norm': 1.0206108093261719, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 12.32it/s]                                              {'loss': 0.3333, 'grad_norm': 1.573882818222046, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 12.32it/s] 37%|███▋      | 11/30 [00:00<00:01, 11.46it/s]                                               {'loss': 0.458, 'grad_norm': 1.2596116065979004, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 11.46it/s]                                               {'loss': 0.139, 'grad_norm': 1.3877094984054565, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 11.46it/s] 43%|████▎     | 13/30 [00:01<00:01, 12.41it/s]                                               {'loss': 0.5602, 'grad_norm': 2.9332327842712402, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 12.41it/s]                                               {'loss': 0.193, 'grad_norm': 3.238313913345337, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 12.41it/s] 50%|█████     | 15/30 [00:01<00:01,  9.43it/s]                                               {'loss': 0.9379, 'grad_norm': 25.16924476623535, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  9.43it/s]                                               {'loss': 0.4661, 'grad_norm': 2.3305304050445557, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  9.43it/s] 57%|█████▋    | 17/30 [00:01<00:01,  8.40it/s]                                               {'loss': 0.2115, 'grad_norm': 1.863574743270874, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01,  8.40it/s]                                               {'loss': 0.1107, 'grad_norm': 1.185591220855713, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01,  8.40it/s] 63%|██████▎   | 19/30 [00:01<00:01,  9.15it/s]                                               {'loss': 0.3555, 'grad_norm': 1.2880550622940063, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:01,  9.15it/s]                                               {'loss': 0.441, 'grad_norm': 2.400826930999756, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:01,  9.15it/s] 70%|███████   | 21/30 [00:02<00:00,  9.68it/s]                                               {'loss': 0.514, 'grad_norm': 2.5907206535339355, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00,  9.68it/s]                                               {'loss': 0.0633, 'grad_norm': 0.872978150844574, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  9.68it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.82it/s]                                               {'loss': 0.7013, 'grad_norm': 4.449495315551758, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  9.82it/s]                                               {'loss': 0.082, 'grad_norm': 1.1895289421081543, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  9.82it/s] 83%|████████▎ | 25/30 [00:02<00:00, 10.67it/s]                                               {'loss': 0.74, 'grad_norm': 8.894265174865723, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 10.67it/s]                                               {'loss': 0.4242, 'grad_norm': 3.4443705081939697, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 10.67it/s] 90%|█████████ | 27/30 [00:02<00:00, 10.27it/s]                                               {'loss': 0.3399, 'grad_norm': 2.404503107070923, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 10.27it/s]                                               {'loss': 0.46, 'grad_norm': 2.2400527000427246, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 10.27it/s] 97%|█████████▋| 29/30 [00:02<00:00,  8.78it/s]                                               {'loss': 0.1957, 'grad_norm': 0.9448429942131042, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00,  8.78it/s]                                               {'loss': 0.0827, 'grad_norm': 1.6429253816604614, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00,  8.78it/s]                                               {'train_runtime': 3.1598, 'train_samples_per_second': 134.502, 'train_steps_per_second': 9.494, 'train_loss': 0.4102999034027259, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.78it/s]100%|██████████| 30/30 [00:03<00:00,  9.51it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:02,  9.75it/s]                                              {'loss': 0.8574, 'grad_norm': 4.049346923828125, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02,  9.75it/s]                                              {'loss': 0.3766, 'grad_norm': 1.804144263267517, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02,  9.75it/s] 10%|█         | 3/30 [00:00<00:02, 11.98it/s]                                              {'loss': 1.0281, 'grad_norm': 5.156614303588867, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.98it/s]                                              {'loss': 0.8671, 'grad_norm': 3.7152230739593506, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.98it/s] 17%|█▋        | 5/30 [00:00<00:01, 12.70it/s]                                              {'loss': 0.8497, 'grad_norm': 4.065838813781738, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:01, 12.70it/s]                                              {'loss': 1.5098, 'grad_norm': 9.633044242858887, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:01, 12.70it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.10it/s]                                              {'loss': 0.3718, 'grad_norm': 4.164183616638184, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.10it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.64it/s]                                              {'loss': 0.5136, 'grad_norm': 1.8721591234207153, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.64it/s] 30%|███       | 9/30 [00:01<00:04,  5.21it/s]                                              {'loss': 0.7094, 'grad_norm': 2.2877755165100098, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  5.21it/s]                                              {'loss': 0.6026, 'grad_norm': 1.4927939176559448, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.21it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.74it/s]                                               {'loss': 0.6931, 'grad_norm': 1.510167121887207, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.74it/s]                                               {'loss': 0.4024, 'grad_norm': 2.168795108795166, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.74it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.79it/s]                                               {'loss': 0.4971, 'grad_norm': 1.8747622966766357, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.79it/s]                                               {'loss': 0.5548, 'grad_norm': 1.3720372915267944, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.79it/s] 50%|█████     | 15/30 [00:02<00:01,  8.82it/s]                                               {'loss': 0.6229, 'grad_norm': 2.3701577186584473, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.82it/s]                                               {'loss': 0.5152, 'grad_norm': 1.6434799432754517, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.82it/s] 57%|█████▋    | 17/30 [00:02<00:01,  9.63it/s]                                               {'loss': 0.5603, 'grad_norm': 2.48258376121521, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  9.63it/s]                                               {'loss': 0.6286, 'grad_norm': 3.3871028423309326, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  9.63it/s] 63%|██████▎   | 19/30 [00:02<00:01, 10.71it/s]                                               {'loss': 0.6191, 'grad_norm': 3.063368797302246, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01, 10.71it/s]                                               {'loss': 0.4083, 'grad_norm': 3.522630214691162, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 10.71it/s] 70%|███████   | 21/30 [00:02<00:00, 10.85it/s]                                               {'loss': 0.6265, 'grad_norm': 1.710494041442871, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 10.85it/s]                                               {'loss': 0.5973, 'grad_norm': 1.7281867265701294, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 10.85it/s] 77%|███████▋  | 23/30 [00:02<00:00, 10.85it/s]                                               {'loss': 0.7193, 'grad_norm': 2.294654369354248, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 10.85it/s]                                               {'loss': 0.4848, 'grad_norm': 2.6383302211761475, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 10.85it/s] 83%|████████▎ | 25/30 [00:02<00:00, 11.18it/s]                                               {'loss': 0.3924, 'grad_norm': 1.5111647844314575, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 11.18it/s]                                               {'loss': 0.5484, 'grad_norm': 1.3152304887771606, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 11.18it/s] 90%|█████████ | 27/30 [00:03<00:00, 11.14it/s]                                               {'loss': 0.6445, 'grad_norm': 1.8468371629714966, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 11.14it/s]                                               {'loss': 0.5448, 'grad_norm': 1.5865623950958252, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 11.14it/s] 97%|█████████▋| 29/30 [00:03<00:00, 11.16it/s]                                               {'loss': 0.605, 'grad_norm': 1.8214094638824463, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 11.16it/s]                                               {'loss': 0.2988, 'grad_norm': 2.9253532886505127, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.16it/s]                                               {'train_runtime': 3.5146, 'train_samples_per_second': 120.925, 'train_steps_per_second': 8.536, 'train_loss': 0.6216511329015096, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.16it/s]100%|██████████| 30/30 [00:03<00:00,  8.54it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.48it/s]                                              {'loss': 0.7128, 'grad_norm': 1.4745185375213623, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.48it/s]  7%|▋         | 2/30 [00:00<00:04,  6.57it/s]                                              {'loss': 0.7725, 'grad_norm': 1.5733685493469238, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.57it/s] 10%|█         | 3/30 [00:00<00:04,  6.67it/s]                                              {'loss': 0.6968, 'grad_norm': 1.8038185834884644, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.67it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.63it/s]                                              {'loss': 0.6744, 'grad_norm': 1.8181406259536743, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.63it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.62it/s]                                              {'loss': 0.7945, 'grad_norm': 1.8628078699111938, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.62it/s]                                              {'loss': 0.7641, 'grad_norm': 2.4482500553131104, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.62it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.87it/s]                                              {'loss': 0.6259, 'grad_norm': 0.7343136072158813, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.87it/s]                                              {'loss': 0.6992, 'grad_norm': 0.6172046065330505, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  8.87it/s] 30%|███       | 9/30 [00:01<00:02, 10.01it/s]                                              {'loss': 0.6997, 'grad_norm': 0.7674118280410767, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02, 10.01it/s]                                              {'loss': 0.6963, 'grad_norm': 0.8862728476524353, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:01, 10.01it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.49it/s]                                               {'loss': 0.6567, 'grad_norm': 1.2183436155319214, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  9.49it/s]                                               {'loss': 0.6205, 'grad_norm': 2.949450969696045, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01,  9.49it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.08it/s]                                               {'loss': 0.6412, 'grad_norm': 1.3934184312820435, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  9.08it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.40it/s]                                               {'loss': 0.6682, 'grad_norm': 0.49865105748176575, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.40it/s] 50%|█████     | 15/30 [00:01<00:01,  7.89it/s]                                               {'loss': 0.6903, 'grad_norm': 0.8311927914619446, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  7.89it/s]                                               {'loss': 0.6357, 'grad_norm': 0.7610020041465759, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  7.89it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.33it/s]                                               {'loss': 0.6052, 'grad_norm': 1.2215253114700317, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01,  9.33it/s]                                               {'loss': 0.5762, 'grad_norm': 1.2964258193969727, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  9.33it/s] 63%|██████▎   | 19/30 [00:02<00:01, 10.56it/s]                                               {'loss': 0.6593, 'grad_norm': 1.986659288406372, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01, 10.56it/s]                                               {'loss': 0.7094, 'grad_norm': 0.6907946467399597, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 10.56it/s] 70%|███████   | 21/30 [00:02<00:00, 11.02it/s]                                               {'loss': 0.6948, 'grad_norm': 0.8571691513061523, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.02it/s]                                               {'loss': 0.6125, 'grad_norm': 0.9527636170387268, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.02it/s] 77%|███████▋  | 23/30 [00:02<00:00, 11.39it/s]                                               {'loss': 0.6826, 'grad_norm': 0.7926493287086487, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 11.39it/s]                                               {'loss': 0.6913, 'grad_norm': 1.3540860414505005, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 11.39it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.97it/s]                                               {'loss': 0.7068, 'grad_norm': 0.9971783757209778, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00,  9.97it/s]                                               {'loss': 0.6738, 'grad_norm': 0.7065957188606262, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00,  9.97it/s] 90%|█████████ | 27/30 [00:03<00:00,  8.86it/s]                                               {'loss': 0.6095, 'grad_norm': 0.7487142086029053, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  8.86it/s] 93%|█████████▎| 28/30 [00:03<00:00,  8.19it/s]                                               {'loss': 0.6451, 'grad_norm': 0.6972373723983765, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  8.19it/s] 97%|█████████▋| 29/30 [00:03<00:00,  8.21it/s]                                               {'loss': 0.6283, 'grad_norm': 0.8718453645706177, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  8.21it/s]                                               {'loss': 0.6594, 'grad_norm': 1.539116621017456, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.21it/s]                                               {'train_runtime': 3.4493, 'train_samples_per_second': 123.214, 'train_steps_per_second': 8.697, 'train_loss': 0.673442417383194, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.21it/s]100%|██████████| 30/30 [00:03<00:00,  8.70it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  4.01it/s]                                              {'loss': 0.6233, 'grad_norm': 1.8624217510223389, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  4.01it/s]  7%|▋         | 2/30 [00:00<00:06,  4.12it/s]                                              {'loss': 0.8501, 'grad_norm': 1.9870270490646362, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.12it/s] 10%|█         | 3/30 [00:00<00:05,  4.82it/s]                                              {'loss': 0.7674, 'grad_norm': 1.5152703523635864, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.82it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.95it/s]                                              {'loss': 0.762, 'grad_norm': 2.24049973487854, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.95it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.74it/s]                                              {'loss': 0.6746, 'grad_norm': 1.1164953708648682, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.74it/s]                                              {'loss': 0.8178, 'grad_norm': 3.7631173133850098, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.74it/s] 23%|██▎       | 7/30 [00:01<00:03,  7.62it/s]                                              {'loss': 0.6509, 'grad_norm': 0.5457208156585693, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  7.62it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.66it/s]                                              {'loss': 0.6791, 'grad_norm': 0.9886685609817505, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.66it/s] 30%|███       | 9/30 [00:01<00:02,  7.40it/s]                                              {'loss': 0.6736, 'grad_norm': 0.584324300289154, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.40it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.08it/s]                                               {'loss': 0.6961, 'grad_norm': 0.653060793876648, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.08it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.74it/s]                                               {'loss': 0.7049, 'grad_norm': 0.7252036333084106, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.74it/s]                                               {'loss': 0.6429, 'grad_norm': 1.7990634441375732, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.74it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.37it/s]                                               {'loss': 0.6832, 'grad_norm': 0.5564752221107483, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.37it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.93it/s]                                               {'loss': 0.687, 'grad_norm': 0.6284202933311462, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.93it/s] 50%|█████     | 15/30 [00:02<00:02,  5.38it/s]                                               {'loss': 0.702, 'grad_norm': 2.0565621852874756, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.38it/s] 53%|█████▎    | 16/30 [00:02<00:02,  4.94it/s]                                               {'loss': 0.6565, 'grad_norm': 0.9786521792411804, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  4.94it/s] 57%|█████▋    | 17/30 [00:02<00:02,  4.85it/s]                                               {'loss': 0.6866, 'grad_norm': 0.607650637626648, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  4.85it/s]                                               {'loss': 0.7286, 'grad_norm': 1.1749573945999146, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.85it/s] 63%|██████▎   | 19/30 [00:03<00:01,  5.65it/s]                                               {'loss': 0.6471, 'grad_norm': 1.7392534017562866, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  5.65it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.32it/s]                                               {'loss': 0.7038, 'grad_norm': 0.43975216150283813, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.32it/s] 70%|███████   | 21/30 [00:03<00:01,  5.09it/s]                                               {'loss': 0.6931, 'grad_norm': 0.41875365376472473, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.09it/s] 73%|███████▎  | 22/30 [00:03<00:01,  4.95it/s]                                               {'loss': 0.6921, 'grad_norm': 0.4878828823566437, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  4.95it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.89it/s]                                               {'loss': 0.8149, 'grad_norm': 1.650805950164795, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.89it/s]                                               {'loss': 0.6704, 'grad_norm': 0.46723535656929016, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  4.89it/s] 83%|████████▎ | 25/30 [00:04<00:00,  5.98it/s]                                               {'loss': 0.7308, 'grad_norm': 0.751008927822113, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  5.98it/s] 87%|████████▋ | 26/30 [00:04<00:00,  5.85it/s]                                               {'loss': 0.6788, 'grad_norm': 0.43224814534187317, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  5.85it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.61it/s]                                               {'loss': 0.6605, 'grad_norm': 0.4426090717315674, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.61it/s] 93%|█████████▎| 28/30 [00:04<00:00,  5.29it/s]                                               {'loss': 0.7075, 'grad_norm': 0.4596308469772339, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  5.29it/s] 97%|█████████▋| 29/30 [00:05<00:00,  5.20it/s]                                               {'loss': 0.7084, 'grad_norm': 1.0088613033294678, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  5.20it/s]                                               {'loss': 0.6573, 'grad_norm': 0.440332293510437, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.20it/s]                                               {'train_runtime': 5.3791, 'train_samples_per_second': 79.01, 'train_steps_per_second': 5.577, 'train_loss': 0.7017141580581665, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.20it/s]100%|██████████| 30/30 [00:05<00:00,  5.58it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:20
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.69it/s]                                              {'loss': 0.5918, 'grad_norm': 5.160315036773682, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.69it/s]  7%|▋         | 2/30 [00:00<00:05,  5.12it/s]                                              {'loss': 0.8928, 'grad_norm': 2.704404592514038, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.12it/s] 10%|█         | 3/30 [00:00<00:05,  4.77it/s]                                              {'loss': 0.7545, 'grad_norm': 2.6219165325164795, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.77it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.67it/s]                                              {'loss': 0.6625, 'grad_norm': 3.571979522705078, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.67it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.48it/s]                                              {'loss': 0.6951, 'grad_norm': 0.9024295210838318, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.48it/s]                                              {'loss': 0.7495, 'grad_norm': 2.525703191757202, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.48it/s] 23%|██▎       | 7/30 [00:01<00:02,  8.43it/s]                                              {'loss': 0.6814, 'grad_norm': 2.3514411449432373, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:02,  8.43it/s]                                              {'loss': 0.6566, 'grad_norm': 0.8362725973129272, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  8.43it/s] 30%|███       | 9/30 [00:01<00:02,  9.36it/s]                                              {'loss': 0.674, 'grad_norm': 0.6090525984764099, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  9.36it/s] 33%|███▎      | 10/30 [00:01<00:02,  8.97it/s]                                               {'loss': 0.6876, 'grad_norm': 0.3625427186489105, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.97it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.00it/s]                                               {'loss': 0.7081, 'grad_norm': 0.44024738669395447, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  9.00it/s]                                               {'loss': 0.9014, 'grad_norm': 2.5785467624664307, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  9.00it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.11it/s]                                               {'loss': 0.7326, 'grad_norm': 0.8455120325088501, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  9.11it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.39it/s]                                               {'loss': 0.7306, 'grad_norm': 0.7152231931686401, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.39it/s] 50%|█████     | 15/30 [00:01<00:01,  7.65it/s]                                               {'loss': 0.53, 'grad_norm': 1.211030125617981, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  7.65it/s]                                               {'loss': 0.6744, 'grad_norm': 0.4179753065109253, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.65it/s] 57%|█████▋    | 17/30 [00:02<00:01,  9.10it/s]                                               {'loss': 0.7617, 'grad_norm': 0.8265992999076843, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  9.10it/s]                                               {'loss': 0.6567, 'grad_norm': 0.6303046345710754, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  9.10it/s] 63%|██████▎   | 19/30 [00:02<00:01, 10.87it/s]                                               {'loss': 0.7909, 'grad_norm': 1.1755868196487427, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01, 10.87it/s]                                               {'loss': 0.6773, 'grad_norm': 0.28253093361854553, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 10.87it/s] 70%|███████   | 21/30 [00:02<00:00, 11.41it/s]                                               {'loss': 0.6889, 'grad_norm': 0.2675948739051819, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.41it/s]                                               {'loss': 0.71, 'grad_norm': 0.42466139793395996, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.41it/s] 77%|███████▋  | 23/30 [00:02<00:00, 11.84it/s]                                               {'loss': 0.6506, 'grad_norm': 0.6925562620162964, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 11.84it/s]                                               {'loss': 0.664, 'grad_norm': 0.5520064830780029, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 11.84it/s] 83%|████████▎ | 25/30 [00:02<00:00, 11.69it/s]                                               {'loss': 0.705, 'grad_norm': 0.4536300599575043, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 11.69it/s]                                               {'loss': 0.6607, 'grad_norm': 0.6788665652275085, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 11.69it/s] 90%|█████████ | 27/30 [00:03<00:00,  9.00it/s]                                               {'loss': 0.6786, 'grad_norm': 0.29919570684432983, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  9.00it/s]                                               {'loss': 0.6841, 'grad_norm': 0.30672916769981384, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  9.00it/s] 97%|█████████▋| 29/30 [00:03<00:00,  8.05it/s]                                               {'loss': 0.6547, 'grad_norm': 0.5241743326187134, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  8.05it/s]                                               {'loss': 0.6735, 'grad_norm': 0.6998785138130188, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.05it/s]                                               {'train_runtime': 3.5852, 'train_samples_per_second': 118.544, 'train_steps_per_second': 8.368, 'train_loss': 0.6993187924226125, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.05it/s]100%|██████████| 30/30 [00:03<00:00,  8.37it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.29it/s]                                              {'loss': 0.7192, 'grad_norm': 1.928479790687561, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.29it/s]  7%|▋         | 2/30 [00:00<00:05,  5.60it/s]                                              {'loss': 0.828, 'grad_norm': 2.123929023742676, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.60it/s] 10%|█         | 3/30 [00:00<00:04,  6.19it/s]                                              {'loss': 0.707, 'grad_norm': 0.9275122880935669, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.19it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.49it/s]                                              {'loss': 0.6777, 'grad_norm': 0.7193959951400757, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.49it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.72it/s]                                              {'loss': 0.7206, 'grad_norm': 0.9243916869163513, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.72it/s]                                              {'loss': 0.7366, 'grad_norm': 1.6515525579452515, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.72it/s] 23%|██▎       | 7/30 [00:01<00:03,  7.45it/s]                                              {'loss': 0.6721, 'grad_norm': 0.642996072769165, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  7.45it/s] 27%|██▋       | 8/30 [00:01<00:03,  7.15it/s]                                              {'loss': 0.7007, 'grad_norm': 0.6115006804466248, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  7.15it/s] 30%|███       | 9/30 [00:01<00:02,  7.27it/s]                                              {'loss': 0.7351, 'grad_norm': 0.7719698548316956, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.27it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.27it/s]                                               {'loss': 0.6691, 'grad_norm': 0.45906102657318115, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.27it/s] 37%|███▋      | 11/30 [00:01<00:03,  6.31it/s]                                               {'loss': 0.692, 'grad_norm': 0.9085365533828735, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  6.31it/s]                                               {'loss': 0.5941, 'grad_norm': 1.8730648756027222, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.31it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.13it/s]                                               {'loss': 0.6458, 'grad_norm': 0.6504207253456116, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.13it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.74it/s]                                               {'loss': 0.6333, 'grad_norm': 0.8820890784263611, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.74it/s] 50%|█████     | 15/30 [00:02<00:02,  6.03it/s]                                               {'loss': 0.7175, 'grad_norm': 1.755470633506775, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.03it/s] 53%|█████▎    | 16/30 [00:02<00:02,  5.36it/s]                                               {'loss': 0.6473, 'grad_norm': 0.8393142223358154, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  5.36it/s] 57%|█████▋    | 17/30 [00:02<00:02,  4.39it/s]                                               {'loss': 0.6378, 'grad_norm': 0.9495005011558533, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  4.39it/s] 60%|██████    | 18/30 [00:03<00:02,  4.69it/s]                                               {'loss': 0.5906, 'grad_norm': 1.2061963081359863, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.69it/s] 63%|██████▎   | 19/30 [00:03<00:02,  3.85it/s]                                               {'loss': 0.5511, 'grad_norm': 1.5567580461502075, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  3.85it/s] 67%|██████▋   | 20/30 [00:03<00:02,  3.48it/s]                                               {'loss': 0.7126, 'grad_norm': 1.3109548091888428, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:02,  3.48it/s] 70%|███████   | 21/30 [00:04<00:02,  3.20it/s]                                               {'loss': 0.728, 'grad_norm': 1.2632057666778564, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:02,  3.20it/s] 73%|███████▎  | 22/30 [00:04<00:02,  3.01it/s]                                               {'loss': 0.5806, 'grad_norm': 1.0192433595657349, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:02,  3.01it/s] 77%|███████▋  | 23/30 [00:04<00:02,  2.86it/s]                                               {'loss': 0.7026, 'grad_norm': 1.5541844367980957, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:02,  2.86it/s] 80%|████████  | 24/30 [00:05<00:01,  3.38it/s]                                               {'loss': 0.5235, 'grad_norm': 3.128117561340332, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  3.38it/s] 83%|████████▎ | 25/30 [00:05<00:01,  3.22it/s]                                               {'loss': 0.6697, 'grad_norm': 1.2172249555587769, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  3.22it/s] 87%|████████▋ | 26/30 [00:05<00:01,  3.09it/s]                                               {'loss': 0.6419, 'grad_norm': 1.0533236265182495, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:01,  3.09it/s] 90%|█████████ | 27/30 [00:06<00:00,  3.09it/s]                                               {'loss': 0.6467, 'grad_norm': 0.9089993834495544, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  3.09it/s] 93%|█████████▎| 28/30 [00:06<00:00,  3.01it/s]                                               {'loss': 0.6446, 'grad_norm': 1.2771501541137695, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  3.01it/s] 97%|█████████▋| 29/30 [00:06<00:00,  3.00it/s]                                               {'loss': 0.5568, 'grad_norm': 1.4546616077423096, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  3.00it/s]100%|██████████| 30/30 [00:06<00:00,  3.68it/s]                                               {'loss': 0.5374, 'grad_norm': 2.0822594165802, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  3.68it/s]                                               {'train_runtime': 7.0604, 'train_samples_per_second': 60.195, 'train_steps_per_second': 4.249, 'train_loss': 0.6606721460819245, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  3.68it/s]100%|██████████| 30/30 [00:07<00:00,  4.25it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
the shape of base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.weight is (768, 768)
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  6%|▌         | 4/66 [00:00<00:01, 32.04it/s] 12%|█▏        | 8/66 [00:00<00:01, 30.24it/s] 18%|█▊        | 12/66 [00:00<00:01, 30.14it/s] 24%|██▍       | 16/66 [00:00<00:01, 29.38it/s] 29%|██▉       | 19/66 [00:00<00:01, 24.27it/s] 33%|███▎      | 22/66 [00:00<00:02, 21.47it/s] 38%|███▊      | 25/66 [00:01<00:02, 18.67it/s] 41%|████      | 27/66 [00:01<00:02, 17.70it/s] 44%|████▍     | 29/66 [00:01<00:02, 17.11it/s] 47%|████▋     | 31/66 [00:01<00:02, 16.38it/s] 50%|█████     | 33/66 [00:01<00:02, 16.04it/s] 55%|█████▍    | 36/66 [00:01<00:01, 19.10it/s] 59%|█████▉    | 39/66 [00:01<00:01, 21.24it/s] 64%|██████▎   | 42/66 [00:01<00:01, 23.17it/s] 68%|██████▊   | 45/66 [00:02<00:00, 24.16it/s] 73%|███████▎  | 48/66 [00:02<00:00, 19.91it/s] 77%|███████▋  | 51/66 [00:02<00:00, 17.49it/s] 80%|████████  | 53/66 [00:02<00:00, 17.36it/s] 83%|████████▎ | 55/66 [00:02<00:00, 16.92it/s] 86%|████████▋ | 57/66 [00:02<00:00, 16.38it/s] 89%|████████▉ | 59/66 [00:03<00:00, 15.99it/s] 92%|█████████▏| 61/66 [00:03<00:00, 15.34it/s] 95%|█████████▌| 63/66 [00:03<00:00, 15.07it/s] 98%|█████████▊| 65/66 [00:03<00:00, 14.86it/s]100%|██████████| 66/66 [00:03<00:00, 19.17it/s]
{'eval_loss': 0.6331945657730103, 'eval_model_preparation_time': 0.0049, 'eval_acc': 0.6951102588686481, 'eval_runtime': 3.5294, 'eval_samples_per_second': 295.518, 'eval_steps_per_second': 18.7}
ROUND:2
CLIENT:75
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.69it/s]                                              {'loss': 0.7577, 'grad_norm': 2.814157009124756, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.69it/s]  7%|▋         | 2/30 [00:00<00:07,  3.77it/s]                                              {'loss': 0.4529, 'grad_norm': 1.785975456237793, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.77it/s] 10%|█         | 3/30 [00:00<00:07,  3.85it/s]                                              {'loss': 0.5082, 'grad_norm': 3.5816617012023926, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.85it/s] 13%|█▎        | 4/30 [00:01<00:06,  3.73it/s]                                              {'loss': 1.1181, 'grad_norm': 4.767806529998779, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:06,  3.73it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.76it/s]                                              {'loss': 1.2119, 'grad_norm': 8.365939140319824, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.76it/s] 20%|██        | 6/30 [00:01<00:05,  4.64it/s]                                              {'loss': 0.9855, 'grad_norm': 13.549015045166016, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.64it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.57it/s]                                              {'loss': 0.4493, 'grad_norm': 1.7891383171081543, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.57it/s] 27%|██▋       | 8/30 [00:01<00:05,  4.20it/s]                                              {'loss': 0.7896, 'grad_norm': 3.350276470184326, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  4.20it/s] 30%|███       | 9/30 [00:02<00:05,  4.08it/s]                                              {'loss': 0.7202, 'grad_norm': 4.064809322357178, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  4.08it/s] 33%|███▎      | 10/30 [00:02<00:05,  3.92it/s]                                               {'loss': 0.6292, 'grad_norm': 1.7148346900939941, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  3.92it/s] 37%|███▋      | 11/30 [00:02<00:04,  3.90it/s]                                               {'loss': 0.6461, 'grad_norm': 2.710869073867798, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  3.90it/s] 40%|████      | 12/30 [00:02<00:03,  4.63it/s]                                               {'loss': 0.5021, 'grad_norm': 2.650333881378174, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.63it/s] 43%|████▎     | 13/30 [00:03<00:03,  4.33it/s]                                               {'loss': 0.5867, 'grad_norm': 1.9241546392440796, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  4.33it/s] 47%|████▋     | 14/30 [00:03<00:04,  3.97it/s]                                               {'loss': 0.6939, 'grad_norm': 2.0464370250701904, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:04,  3.97it/s] 50%|█████     | 15/30 [00:03<00:04,  3.73it/s]                                               {'loss': 0.896, 'grad_norm': 3.888310670852661, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:04,  3.73it/s] 53%|█████▎    | 16/30 [00:03<00:03,  3.95it/s]                                               {'loss': 0.5238, 'grad_norm': 3.0483360290527344, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  3.95it/s] 57%|█████▋    | 17/30 [00:04<00:03,  4.14it/s]                                               {'loss': 0.6154, 'grad_norm': 1.7379565238952637, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:03,  4.14it/s] 60%|██████    | 18/30 [00:04<00:02,  4.85it/s]                                               {'loss': 0.7142, 'grad_norm': 2.312516450881958, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.85it/s] 63%|██████▎   | 19/30 [00:04<00:02,  4.80it/s]                                               {'loss': 0.6501, 'grad_norm': 1.7272063493728638, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  4.80it/s] 67%|██████▋   | 20/30 [00:04<00:01,  5.54it/s]                                               {'loss': 0.6445, 'grad_norm': 0.815484344959259, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  5.54it/s] 70%|███████   | 21/30 [00:04<00:01,  6.32it/s]                                               {'loss': 0.6967, 'grad_norm': 1.4818542003631592, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  6.32it/s] 73%|███████▎  | 22/30 [00:04<00:01,  7.00it/s]                                               {'loss': 0.6243, 'grad_norm': 2.1088614463806152, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  7.00it/s]                                               {'loss': 0.6117, 'grad_norm': 1.9686497449874878, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  7.00it/s] 80%|████████  | 24/30 [00:04<00:00,  9.78it/s]                                               {'loss': 0.614, 'grad_norm': 2.005204677581787, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  9.78it/s]                                               {'loss': 0.6396, 'grad_norm': 2.643827199935913, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  9.78it/s] 87%|████████▋ | 26/30 [00:05<00:00,  9.91it/s]                                               {'loss': 0.5497, 'grad_norm': 1.9813151359558105, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  9.91it/s]                                               {'loss': 0.6434, 'grad_norm': 2.066056489944458, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  9.91it/s] 93%|█████████▎| 28/30 [00:05<00:00, 10.24it/s]                                               {'loss': 0.6798, 'grad_norm': 1.7827695608139038, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00, 10.24it/s]                                               {'loss': 0.6661, 'grad_norm': 3.325737237930298, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00, 10.24it/s]100%|██████████| 30/30 [00:05<00:00, 11.93it/s]                                               {'loss': 0.5662, 'grad_norm': 10.13250732421875, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00, 11.93it/s]                                               {'train_runtime': 5.5796, 'train_samples_per_second': 76.171, 'train_steps_per_second': 5.377, 'train_loss': 0.679561224579811, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00, 11.93it/s]100%|██████████| 30/30 [00:05<00:00,  5.38it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.58it/s]                                              {'loss': 0.5895, 'grad_norm': 1.9627259969711304, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.58it/s]  7%|▋         | 2/30 [00:00<00:06,  4.46it/s]                                              {'loss': 0.8525, 'grad_norm': 2.209906578063965, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.46it/s] 10%|█         | 3/30 [00:00<00:05,  4.54it/s]                                              {'loss': 0.7799, 'grad_norm': 1.5922824144363403, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.54it/s] 13%|█▎        | 4/30 [00:00<00:05,  5.08it/s]                                              {'loss': 0.7, 'grad_norm': 3.907836437225342, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  5.08it/s]                                              {'loss': 0.6852, 'grad_norm': 3.966970443725586, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.08it/s] 20%|██        | 6/30 [00:00<00:03,  7.99it/s]                                              {'loss': 0.4592, 'grad_norm': 6.973361968994141, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.99it/s]                                              {'loss': 0.7087, 'grad_norm': 1.8891159296035767, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:02,  7.99it/s] 27%|██▋       | 8/30 [00:01<00:02,  9.36it/s]                                              {'loss': 0.9473, 'grad_norm': 5.567966461181641, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  9.36it/s]                                              {'loss': 0.543, 'grad_norm': 0.8581175208091736, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  9.36it/s] 33%|███▎      | 10/30 [00:01<00:01, 10.33it/s]                                               {'loss': 0.681, 'grad_norm': 2.4875197410583496, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:01, 10.33it/s]                                               {'loss': 0.6432, 'grad_norm': 0.7863005995750427, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 10.33it/s] 40%|████      | 12/30 [00:01<00:01, 11.65it/s]                                               {'loss': 0.5619, 'grad_norm': 18.39288330078125, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 11.65it/s]                                               {'loss': 0.6141, 'grad_norm': 0.6361706256866455, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 11.65it/s] 47%|████▋     | 14/30 [00:01<00:01, 11.82it/s]                                               {'loss': 0.6131, 'grad_norm': 0.9098517298698425, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.82it/s]                                               {'loss': 0.8045, 'grad_norm': 2.6331582069396973, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 11.82it/s] 53%|█████▎    | 16/30 [00:01<00:01, 11.97it/s]                                               {'loss': 0.7134, 'grad_norm': 1.6177241802215576, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.97it/s]                                               {'loss': 0.6406, 'grad_norm': 0.9113146662712097, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.97it/s] 60%|██████    | 18/30 [00:01<00:00, 13.49it/s]                                               {'loss': 0.6816, 'grad_norm': 1.8269375562667847, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.49it/s]                                               {'loss': 0.6336, 'grad_norm': 1.2788115739822388, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.49it/s] 67%|██████▋   | 20/30 [00:01<00:00, 13.41it/s]                                               {'loss': 0.6878, 'grad_norm': 1.034805178642273, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.41it/s]                                               {'loss': 0.6541, 'grad_norm': 0.9762026071548462, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 13.41it/s] 73%|███████▎  | 22/30 [00:02<00:00, 13.12it/s]                                               {'loss': 0.6551, 'grad_norm': 1.4868501424789429, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 13.12it/s]                                               {'loss': 0.6464, 'grad_norm': 1.0019689798355103, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 13.12it/s] 80%|████████  | 24/30 [00:02<00:00, 14.38it/s]                                               {'loss': 0.6457, 'grad_norm': 1.7965435981750488, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 14.38it/s]                                               {'loss': 0.6751, 'grad_norm': 2.3216893672943115, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 14.38it/s] 87%|████████▋ | 26/30 [00:02<00:00, 13.73it/s]                                               {'loss': 0.6564, 'grad_norm': 0.9481802582740784, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.73it/s]                                               {'loss': 0.658, 'grad_norm': 2.759866714477539, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.73it/s] 93%|█████████▎| 28/30 [00:02<00:00,  9.07it/s]                                               {'loss': 0.6416, 'grad_norm': 1.1375892162322998, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00,  9.07it/s]                                               {'loss': 0.698, 'grad_norm': 1.9069184064865112, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  9.07it/s]100%|██████████| 30/30 [00:03<00:00,  8.23it/s]                                               {'loss': 0.571, 'grad_norm': 2.455228567123413, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.23it/s]                                               {'train_runtime': 3.2053, 'train_samples_per_second': 132.595, 'train_steps_per_second': 9.36, 'train_loss': 0.6680456688006718, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.23it/s]100%|██████████| 30/30 [00:03<00:00,  9.37it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:46
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  4.09it/s]                                              {'loss': 0.6218, 'grad_norm': 2.3445277214050293, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  4.09it/s]  7%|▋         | 2/30 [00:00<00:06,  4.35it/s]                                              {'loss': 0.8044, 'grad_norm': 1.6413995027542114, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.35it/s] 10%|█         | 3/30 [00:00<00:05,  5.20it/s]                                              {'loss': 0.7624, 'grad_norm': 1.3923753499984741, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  5.20it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.87it/s]                                              {'loss': 0.669, 'grad_norm': 1.1933997869491577, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.87it/s]                                              {'loss': 0.7012, 'grad_norm': 0.8482056856155396, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.87it/s] 20%|██        | 6/30 [00:01<00:06,  3.54it/s]                                              {'loss': 0.8205, 'grad_norm': 3.4620261192321777, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.54it/s] 23%|██▎       | 7/30 [00:01<00:06,  3.78it/s]                                              {'loss': 0.6641, 'grad_norm': 0.5500741600990295, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:06,  3.78it/s] 27%|██▋       | 8/30 [00:01<00:05,  3.99it/s]                                              {'loss': 0.6779, 'grad_norm': 0.7094672918319702, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  3.99it/s] 30%|███       | 9/30 [00:02<00:05,  4.19it/s]                                              {'loss': 0.6685, 'grad_norm': 0.7609333992004395, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  4.19it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.62it/s]                                               {'loss': 0.6784, 'grad_norm': 0.2578211724758148, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.62it/s] 37%|███▋      | 11/30 [00:02<00:03,  4.90it/s]                                               {'loss': 0.6703, 'grad_norm': 0.29776304960250854, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  4.90it/s]                                               {'loss': 0.5394, 'grad_norm': 1.739675760269165, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.90it/s] 43%|████▎     | 13/30 [00:02<00:02,  5.92it/s]                                               {'loss': 0.6819, 'grad_norm': 0.7285352945327759, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  5.92it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.98it/s]                                               {'loss': 0.6654, 'grad_norm': 0.7356575131416321, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.98it/s] 50%|█████     | 15/30 [00:03<00:02,  6.07it/s]                                               {'loss': 0.5859, 'grad_norm': 1.8220654726028442, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  6.07it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.51it/s]                                               {'loss': 0.6612, 'grad_norm': 0.6737930178642273, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.51it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.55it/s]                                               {'loss': 0.7138, 'grad_norm': 0.7076150178909302, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.55it/s] 60%|██████    | 18/30 [00:03<00:01,  6.11it/s]                                               {'loss': 0.6373, 'grad_norm': 0.658220648765564, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  6.11it/s] 63%|██████▎   | 19/30 [00:03<00:01,  5.83it/s]                                               {'loss': 0.73, 'grad_norm': 1.5330179929733276, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  5.83it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.73it/s]                                               {'loss': 0.6604, 'grad_norm': 0.48045507073402405, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.73it/s] 70%|███████   | 21/30 [00:04<00:01,  5.93it/s]                                               {'loss': 0.6718, 'grad_norm': 0.4524347484111786, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.93it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.89it/s]                                               {'loss': 0.6806, 'grad_norm': 0.561866819858551, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.89it/s] 77%|███████▋  | 23/30 [00:04<00:01,  5.85it/s]                                               {'loss': 0.6454, 'grad_norm': 1.145146131515503, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  5.85it/s] 80%|████████  | 24/30 [00:04<00:00,  6.48it/s]                                               {'loss': 0.639, 'grad_norm': 0.80705726146698, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  6.48it/s] 83%|████████▎ | 25/30 [00:04<00:00,  7.12it/s]                                               {'loss': 0.6598, 'grad_norm': 0.9819260239601135, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  7.12it/s] 87%|████████▋ | 26/30 [00:04<00:00,  7.50it/s]                                               {'loss': 0.6268, 'grad_norm': 1.0855135917663574, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  7.50it/s] 90%|█████████ | 27/30 [00:04<00:00,  7.06it/s]                                               {'loss': 0.6818, 'grad_norm': 1.1721006631851196, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  7.06it/s] 93%|█████████▎| 28/30 [00:05<00:00,  6.43it/s]                                               {'loss': 0.6534, 'grad_norm': 0.5260393619537354, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  6.43it/s] 97%|█████████▋| 29/30 [00:05<00:00,  6.13it/s]                                               {'loss': 0.6363, 'grad_norm': 1.2753942012786865, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  6.13it/s]100%|██████████| 30/30 [00:05<00:00,  6.12it/s]                                               {'loss': 0.6532, 'grad_norm': 0.9826475381851196, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.12it/s]                                               {'train_runtime': 5.7132, 'train_samples_per_second': 74.39, 'train_steps_per_second': 5.251, 'train_loss': 0.6720637301603953, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.12it/s]100%|██████████| 30/30 [00:05<00:00,  5.26it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:09,  2.95it/s]                                              {'loss': 0.5429, 'grad_norm': 2.2878692150115967, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:09,  2.95it/s]  7%|▋         | 2/30 [00:01<00:15,  1.84it/s]                                              {'loss': 0.7827, 'grad_norm': 3.245112895965576, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:15,  1.84it/s] 10%|█         | 3/30 [00:01<00:11,  2.35it/s]                                              {'loss': 0.7039, 'grad_norm': 1.7047085762023926, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:11,  2.35it/s] 13%|█▎        | 4/30 [00:01<00:08,  2.98it/s]                                              {'loss': 0.7173, 'grad_norm': 13.276971817016602, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  2.98it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.79it/s]                                              {'loss': 0.8014, 'grad_norm': 2.9155466556549072, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.79it/s]                                              {'loss': 0.9301, 'grad_norm': 3.91261625289917, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.79it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.28it/s]                                              {'loss': 0.6402, 'grad_norm': 2.2876179218292236, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.28it/s] 27%|██▋       | 8/30 [00:02<00:03,  5.52it/s]                                              {'loss': 0.7158, 'grad_norm': 1.8855907917022705, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:03,  5.52it/s] 30%|███       | 9/30 [00:02<00:03,  5.78it/s]                                              {'loss': 0.7063, 'grad_norm': 1.7303546667099, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:03,  5.78it/s] 33%|███▎      | 10/30 [00:02<00:03,  6.27it/s]                                               {'loss': 0.6626, 'grad_norm': 1.3888188600540161, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  6.27it/s] 37%|███▋      | 11/30 [00:02<00:02,  6.58it/s]                                               {'loss': 0.6554, 'grad_norm': 1.2442090511322021, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:02,  6.58it/s]                                               {'loss': 0.6011, 'grad_norm': 3.569026231765747, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.58it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.42it/s]                                               {'loss': 0.5815, 'grad_norm': 2.3245139122009277, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.42it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.04it/s]                                               {'loss': 0.6377, 'grad_norm': 3.4530563354492188, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.04it/s] 50%|█████     | 15/30 [00:02<00:02,  7.14it/s]                                               {'loss': 0.887, 'grad_norm': 8.80202865600586, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.14it/s] 53%|█████▎    | 16/30 [00:03<00:02,  6.92it/s]                                               {'loss': 0.627, 'grad_norm': 9.951394081115723, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  6.92it/s] 57%|█████▋    | 17/30 [00:03<00:01,  6.88it/s]                                               {'loss': 0.607, 'grad_norm': 3.1835033893585205, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:01,  6.88it/s]                                               {'loss': 0.6092, 'grad_norm': 7.269467830657959, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  6.88it/s] 63%|██████▎   | 19/30 [00:03<00:01,  8.25it/s]                                               {'loss': 0.5795, 'grad_norm': 3.76252818107605, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  8.25it/s] 67%|██████▋   | 20/30 [00:03<00:01,  8.17it/s]                                               {'loss': 0.7713, 'grad_norm': 4.902004241943359, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  8.17it/s] 70%|███████   | 21/30 [00:03<00:01,  8.52it/s]                                               {'loss': 0.667, 'grad_norm': 5.283078670501709, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  8.52it/s] 73%|███████▎  | 22/30 [00:03<00:00,  8.15it/s]                                               {'loss': 0.7188, 'grad_norm': 5.203037261962891, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00,  8.15it/s]                                               {'loss': 0.66, 'grad_norm': 4.466771125793457, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  8.15it/s] 80%|████████  | 24/30 [00:03<00:00, 10.30it/s]                                               {'loss': 0.6623, 'grad_norm': 5.575464248657227, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00, 10.30it/s]                                               {'loss': 0.7257, 'grad_norm': 3.0691020488739014, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00, 10.30it/s] 87%|████████▋ | 26/30 [00:04<00:00,  9.16it/s]                                               {'loss': 0.6349, 'grad_norm': 3.9052512645721436, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  9.16it/s] 90%|█████████ | 27/30 [00:04<00:00,  9.22it/s]                                               {'loss': 0.633, 'grad_norm': 3.312427043914795, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  9.22it/s] 93%|█████████▎| 28/30 [00:04<00:00,  8.26it/s]                                               {'loss': 0.6075, 'grad_norm': 6.930206775665283, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  8.26it/s]                                               {'loss': 0.6624, 'grad_norm': 4.385112762451172, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  8.26it/s]100%|██████████| 30/30 [00:04<00:00,  9.80it/s]                                               {'loss': 0.6521, 'grad_norm': 7.109428405761719, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  9.80it/s]                                               {'train_runtime': 4.7483, 'train_samples_per_second': 89.505, 'train_steps_per_second': 6.318, 'train_loss': 0.6794570545355479, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  9.80it/s]100%|██████████| 30/30 [00:04<00:00,  6.32it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.62it/s]                                              {'loss': 0.7172, 'grad_norm': 1.6335399150848389, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.62it/s]                                              {'loss': 0.7576, 'grad_norm': 1.4861499071121216, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.62it/s] 10%|█         | 3/30 [00:00<00:02, 11.15it/s]                                              {'loss': 0.6859, 'grad_norm': 1.7130401134490967, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.15it/s]                                              {'loss': 0.6701, 'grad_norm': 1.683681607246399, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.15it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.75it/s]                                              {'loss': 0.777, 'grad_norm': 1.9824442863464355, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.75it/s]                                              {'loss': 0.7949, 'grad_norm': 7.440006256103516, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.75it/s] 23%|██▎       | 7/30 [00:00<00:01, 12.87it/s]                                              {'loss': 0.6261, 'grad_norm': 0.9511284232139587, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 12.87it/s]                                              {'loss': 0.7003, 'grad_norm': 0.7234774231910706, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 12.87it/s] 30%|███       | 9/30 [00:00<00:01, 12.48it/s]                                              {'loss': 0.7098, 'grad_norm': 1.3646363019943237, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 12.48it/s]                                              {'loss': 0.731, 'grad_norm': 1.685821533203125, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 12.48it/s] 37%|███▋      | 11/30 [00:00<00:01, 12.57it/s]                                               {'loss': 0.6632, 'grad_norm': 1.1886045932769775, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.57it/s]                                               {'loss': 0.6387, 'grad_norm': 3.7947590351104736, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 12.57it/s] 43%|████▎     | 13/30 [00:01<00:01, 12.92it/s]                                               {'loss': 0.6528, 'grad_norm': 0.9054310917854309, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 12.92it/s]                                               {'loss': 0.664, 'grad_norm': 0.7349648475646973, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 12.92it/s] 50%|█████     | 15/30 [00:01<00:01, 12.41it/s]                                               {'loss': 0.7011, 'grad_norm': 2.1081478595733643, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 12.41it/s]                                               {'loss': 0.5997, 'grad_norm': 0.968420147895813, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 12.41it/s] 57%|█████▋    | 17/30 [00:01<00:01, 12.52it/s]                                               {'loss': 0.5918, 'grad_norm': 1.0331782102584839, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.52it/s]                                               {'loss': 0.5677, 'grad_norm': 1.3524737358093262, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 12.52it/s] 63%|██████▎   | 19/30 [00:01<00:00, 13.68it/s]                                               {'loss': 0.6346, 'grad_norm': 1.5352846384048462, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.68it/s]                                               {'loss': 0.7124, 'grad_norm': 0.7013242840766907, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.68it/s] 70%|███████   | 21/30 [00:01<00:00, 13.15it/s]                                               {'loss': 0.6617, 'grad_norm': 1.5246925354003906, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.15it/s]                                               {'loss': 0.5913, 'grad_norm': 0.9486814141273499, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.15it/s] 77%|███████▋  | 23/30 [00:01<00:00, 12.77it/s]                                               {'loss': 0.6638, 'grad_norm': 0.9773384928703308, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 12.77it/s]                                               {'loss': 0.748, 'grad_norm': 2.7536566257476807, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 12.77it/s] 83%|████████▎ | 25/30 [00:02<00:00, 12.24it/s]                                               {'loss': 0.7511, 'grad_norm': 1.3082189559936523, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.24it/s]                                               {'loss': 0.7063, 'grad_norm': 0.9893481135368347, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 12.24it/s] 90%|█████████ | 27/30 [00:02<00:00, 11.77it/s]                                               {'loss': 0.6311, 'grad_norm': 0.9351538419723511, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 11.77it/s]                                               {'loss': 0.6658, 'grad_norm': 0.8827695846557617, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 11.77it/s] 97%|█████████▋| 29/30 [00:02<00:00, 11.95it/s]                                               {'loss': 0.6848, 'grad_norm': 1.2153797149658203, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 11.95it/s]                                               {'loss': 0.7009, 'grad_norm': 1.5609889030456543, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 11.95it/s]                                               {'train_runtime': 2.4688, 'train_samples_per_second': 172.146, 'train_steps_per_second': 12.152, 'train_loss': 0.6800168712933858, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 11.95it/s]100%|██████████| 30/30 [00:02<00:00, 12.16it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:39
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:01<00:33,  1.17s/it]                                              {'loss': 0.6326, 'grad_norm': 1.2632158994674683, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:01<00:33,  1.17s/it]  7%|▋         | 2/30 [00:01<00:16,  1.71it/s]                                              {'loss': 0.5392, 'grad_norm': 1.8151823282241821, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:16,  1.71it/s] 10%|█         | 3/30 [00:01<00:10,  2.62it/s]                                              {'loss': 0.6, 'grad_norm': 1.7751586437225342, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:10,  2.62it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.55it/s]                                              {'loss': 0.4121, 'grad_norm': 16.797775268554688, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.55it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.61it/s]                                              {'loss': 0.755, 'grad_norm': 2.7629077434539795, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.61it/s] 20%|██        | 6/30 [00:01<00:04,  5.28it/s]                                              {'loss': 0.4811, 'grad_norm': 2.0930025577545166, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.28it/s] 23%|██▎       | 7/30 [00:02<00:04,  5.70it/s]                                              {'loss': 0.5599, 'grad_norm': 0.9927311539649963, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:04,  5.70it/s] 27%|██▋       | 8/30 [00:02<00:03,  6.56it/s]                                              {'loss': 0.5694, 'grad_norm': 1.1325587034225464, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:03,  6.56it/s] 30%|███       | 9/30 [00:02<00:03,  6.95it/s]                                              {'loss': 0.641, 'grad_norm': 1.1793309450149536, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:03,  6.95it/s] 33%|███▎      | 10/30 [00:02<00:02,  7.24it/s]                                               {'loss': 0.4899, 'grad_norm': 1.5925661325454712, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:02,  7.24it/s] 37%|███▋      | 11/30 [00:02<00:02,  7.89it/s]                                               {'loss': 0.5043, 'grad_norm': 1.2949200868606567, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:02,  7.89it/s]                                               {'loss': 0.4036, 'grad_norm': 1.4039548635482788, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  7.89it/s] 43%|████▎     | 13/30 [00:02<00:01,  8.65it/s]                                               {'loss': 0.5472, 'grad_norm': 0.9738976955413818, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:01,  8.65it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.74it/s]                                               {'loss': 0.4805, 'grad_norm': 1.164407730102539, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.74it/s] 50%|█████     | 15/30 [00:03<00:02,  6.52it/s]                                               {'loss': 0.8707, 'grad_norm': 4.564242362976074, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  6.52it/s] 53%|█████▎    | 16/30 [00:03<00:02,  6.00it/s]                                               {'loss': 0.4688, 'grad_norm': 1.279396414756775, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  6.00it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.62it/s]                                               {'loss': 0.4833, 'grad_norm': 1.1496907472610474, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.62it/s]                                               {'loss': 0.1963, 'grad_norm': 2.4035086631774902, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.62it/s] 63%|██████▎   | 19/30 [00:03<00:01,  6.91it/s]                                               {'loss': 0.4122, 'grad_norm': 1.3138127326965332, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  6.91it/s] 67%|██████▋   | 20/30 [00:03<00:01,  6.45it/s]                                               {'loss': 0.6335, 'grad_norm': 2.1708035469055176, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  6.45it/s] 70%|███████   | 21/30 [00:04<00:01,  5.97it/s]                                               {'loss': 0.5343, 'grad_norm': 2.0329012870788574, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.97it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.38it/s]                                               {'loss': 0.517, 'grad_norm': 1.4450132846832275, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.38it/s] 77%|███████▋  | 23/30 [00:04<00:01,  5.73it/s]                                               {'loss': 0.6227, 'grad_norm': 1.8654735088348389, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  5.73it/s] 80%|████████  | 24/30 [00:04<00:01,  5.50it/s]                                               {'loss': 0.22, 'grad_norm': 2.2168562412261963, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  5.50it/s] 83%|████████▎ | 25/30 [00:04<00:00,  5.13it/s]                                               {'loss': 0.2939, 'grad_norm': 2.1903693675994873, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  5.13it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.22it/s]                                               {'loss': 0.4967, 'grad_norm': 1.4734410047531128, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.22it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.89it/s]                                               {'loss': 0.4339, 'grad_norm': 1.7062804698944092, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.89it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.48it/s]                                               {'loss': 0.5744, 'grad_norm': 2.429028034210205, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.48it/s] 97%|█████████▋| 29/30 [00:05<00:00,  4.13it/s]                                               {'loss': 0.6386, 'grad_norm': 3.516227960586548, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  4.13it/s]100%|██████████| 30/30 [00:06<00:00,  4.59it/s]                                               {'loss': 0.648, 'grad_norm': 4.67569637298584, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.59it/s]                                               {'train_runtime': 6.4429, 'train_samples_per_second': 65.965, 'train_steps_per_second': 4.656, 'train_loss': 0.5219982142249743, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.59it/s]100%|██████████| 30/30 [00:06<00:00,  4.66it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:23
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.64it/s]                                              {'loss': 0.8247, 'grad_norm': 3.6728732585906982, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.64it/s]  7%|▋         | 2/30 [00:00<00:06,  4.39it/s]                                              {'loss': 0.2706, 'grad_norm': 2.5884220600128174, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.39it/s] 10%|█         | 3/30 [00:00<00:06,  4.10it/s]                                              {'loss': 0.3202, 'grad_norm': 1.8305984735488892, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.10it/s] 13%|█▎        | 4/30 [00:00<00:06,  4.21it/s]                                              {'loss': 0.0621, 'grad_norm': 0.8022743463516235, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:06,  4.21it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.17it/s]                                              {'loss': 0.4211, 'grad_norm': 2.7362184524536133, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.17it/s] 20%|██        | 6/30 [00:01<00:04,  5.05it/s]                                              {'loss': 0.0141, 'grad_norm': 0.3954324424266815, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.05it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.70it/s]                                              {'loss': 0.0281, 'grad_norm': 1.5141950845718384, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.70it/s] 27%|██▋       | 8/30 [00:01<00:05,  4.23it/s]                                              {'loss': 0.2823, 'grad_norm': 1.4862780570983887, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  4.23it/s] 30%|███       | 9/30 [00:02<00:05,  4.18it/s]                                              {'loss': 0.2696, 'grad_norm': 1.3793538808822632, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  4.18it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.08it/s]                                               {'loss': 0.2633, 'grad_norm': 1.570259690284729, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.08it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.02it/s]                                               {'loss': 0.4512, 'grad_norm': 1.9650957584381104, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.02it/s] 40%|████      | 12/30 [00:02<00:03,  4.60it/s]                                               {'loss': 0.5339, 'grad_norm': 2.6853325366973877, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.60it/s] 43%|████▎     | 13/30 [00:03<00:03,  4.29it/s]                                               {'loss': 0.5082, 'grad_norm': 0.9883297681808472, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  4.29it/s] 47%|████▋     | 14/30 [00:03<00:04,  3.82it/s]                                               {'loss': 0.2799, 'grad_norm': 0.8783416152000427, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:04,  3.82it/s] 50%|█████     | 15/30 [00:03<00:04,  3.60it/s]                                               {'loss': 0.1491, 'grad_norm': 1.3441489934921265, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:04,  3.60it/s] 53%|█████▎    | 16/30 [00:04<00:04,  3.36it/s]                                               {'loss': 0.2653, 'grad_norm': 0.7390162348747253, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:04,  3.36it/s] 57%|█████▋    | 17/30 [00:04<00:04,  3.19it/s]                                               {'loss': 0.4454, 'grad_norm': 0.47096771001815796, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:04,  3.19it/s] 60%|██████    | 18/30 [00:04<00:03,  3.65it/s]                                               {'loss': 0.1859, 'grad_norm': 1.460131287574768, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:03,  3.65it/s] 63%|██████▎   | 19/30 [00:04<00:03,  3.57it/s]                                               {'loss': 0.2409, 'grad_norm': 0.7188165783882141, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:03,  3.57it/s] 67%|██████▋   | 20/30 [00:05<00:02,  3.67it/s]                                               {'loss': 0.3523, 'grad_norm': 0.44393253326416016, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  3.67it/s] 70%|███████   | 21/30 [00:05<00:02,  3.60it/s]                                               {'loss': 0.3747, 'grad_norm': 1.0450254678726196, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:02,  3.60it/s] 73%|███████▎  | 22/30 [00:05<00:02,  3.37it/s]                                               {'loss': 0.4525, 'grad_norm': 0.675345242023468, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:02,  3.37it/s] 77%|███████▋  | 23/30 [00:06<00:02,  3.14it/s]                                               {'loss': 0.1188, 'grad_norm': 1.2530964612960815, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:02,  3.14it/s] 80%|████████  | 24/30 [00:06<00:01,  3.56it/s]                                               {'loss': 0.114, 'grad_norm': 1.3032934665679932, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  3.56it/s] 83%|████████▎ | 25/30 [00:06<00:01,  3.30it/s]                                               {'loss': 0.0936, 'grad_norm': 1.0476404428482056, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:01,  3.30it/s] 87%|████████▋ | 26/30 [00:07<00:01,  3.11it/s]                                               {'loss': 0.3754, 'grad_norm': 0.5553988218307495, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:07<00:01,  3.11it/s] 90%|█████████ | 27/30 [00:07<00:00,  3.16it/s]                                               {'loss': 0.3979, 'grad_norm': 0.8053838610649109, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:07<00:00,  3.16it/s] 93%|█████████▎| 28/30 [00:07<00:00,  3.26it/s]                                               {'loss': 0.2353, 'grad_norm': 0.3732733726501465, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  3.26it/s] 97%|█████████▋| 29/30 [00:07<00:00,  3.34it/s]                                               {'loss': 0.2024, 'grad_norm': 0.25185203552246094, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  3.34it/s]100%|██████████| 30/30 [00:08<00:00,  4.05it/s]                                               {'loss': 0.1003, 'grad_norm': 1.110859751701355, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.05it/s]                                               {'train_runtime': 8.0986, 'train_samples_per_second': 52.478, 'train_steps_per_second': 3.704, 'train_loss': 0.2877662514646848, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.05it/s]100%|██████████| 30/30 [00:08<00:00,  3.71it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:20
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.24it/s]                                              {'loss': 0.6149, 'grad_norm': 2.4523210525512695, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.24it/s]  7%|▋         | 2/30 [00:00<00:08,  3.33it/s]                                              {'loss': 0.8837, 'grad_norm': 3.1581003665924072, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:08,  3.33it/s] 10%|█         | 3/30 [00:00<00:07,  3.46it/s]                                              {'loss': 0.7904, 'grad_norm': 1.5224318504333496, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.46it/s] 13%|█▎        | 4/30 [00:01<00:06,  3.79it/s]                                              {'loss': 0.6948, 'grad_norm': 1.676892638206482, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:06,  3.79it/s] 17%|█▋        | 5/30 [00:01<00:06,  4.06it/s]                                              {'loss': 0.6694, 'grad_norm': 0.6048974990844727, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  4.06it/s]                                              {'loss': 0.678, 'grad_norm': 2.150420665740967, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.06it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.92it/s]                                              {'loss': 0.7032, 'grad_norm': 0.6436122059822083, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.92it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.88it/s]                                              {'loss': 0.653, 'grad_norm': 0.8225873112678528, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.88it/s] 30%|███       | 9/30 [00:02<00:04,  4.81it/s]                                              {'loss': 0.6663, 'grad_norm': 0.5664159059524536, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.81it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.76it/s]                                               {'loss': 0.6832, 'grad_norm': 0.3354058563709259, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.76it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.67it/s]                                               {'loss': 0.6966, 'grad_norm': 0.41653549671173096, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.67it/s]                                               {'loss': 0.7964, 'grad_norm': 2.004622459411621, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.67it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.47it/s]                                               {'loss': 0.7135, 'grad_norm': 0.7060235142707825, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.47it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.39it/s]                                               {'loss': 0.7035, 'grad_norm': 0.5203807950019836, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.39it/s] 50%|█████     | 15/30 [00:03<00:02,  5.18it/s]                                               {'loss': 0.5781, 'grad_norm': 1.3467010259628296, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.18it/s] 53%|█████▎    | 16/30 [00:03<00:02,  4.93it/s]                                               {'loss': 0.6661, 'grad_norm': 0.42111656069755554, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  4.93it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.59it/s]                                               {'loss': 0.7403, 'grad_norm': 0.8014214038848877, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.59it/s] 60%|██████    | 18/30 [00:03<00:02,  5.15it/s]                                               {'loss': 0.6613, 'grad_norm': 0.7728962302207947, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.15it/s] 63%|██████▎   | 19/30 [00:04<00:02,  4.78it/s]                                               {'loss': 0.7294, 'grad_norm': 1.0777580738067627, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  4.78it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.49it/s]                                               {'loss': 0.6739, 'grad_norm': 0.30236560106277466, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.49it/s] 70%|███████   | 21/30 [00:04<00:02,  4.49it/s]                                               {'loss': 0.6932, 'grad_norm': 0.29366207122802734, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:02,  4.49it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.45it/s]                                               {'loss': 0.7042, 'grad_norm': 0.4533179998397827, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.45it/s] 77%|███████▋  | 23/30 [00:05<00:01,  4.45it/s]                                               {'loss': 0.6582, 'grad_norm': 0.6680086851119995, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  4.45it/s]                                               {'loss': 0.6561, 'grad_norm': 0.5602076649665833, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  4.45it/s] 83%|████████▎ | 25/30 [00:05<00:00,  5.30it/s]                                               {'loss': 0.6971, 'grad_norm': 0.36681538820266724, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  5.30it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.02it/s]                                               {'loss': 0.6624, 'grad_norm': 0.30244719982147217, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.02it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.68it/s]                                               {'loss': 0.6859, 'grad_norm': 0.33161765336990356, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.68it/s] 93%|█████████▎| 28/30 [00:06<00:00,  4.54it/s]                                               {'loss': 0.6845, 'grad_norm': 0.3227863907814026, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  4.54it/s] 97%|█████████▋| 29/30 [00:06<00:00,  4.52it/s]                                               {'loss': 0.6514, 'grad_norm': 0.5833477973937988, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  4.52it/s]100%|██████████| 30/30 [00:06<00:00,  5.20it/s]                                               {'loss': 0.6507, 'grad_norm': 0.5367411375045776, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.20it/s]                                               {'train_runtime': 6.6407, 'train_samples_per_second': 63.999, 'train_steps_per_second': 4.518, 'train_loss': 0.6913279831409455, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.20it/s]100%|██████████| 30/30 [00:06<00:00,  4.52it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:70
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7904, 'grad_norm': 4.888942718505859, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 11.61it/s]  7%|▋         | 2/30 [00:00<00:02, 12.30it/s]                                              {'loss': 0.2607, 'grad_norm': 2.3752920627593994, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 12.30it/s]                                              {'loss': 0.0555, 'grad_norm': 0.6004407405853271, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 12.30it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.63it/s]                                              {'loss': 0.6291, 'grad_norm': 2.5683701038360596, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.63it/s]                                              {'loss': 0.0094, 'grad_norm': 0.13249167799949646, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:01, 12.63it/s] 20%|██        | 6/30 [00:00<00:01, 14.82it/s]                                              {'loss': 2.6123, 'grad_norm': 11.991643905639648, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 14.82it/s]                                              {'loss': 0.2534, 'grad_norm': 1.4337904453277588, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.82it/s] 27%|██▋       | 8/30 [00:00<00:01, 13.68it/s]                                              {'loss': 0.0361, 'grad_norm': 0.5653958320617676, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.68it/s]                                              {'loss': 0.4852, 'grad_norm': 2.228811740875244, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.68it/s] 33%|███▎      | 10/30 [00:00<00:01, 13.29it/s]                                               {'loss': 0.445, 'grad_norm': 1.7572890520095825, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.29it/s]                                               {'loss': 0.2142, 'grad_norm': 0.8690346479415894, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.29it/s] 40%|████      | 12/30 [00:00<00:01, 14.90it/s]                                               {'loss': 0.0536, 'grad_norm': 1.20443594455719, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 14.90it/s]                                               {'loss': 0.0676, 'grad_norm': 0.8465163707733154, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 14.90it/s] 47%|████▋     | 14/30 [00:01<00:01, 13.97it/s]                                               {'loss': 0.0285, 'grad_norm': 0.41691964864730835, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.97it/s]                                               {'loss': 0.7516, 'grad_norm': 8.42219352722168, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.97it/s] 53%|█████▎    | 16/30 [00:01<00:01, 13.56it/s]                                               {'loss': 0.3535, 'grad_norm': 7.644586563110352, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.56it/s]                                               {'loss': 0.5834, 'grad_norm': 7.27717924118042, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.56it/s] 60%|██████    | 18/30 [00:01<00:00, 14.92it/s]                                               {'loss': 1.1677, 'grad_norm': 18.271774291992188, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 14.92it/s]                                               {'loss': 0.2474, 'grad_norm': 6.647526264190674, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.92it/s] 67%|██████▋   | 20/30 [00:01<00:00, 14.48it/s]                                               {'loss': 0.0337, 'grad_norm': 0.6626251935958862, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.48it/s]                                               {'loss': 0.0284, 'grad_norm': 0.5181536674499512, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.48it/s] 73%|███████▎  | 22/30 [00:01<00:00, 14.03it/s]                                               {'loss': 0.5951, 'grad_norm': 17.990509033203125, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 14.03it/s]                                               {'loss': 0.8517, 'grad_norm': 14.727509498596191, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 14.03it/s] 80%|████████  | 24/30 [00:01<00:00, 15.37it/s]                                               {'loss': 0.0205, 'grad_norm': 1.1436079740524292, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 15.37it/s]                                               {'loss': 0.3019, 'grad_norm': 4.526949882507324, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 15.37it/s] 87%|████████▋ | 26/30 [00:01<00:00, 14.61it/s]                                               {'loss': 0.1839, 'grad_norm': 1.9257179498672485, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 14.61it/s]                                               {'loss': 0.4813, 'grad_norm': 4.805994033813477, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 14.61it/s] 93%|█████████▎| 28/30 [00:01<00:00, 14.04it/s]                                               {'loss': 0.017, 'grad_norm': 0.44216489791870117, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:01<00:00, 14.04it/s]                                               {'loss': 0.7176, 'grad_norm': 2.723130941390991, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 14.04it/s]100%|██████████| 30/30 [00:02<00:00, 15.28it/s]                                               {'loss': 0.0112, 'grad_norm': 0.2779194414615631, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 15.28it/s]                                               {'train_runtime': 2.1727, 'train_samples_per_second': 195.605, 'train_steps_per_second': 13.807, 'train_loss': 0.4095624850131571, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 15.28it/s]100%|██████████| 30/30 [00:02<00:00, 13.81it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.6054, 'grad_norm': 1.5316684246063232, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 11.27it/s]  7%|▋         | 2/30 [00:00<00:02, 11.71it/s]                                              {'loss': 0.8304, 'grad_norm': 2.0552399158477783, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 11.71it/s]                                              {'loss': 0.7967, 'grad_norm': 1.6763044595718384, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.71it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.43it/s]                                              {'loss': 0.7632, 'grad_norm': 1.925892949104309, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.43it/s]                                              {'loss': 0.6785, 'grad_norm': 0.806039571762085, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.43it/s] 20%|██        | 6/30 [00:00<00:01, 14.75it/s]                                              {'loss': 0.8875, 'grad_norm': 4.03883695602417, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 14.75it/s]                                              {'loss': 0.6741, 'grad_norm': 1.3495821952819824, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.75it/s] 27%|██▋       | 8/30 [00:00<00:01, 13.27it/s]                                              {'loss': 0.7108, 'grad_norm': 1.1711316108703613, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.27it/s]                                              {'loss': 0.6792, 'grad_norm': 0.8227392435073853, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.27it/s] 33%|███▎      | 10/30 [00:00<00:01, 10.31it/s]                                               {'loss': 0.678, 'grad_norm': 0.6944236755371094, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 10.31it/s]                                               {'loss': 0.6801, 'grad_norm': 0.4193609654903412, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 10.31it/s] 40%|████      | 12/30 [00:01<00:01, 10.09it/s]                                               {'loss': 0.5548, 'grad_norm': 1.6871353387832642, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 10.09it/s]                                               {'loss': 0.6871, 'grad_norm': 0.8813164234161377, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 10.09it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.90it/s]                                               {'loss': 0.6536, 'grad_norm': 0.7531105279922485, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.90it/s] 50%|█████     | 15/30 [00:01<00:01,  8.64it/s]                                               {'loss': 0.7521, 'grad_norm': 1.9545363187789917, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  8.64it/s] 53%|█████▎    | 16/30 [00:01<00:01,  8.30it/s]                                               {'loss': 0.6528, 'grad_norm': 0.7179670333862305, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  8.30it/s] 57%|█████▋    | 17/30 [00:01<00:01,  8.14it/s]                                               {'loss': 0.6542, 'grad_norm': 0.646746814250946, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01,  8.14it/s]                                               {'loss': 0.7323, 'grad_norm': 1.4555178880691528, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01,  8.14it/s] 63%|██████▎   | 19/30 [00:01<00:01,  8.61it/s]                                               {'loss': 0.6938, 'grad_norm': 1.1600311994552612, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:01,  8.61it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.38it/s]                                               {'loss': 0.673, 'grad_norm': 0.4566165506839752, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.38it/s] 70%|███████   | 21/30 [00:02<00:01,  8.06it/s]                                               {'loss': 0.674, 'grad_norm': 0.5823735594749451, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.06it/s] 73%|███████▎  | 22/30 [00:02<00:01,  7.81it/s]                                               {'loss': 0.7194, 'grad_norm': 1.2855817079544067, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  7.81it/s] 77%|███████▋  | 23/30 [00:02<00:00,  7.74it/s]                                               {'loss': 0.7837, 'grad_norm': 1.7048345804214478, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  7.74it/s]                                               {'loss': 0.6644, 'grad_norm': 0.7330572009086609, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  7.74it/s] 83%|████████▎ | 25/30 [00:02<00:00,  8.64it/s]                                               {'loss': 0.6705, 'grad_norm': 0.9405356645584106, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00,  8.64it/s] 87%|████████▋ | 26/30 [00:02<00:00,  8.47it/s]                                               {'loss': 0.6524, 'grad_norm': 0.7077070474624634, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00,  8.47it/s] 90%|█████████ | 27/30 [00:02<00:00,  8.16it/s]                                               {'loss': 0.6174, 'grad_norm': 1.0805933475494385, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00,  8.16it/s] 93%|█████████▎| 28/30 [00:03<00:00,  7.93it/s]                                               {'loss': 0.6652, 'grad_norm': 0.41036510467529297, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  7.93it/s] 97%|█████████▋| 29/30 [00:03<00:00,  7.70it/s]                                               {'loss': 0.661, 'grad_norm': 1.4893580675125122, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  7.70it/s]                                               {'loss': 0.6349, 'grad_norm': 1.3320447206497192, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  7.70it/s]                                               {'train_runtime': 3.3865, 'train_samples_per_second': 125.498, 'train_steps_per_second': 8.859, 'train_loss': 0.6926813880602519, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  7.70it/s]100%|██████████| 30/30 [00:03<00:00,  8.86it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
the shape of base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.weight is (768, 768)
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  5%|▍         | 3/66 [00:00<00:03, 16.34it/s]  8%|▊         | 5/66 [00:00<00:04, 13.51it/s] 11%|█         | 7/66 [00:00<00:04, 12.27it/s] 14%|█▎        | 9/66 [00:00<00:05, 11.38it/s] 17%|█▋        | 11/66 [00:00<00:04, 11.01it/s] 20%|█▉        | 13/66 [00:01<00:04, 12.61it/s] 26%|██▌       | 17/66 [00:01<00:02, 17.60it/s] 32%|███▏      | 21/66 [00:01<00:02, 21.30it/s] 38%|███▊      | 25/66 [00:01<00:01, 23.84it/s] 44%|████▍     | 29/66 [00:01<00:01, 25.84it/s] 48%|████▊     | 32/66 [00:01<00:01, 26.77it/s] 55%|█████▍    | 36/66 [00:01<00:01, 27.87it/s] 61%|██████    | 40/66 [00:01<00:00, 28.62it/s] 67%|██████▋   | 44/66 [00:02<00:00, 29.39it/s] 73%|███████▎  | 48/66 [00:02<00:00, 29.98it/s] 79%|███████▉  | 52/66 [00:02<00:00, 30.11it/s] 85%|████████▍ | 56/66 [00:02<00:00, 30.23it/s] 91%|█████████ | 60/66 [00:02<00:00, 30.34it/s] 97%|█████████▋| 64/66 [00:02<00:00, 30.28it/s]100%|██████████| 66/66 [00:02<00:00, 23.94it/s]
{'eval_loss': 0.6231513023376465, 'eval_model_preparation_time': 0.0062, 'eval_acc': 0.6912751677852349, 'eval_runtime': 2.8539, 'eval_samples_per_second': 365.47, 'eval_steps_per_second': 23.127}
ROUND:3
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7465, 'grad_norm': 2.1836535930633545, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 11.52it/s]  7%|▋         | 2/30 [00:00<00:02, 12.26it/s]                                              {'loss': 0.7936, 'grad_norm': 2.0660786628723145, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 12.26it/s]                                              {'loss': 0.702, 'grad_norm': 1.0249323844909668, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 12.26it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.70it/s]                                              {'loss': 0.669, 'grad_norm': 0.7028477191925049, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.70it/s]                                              {'loss': 0.7146, 'grad_norm': 0.8589537739753723, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:01, 12.70it/s] 20%|██        | 6/30 [00:00<00:01, 14.37it/s]                                              {'loss': 0.7303, 'grad_norm': 1.6026904582977295, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 14.37it/s]                                              {'loss': 0.6779, 'grad_norm': 0.7774226665496826, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.37it/s] 27%|██▋       | 8/30 [00:00<00:01, 13.68it/s]                                              {'loss': 0.7035, 'grad_norm': 0.7669726014137268, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.68it/s]                                              {'loss': 0.736, 'grad_norm': 0.7778874039649963, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.68it/s] 33%|███▎      | 10/30 [00:00<00:01, 13.21it/s]                                               {'loss': 0.6599, 'grad_norm': 0.5821765065193176, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.21it/s]                                               {'loss': 0.6856, 'grad_norm': 0.7430657148361206, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.21it/s]                                               {'loss': 0.5789, 'grad_norm': 2.0074002742767334, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 13.21it/s] 43%|████▎     | 13/30 [00:00<00:01, 14.53it/s]                                               {'loss': 0.6448, 'grad_norm': 0.7041914463043213, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 14.53it/s]                                               {'loss': 0.6307, 'grad_norm': 1.0602998733520508, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 14.53it/s] 50%|█████     | 15/30 [00:01<00:01, 13.94it/s]                                               {'loss': 0.749, 'grad_norm': 2.370574712753296, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.94it/s]                                               {'loss': 0.6258, 'grad_norm': 1.0243233442306519, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.94it/s] 57%|█████▋    | 17/30 [00:01<00:00, 13.62it/s]                                               {'loss': 0.6376, 'grad_norm': 1.0948693752288818, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.62it/s]                                               {'loss': 0.5635, 'grad_norm': 1.5324150323867798, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.62it/s] 63%|██████▎   | 19/30 [00:01<00:00, 14.64it/s]                                               {'loss': 0.5391, 'grad_norm': 1.9033983945846558, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.64it/s]                                               {'loss': 0.7347, 'grad_norm': 1.8256313800811768, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.64it/s] 70%|███████   | 21/30 [00:01<00:00, 14.11it/s]                                               {'loss': 0.7473, 'grad_norm': 1.6155871152877808, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.11it/s]                                               {'loss': 0.5677, 'grad_norm': 1.1799211502075195, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 14.11it/s] 77%|███████▋  | 23/30 [00:01<00:00, 13.95it/s]                                               {'loss': 0.7543, 'grad_norm': 2.113957166671753, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.95it/s]                                               {'loss': 0.5168, 'grad_norm': 3.336933135986328, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 13.95it/s] 83%|████████▎ | 25/30 [00:01<00:00, 14.90it/s]                                               {'loss': 0.7018, 'grad_norm': 1.504141092300415, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.90it/s]                                               {'loss': 0.6682, 'grad_norm': 1.2723798751831055, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 14.90it/s] 90%|█████████ | 27/30 [00:01<00:00, 14.10it/s]                                               {'loss': 0.6725, 'grad_norm': 1.2095335721969604, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 14.10it/s]                                               {'loss': 0.7041, 'grad_norm': 1.7380943298339844, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 14.10it/s] 97%|█████████▋| 29/30 [00:02<00:00, 13.90it/s]                                               {'loss': 0.5827, 'grad_norm': 1.7384998798370361, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.90it/s]                                               {'loss': 0.5168, 'grad_norm': 2.1495378017425537, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.90it/s]                                               {'train_runtime': 2.2022, 'train_samples_per_second': 192.991, 'train_steps_per_second': 13.623, 'train_loss': 0.6651763995488484, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.90it/s]100%|██████████| 30/30 [00:02<00:00, 13.63it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7923, 'grad_norm': 3.6050288677215576, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 11.51it/s]  7%|▋         | 2/30 [00:00<00:02, 12.03it/s]                                              {'loss': 0.703, 'grad_norm': 1.4364864826202393, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 12.03it/s]                                              {'loss': 0.719, 'grad_norm': 1.8736546039581299, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 12.03it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.29it/s]                                              {'loss': 0.698, 'grad_norm': 1.6689505577087402, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.29it/s]                                              {'loss': 0.7236, 'grad_norm': 1.6488298177719116, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.29it/s] 20%|██        | 6/30 [00:00<00:01, 14.29it/s]                                              {'loss': 0.7044, 'grad_norm': 2.6498634815216064, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 14.29it/s]                                              {'loss': 0.6891, 'grad_norm': 0.8631763458251953, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.29it/s] 27%|██▋       | 8/30 [00:00<00:01, 13.41it/s]                                              {'loss': 0.65, 'grad_norm': 0.6123047471046448, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.41it/s]                                              {'loss': 0.6395, 'grad_norm': 0.7123621106147766, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.41it/s] 33%|███▎      | 10/30 [00:00<00:01, 13.08it/s]                                               {'loss': 0.7111, 'grad_norm': 0.9162570238113403, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.08it/s]                                               {'loss': 0.7025, 'grad_norm': 0.6638796329498291, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.08it/s] 40%|████      | 12/30 [00:00<00:01, 14.73it/s]                                               {'loss': 0.8295, 'grad_norm': 2.5849595069885254, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 14.73it/s]                                               {'loss': 0.7149, 'grad_norm': 1.2615134716033936, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 14.73it/s] 47%|████▋     | 14/30 [00:01<00:01, 14.01it/s]                                               {'loss': 0.6802, 'grad_norm': 1.4672150611877441, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 14.01it/s]                                               {'loss': 0.6387, 'grad_norm': 3.0415139198303223, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 14.01it/s] 53%|█████▎    | 16/30 [00:01<00:01, 13.38it/s]                                               {'loss': 0.6575, 'grad_norm': 1.2191892862319946, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.38it/s]                                               {'loss': 0.6659, 'grad_norm': 1.4608365297317505, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.38it/s] 60%|██████    | 18/30 [00:01<00:00, 14.81it/s]                                               {'loss': 0.6602, 'grad_norm': 3.89795184135437, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 14.81it/s]                                               {'loss': 0.6956, 'grad_norm': 1.9364434480667114, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.81it/s] 67%|██████▋   | 20/30 [00:01<00:00, 14.21it/s]                                               {'loss': 0.682, 'grad_norm': 2.0076498985290527, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.21it/s]                                               {'loss': 0.7079, 'grad_norm': 1.6832205057144165, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.21it/s] 73%|███████▎  | 22/30 [00:01<00:00, 13.71it/s]                                               {'loss': 0.7283, 'grad_norm': 1.5327092409133911, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.71it/s]                                               {'loss': 0.6834, 'grad_norm': 2.102691411972046, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.71it/s]                                               {'loss': 0.6348, 'grad_norm': 2.5058860778808594, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 13.71it/s] 83%|████████▎ | 25/30 [00:01<00:00, 14.88it/s]                                               {'loss': 0.7124, 'grad_norm': 2.665898561477661, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.88it/s]                                               {'loss': 0.6914, 'grad_norm': 1.7453402280807495, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 14.88it/s] 90%|█████████ | 27/30 [00:01<00:00, 14.59it/s]                                               {'loss': 0.6899, 'grad_norm': 1.5109145641326904, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 14.59it/s]                                               {'loss': 0.6726, 'grad_norm': 1.349825143814087, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:01<00:00, 14.59it/s] 97%|█████████▋| 29/30 [00:02<00:00, 14.21it/s]                                               {'loss': 0.6536, 'grad_norm': 2.118626356124878, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 14.21it/s]                                               {'loss': 0.8399, 'grad_norm': 5.238856315612793, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 14.21it/s]                                               {'train_runtime': 2.1892, 'train_samples_per_second': 194.131, 'train_steps_per_second': 13.703, 'train_loss': 0.699040017525355, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 14.21it/s]100%|██████████| 30/30 [00:02<00:00, 13.71it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:76
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.04it/s]                                              {'loss': 0.7884, 'grad_norm': 1.7691506147384644, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.04it/s]                                              {'loss': 0.6639, 'grad_norm': 2.0200212001800537, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.04it/s] 10%|█         | 3/30 [00:00<00:02,  9.65it/s]                                              {'loss': 0.7137, 'grad_norm': 1.1537144184112549, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02,  9.65it/s]                                              {'loss': 0.659, 'grad_norm': 1.7560573816299438, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  9.65it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.37it/s]                                              {'loss': 0.6643, 'grad_norm': 1.31693696975708, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.37it/s]                                              {'loss': 0.8896, 'grad_norm': 4.261265277862549, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:02, 11.37it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.45it/s]                                              {'loss': 0.6628, 'grad_norm': 1.48739492893219, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.45it/s]                                              {'loss': 0.6898, 'grad_norm': 0.6092138290405273, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.45it/s] 30%|███       | 9/30 [00:01<00:03,  5.93it/s]                                              {'loss': 0.6631, 'grad_norm': 0.7722357511520386, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.93it/s]                                              {'loss': 0.7248, 'grad_norm': 1.1712088584899902, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.93it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.29it/s]                                               {'loss': 0.7087, 'grad_norm': 0.9654185771942139, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.29it/s]                                               {'loss': 0.7317, 'grad_norm': 3.143052577972412, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.29it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.14it/s]                                               {'loss': 0.7004, 'grad_norm': 0.7407251000404358, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  9.14it/s]                                               {'loss': 0.6923, 'grad_norm': 0.487398236989975, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  9.14it/s] 50%|█████     | 15/30 [00:01<00:01, 10.11it/s]                                               {'loss': 0.6757, 'grad_norm': 1.8155921697616577, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 10.11it/s]                                               {'loss': 0.6825, 'grad_norm': 0.8579186201095581, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 10.11it/s] 57%|█████▋    | 17/30 [00:02<00:01, 10.89it/s]                                               {'loss': 0.6852, 'grad_norm': 1.4500030279159546, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01, 10.89it/s]                                               {'loss': 0.662, 'grad_norm': 0.8331460356712341, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 10.89it/s] 63%|██████▎   | 19/30 [00:02<00:00, 12.59it/s]                                               {'loss': 0.6296, 'grad_norm': 1.3990240097045898, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 12.59it/s]                                               {'loss': 0.7298, 'grad_norm': 1.0250916481018066, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 12.59it/s] 70%|███████   | 21/30 [00:02<00:00, 12.67it/s]                                               {'loss': 0.6804, 'grad_norm': 0.3446696102619171, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 12.67it/s]                                               {'loss': 0.695, 'grad_norm': 0.7009459137916565, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 12.67it/s] 77%|███████▋  | 23/30 [00:02<00:00, 12.66it/s]                                               {'loss': 0.6948, 'grad_norm': 0.5190560817718506, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 12.66it/s]                                               {'loss': 0.6071, 'grad_norm': 1.0289928913116455, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 12.66it/s] 83%|████████▎ | 25/30 [00:02<00:00, 13.44it/s]                                               {'loss': 0.6953, 'grad_norm': 0.3703021705150604, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 13.44it/s]                                               {'loss': 0.7043, 'grad_norm': 0.8608470559120178, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.44it/s] 90%|█████████ | 27/30 [00:02<00:00, 13.20it/s]                                               {'loss': 0.7173, 'grad_norm': 0.6254949569702148, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.20it/s]                                               {'loss': 0.7027, 'grad_norm': 0.41497603058815, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.20it/s] 97%|█████████▋| 29/30 [00:02<00:00, 13.33it/s]                                               {'loss': 0.6985, 'grad_norm': 0.8570516109466553, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.33it/s]                                               {'loss': 0.5649, 'grad_norm': 1.5071821212768555, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.33it/s]                                               {'train_runtime': 2.9702, 'train_samples_per_second': 143.087, 'train_steps_per_second': 10.1, 'train_loss': 0.69258953332901, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.33it/s]100%|██████████| 30/30 [00:02<00:00, 10.10it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7259, 'grad_norm': 1.8640366792678833, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 10.97it/s]  7%|▋         | 2/30 [00:00<00:02, 11.76it/s]                                              {'loss': 0.7446, 'grad_norm': 1.4366369247436523, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 11.76it/s]                                              {'loss': 0.6754, 'grad_norm': 1.6649491786956787, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.76it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.08it/s]                                              {'loss': 0.6591, 'grad_norm': 1.5738246440887451, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.08it/s]                                              {'loss': 0.7703, 'grad_norm': 2.7413129806518555, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.08it/s] 20%|██        | 6/30 [00:00<00:01, 13.59it/s]                                              {'loss': 0.8018, 'grad_norm': 3.195998191833496, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 13.59it/s]                                              {'loss': 0.6152, 'grad_norm': 1.0393257141113281, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 13.59it/s] 27%|██▋       | 8/30 [00:00<00:01, 13.26it/s]                                              {'loss': 0.7033, 'grad_norm': 0.7720293402671814, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.26it/s]                                              {'loss': 0.6966, 'grad_norm': 0.9663235545158386, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.26it/s] 33%|███▎      | 10/30 [00:00<00:01, 13.15it/s]                                               {'loss': 0.7364, 'grad_norm': 2.265629529953003, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.15it/s]                                               {'loss': 0.6571, 'grad_norm': 1.4844294786453247, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.15it/s] 40%|████      | 12/30 [00:00<00:01, 14.72it/s]                                               {'loss': 0.5133, 'grad_norm': 4.773102760314941, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 14.72it/s]                                               {'loss': 0.6734, 'grad_norm': 1.1599911451339722, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 14.72it/s] 47%|████▋     | 14/30 [00:01<00:01, 13.99it/s]                                               {'loss': 0.6133, 'grad_norm': 1.5425047874450684, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.99it/s]                                               {'loss': 0.7958, 'grad_norm': 2.883633613586426, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.99it/s] 53%|█████▎    | 16/30 [00:01<00:01, 13.56it/s]                                               {'loss': 0.7001, 'grad_norm': 4.476696968078613, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.56it/s]                                               {'loss': 0.6106, 'grad_norm': 2.6381258964538574, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.56it/s] 60%|██████    | 18/30 [00:01<00:00, 14.93it/s]                                               {'loss': 0.5926, 'grad_norm': 2.9193620681762695, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 14.93it/s]                                               {'loss': 0.5674, 'grad_norm': 2.865509510040283, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.93it/s] 67%|██████▋   | 20/30 [00:01<00:00, 14.13it/s]                                               {'loss': 0.7058, 'grad_norm': 2.747278928756714, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.13it/s]                                               {'loss': 0.7542, 'grad_norm': 2.878664016723633, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.13it/s] 73%|███████▎  | 22/30 [00:01<00:00, 13.86it/s]                                               {'loss': 0.6388, 'grad_norm': 2.385119915008545, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.86it/s]                                               {'loss': 0.6754, 'grad_norm': 2.3605964183807373, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.86it/s] 80%|████████  | 24/30 [00:01<00:00, 14.71it/s]                                               {'loss': 0.711, 'grad_norm': 4.46156120300293, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 14.71it/s]                                               {'loss': 0.773, 'grad_norm': 2.2367706298828125, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.71it/s] 87%|████████▋ | 26/30 [00:01<00:00, 13.98it/s]                                               {'loss': 0.7093, 'grad_norm': 2.0228757858276367, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 13.98it/s]                                               {'loss': 0.5979, 'grad_norm': 1.9743521213531494, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 13.98it/s] 93%|█████████▎| 28/30 [00:02<00:00, 13.47it/s]                                               {'loss': 0.7535, 'grad_norm': 2.523164749145508, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.47it/s]                                               {'loss': 0.6307, 'grad_norm': 2.619239091873169, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.47it/s]100%|██████████| 30/30 [00:02<00:00, 14.84it/s]                                               {'loss': 0.6257, 'grad_norm': 4.348791599273682, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 14.84it/s]                                               {'train_runtime': 2.2288, 'train_samples_per_second': 190.687, 'train_steps_per_second': 13.46, 'train_loss': 0.6809250970681509, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 14.84it/s]100%|██████████| 30/30 [00:02<00:00, 13.46it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:56
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.6909, 'grad_norm': 3.1185052394866943, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 12.60it/s]  7%|▋         | 2/30 [00:00<00:02, 12.69it/s]                                              {'loss': 0.3987, 'grad_norm': 1.5957552194595337, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 12.69it/s]                                              {'loss': 0.6693, 'grad_norm': 2.424198865890503, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 12.69it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.75it/s]                                              {'loss': 1.1219, 'grad_norm': 9.734980583190918, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.75it/s]                                              {'loss': 0.9796, 'grad_norm': 8.974181175231934, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:01, 12.75it/s] 20%|██        | 6/30 [00:00<00:01, 14.79it/s]                                              {'loss': 1.2654, 'grad_norm': 8.533181190490723, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 14.79it/s]                                              {'loss': 0.4353, 'grad_norm': 1.6234267950057983, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.79it/s] 27%|██▋       | 8/30 [00:00<00:01, 13.76it/s]                                              {'loss': 0.6064, 'grad_norm': 0.7329508066177368, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.76it/s]                                              {'loss': 0.8171, 'grad_norm': 2.2841246128082275, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.76it/s] 33%|███▎      | 10/30 [00:00<00:01, 13.41it/s]                                               {'loss': 0.6038, 'grad_norm': 0.5001178979873657, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.41it/s]                                               {'loss': 0.6673, 'grad_norm': 0.7614433169364929, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.41it/s] 40%|████      | 12/30 [00:00<00:01, 14.92it/s]                                               {'loss': 0.5041, 'grad_norm': 1.5347687005996704, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 14.92it/s]                                               {'loss': 0.5672, 'grad_norm': 0.8832521438598633, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 14.92it/s] 47%|████▋     | 14/30 [00:00<00:01, 14.49it/s]                                               {'loss': 0.5136, 'grad_norm': 2.148322582244873, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:00<00:01, 14.49it/s]                                               {'loss': 0.7645, 'grad_norm': 1.979967713356018, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 14.49it/s] 53%|█████▎    | 16/30 [00:01<00:01, 13.97it/s]                                               {'loss': 0.6033, 'grad_norm': 0.455294132232666, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.97it/s]                                               {'loss': 0.5951, 'grad_norm': 0.3530891537666321, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.97it/s] 60%|██████    | 18/30 [00:01<00:00, 15.25it/s]                                               {'loss': 0.8865, 'grad_norm': 2.7233941555023193, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 15.25it/s]                                               {'loss': 0.6135, 'grad_norm': 0.5188932418823242, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 15.25it/s] 67%|██████▋   | 20/30 [00:01<00:00, 14.29it/s]                                               {'loss': 0.6367, 'grad_norm': 0.4137088358402252, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.29it/s]                                               {'loss': 0.5668, 'grad_norm': 1.1153206825256348, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.29it/s] 73%|███████▎  | 22/30 [00:01<00:00, 13.93it/s]                                               {'loss': 0.6427, 'grad_norm': 0.6257404685020447, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.93it/s]                                               {'loss': 0.6402, 'grad_norm': 0.5138520002365112, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.93it/s] 80%|████████  | 24/30 [00:01<00:00, 14.95it/s]                                               {'loss': 0.7008, 'grad_norm': 0.8545650839805603, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 14.95it/s]                                               {'loss': 0.6067, 'grad_norm': 0.7168362736701965, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.95it/s] 87%|████████▋ | 26/30 [00:01<00:00, 13.98it/s]                                               {'loss': 0.6195, 'grad_norm': 0.7952945232391357, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 13.98it/s]                                               {'loss': 0.6259, 'grad_norm': 0.5656968951225281, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 13.98it/s] 93%|█████████▎| 28/30 [00:01<00:00, 13.43it/s]                                               {'loss': 0.6508, 'grad_norm': 0.4275444746017456, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.43it/s]                                               {'loss': 0.7547, 'grad_norm': 1.4031320810317993, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.43it/s]100%|██████████| 30/30 [00:02<00:00, 14.81it/s]                                               {'loss': 0.5863, 'grad_norm': 1.0452795028686523, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 14.81it/s]                                               {'train_runtime': 2.2291, 'train_samples_per_second': 190.657, 'train_steps_per_second': 13.458, 'train_loss': 0.6778174936771393, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 14.81it/s]100%|██████████| 30/30 [00:02<00:00, 13.46it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.52it/s]                                              {'loss': 0.6856, 'grad_norm': 5.2421345710754395, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.52it/s]  7%|▋         | 2/30 [00:00<00:03,  7.73it/s]                                              {'loss': 0.2811, 'grad_norm': 2.380887746810913, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.73it/s]                                              {'loss': 0.0425, 'grad_norm': 1.6028053760528564, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.73it/s] 13%|█▎        | 4/30 [00:00<00:02, 10.16it/s]                                              {'loss': 0.346, 'grad_norm': 1.6221513748168945, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.16it/s]                                              {'loss': 0.3428, 'grad_norm': 1.6647955179214478, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.16it/s] 20%|██        | 6/30 [00:00<00:01, 12.65it/s]                                              {'loss': 0.0165, 'grad_norm': 0.3582044243812561, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 12.65it/s]                                              {'loss': 0.346, 'grad_norm': 3.1884379386901855, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 12.65it/s] 27%|██▋       | 8/30 [00:00<00:01, 12.53it/s]                                              {'loss': 0.0095, 'grad_norm': 0.20483005046844482, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 12.53it/s]                                              {'loss': 0.3886, 'grad_norm': 4.199128150939941, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 12.53it/s] 33%|███▎      | 10/30 [00:00<00:01, 12.59it/s]                                               {'loss': 0.0213, 'grad_norm': 1.2847188711166382, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 12.59it/s]                                               {'loss': 0.0137, 'grad_norm': 0.3677918612957001, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.59it/s] 40%|████      | 12/30 [00:00<00:01, 14.38it/s]                                               {'loss': 0.0023, 'grad_norm': 0.09276553988456726, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 14.38it/s]                                               {'loss': 0.0029, 'grad_norm': 0.09022790193557739, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 14.38it/s] 47%|████▋     | 14/30 [00:01<00:01, 13.53it/s]                                               {'loss': 0.0027, 'grad_norm': 0.07051509618759155, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.53it/s]                                               {'loss': 0.3713, 'grad_norm': 4.487423896789551, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.53it/s] 53%|█████▎    | 16/30 [00:01<00:01, 13.11it/s]                                               {'loss': 0.0025, 'grad_norm': 0.04447449743747711, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.11it/s]                                               {'loss': 0.3053, 'grad_norm': 2.6105706691741943, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.11it/s] 60%|██████    | 18/30 [00:01<00:00, 14.52it/s]                                               {'loss': 0.0038, 'grad_norm': 0.08501525223255157, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 14.52it/s]                                               {'loss': 0.0027, 'grad_norm': 0.04983774200081825, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.52it/s] 67%|██████▋   | 20/30 [00:01<00:00, 13.96it/s]                                               {'loss': 0.3263, 'grad_norm': 2.3493454456329346, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.96it/s]                                               {'loss': 0.0063, 'grad_norm': 0.15814873576164246, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.96it/s] 73%|███████▎  | 22/30 [00:01<00:00, 13.51it/s]                                               {'loss': 0.0088, 'grad_norm': 0.2298869788646698, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.51it/s]                                               {'loss': 0.2614, 'grad_norm': 3.463979959487915, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.51it/s] 80%|████████  | 24/30 [00:01<00:00, 14.69it/s]                                               {'loss': 0.0088, 'grad_norm': 0.2570295035839081, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 14.69it/s]                                               {'loss': 0.6685, 'grad_norm': 3.323828935623169, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.69it/s] 87%|████████▋ | 26/30 [00:01<00:00, 13.91it/s]                                               {'loss': 0.016, 'grad_norm': 0.3844221234321594, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 13.91it/s]                                               {'loss': 0.0176, 'grad_norm': 0.414884477853775, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.91it/s] 93%|█████████▎| 28/30 [00:02<00:00, 13.59it/s]                                               {'loss': 0.0159, 'grad_norm': 0.3854072093963623, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.59it/s]                                               {'loss': 0.0135, 'grad_norm': 0.2921343147754669, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.59it/s]100%|██████████| 30/30 [00:02<00:00, 15.00it/s]                                               {'loss': 0.0179, 'grad_norm': 0.44697844982147217, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 15.00it/s]                                               {'train_runtime': 2.3152, 'train_samples_per_second': 183.568, 'train_steps_per_second': 12.958, 'train_loss': 0.15160690067956845, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 15.00it/s]100%|██████████| 30/30 [00:02<00:00, 12.97it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7818, 'grad_norm': 3.633368968963623, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 11.62it/s]  7%|▋         | 2/30 [00:00<00:02, 12.03it/s]                                              {'loss': 0.3442, 'grad_norm': 1.5038131475448608, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 12.03it/s]                                              {'loss': 1.0424, 'grad_norm': 4.951995849609375, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 12.03it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.45it/s]                                              {'loss': 0.8716, 'grad_norm': 3.7234416007995605, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.45it/s]                                              {'loss': 0.8709, 'grad_norm': 4.506917476654053, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.45it/s] 20%|██        | 6/30 [00:00<00:01, 14.81it/s]                                              {'loss': 1.4817, 'grad_norm': 11.56848430633545, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 14.81it/s]                                              {'loss': 0.3462, 'grad_norm': 1.589478850364685, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.81it/s] 27%|██▋       | 8/30 [00:00<00:01, 13.74it/s]                                              {'loss': 0.5463, 'grad_norm': 0.36863699555397034, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.74it/s]                                              {'loss': 0.7555, 'grad_norm': 1.575881004333496, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.74it/s] 33%|███▎      | 10/30 [00:00<00:01, 13.35it/s]                                               {'loss': 0.61, 'grad_norm': 0.42796024680137634, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.35it/s]                                               {'loss': 0.6614, 'grad_norm': 0.629315197467804, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.35it/s] 40%|████      | 12/30 [00:00<00:01, 14.73it/s]                                               {'loss': 0.4185, 'grad_norm': 0.732552707195282, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 14.73it/s]                                               {'loss': 0.5474, 'grad_norm': 0.4702242910861969, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 14.73it/s] 47%|████▋     | 14/30 [00:01<00:01, 10.52it/s]                                               {'loss': 0.5981, 'grad_norm': 0.3711211681365967, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 10.52it/s]                                               {'loss': 0.6795, 'grad_norm': 1.1655467748641968, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 10.52it/s] 53%|█████▎    | 16/30 [00:01<00:01,  7.99it/s]                                               {'loss': 0.6054, 'grad_norm': 0.6847620606422424, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  7.99it/s]                                               {'loss': 0.5302, 'grad_norm': 1.2786349058151245, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01,  7.99it/s] 60%|██████    | 18/30 [00:01<00:01,  8.38it/s]                                               {'loss': 0.6335, 'grad_norm': 0.550312340259552, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01,  8.38it/s]                                               {'loss': 0.5867, 'grad_norm': 0.5553247332572937, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:01,  8.38it/s] 67%|██████▋   | 20/30 [00:01<00:01,  8.67it/s]                                               {'loss': 0.4764, 'grad_norm': 1.4434280395507812, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:01,  8.67it/s]                                               {'loss': 0.6734, 'grad_norm': 0.6054072976112366, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.67it/s] 73%|███████▎  | 22/30 [00:02<00:00,  8.64it/s]                                               {'loss': 0.6495, 'grad_norm': 0.6350577473640442, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  8.64it/s] 77%|███████▋  | 23/30 [00:02<00:00,  8.45it/s]                                               {'loss': 0.7468, 'grad_norm': 1.1947681903839111, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  8.45it/s]                                               {'loss': 0.5705, 'grad_norm': 0.9239544868469238, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  8.45it/s] 83%|████████▎ | 25/30 [00:02<00:00,  7.05it/s]                                               {'loss': 0.448, 'grad_norm': 1.2438615560531616, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00,  7.05it/s] 87%|████████▋ | 26/30 [00:02<00:00,  6.07it/s]                                               {'loss': 0.6312, 'grad_norm': 0.7356287837028503, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00,  6.07it/s] 90%|█████████ | 27/30 [00:03<00:00,  5.26it/s]                                               {'loss': 0.7269, 'grad_norm': 1.0758484601974487, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  5.26it/s] 93%|█████████▎| 28/30 [00:03<00:00,  4.73it/s]                                               {'loss': 0.6005, 'grad_norm': 0.4848659038543701, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  4.73it/s] 97%|█████████▋| 29/30 [00:03<00:00,  4.42it/s]                                               {'loss': 0.6981, 'grad_norm': 1.2612898349761963, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  4.42it/s]100%|██████████| 30/30 [00:03<00:00,  5.10it/s]                                               {'loss': 0.4095, 'grad_norm': 2.3087899684906006, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  5.10it/s]                                               {'train_runtime': 3.995, 'train_samples_per_second': 106.382, 'train_steps_per_second': 7.509, 'train_loss': 0.6513993392388026, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  5.10it/s]100%|██████████| 30/30 [00:03<00:00,  7.51it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7647, 'grad_norm': 4.714115142822266, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 10.00it/s]  7%|▋         | 2/30 [00:00<00:02, 10.76it/s]                                              {'loss': 0.2771, 'grad_norm': 2.193617582321167, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 10.76it/s]                                              {'loss': 0.0676, 'grad_norm': 1.0627580881118774, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.76it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.07it/s]                                              {'loss': 0.3057, 'grad_norm': 1.5667587518692017, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.07it/s]                                              {'loss': 0.0068, 'grad_norm': 0.10968998819589615, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.07it/s] 20%|██        | 6/30 [00:00<00:01, 14.56it/s]                                              {'loss': 0.008, 'grad_norm': 0.2885213792324066, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 14.56it/s]                                              {'loss': 0.0062, 'grad_norm': 0.15778207778930664, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.56it/s] 27%|██▋       | 8/30 [00:00<00:01, 13.90it/s]                                              {'loss': 0.0061, 'grad_norm': 0.2907610237598419, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.90it/s]                                              {'loss': 0.0043, 'grad_norm': 0.07424455136060715, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.90it/s] 33%|███▎      | 10/30 [00:00<00:01, 13.46it/s]                                               {'loss': 0.2384, 'grad_norm': 2.7848126888275146, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.46it/s]                                               {'loss': 0.0017, 'grad_norm': 0.031027724966406822, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.46it/s] 40%|████      | 12/30 [00:00<00:01, 13.79it/s]                                               {'loss': 0.0025, 'grad_norm': 0.06322536617517471, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 13.79it/s]                                               {'loss': 0.0016, 'grad_norm': 0.06448493897914886, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 13.79it/s] 47%|████▋     | 14/30 [00:01<00:01, 13.20it/s]                                               {'loss': 0.0027, 'grad_norm': 0.0683644562959671, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.20it/s]                                               {'loss': 0.4247, 'grad_norm': 1.8261680603027344, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.20it/s] 53%|█████▎    | 16/30 [00:01<00:01, 13.13it/s]                                               {'loss': 0.0025, 'grad_norm': 0.046706803143024445, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.13it/s]                                               {'loss': 0.0089, 'grad_norm': 0.1996641904115677, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.13it/s] 60%|██████    | 18/30 [00:01<00:00, 14.56it/s]                                               {'loss': 0.0018, 'grad_norm': 0.044722191989421844, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 14.56it/s]                                               {'loss': 0.0018, 'grad_norm': 0.029025999829173088, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.56it/s] 67%|██████▋   | 20/30 [00:01<00:00, 13.89it/s]                                               {'loss': 0.0046, 'grad_norm': 0.08033126592636108, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.89it/s]                                               {'loss': 0.0042, 'grad_norm': 0.06373543292284012, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.89it/s] 73%|███████▎  | 22/30 [00:01<00:00, 13.06it/s]                                               {'loss': 0.0037, 'grad_norm': 0.05168442055583, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.06it/s]                                               {'loss': 0.5002, 'grad_norm': 1.9574387073516846, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.06it/s] 80%|████████  | 24/30 [00:01<00:00, 14.35it/s]                                               {'loss': 0.0028, 'grad_norm': 0.04989749938249588, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 14.35it/s]                                               {'loss': 0.008, 'grad_norm': 0.12979771196842194, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.35it/s] 87%|████████▋ | 26/30 [00:01<00:00, 12.82it/s]                                               {'loss': 0.0043, 'grad_norm': 0.06483553349971771, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 12.82it/s]                                               {'loss': 0.3304, 'grad_norm': 1.3764270544052124, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 12.82it/s] 93%|█████████▎| 28/30 [00:02<00:00, 12.26it/s]                                               {'loss': 0.0033, 'grad_norm': 0.062004510313272476, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.26it/s]                                               {'loss': 0.0057, 'grad_norm': 0.10926157981157303, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.26it/s]100%|██████████| 30/30 [00:02<00:00, 13.80it/s]                                               {'loss': 0.0061, 'grad_norm': 0.09935538470745087, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.80it/s]                                               {'train_runtime': 2.3141, 'train_samples_per_second': 183.655, 'train_steps_per_second': 12.964, 'train_loss': 0.10021352997670571, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.80it/s]100%|██████████| 30/30 [00:02<00:00, 12.97it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:60
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7927, 'grad_norm': 4.693998336791992, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 11.36it/s]  7%|▋         | 2/30 [00:00<00:02, 11.78it/s]                                              {'loss': 0.3727, 'grad_norm': 1.3376320600509644, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 11.78it/s]                                              {'loss': 0.2903, 'grad_norm': 1.558827519416809, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.78it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.27it/s]                                              {'loss': 0.9306, 'grad_norm': 3.2757997512817383, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.27it/s]                                              {'loss': 0.7051, 'grad_norm': 3.604661464691162, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.27it/s] 20%|██        | 6/30 [00:00<00:01, 14.29it/s]                                              {'loss': 0.0847, 'grad_norm': 1.2639027833938599, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 14.29it/s]                                              {'loss': 0.5139, 'grad_norm': 2.453720808029175, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.29it/s] 27%|██▋       | 8/30 [00:00<00:01, 13.65it/s]                                              {'loss': 0.5781, 'grad_norm': 3.7348124980926514, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.65it/s]                                              {'loss': 0.3542, 'grad_norm': 0.8157140016555786, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.65it/s] 33%|███▎      | 10/30 [00:00<00:01, 13.03it/s]                                               {'loss': 0.3258, 'grad_norm': 1.4698721170425415, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.03it/s]                                               {'loss': 0.257, 'grad_norm': 1.3527783155441284, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.03it/s] 40%|████      | 12/30 [00:00<00:01, 13.91it/s]                                               {'loss': 0.1282, 'grad_norm': 3.0311226844787598, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 13.91it/s]                                               {'loss': 0.5037, 'grad_norm': 1.7286609411239624, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 13.91it/s] 47%|████▋     | 14/30 [00:01<00:01, 13.15it/s]                                               {'loss': 0.3213, 'grad_norm': 0.9748237729072571, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.15it/s]                                               {'loss': 0.8136, 'grad_norm': 2.706245183944702, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.15it/s] 53%|█████▎    | 16/30 [00:01<00:01, 12.86it/s]                                               {'loss': 0.2538, 'grad_norm': 1.0724645853042603, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 12.86it/s]                                               {'loss': 0.2168, 'grad_norm': 0.8233691453933716, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.86it/s] 60%|██████    | 18/30 [00:01<00:00, 14.20it/s]                                               {'loss': 0.1279, 'grad_norm': 1.3868157863616943, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 14.20it/s]                                               {'loss': 0.3573, 'grad_norm': 1.3286032676696777, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.20it/s] 67%|██████▋   | 20/30 [00:01<00:00, 14.04it/s]                                               {'loss': 0.3317, 'grad_norm': 1.111776351928711, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.04it/s]                                               {'loss': 0.6953, 'grad_norm': 2.42297101020813, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.04it/s] 73%|███████▎  | 22/30 [00:01<00:00, 13.50it/s]                                               {'loss': 0.2345, 'grad_norm': 0.9881375432014465, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.50it/s]                                               {'loss': 0.4604, 'grad_norm': 2.471759557723999, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.50it/s] 80%|████████  | 24/30 [00:01<00:00, 14.88it/s]                                               {'loss': 0.0644, 'grad_norm': 1.028298258781433, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 14.88it/s]                                               {'loss': 0.3353, 'grad_norm': 1.1250873804092407, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.88it/s] 87%|████████▋ | 26/30 [00:01<00:00, 13.99it/s]                                               {'loss': 0.4117, 'grad_norm': 1.820529818534851, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 13.99it/s]                                               {'loss': 0.1987, 'grad_norm': 1.088620901107788, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 13.99it/s] 93%|█████████▎| 28/30 [00:02<00:00, 12.97it/s]                                               {'loss': 0.4174, 'grad_norm': 1.8504524230957031, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.97it/s]                                               {'loss': 0.5155, 'grad_norm': 2.480963945388794, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.97it/s]100%|██████████| 30/30 [00:02<00:00, 14.06it/s]                                               {'loss': 0.1006, 'grad_norm': 1.44585382938385, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 14.06it/s]                                               {'train_runtime': 2.3273, 'train_samples_per_second': 182.617, 'train_steps_per_second': 12.891, 'train_loss': 0.3897687810162703, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 14.06it/s]100%|██████████| 30/30 [00:02<00:00, 12.89it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:67
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  7.30it/s]                                              {'loss': 0.6456, 'grad_norm': 2.721999406814575, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  7.30it/s]  7%|▋         | 2/30 [00:00<00:03,  7.66it/s]                                              {'loss': 0.7999, 'grad_norm': 1.6710549592971802, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.66it/s] 10%|█         | 3/30 [00:00<00:04,  6.40it/s]                                              {'loss': 0.7283, 'grad_norm': 1.3883858919143677, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.40it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.04it/s]                                              {'loss': 0.6773, 'grad_norm': 1.0149866342544556, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.04it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.55it/s]                                              {'loss': 0.6823, 'grad_norm': 1.024076223373413, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.55it/s]                                              {'loss': 0.6924, 'grad_norm': 2.506850242614746, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.55it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.75it/s]                                              {'loss': 0.6875, 'grad_norm': 0.9115414023399353, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.75it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.87it/s]                                              {'loss': 0.6647, 'grad_norm': 0.5687363743782043, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.87it/s] 30%|███       | 9/30 [00:01<00:03,  5.44it/s]                                              {'loss': 0.647, 'grad_norm': 0.8797879815101624, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.44it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.78it/s]                                               {'loss': 0.685, 'grad_norm': 0.45684510469436646, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.78it/s] 37%|███▋      | 11/30 [00:01<00:03,  5.47it/s]                                               {'loss': 0.6698, 'grad_norm': 0.36491450667381287, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  5.47it/s] 40%|████      | 12/30 [00:01<00:02,  6.24it/s]                                               {'loss': 0.7103, 'grad_norm': 2.6375834941864014, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.24it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.66it/s]                                               {'loss': 0.6769, 'grad_norm': 1.2051278352737427, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.66it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.86it/s]                                               {'loss': 0.6838, 'grad_norm': 1.6147140264511108, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.86it/s] 50%|█████     | 15/30 [00:02<00:02,  5.58it/s]                                               {'loss': 0.7053, 'grad_norm': 4.183749198913574, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.58it/s] 53%|█████▎    | 16/30 [00:02<00:02,  5.41it/s]                                               {'loss': 0.6442, 'grad_norm': 2.76692795753479, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  5.41it/s] 57%|█████▋    | 17/30 [00:02<00:02,  5.13it/s]                                               {'loss': 0.6959, 'grad_norm': 2.880378246307373, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  5.13it/s]                                               {'loss': 0.7112, 'grad_norm': 5.756591320037842, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.13it/s] 63%|██████▎   | 19/30 [00:03<00:01,  5.56it/s]                                               {'loss': 0.7182, 'grad_norm': 10.212835311889648, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  5.56it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.61it/s]                                               {'loss': 0.6935, 'grad_norm': 2.7808072566986084, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.61it/s] 70%|███████   | 21/30 [00:03<00:01,  5.29it/s]                                               {'loss': 0.7298, 'grad_norm': 4.162937641143799, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.29it/s] 73%|███████▎  | 22/30 [00:03<00:01,  5.07it/s]                                               {'loss': 0.7174, 'grad_norm': 2.709132432937622, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  5.07it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.98it/s]                                               {'loss': 0.6531, 'grad_norm': 2.717531204223633, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.98it/s]                                               {'loss': 0.6593, 'grad_norm': 1.462087631225586, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  4.98it/s] 83%|████████▎ | 25/30 [00:04<00:00,  5.87it/s]                                               {'loss': 0.705, 'grad_norm': 2.096052885055542, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  5.87it/s] 87%|████████▋ | 26/30 [00:04<00:00,  5.41it/s]                                               {'loss': 0.6443, 'grad_norm': 1.1744451522827148, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  5.41it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.63it/s]                                               {'loss': 0.6695, 'grad_norm': 1.484987497329712, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.63it/s] 93%|█████████▎| 28/30 [00:04<00:00,  5.45it/s]                                               {'loss': 0.6862, 'grad_norm': 0.9432705044746399, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  5.45it/s] 97%|█████████▋| 29/30 [00:05<00:00,  5.38it/s]                                               {'loss': 0.6808, 'grad_norm': 5.184788703918457, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  5.38it/s]                                               {'loss': 0.636, 'grad_norm': 1.6579511165618896, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.38it/s]                                               {'train_runtime': 5.2873, 'train_samples_per_second': 80.381, 'train_steps_per_second': 5.674, 'train_loss': 0.6866954227288564, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.38it/s]100%|██████████| 30/30 [00:05<00:00,  5.68it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
the shape of base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.weight is (768, 768)
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  6%|▌         | 4/66 [00:00<00:01, 38.88it/s] 12%|█▏        | 8/66 [00:00<00:01, 30.93it/s] 18%|█▊        | 12/66 [00:00<00:02, 25.48it/s] 23%|██▎       | 15/66 [00:00<00:02, 24.87it/s] 27%|██▋       | 18/66 [00:00<00:02, 21.91it/s] 32%|███▏      | 21/66 [00:00<00:02, 20.32it/s] 36%|███▋      | 24/66 [00:01<00:02, 17.90it/s] 39%|███▉      | 26/66 [00:01<00:02, 17.32it/s] 42%|████▏     | 28/66 [00:01<00:02, 16.51it/s] 45%|████▌     | 30/66 [00:01<00:02, 15.66it/s] 48%|████▊     | 32/66 [00:01<00:02, 13.71it/s] 52%|█████▏    | 34/66 [00:01<00:02, 12.10it/s] 55%|█████▍    | 36/66 [00:02<00:02, 12.34it/s] 58%|█████▊    | 38/66 [00:02<00:02, 12.29it/s] 61%|██████    | 40/66 [00:02<00:02, 11.97it/s] 64%|██████▎   | 42/66 [00:02<00:02, 11.35it/s] 67%|██████▋   | 44/66 [00:02<00:01, 12.38it/s] 70%|██████▉   | 46/66 [00:02<00:01, 13.07it/s] 73%|███████▎  | 48/66 [00:03<00:01, 13.03it/s] 76%|███████▌  | 50/66 [00:03<00:01, 12.54it/s] 79%|███████▉  | 52/66 [00:03<00:01, 13.18it/s] 82%|████████▏ | 54/66 [00:03<00:00, 13.78it/s] 85%|████████▍ | 56/66 [00:03<00:00, 13.80it/s] 88%|████████▊ | 58/66 [00:03<00:00, 12.42it/s] 91%|█████████ | 60/66 [00:04<00:00, 11.50it/s] 94%|█████████▍| 62/66 [00:04<00:00, 11.33it/s] 97%|█████████▋| 64/66 [00:04<00:00, 11.01it/s]100%|██████████| 66/66 [00:04<00:00, 11.73it/s]100%|██████████| 66/66 [00:04<00:00, 14.46it/s]
{'eval_loss': 0.6150627732276917, 'eval_model_preparation_time': 0.0083, 'eval_acc': 0.6912751677852349, 'eval_runtime': 4.6047, 'eval_samples_per_second': 226.506, 'eval_steps_per_second': 14.333}
ROUND:4
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  9.13it/s]                                              {'loss': 0.4991, 'grad_norm': 3.813401460647583, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  9.13it/s]  7%|▋         | 2/30 [00:00<00:03,  7.70it/s]                                              {'loss': 0.4259, 'grad_norm': 4.006446838378906, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.70it/s] 10%|█         | 3/30 [00:00<00:03,  7.30it/s]                                              {'loss': 0.4102, 'grad_norm': 1.9575986862182617, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.30it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.41it/s]                                              {'loss': 0.3251, 'grad_norm': 1.5527637004852295, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.41it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.09it/s]                                              {'loss': 0.6645, 'grad_norm': 3.0162832736968994, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.09it/s]                                              {'loss': 0.0639, 'grad_norm': 1.0972758531570435, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.09it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.38it/s]                                              {'loss': 0.1319, 'grad_norm': 1.9143171310424805, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.38it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.77it/s]                                              {'loss': 0.4471, 'grad_norm': 2.1892995834350586, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.77it/s] 30%|███       | 9/30 [00:01<00:02,  7.17it/s]                                              {'loss': 0.5664, 'grad_norm': 2.5727884769439697, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.17it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.00it/s]                                               {'loss': 0.6622, 'grad_norm': 3.674607515335083, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.00it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.30it/s]                                               {'loss': 0.2479, 'grad_norm': 1.4292632341384888, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.30it/s]                                               {'loss': 0.4276, 'grad_norm': 2.4476208686828613, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.30it/s] 43%|████▎     | 13/30 [00:01<00:01,  8.53it/s]                                               {'loss': 0.3571, 'grad_norm': 4.420238971710205, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  8.53it/s]                                               {'loss': 0.3287, 'grad_norm': 1.3664523363113403, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.53it/s] 50%|█████     | 15/30 [00:01<00:01,  9.75it/s]                                               {'loss': 0.4515, 'grad_norm': 2.028165578842163, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  9.75it/s]                                               {'loss': 0.4281, 'grad_norm': 2.076371908187866, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  9.75it/s] 57%|█████▋    | 17/30 [00:01<00:01, 10.67it/s]                                               {'loss': 0.1717, 'grad_norm': 2.366895914077759, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 10.67it/s]                                               {'loss': 0.5061, 'grad_norm': 3.7887749671936035, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 10.67it/s] 63%|██████▎   | 19/30 [00:02<00:00, 12.02it/s]                                               {'loss': 0.3125, 'grad_norm': 1.469964861869812, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 12.02it/s]                                               {'loss': 0.4534, 'grad_norm': 6.147665500640869, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 12.02it/s] 70%|███████   | 21/30 [00:02<00:00, 11.94it/s]                                               {'loss': 0.3275, 'grad_norm': 2.643531322479248, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.94it/s]                                               {'loss': 0.4357, 'grad_norm': 3.7565762996673584, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.94it/s] 77%|███████▋  | 23/30 [00:02<00:00, 10.99it/s]                                               {'loss': 0.089, 'grad_norm': 1.0914289951324463, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 10.99it/s]                                               {'loss': 0.3275, 'grad_norm': 3.0848886966705322, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 10.99it/s] 83%|████████▎ | 25/30 [00:02<00:00, 11.19it/s]                                               {'loss': 0.3494, 'grad_norm': 1.3951013088226318, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 11.19it/s]                                               {'loss': 0.4592, 'grad_norm': 2.231644630432129, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 11.19it/s] 90%|█████████ | 27/30 [00:02<00:00, 11.17it/s]                                               {'loss': 0.4171, 'grad_norm': 2.7006964683532715, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 11.17it/s]                                               {'loss': 0.0652, 'grad_norm': 0.935353696346283, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 11.17it/s] 97%|█████████▋| 29/30 [00:03<00:00, 11.02it/s]                                               {'loss': 0.2996, 'grad_norm': 0.9686684012413025, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 11.02it/s]                                               {'loss': 0.2912, 'grad_norm': 2.9309988021850586, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.02it/s]                                               {'train_runtime': 3.2025, 'train_samples_per_second': 132.707, 'train_steps_per_second': 9.368, 'train_loss': 0.364740922053655, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.02it/s]100%|██████████| 30/30 [00:03<00:00,  9.37it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.597, 'grad_norm': 2.6454758644104004, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02,  9.90it/s]  7%|▋         | 2/30 [00:00<00:03,  7.98it/s]                                              {'loss': 0.262, 'grad_norm': 2.0203864574432373, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.98it/s]                                              {'loss': 0.3754, 'grad_norm': 1.962077260017395, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.98it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.28it/s]                                              {'loss': 0.036, 'grad_norm': 0.4984554946422577, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  9.28it/s]                                              {'loss': 0.4417, 'grad_norm': 3.1169846057891846, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  9.28it/s] 20%|██        | 6/30 [00:00<00:02, 10.83it/s]                                              {'loss': 0.013, 'grad_norm': 0.36769360303878784, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 10.83it/s]                                              {'loss': 0.2442, 'grad_norm': 2.4711759090423584, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02, 10.83it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.35it/s]                                              {'loss': 0.998, 'grad_norm': 5.041591167449951, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  9.35it/s]                                              {'loss': 0.0446, 'grad_norm': 0.73678058385849, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:02,  9.35it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.23it/s]                                               {'loss': 0.2467, 'grad_norm': 2.231381893157959, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  9.23it/s] 37%|███▋      | 11/30 [00:01<00:02,  8.66it/s]                                               {'loss': 0.0563, 'grad_norm': 0.9514362215995789, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.66it/s]                                               {'loss': 0.0163, 'grad_norm': 0.47118374705314636, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.66it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.20it/s]                                               {'loss': 0.464, 'grad_norm': 4.021585464477539, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.20it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.03it/s]                                               {'loss': 0.025, 'grad_norm': 0.4860118329524994, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.03it/s] 50%|█████     | 15/30 [00:01<00:02,  6.99it/s]                                               {'loss': 0.6607, 'grad_norm': 2.502201795578003, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:02,  6.99it/s] 53%|█████▎    | 16/30 [00:01<00:02,  6.73it/s]                                               {'loss': 0.3237, 'grad_norm': 1.6124035120010376, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:02,  6.73it/s] 57%|█████▋    | 17/30 [00:02<00:01,  6.90it/s]                                               {'loss': 0.0282, 'grad_norm': 0.5266656279563904, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  6.90it/s]                                               {'loss': 0.7878, 'grad_norm': 7.78900671005249, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  6.90it/s] 63%|██████▎   | 19/30 [00:02<00:01,  7.20it/s]                                               {'loss': 0.0266, 'grad_norm': 0.5002536773681641, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.20it/s] 67%|██████▋   | 20/30 [00:02<00:01,  6.69it/s]                                               {'loss': 0.7565, 'grad_norm': 4.542727947235107, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  6.69it/s] 70%|███████   | 21/30 [00:02<00:01,  6.07it/s]                                               {'loss': 0.2925, 'grad_norm': 2.31994366645813, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  6.07it/s] 73%|███████▎  | 22/30 [00:02<00:01,  6.11it/s]                                               {'loss': 0.1881, 'grad_norm': 1.2058947086334229, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  6.11it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.69it/s]                                               {'loss': 0.2041, 'grad_norm': 1.222093105316162, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.69it/s] 80%|████████  | 24/30 [00:03<00:01,  5.81it/s]                                               {'loss': 0.0652, 'grad_norm': 1.163265585899353, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:01,  5.81it/s] 83%|████████▎ | 25/30 [00:03<00:01,  4.80it/s]                                               {'loss': 0.2387, 'grad_norm': 1.8145697116851807, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:01,  4.80it/s] 87%|████████▋ | 26/30 [00:03<00:00,  4.08it/s]                                               {'loss': 0.0571, 'grad_norm': 0.9115403294563293, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  4.08it/s] 90%|█████████ | 27/30 [00:04<00:00,  3.67it/s]                                               {'loss': 0.1913, 'grad_norm': 0.9480105638504028, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  3.67it/s] 93%|█████████▎| 28/30 [00:04<00:00,  3.35it/s]                                               {'loss': 0.421, 'grad_norm': 2.2453088760375977, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  3.35it/s] 97%|█████████▋| 29/30 [00:04<00:00,  3.42it/s]                                               {'loss': 0.1847, 'grad_norm': 1.7726999521255493, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  3.42it/s]                                               {'loss': 0.5081, 'grad_norm': 3.4838924407958984, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  3.42it/s]                                               {'train_runtime': 5.1302, 'train_samples_per_second': 82.844, 'train_steps_per_second': 5.848, 'train_loss': 0.2918125819414854, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  3.42it/s]100%|██████████| 30/30 [00:05<00:00,  5.85it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:79
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.37it/s]                                              {'loss': 0.7603, 'grad_norm': 3.4494130611419678, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.37it/s]  7%|▋         | 2/30 [00:00<00:08,  3.42it/s]                                              {'loss': 0.2017, 'grad_norm': 2.1603641510009766, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:08,  3.42it/s] 10%|█         | 3/30 [00:00<00:08,  3.37it/s]                                              {'loss': 0.3288, 'grad_norm': 2.366650104522705, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:08,  3.37it/s] 13%|█▎        | 4/30 [00:01<00:08,  3.24it/s]                                              {'loss': 0.0288, 'grad_norm': 0.5145721435546875, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  3.24it/s] 17%|█▋        | 5/30 [00:01<00:07,  3.25it/s]                                              {'loss': 0.2944, 'grad_norm': 1.2631537914276123, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:07,  3.25it/s] 20%|██        | 6/30 [00:01<00:05,  4.02it/s]                                              {'loss': 1.2923, 'grad_norm': 4.181050777435303, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.02it/s] 23%|██▎       | 7/30 [00:01<00:06,  3.63it/s]                                              {'loss': 0.213, 'grad_norm': 0.9775800108909607, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:06,  3.63it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.51it/s]                                              {'loss': 0.5145, 'grad_norm': 2.18796443939209, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.51it/s] 30%|███       | 9/30 [00:02<00:06,  3.45it/s]                                              {'loss': 0.5015, 'grad_norm': 1.9933747053146362, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:06,  3.45it/s] 33%|███▎      | 10/30 [00:02<00:05,  3.40it/s]                                               {'loss': 0.1364, 'grad_norm': 9.345993995666504, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  3.40it/s] 37%|███▋      | 11/30 [00:03<00:05,  3.42it/s]                                               {'loss': 0.0405, 'grad_norm': 0.4286499619483948, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:05,  3.42it/s] 40%|████      | 12/30 [00:03<00:04,  4.19it/s]                                               {'loss': 0.0425, 'grad_norm': 0.5035702586174011, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:04,  4.19it/s] 43%|████▎     | 13/30 [00:03<00:04,  3.86it/s]                                               {'loss': 0.4302, 'grad_norm': 2.0314669609069824, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:04,  3.86it/s] 47%|████▋     | 14/30 [00:03<00:04,  3.79it/s]                                               {'loss': 0.0407, 'grad_norm': 0.4270022511482239, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:04,  3.79it/s] 50%|█████     | 15/30 [00:04<00:04,  3.71it/s]                                               {'loss': 0.676, 'grad_norm': 1.9184496402740479, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:04,  3.71it/s] 53%|█████▎    | 16/30 [00:04<00:03,  3.55it/s]                                               {'loss': 0.2067, 'grad_norm': 0.8039335012435913, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:03,  3.55it/s] 57%|█████▋    | 17/30 [00:04<00:03,  3.40it/s]                                               {'loss': 0.0607, 'grad_norm': 0.5419349074363708, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:03,  3.40it/s] 60%|██████    | 18/30 [00:04<00:02,  4.03it/s]                                               {'loss': 0.0638, 'grad_norm': 0.6249538660049438, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.03it/s] 63%|██████▎   | 19/30 [00:05<00:02,  3.86it/s]                                               {'loss': 0.386, 'grad_norm': 1.4621729850769043, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  3.86it/s] 67%|██████▋   | 20/30 [00:05<00:02,  3.93it/s]                                               {'loss': 0.2208, 'grad_norm': 0.7112722992897034, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  3.93it/s] 70%|███████   | 21/30 [00:05<00:02,  4.11it/s]                                               {'loss': 0.0748, 'grad_norm': 0.7464629411697388, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:02,  4.11it/s] 73%|███████▎  | 22/30 [00:05<00:01,  4.21it/s]                                               {'loss': 0.4158, 'grad_norm': 1.250872254371643, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:01,  4.21it/s] 77%|███████▋  | 23/30 [00:06<00:01,  3.99it/s]                                               {'loss': 0.2367, 'grad_norm': 1.111541986465454, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  3.99it/s] 80%|████████  | 24/30 [00:06<00:01,  4.50it/s]                                               {'loss': 0.0754, 'grad_norm': 0.7478059530258179, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  4.50it/s] 83%|████████▎ | 25/30 [00:06<00:01,  4.24it/s]                                               {'loss': 0.0628, 'grad_norm': 0.5414530038833618, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:01,  4.24it/s] 87%|████████▋ | 26/30 [00:06<00:00,  4.26it/s]                                               {'loss': 0.3853, 'grad_norm': 1.0128237009048462, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:00,  4.26it/s] 90%|█████████ | 27/30 [00:07<00:00,  4.29it/s]                                               {'loss': 0.0694, 'grad_norm': 0.6495583057403564, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:07<00:00,  4.29it/s] 93%|█████████▎| 28/30 [00:07<00:00,  4.32it/s]                                               {'loss': 0.2171, 'grad_norm': 1.746461272239685, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  4.32it/s] 97%|█████████▋| 29/30 [00:07<00:00,  4.38it/s]                                               {'loss': 0.5412, 'grad_norm': 1.3263192176818848, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  4.38it/s]100%|██████████| 30/30 [00:07<00:00,  5.26it/s]                                               {'loss': 0.0519, 'grad_norm': 0.5903096199035645, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  5.26it/s]                                               {'train_runtime': 7.7684, 'train_samples_per_second': 54.709, 'train_steps_per_second': 3.862, 'train_loss': 0.2856730151300629, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  5.26it/s]100%|██████████| 30/30 [00:07<00:00,  3.86it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:14
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.59it/s]                                              {'loss': 0.6233, 'grad_norm': 1.8449561595916748, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.59it/s]  7%|▋         | 2/30 [00:00<00:06,  4.52it/s]                                              {'loss': 0.7694, 'grad_norm': 1.9882557392120361, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.52it/s] 10%|█         | 3/30 [00:00<00:05,  4.94it/s]                                              {'loss': 0.7529, 'grad_norm': 2.0362634658813477, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.94it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.47it/s]                                              {'loss': 0.6819, 'grad_norm': 1.488295316696167, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.47it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.46it/s]                                              {'loss': 0.7546, 'grad_norm': 1.2237343788146973, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.46it/s] 20%|██        | 6/30 [00:01<00:07,  3.06it/s]                                              {'loss': 0.9477, 'grad_norm': 3.4399173259735107, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:07,  3.06it/s] 23%|██▎       | 7/30 [00:01<00:06,  3.75it/s]                                              {'loss': 0.6322, 'grad_norm': 0.7290734648704529, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:06,  3.75it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.60it/s]                                              {'loss': 0.7205, 'grad_norm': 0.9496342539787292, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.60it/s] 30%|███       | 9/30 [00:01<00:04,  4.95it/s]                                              {'loss': 0.7008, 'grad_norm': 0.8572696447372437, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.95it/s]                                              {'loss': 0.6354, 'grad_norm': 0.6854708194732666, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.95it/s] 37%|███▋      | 11/30 [00:02<00:03,  6.24it/s]                                               {'loss': 0.6365, 'grad_norm': 0.7541205286979675, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  6.24it/s]                                               {'loss': 0.5829, 'grad_norm': 1.5881232023239136, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.24it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.64it/s]                                               {'loss': 0.6053, 'grad_norm': 0.9103790521621704, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.64it/s]                                               {'loss': 0.5797, 'grad_norm': 2.348768949508667, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.64it/s] 50%|█████     | 15/30 [00:02<00:02,  7.05it/s]                                               {'loss': 0.7825, 'grad_norm': 2.0760464668273926, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.05it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.49it/s]                                               {'loss': 0.6057, 'grad_norm': 0.611729085445404, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.49it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.47it/s]                                               {'loss': 0.594, 'grad_norm': 0.7327563762664795, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.47it/s]                                               {'loss': 0.8289, 'grad_norm': 2.6182563304901123, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  7.47it/s] 63%|██████▎   | 19/30 [00:03<00:01,  8.24it/s]                                               {'loss': 0.5047, 'grad_norm': 1.2171753644943237, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  8.24it/s] 67%|██████▋   | 20/30 [00:03<00:01,  8.55it/s]                                               {'loss': 0.6626, 'grad_norm': 1.1281208992004395, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  8.55it/s] 70%|███████   | 21/30 [00:03<00:01,  8.05it/s]                                               {'loss': 0.5897, 'grad_norm': 1.2002357244491577, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  8.05it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.99it/s]                                               {'loss': 0.6064, 'grad_norm': 0.8207638263702393, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.99it/s]                                               {'loss': 0.6194, 'grad_norm': 1.635932207107544, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.99it/s] 80%|████████  | 24/30 [00:03<00:00,  9.13it/s]                                               {'loss': 0.6257, 'grad_norm': 2.0976362228393555, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  9.13it/s] 83%|████████▎ | 25/30 [00:03<00:00,  9.05it/s]                                               {'loss': 0.5556, 'grad_norm': 1.1656849384307861, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  9.05it/s]                                               {'loss': 0.6226, 'grad_norm': 1.6275053024291992, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  9.05it/s] 90%|█████████ | 27/30 [00:03<00:00, 10.05it/s]                                               {'loss': 0.5884, 'grad_norm': 1.3005175590515137, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 10.05it/s]                                               {'loss': 0.5634, 'grad_norm': 1.911665439605713, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00, 10.05it/s] 97%|█████████▋| 29/30 [00:04<00:00, 10.47it/s]                                               {'loss': 0.5056, 'grad_norm': 1.4365798234939575, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00, 10.47it/s]                                               {'loss': 0.4615, 'grad_norm': 2.465639114379883, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00, 10.47it/s]                                               {'train_runtime': 4.2894, 'train_samples_per_second': 99.082, 'train_steps_per_second': 6.994, 'train_loss': 0.6446656386057535, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00, 10.47it/s]100%|██████████| 30/30 [00:04<00:00,  7.00it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.83it/s]                                              {'loss': 0.6939, 'grad_norm': 2.607740879058838, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.83it/s]  7%|▋         | 2/30 [00:00<00:03,  7.08it/s]                                              {'loss': 0.7678, 'grad_norm': 1.7827715873718262, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.08it/s] 10%|█         | 3/30 [00:00<00:04,  6.58it/s]                                              {'loss': 0.7037, 'grad_norm': 1.1735409498214722, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.58it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.44it/s]                                              {'loss': 0.7104, 'grad_norm': 1.708796501159668, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.44it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.36it/s]                                              {'loss': 0.7262, 'grad_norm': 1.8392068147659302, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.36it/s]                                              {'loss': 0.9345, 'grad_norm': 4.017821311950684, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.36it/s] 23%|██▎       | 7/30 [00:00<00:03,  7.31it/s]                                              {'loss': 0.665, 'grad_norm': 0.5131437182426453, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:03,  7.31it/s] 27%|██▋       | 8/30 [00:01<00:03,  7.00it/s]                                              {'loss': 0.7187, 'grad_norm': 1.2158094644546509, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  7.00it/s] 30%|███       | 9/30 [00:01<00:03,  6.76it/s]                                              {'loss': 0.6859, 'grad_norm': 0.6748810410499573, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.76it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.43it/s]                                               {'loss': 0.6737, 'grad_norm': 0.5330260992050171, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.43it/s] 37%|███▋      | 11/30 [00:01<00:03,  6.29it/s]                                               {'loss': 0.6831, 'grad_norm': 0.6693497896194458, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  6.29it/s]                                               {'loss': 0.6166, 'grad_norm': 1.4338245391845703, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.29it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.12it/s]                                               {'loss': 0.6701, 'grad_norm': 0.5155903100967407, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.12it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.22it/s]                                               {'loss': 0.6855, 'grad_norm': 0.4887976050376892, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.22it/s] 50%|█████     | 15/30 [00:02<00:02,  6.71it/s]                                               {'loss': 0.7551, 'grad_norm': 2.332260847091675, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.71it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.31it/s]                                               {'loss': 0.6521, 'grad_norm': 0.8025723695755005, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.31it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.13it/s]                                               {'loss': 0.6709, 'grad_norm': 0.5339898467063904, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.13it/s]                                               {'loss': 0.7436, 'grad_norm': 1.243334412574768, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.13it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.09it/s]                                               {'loss': 0.6345, 'grad_norm': 1.6332104206085205, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.09it/s] 67%|██████▋   | 20/30 [00:02<00:01,  6.71it/s]                                               {'loss': 0.7126, 'grad_norm': 0.522750198841095, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  6.71it/s] 70%|███████   | 21/30 [00:03<00:01,  6.13it/s]                                               {'loss': 0.6999, 'grad_norm': 0.3997850716114044, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  6.13it/s] 73%|███████▎  | 22/30 [00:03<00:01,  5.50it/s]                                               {'loss': 0.6932, 'grad_norm': 0.4264017343521118, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  5.50it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.16it/s]                                               {'loss': 0.7839, 'grad_norm': 1.6015523672103882, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.16it/s]                                               {'loss': 0.6705, 'grad_norm': 0.5417402386665344, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:01,  5.16it/s] 83%|████████▎ | 25/30 [00:03<00:00,  5.91it/s]                                               {'loss': 0.716, 'grad_norm': 0.6557140350341797, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  5.91it/s] 87%|████████▋ | 26/30 [00:03<00:00,  6.17it/s]                                               {'loss': 0.6734, 'grad_norm': 0.4147863984107971, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  6.17it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.62it/s]                                               {'loss': 0.6745, 'grad_norm': 0.3938503861427307, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.62it/s] 93%|█████████▎| 28/30 [00:04<00:00,  5.54it/s]                                               {'loss': 0.7129, 'grad_norm': 0.5063799619674683, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  5.54it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.45it/s]                                               {'loss': 0.6872, 'grad_norm': 0.8494353294372559, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.45it/s]                                               {'loss': 0.647, 'grad_norm': 0.4439639151096344, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.45it/s]                                               {'train_runtime': 4.861, 'train_samples_per_second': 87.43, 'train_steps_per_second': 6.172, 'train_loss': 0.7020768781503042, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.45it/s]100%|██████████| 30/30 [00:04<00:00,  6.17it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.31it/s]                                              {'loss': 0.7415, 'grad_norm': 2.1558001041412354, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.31it/s]  7%|▋         | 2/30 [00:00<00:08,  3.48it/s]                                              {'loss': 0.7313, 'grad_norm': 1.399830937385559, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:08,  3.48it/s] 10%|█         | 3/30 [00:00<00:07,  3.38it/s]                                              {'loss': 0.6636, 'grad_norm': 1.628257393836975, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.38it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.42it/s]                                              {'loss': 0.6463, 'grad_norm': 1.4405900239944458, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.42it/s] 17%|█▋        | 5/30 [00:01<00:07,  3.49it/s]                                              {'loss': 0.7584, 'grad_norm': 1.7654401063919067, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:07,  3.49it/s]                                              {'loss': 0.7807, 'grad_norm': 2.9030628204345703, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.49it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.20it/s]                                              {'loss': 0.6313, 'grad_norm': 0.9649324417114258, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.20it/s] 27%|██▋       | 8/30 [00:02<00:05,  3.92it/s]                                              {'loss': 0.6987, 'grad_norm': 0.6050338745117188, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:05,  3.92it/s] 30%|███       | 9/30 [00:02<00:05,  3.82it/s]                                              {'loss': 0.6867, 'grad_norm': 0.7540744543075562, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  3.82it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.02it/s]                                               {'loss': 0.709, 'grad_norm': 0.5779072642326355, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.02it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.18it/s]                                               {'loss': 0.6612, 'grad_norm': 0.4586382508277893, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.18it/s]                                               {'loss': 0.6574, 'grad_norm': 3.3864657878875732, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:04,  4.18it/s] 43%|████▎     | 13/30 [00:03<00:03,  5.35it/s]                                               {'loss': 0.6262, 'grad_norm': 0.7744380235671997, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  5.35it/s] 47%|████▋     | 14/30 [00:03<00:03,  5.10it/s]                                               {'loss': 0.6785, 'grad_norm': 0.5336123108863831, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  5.10it/s] 50%|█████     | 15/30 [00:03<00:02,  5.01it/s]                                               {'loss': 0.6811, 'grad_norm': 0.7663955688476562, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.01it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.24it/s]                                               {'loss': 0.6246, 'grad_norm': 0.567841649055481, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.24it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.73it/s]                                               {'loss': 0.5854, 'grad_norm': 1.013525128364563, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.73it/s]                                               {'loss': 0.5737, 'grad_norm': 1.1790674924850464, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.73it/s] 63%|██████▎   | 19/30 [00:03<00:01,  7.36it/s]                                               {'loss': 0.6729, 'grad_norm': 1.0824164152145386, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:01,  7.36it/s] 67%|██████▋   | 20/30 [00:04<00:01,  7.35it/s]                                               {'loss': 0.7226, 'grad_norm': 0.6809068322181702, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  7.35it/s] 70%|███████   | 21/30 [00:04<00:01,  6.44it/s]                                               {'loss': 0.6538, 'grad_norm': 0.7453944683074951, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  6.44it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.93it/s]                                               {'loss': 0.59, 'grad_norm': 0.6509373188018799, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.93it/s] 77%|███████▋  | 23/30 [00:04<00:01,  5.65it/s]                                               {'loss': 0.6473, 'grad_norm': 0.7360918521881104, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  5.65it/s]                                               {'loss': 0.6734, 'grad_norm': 1.2048612833023071, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  5.65it/s] 83%|████████▎ | 25/30 [00:05<00:00,  6.24it/s]                                               {'loss': 0.6996, 'grad_norm': 0.713458776473999, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  6.24it/s] 87%|████████▋ | 26/30 [00:05<00:00,  6.38it/s]                                               {'loss': 0.6629, 'grad_norm': 0.5962964296340942, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  6.38it/s] 90%|█████████ | 27/30 [00:05<00:00,  6.38it/s]                                               {'loss': 0.6257, 'grad_norm': 0.7577196955680847, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  6.38it/s] 93%|█████████▎| 28/30 [00:05<00:00,  6.28it/s]                                               {'loss': 0.669, 'grad_norm': 0.7064192295074463, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  6.28it/s] 97%|█████████▋| 29/30 [00:05<00:00,  6.42it/s]                                               {'loss': 0.6493, 'grad_norm': 0.9365267753601074, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  6.42it/s]                                               {'loss': 0.6642, 'grad_norm': 1.6823147535324097, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.42it/s]                                               {'train_runtime': 5.804, 'train_samples_per_second': 73.225, 'train_steps_per_second': 5.169, 'train_loss': 0.6688760181268056, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.42it/s]100%|██████████| 30/30 [00:05<00:00,  5.17it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:19
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.65it/s]                                              {'loss': 0.6957, 'grad_norm': 1.8211907148361206, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.65it/s]  7%|▋         | 2/30 [00:00<00:05,  5.51it/s]                                              {'loss': 0.7805, 'grad_norm': 2.3340682983398438, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.51it/s] 10%|█         | 3/30 [00:00<00:05,  5.22it/s]                                              {'loss': 0.6878, 'grad_norm': 1.017187237739563, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  5.22it/s] 13%|█▎        | 4/30 [00:00<00:05,  5.00it/s]                                              {'loss': 0.6714, 'grad_norm': 0.8439154624938965, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  5.00it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.48it/s]                                              {'loss': 0.7025, 'grad_norm': 0.884887158870697, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.48it/s] 20%|██        | 6/30 [00:01<00:04,  5.51it/s]                                              {'loss': 0.8886, 'grad_norm': 3.0184812545776367, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.51it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.14it/s]                                              {'loss': 0.6776, 'grad_norm': 0.5613775849342346, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.14it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.35it/s]                                              {'loss': 0.7137, 'grad_norm': 0.6190128922462463, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.35it/s] 30%|███       | 9/30 [00:01<00:04,  5.10it/s]                                              {'loss': 0.6937, 'grad_norm': 0.8844289183616638, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  5.10it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.35it/s]                                               {'loss': 0.6751, 'grad_norm': 0.47976788878440857, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.35it/s] 37%|███▋      | 11/30 [00:01<00:03,  6.15it/s]                                               {'loss': 0.6744, 'grad_norm': 0.5057736039161682, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  6.15it/s]                                               {'loss': 0.621, 'grad_norm': 2.0052313804626465, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.15it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.23it/s]                                               {'loss': 0.6337, 'grad_norm': 1.0504440069198608, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.23it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.68it/s]                                               {'loss': 0.6976, 'grad_norm': 0.7510248422622681, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.68it/s]                                               {'loss': 0.7624, 'grad_norm': 1.4047573804855347, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.68it/s] 53%|█████▎    | 16/30 [00:02<00:01,  8.35it/s]                                               {'loss': 0.6588, 'grad_norm': 0.7186136841773987, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.35it/s] 57%|█████▋    | 17/30 [00:02<00:01,  8.23it/s]                                               {'loss': 0.65, 'grad_norm': 0.816737174987793, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  8.23it/s] 60%|██████    | 18/30 [00:02<00:01,  7.65it/s]                                               {'loss': 0.6335, 'grad_norm': 1.193264365196228, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.65it/s] 63%|██████▎   | 19/30 [00:03<00:01,  6.71it/s]                                               {'loss': 0.6668, 'grad_norm': 0.8278419971466064, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  6.71it/s] 67%|██████▋   | 20/30 [00:03<00:01,  6.84it/s]                                               {'loss': 0.6768, 'grad_norm': 0.49646487832069397, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  6.84it/s]                                               {'loss': 0.6255, 'grad_norm': 0.8234760165214539, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  6.84it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.99it/s]                                               {'loss': 0.6379, 'grad_norm': 0.6862037181854248, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.99it/s]                                               {'loss': 0.6964, 'grad_norm': 1.19809889793396, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.99it/s] 80%|████████  | 24/30 [00:03<00:00,  8.72it/s]                                               {'loss': 0.5895, 'grad_norm': 1.579562783241272, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  8.72it/s] 83%|████████▎ | 25/30 [00:03<00:00,  7.83it/s]                                               {'loss': 0.6241, 'grad_norm': 0.8286128640174866, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  7.83it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.13it/s]                                               {'loss': 0.6145, 'grad_norm': 0.665858268737793, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.13it/s] 90%|█████████ | 27/30 [00:04<00:00,  6.85it/s]                                               {'loss': 0.6197, 'grad_norm': 0.9607662558555603, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  6.85it/s] 93%|█████████▎| 28/30 [00:04<00:00,  7.05it/s]                                               {'loss': 0.6638, 'grad_norm': 0.8013854026794434, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  7.05it/s] 97%|█████████▋| 29/30 [00:04<00:00,  6.67it/s]                                               {'loss': 0.6319, 'grad_norm': 1.2896896600723267, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  6.67it/s]                                               {'loss': 0.4699, 'grad_norm': 1.487750768661499, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.67it/s]                                               {'train_runtime': 4.658, 'train_samples_per_second': 91.241, 'train_steps_per_second': 6.441, 'train_loss': 0.66782599290212, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.67it/s]100%|██████████| 30/30 [00:04<00:00,  6.45it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:28
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:10,  2.87it/s]                                              {'loss': 0.7349, 'grad_norm': 3.5849411487579346, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:10,  2.87it/s]  7%|▋         | 2/30 [00:00<00:09,  2.98it/s]                                              {'loss': 0.7605, 'grad_norm': 1.679738998413086, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:09,  2.98it/s] 10%|█         | 3/30 [00:01<00:08,  3.00it/s]                                              {'loss': 0.7104, 'grad_norm': 0.9223361015319824, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:08,  3.00it/s] 13%|█▎        | 4/30 [00:01<00:08,  2.93it/s]                                              {'loss': 0.6873, 'grad_norm': 0.9691614508628845, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  2.93it/s] 17%|█▋        | 5/30 [00:01<00:08,  3.02it/s]                                              {'loss': 0.704, 'grad_norm': 1.0968773365020752, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:08,  3.02it/s] 20%|██        | 6/30 [00:02<00:07,  3.01it/s]                                              {'loss': 0.7655, 'grad_norm': 2.3725624084472656, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:07,  3.01it/s] 23%|██▎       | 7/30 [00:02<00:07,  3.09it/s]                                              {'loss': 0.6651, 'grad_norm': 0.5350676774978638, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:07,  3.09it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.37it/s]                                              {'loss': 0.6636, 'grad_norm': 0.905047595500946, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.37it/s] 30%|███       | 9/30 [00:02<00:06,  3.46it/s]                                              {'loss': 0.6406, 'grad_norm': 0.6509366035461426, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:06,  3.46it/s] 33%|███▎      | 10/30 [00:03<00:05,  3.44it/s]                                               {'loss': 0.6812, 'grad_norm': 0.4001210927963257, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:05,  3.44it/s] 37%|███▋      | 11/30 [00:03<00:05,  3.56it/s]                                               {'loss': 0.6911, 'grad_norm': 0.5690896511077881, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:05,  3.56it/s] 40%|████      | 12/30 [00:03<00:04,  4.20it/s]                                               {'loss': 0.7951, 'grad_norm': 3.378309488296509, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:04,  4.20it/s] 43%|████▎     | 13/30 [00:03<00:04,  4.01it/s]                                               {'loss': 0.6946, 'grad_norm': 0.8445691466331482, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:04,  4.01it/s] 47%|████▋     | 14/30 [00:04<00:03,  4.05it/s]                                               {'loss': 0.7, 'grad_norm': 0.8166993856430054, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:03,  4.05it/s] 50%|█████     | 15/30 [00:04<00:03,  3.91it/s]                                               {'loss': 0.5345, 'grad_norm': 1.3888665437698364, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:03,  3.91it/s] 53%|█████▎    | 16/30 [00:04<00:03,  3.79it/s]                                               {'loss': 0.6501, 'grad_norm': 0.722678005695343, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:03,  3.79it/s] 57%|█████▋    | 17/30 [00:04<00:03,  3.74it/s]                                               {'loss': 0.6826, 'grad_norm': 0.7279370427131653, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:03,  3.74it/s] 60%|██████    | 18/30 [00:05<00:03,  3.61it/s]                                               {'loss': 0.6256, 'grad_norm': 0.9493667483329773, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:03,  3.61it/s] 63%|██████▎   | 19/30 [00:05<00:03,  3.46it/s]                                               {'loss': 0.7141, 'grad_norm': 0.9336743950843811, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:03,  3.46it/s] 67%|██████▋   | 20/30 [00:05<00:02,  3.62it/s]                                               {'loss': 0.6451, 'grad_norm': 0.4233340322971344, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  3.62it/s] 70%|███████   | 21/30 [00:05<00:02,  3.78it/s]                                               {'loss': 0.6609, 'grad_norm': 0.46139997243881226, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:02,  3.78it/s] 73%|███████▎  | 22/30 [00:06<00:01,  4.03it/s]                                               {'loss': 0.6496, 'grad_norm': 0.5686037540435791, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:06<00:01,  4.03it/s] 77%|███████▋  | 23/30 [00:06<00:01,  4.49it/s]                                               {'loss': 0.6181, 'grad_norm': 1.127440094947815, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  4.49it/s] 80%|████████  | 24/30 [00:06<00:01,  4.60it/s]                                               {'loss': 0.5834, 'grad_norm': 0.7594359517097473, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  4.60it/s] 83%|████████▎ | 25/30 [00:06<00:01,  4.27it/s]                                               {'loss': 0.6872, 'grad_norm': 0.6630369424819946, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:01,  4.27it/s] 87%|████████▋ | 26/30 [00:07<00:00,  4.40it/s]                                               {'loss': 0.649, 'grad_norm': 0.8593541383743286, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:07<00:00,  4.40it/s] 90%|█████████ | 27/30 [00:07<00:00,  4.49it/s]                                               {'loss': 0.6488, 'grad_norm': 0.5865784883499146, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:07<00:00,  4.49it/s] 93%|█████████▎| 28/30 [00:07<00:00,  4.28it/s]                                               {'loss': 0.6198, 'grad_norm': 0.5750671625137329, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  4.28it/s] 97%|█████████▋| 29/30 [00:07<00:00,  4.20it/s]                                               {'loss': 0.5588, 'grad_norm': 0.9286075830459595, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  4.20it/s]100%|██████████| 30/30 [00:07<00:00,  4.26it/s]                                               {'loss': 0.5657, 'grad_norm': 0.9545892477035522, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  4.26it/s]                                               {'train_runtime': 8.362, 'train_samples_per_second': 50.825, 'train_steps_per_second': 3.588, 'train_loss': 0.6662342806657156, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.26it/s]100%|██████████| 30/30 [00:08<00:00,  3.59it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:94
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.94it/s]                                              {'loss': 0.7033, 'grad_norm': 2.94583797454834, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.94it/s]  7%|▋         | 2/30 [00:00<00:04,  6.83it/s]                                              {'loss': 0.7628, 'grad_norm': 1.510892629623413, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.83it/s] 10%|█         | 3/30 [00:00<00:03,  6.83it/s]                                              {'loss': 0.7474, 'grad_norm': 1.7129132747650146, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  6.83it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.27it/s]                                              {'loss': 0.6946, 'grad_norm': 1.4594712257385254, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.27it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.37it/s]                                              {'loss': 0.7248, 'grad_norm': 1.366557002067566, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.37it/s] 20%|██        | 6/30 [00:01<00:05,  4.21it/s]                                              {'loss': 0.8404, 'grad_norm': 4.1406331062316895, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.21it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.82it/s]                                              {'loss': 0.6857, 'grad_norm': 0.6561694145202637, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.82it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.41it/s]                                              {'loss': 0.7018, 'grad_norm': 11.168302536010742, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.41it/s] 30%|███       | 9/30 [00:01<00:03,  5.80it/s]                                              {'loss': 0.7084, 'grad_norm': 1.1459858417510986, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.80it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.02it/s]                                               {'loss': 0.6735, 'grad_norm': 0.7290362119674683, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.02it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.61it/s]                                               {'loss': 0.6691, 'grad_norm': 0.9316948652267456, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.61it/s]                                               {'loss': 0.5846, 'grad_norm': 1.7549161911010742, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.61it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.60it/s]                                               {'loss': 0.6931, 'grad_norm': 2.519859790802002, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.60it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.23it/s]                                               {'loss': 0.6878, 'grad_norm': 1.5592546463012695, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.23it/s] 50%|█████     | 15/30 [00:02<00:02,  7.03it/s]                                               {'loss': 0.8443, 'grad_norm': 5.916346073150635, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.03it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.09it/s]                                               {'loss': 0.6759, 'grad_norm': 2.64162278175354, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.09it/s] 57%|█████▋    | 17/30 [00:02<00:01,  6.97it/s]                                               {'loss': 0.739, 'grad_norm': 3.394700050354004, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  6.97it/s]                                               {'loss': 0.6425, 'grad_norm': 6.178571701049805, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  6.97it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.39it/s]                                               {'loss': 0.7794, 'grad_norm': 2.9762566089630127, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.39it/s]                                               {'loss': 0.6962, 'grad_norm': 2.1089439392089844, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.39it/s] 70%|███████   | 21/30 [00:02<00:00,  9.57it/s]                                               {'loss': 0.7168, 'grad_norm': 1.7154062986373901, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00,  9.57it/s]                                               {'loss': 0.7061, 'grad_norm': 1.6530653238296509, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00,  9.57it/s] 77%|███████▋  | 23/30 [00:03<00:00, 10.06it/s]                                               {'loss': 0.6654, 'grad_norm': 1.7291982173919678, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00, 10.06it/s]                                               {'loss': 0.6882, 'grad_norm': 2.368823766708374, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00, 10.06it/s] 83%|████████▎ | 25/30 [00:03<00:00, 10.86it/s]                                               {'loss': 0.6735, 'grad_norm': 2.128239154815674, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 10.86it/s]                                               {'loss': 0.668, 'grad_norm': 1.7233988046646118, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 10.86it/s] 90%|█████████ | 27/30 [00:03<00:00, 11.31it/s]                                               {'loss': 0.6753, 'grad_norm': 1.6947307586669922, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 11.31it/s]                                               {'loss': 0.6938, 'grad_norm': 1.9698878526687622, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 11.31it/s] 97%|█████████▋| 29/30 [00:03<00:00, 11.79it/s]                                               {'loss': 0.7114, 'grad_norm': 2.21220326423645, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 11.79it/s]                                               {'loss': 0.6679, 'grad_norm': 6.169670104980469, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.79it/s]                                               {'train_runtime': 3.7667, 'train_samples_per_second': 112.83, 'train_steps_per_second': 7.964, 'train_loss': 0.704038514693578, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.79it/s]100%|██████████| 30/30 [00:03<00:00,  7.97it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.6763, 'grad_norm': 1.5054047107696533, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 10.21it/s]  7%|▋         | 2/30 [00:00<00:02, 11.14it/s]                                              {'loss': 0.4809, 'grad_norm': 1.9361212253570557, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 11.14it/s]                                              {'loss': 0.557, 'grad_norm': 3.4247870445251465, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.14it/s] 13%|█▎        | 4/30 [00:00<00:02, 11.74it/s]                                              {'loss': 1.0056, 'grad_norm': 4.815847396850586, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.74it/s]                                              {'loss': 1.1913, 'grad_norm': 5.968902111053467, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.74it/s] 20%|██        | 6/30 [00:00<00:01, 13.23it/s]                                              {'loss': 1.311, 'grad_norm': 7.152474880218506, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 13.23it/s]                                              {'loss': 0.435, 'grad_norm': 1.0444608926773071, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 13.23it/s] 27%|██▋       | 8/30 [00:00<00:01, 12.42it/s]                                              {'loss': 0.7678, 'grad_norm': 2.0708746910095215, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 12.42it/s]                                              {'loss': 0.6236, 'grad_norm': 1.27133309841156, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 12.42it/s] 33%|███▎      | 10/30 [00:00<00:01, 12.16it/s]                                               {'loss': 0.6416, 'grad_norm': 0.9168925285339355, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 12.16it/s]                                               {'loss': 0.622, 'grad_norm': 1.036588430404663, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.16it/s] 40%|████      | 12/30 [00:00<00:01, 13.53it/s]                                               {'loss': 0.4993, 'grad_norm': 3.375200033187866, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 13.53it/s]                                               {'loss': 0.6874, 'grad_norm': 1.0882688760757446, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 13.53it/s] 47%|████▋     | 14/30 [00:01<00:01, 13.25it/s]                                               {'loss': 0.5816, 'grad_norm': 1.190299153327942, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.25it/s]                                               {'loss': 0.8524, 'grad_norm': 2.644181489944458, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.25it/s] 53%|█████▎    | 16/30 [00:01<00:01, 12.68it/s]                                               {'loss': 0.5343, 'grad_norm': 1.1088371276855469, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 12.68it/s]                                               {'loss': 0.4531, 'grad_norm': 1.5029096603393555, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.68it/s] 60%|██████    | 18/30 [00:01<00:01, 11.63it/s]                                               {'loss': 0.3611, 'grad_norm': 2.1105563640594482, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 11.63it/s]                                               {'loss': 0.5043, 'grad_norm': 0.9198812246322632, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 11.63it/s] 67%|██████▋   | 20/30 [00:01<00:01,  9.00it/s]                                               {'loss': 0.6804, 'grad_norm': 1.0222090482711792, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:01,  9.00it/s]                                               {'loss': 0.6081, 'grad_norm': 1.129929542541504, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00,  9.00it/s] 73%|███████▎  | 22/30 [00:02<00:00,  8.54it/s]                                               {'loss': 0.5061, 'grad_norm': 1.0625649690628052, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  8.54it/s]                                               {'loss': 0.5862, 'grad_norm': 1.1747937202453613, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  8.54it/s] 80%|████████  | 24/30 [00:02<00:00, 10.10it/s]                                               {'loss': 0.5063, 'grad_norm': 1.3563638925552368, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 10.10it/s]                                               {'loss': 0.6374, 'grad_norm': 1.2662829160690308, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 10.10it/s] 87%|████████▋ | 26/30 [00:02<00:00,  8.76it/s]                                               {'loss': 0.5139, 'grad_norm': 0.8656024932861328, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00,  8.76it/s]                                               {'loss': 0.5234, 'grad_norm': 1.203842282295227, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00,  8.76it/s] 93%|█████████▎| 28/30 [00:02<00:00,  7.48it/s]                                               {'loss': 0.5281, 'grad_norm': 0.7491782903671265, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00,  7.48it/s] 97%|█████████▋| 29/30 [00:03<00:00,  6.71it/s]                                               {'loss': 0.6621, 'grad_norm': 1.8586595058441162, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  6.71it/s]                                               {'loss': 0.4323, 'grad_norm': 1.1588281393051147, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  6.71it/s]                                               {'train_runtime': 3.2711, 'train_samples_per_second': 129.927, 'train_steps_per_second': 9.171, 'train_loss': 0.6323279827833176, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  6.71it/s]100%|██████████| 30/30 [00:03<00:00,  9.19it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
the shape of base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.weight is (768, 768)
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  3%|▎         | 2/66 [00:00<00:03, 18.54it/s]  6%|▌         | 4/66 [00:00<00:05, 12.15it/s]  9%|▉         | 6/66 [00:00<00:05, 11.80it/s] 12%|█▏        | 8/66 [00:00<00:05, 11.25it/s] 15%|█▌        | 10/66 [00:00<00:05, 11.13it/s] 18%|█▊        | 12/66 [00:01<00:04, 11.27it/s] 21%|██        | 14/66 [00:01<00:04, 10.75it/s] 24%|██▍       | 16/66 [00:01<00:04, 10.49it/s] 27%|██▋       | 18/66 [00:01<00:04, 11.51it/s] 30%|███       | 20/66 [00:01<00:03, 12.07it/s] 33%|███▎      | 22/66 [00:01<00:03, 12.75it/s] 36%|███▋      | 24/66 [00:01<00:03, 13.51it/s] 39%|███▉      | 26/66 [00:02<00:02, 13.71it/s] 42%|████▏     | 28/66 [00:02<00:02, 14.20it/s] 45%|████▌     | 30/66 [00:02<00:02, 14.57it/s] 48%|████▊     | 32/66 [00:02<00:02, 14.17it/s] 52%|█████▏    | 34/66 [00:02<00:02, 14.22it/s] 55%|█████▍    | 36/66 [00:02<00:02, 14.62it/s] 58%|█████▊    | 38/66 [00:02<00:01, 14.91it/s] 61%|██████    | 40/66 [00:03<00:01, 15.10it/s] 64%|██████▎   | 42/66 [00:03<00:01, 14.76it/s] 67%|██████▋   | 44/66 [00:03<00:01, 14.57it/s] 70%|██████▉   | 46/66 [00:03<00:01, 14.95it/s] 73%|███████▎  | 48/66 [00:03<00:01, 14.76it/s] 76%|███████▌  | 50/66 [00:03<00:01, 14.66it/s] 79%|███████▉  | 52/66 [00:03<00:00, 14.69it/s] 82%|████████▏ | 54/66 [00:04<00:00, 14.68it/s] 85%|████████▍ | 56/66 [00:04<00:00, 14.59it/s] 88%|████████▊ | 58/66 [00:04<00:00, 14.60it/s] 91%|█████████ | 60/66 [00:04<00:00, 14.74it/s] 94%|█████████▍| 62/66 [00:04<00:00, 14.88it/s] 97%|█████████▋| 64/66 [00:04<00:00, 15.21it/s]100%|██████████| 66/66 [00:04<00:00, 13.86it/s]
{'eval_loss': 0.6098011136054993, 'eval_model_preparation_time': 0.006, 'eval_acc': 0.6912751677852349, 'eval_runtime': 4.8512, 'eval_samples_per_second': 214.997, 'eval_steps_per_second': 13.605}
ROUND:5
CLIENT:79
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.95it/s]                                              {'loss': 0.7367, 'grad_norm': 3.3453893661499023, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.95it/s]  7%|▋         | 2/30 [00:00<00:05,  4.67it/s]                                              {'loss': 0.1831, 'grad_norm': 2.0340633392333984, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  4.67it/s] 10%|█         | 3/30 [00:00<00:05,  4.63it/s]                                              {'loss': 0.3171, 'grad_norm': 2.774193286895752, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.63it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.87it/s]                                              {'loss': 0.0292, 'grad_norm': 0.4321573078632355, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.87it/s] 17%|█▋        | 5/30 [00:01<00:04,  5.04it/s]                                              {'loss': 0.2866, 'grad_norm': 1.2004679441452026, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:04,  5.04it/s]                                              {'loss': 1.2868, 'grad_norm': 4.159916877746582, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.04it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.10it/s]                                              {'loss': 0.2155, 'grad_norm': 1.2121163606643677, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.10it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.55it/s]                                              {'loss': 0.5146, 'grad_norm': 2.1500096321105957, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.55it/s] 30%|███       | 9/30 [00:01<00:03,  5.31it/s]                                              {'loss': 0.5093, 'grad_norm': 2.0181307792663574, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.31it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.32it/s]                                               {'loss': 0.1653, 'grad_norm': 16.749177932739258, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.32it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.19it/s]                                               {'loss': 0.0392, 'grad_norm': 1.88465416431427, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.19it/s]                                               {'loss': 0.0436, 'grad_norm': 0.49505335092544556, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.19it/s] 43%|████▎     | 13/30 [00:02<00:02,  5.69it/s]                                               {'loss': 0.4244, 'grad_norm': 1.5083396434783936, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  5.69it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.40it/s]                                               {'loss': 0.0407, 'grad_norm': 0.41738542914390564, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.40it/s] 50%|█████     | 15/30 [00:02<00:02,  5.19it/s]                                               {'loss': 0.6668, 'grad_norm': 1.7528806924819946, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.19it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.05it/s]                                               {'loss': 0.2055, 'grad_norm': 0.744036078453064, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.05it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.59it/s]                                               {'loss': 0.0591, 'grad_norm': 0.5142228007316589, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.59it/s]                                               {'loss': 0.0647, 'grad_norm': 0.6132544279098511, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.59it/s] 63%|██████▎   | 19/30 [00:03<00:01,  5.51it/s]                                               {'loss': 0.3789, 'grad_norm': 1.169943928718567, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  5.51it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.22it/s]                                               {'loss': 0.224, 'grad_norm': 0.655364453792572, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.22it/s] 70%|███████   | 21/30 [00:04<00:01,  5.01it/s]                                               {'loss': 0.0766, 'grad_norm': 0.7348372340202332, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.01it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.85it/s]                                               {'loss': 0.4188, 'grad_norm': 2.2182931900024414, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.85it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.39it/s]                                               {'loss': 0.2305, 'grad_norm': 0.8945853114128113, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.39it/s] 80%|████████  | 24/30 [00:04<00:01,  5.06it/s]                                               {'loss': 0.0797, 'grad_norm': 0.7004655599594116, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  5.06it/s] 83%|████████▎ | 25/30 [00:04<00:01,  4.55it/s]                                               {'loss': 0.0623, 'grad_norm': 0.5032055377960205, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:01,  4.55it/s] 87%|████████▋ | 26/30 [00:05<00:00,  4.32it/s]                                               {'loss': 0.394, 'grad_norm': 0.8061059713363647, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  4.32it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.02it/s]                                               {'loss': 0.0764, 'grad_norm': 0.6900414228439331, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.02it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.18it/s]                                               {'loss': 0.2035, 'grad_norm': 0.6481911540031433, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.18it/s] 97%|█████████▋| 29/30 [00:05<00:00,  4.29it/s]                                               {'loss': 0.5124, 'grad_norm': 1.2704981565475464, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  4.29it/s]                                               {'loss': 0.0495, 'grad_norm': 0.49968788027763367, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.29it/s]                                               {'train_runtime': 6.1251, 'train_samples_per_second': 69.387, 'train_steps_per_second': 4.898, 'train_loss': 0.28315663101772465, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.29it/s]100%|██████████| 30/30 [00:06<00:00,  4.90it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:75
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.83it/s]                                              {'loss': 0.7394, 'grad_norm': 2.6751978397369385, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.83it/s]  7%|▋         | 2/30 [00:00<00:07,  3.76it/s]                                              {'loss': 0.4824, 'grad_norm': 1.7061686515808105, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.76it/s] 10%|█         | 3/30 [00:00<00:06,  4.00it/s]                                              {'loss': 0.5001, 'grad_norm': 1.167237639427185, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.00it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.47it/s]                                              {'loss': 0.8537, 'grad_norm': 5.51491117477417, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.47it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.57it/s]                                              {'loss': 0.9704, 'grad_norm': 5.286900520324707, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.57it/s]                                              {'loss': 0.7512, 'grad_norm': 5.020430088043213, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.57it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.47it/s]                                              {'loss': 0.468, 'grad_norm': 1.3139573335647583, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.47it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.12it/s]                                              {'loss': 0.6859, 'grad_norm': 1.6514172554016113, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.12it/s] 30%|███       | 9/30 [00:01<00:04,  4.95it/s]                                              {'loss': 0.6909, 'grad_norm': 2.0522477626800537, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.95it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.78it/s]                                               {'loss': 0.6292, 'grad_norm': 2.612609624862671, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.78it/s] 37%|███▋      | 11/30 [00:02<00:03,  4.81it/s]                                               {'loss': 0.6687, 'grad_norm': 1.6797223091125488, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  4.81it/s]                                               {'loss': 0.5129, 'grad_norm': 1.934879183769226, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.81it/s] 43%|████▎     | 13/30 [00:02<00:02,  5.93it/s]                                               {'loss': 0.5904, 'grad_norm': 1.7083320617675781, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  5.93it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.08it/s]                                               {'loss': 0.6339, 'grad_norm': 4.521571636199951, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.08it/s] 50%|█████     | 15/30 [00:02<00:02,  6.25it/s]                                               {'loss': 0.8874, 'grad_norm': 4.071991920471191, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.25it/s] 53%|█████▎    | 16/30 [00:03<00:02,  6.38it/s]                                               {'loss': 0.5545, 'grad_norm': 3.0117034912109375, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  6.38it/s]                                               {'loss': 0.6143, 'grad_norm': 1.1612945795059204, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  6.38it/s] 60%|██████    | 18/30 [00:03<00:01,  8.68it/s]                                               {'loss': 0.6977, 'grad_norm': 5.878935813903809, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  8.68it/s]                                               {'loss': 0.6397, 'grad_norm': 5.884174823760986, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  8.68it/s] 67%|██████▋   | 20/30 [00:03<00:01,  9.61it/s]                                               {'loss': 0.6553, 'grad_norm': 0.9594093561172485, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  9.61it/s]                                               {'loss': 0.7359, 'grad_norm': 2.776790142059326, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:00,  9.61it/s] 73%|███████▎  | 22/30 [00:03<00:00, 10.21it/s]                                               {'loss': 0.6423, 'grad_norm': 3.510446786880493, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00, 10.21it/s]                                               {'loss': 0.6555, 'grad_norm': 2.935887098312378, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00, 10.21it/s] 80%|████████  | 24/30 [00:03<00:00, 11.99it/s]                                               {'loss': 0.6851, 'grad_norm': 2.7303659915924072, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00, 11.99it/s]                                               {'loss': 0.6881, 'grad_norm': 1.1361738443374634, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 11.99it/s] 87%|████████▋ | 26/30 [00:03<00:00,  9.55it/s]                                               {'loss': 0.5043, 'grad_norm': 1.5557823181152344, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  9.55it/s]                                               {'loss': 0.7546, 'grad_norm': 1.437834620475769, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  9.55it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.92it/s]                                               {'loss': 0.6808, 'grad_norm': 5.453959941864014, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.92it/s] 97%|█████████▋| 29/30 [00:04<00:00,  6.11it/s]                                               {'loss': 0.7433, 'grad_norm': 3.6319470405578613, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  6.11it/s]100%|██████████| 30/30 [00:04<00:00,  6.26it/s]                                               {'loss': 0.5849, 'grad_norm': 1.189568281173706, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.26it/s]                                               {'train_runtime': 4.8668, 'train_samples_per_second': 87.327, 'train_steps_per_second': 6.164, 'train_loss': 0.6633714238802592, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.26it/s]100%|██████████| 30/30 [00:04<00:00,  6.17it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.23it/s]                                              {'loss': 0.6081, 'grad_norm': 1.4433622360229492, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.23it/s]                                              {'loss': 0.7217, 'grad_norm': 1.6530789136886597, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.23it/s] 10%|█         | 3/30 [00:00<00:02, 10.90it/s]                                              {'loss': 0.6687, 'grad_norm': 0.8021077513694763, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.90it/s]                                              {'loss': 0.6116, 'grad_norm': 1.6059805154800415, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.90it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.43it/s]                                              {'loss': 0.6955, 'grad_norm': 0.8268492817878723, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.43it/s]                                              {'loss': 0.873, 'grad_norm': 2.5610811710357666, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.43it/s] 23%|██▎       | 7/30 [00:00<00:01, 13.32it/s]                                              {'loss': 0.6084, 'grad_norm': 0.6927856206893921, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 13.32it/s]                                              {'loss': 0.7928, 'grad_norm': 1.782041311264038, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.32it/s] 30%|███       | 9/30 [00:00<00:01, 13.08it/s]                                              {'loss': 0.6628, 'grad_norm': 0.7789046168327332, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.08it/s]                                              {'loss': 0.5855, 'grad_norm': 0.8941002488136292, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.08it/s] 37%|███▋      | 11/30 [00:00<00:01, 11.39it/s]                                               {'loss': 0.6502, 'grad_norm': 0.715925931930542, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 11.39it/s]                                               {'loss': 0.4382, 'grad_norm': 1.6399356126785278, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 11.39it/s] 43%|████▎     | 13/30 [00:01<00:01, 11.35it/s]                                               {'loss': 0.5878, 'grad_norm': 1.3337020874023438, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 11.35it/s]                                               {'loss': 0.5247, 'grad_norm': 0.7659364342689514, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.35it/s] 50%|█████     | 15/30 [00:01<00:01, 11.47it/s]                                               {'loss': 0.9193, 'grad_norm': 3.835949182510376, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 11.47it/s]                                               {'loss': 0.6099, 'grad_norm': 1.6415741443634033, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.47it/s] 57%|█████▋    | 17/30 [00:01<00:01, 11.46it/s]                                               {'loss': 0.473, 'grad_norm': 1.2448028326034546, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.46it/s]                                               {'loss': 0.7052, 'grad_norm': 2.4435415267944336, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 11.46it/s] 63%|██████▎   | 19/30 [00:01<00:00, 12.37it/s]                                               {'loss': 0.469, 'grad_norm': 1.450938105583191, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 12.37it/s]                                               {'loss': 0.612, 'grad_norm': 1.2201281785964966, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 12.37it/s] 70%|███████   | 21/30 [00:01<00:00, 11.73it/s]                                               {'loss': 0.5579, 'grad_norm': 1.4133156538009644, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 11.73it/s]                                               {'loss': 0.5264, 'grad_norm': 1.6216959953308105, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 11.73it/s] 77%|███████▋  | 23/30 [00:01<00:00, 11.28it/s]                                               {'loss': 0.8775, 'grad_norm': 4.295708179473877, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 11.28it/s]                                               {'loss': 0.4201, 'grad_norm': 2.4465548992156982, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 11.28it/s] 83%|████████▎ | 25/30 [00:02<00:00, 11.05it/s]                                               {'loss': 0.4757, 'grad_norm': 1.0306745767593384, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 11.05it/s]                                               {'loss': 0.6065, 'grad_norm': 1.5599159002304077, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 11.05it/s] 90%|█████████ | 27/30 [00:02<00:00, 10.92it/s]                                               {'loss': 0.5593, 'grad_norm': 2.364177942276001, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 10.92it/s]                                               {'loss': 0.6118, 'grad_norm': 1.6633936166763306, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 10.92it/s] 97%|█████████▋| 29/30 [00:02<00:00, 11.38it/s]                                               {'loss': 0.5083, 'grad_norm': 2.4124529361724854, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 11.38it/s]                                               {'loss': 0.4242, 'grad_norm': 2.473684787750244, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 11.38it/s]                                               {'train_runtime': 2.6648, 'train_samples_per_second': 159.484, 'train_steps_per_second': 11.258, 'train_loss': 0.612844122449557, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 11.38it/s]100%|██████████| 30/30 [00:02<00:00, 11.26it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:15
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.57it/s]                                              {'loss': 0.6539, 'grad_norm': 4.481317520141602, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.57it/s]  7%|▋         | 2/30 [00:00<00:05,  5.05it/s]                                              {'loss': 0.1613, 'grad_norm': 1.9425805807113647, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.05it/s] 10%|█         | 3/30 [00:00<00:05,  5.24it/s]                                              {'loss': 0.0639, 'grad_norm': 0.7249494791030884, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  5.24it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.97it/s]                                              {'loss': 0.015, 'grad_norm': 0.23094959557056427, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.97it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.18it/s]                                              {'loss': 0.0053, 'grad_norm': 0.11451045423746109, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.18it/s] 20%|██        | 6/30 [00:01<00:04,  5.58it/s]                                              {'loss': 0.0031, 'grad_norm': 0.07060504704713821, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.58it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.55it/s]                                              {'loss': 0.0034, 'grad_norm': 0.06741376221179962, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.55it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.11it/s]                                              {'loss': 0.0019, 'grad_norm': 0.04217958450317383, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.11it/s] 30%|███       | 9/30 [00:01<00:04,  4.86it/s]                                              {'loss': 0.0011, 'grad_norm': 0.021999962627887726, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.86it/s] 33%|███▎      | 10/30 [00:01<00:04,  4.72it/s]                                               {'loss': 0.0006, 'grad_norm': 0.013060344383120537, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.72it/s] 37%|███▋      | 11/30 [00:02<00:03,  4.91it/s]                                               {'loss': 0.0004, 'grad_norm': 0.007157893851399422, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  4.91it/s] 40%|████      | 12/30 [00:02<00:03,  5.79it/s]                                               {'loss': 0.0003, 'grad_norm': 0.006074939388781786, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.79it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.04it/s]                                               {'loss': 0.0002, 'grad_norm': 0.003231787821277976, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.04it/s] 47%|████▋     | 14/30 [00:02<00:03,  4.90it/s]                                               {'loss': 0.0002, 'grad_norm': 0.0017516701482236385, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  4.90it/s] 50%|█████     | 15/30 [00:02<00:03,  4.78it/s]                                               {'loss': 0.0002, 'grad_norm': 0.0021308399736881256, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:03,  4.78it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.32it/s]                                               {'loss': 0.0002, 'grad_norm': 0.0014432421885430813, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.32it/s] 57%|█████▋    | 17/30 [00:03<00:03,  4.03it/s]                                               {'loss': 0.0003, 'grad_norm': 0.002362178871408105, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  4.03it/s]                                               {'loss': 0.0002, 'grad_norm': 0.00126612966414541, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.03it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.52it/s]                                               {'loss': 0.0002, 'grad_norm': 0.0009689974249340594, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.52it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.22it/s]                                               {'loss': 0.0002, 'grad_norm': 0.0012802737765014172, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.22it/s] 70%|███████   | 21/30 [00:04<00:02,  4.17it/s]                                               {'loss': 0.0002, 'grad_norm': 0.0013749408535659313, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:02,  4.17it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.26it/s]                                               {'loss': 0.0002, 'grad_norm': 0.0011260249884799123, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.26it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.38it/s]                                               {'loss': 0.0002, 'grad_norm': 0.001096147345378995, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.38it/s]                                               {'loss': 0.0001, 'grad_norm': 0.0010583968833088875, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  4.38it/s] 83%|████████▎ | 25/30 [00:05<00:00,  5.73it/s]                                               {'loss': 0.0002, 'grad_norm': 0.0015274293255060911, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  5.73it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.69it/s]                                               {'loss': 0.0002, 'grad_norm': 0.001621246337890625, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.69it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.96it/s]                                               {'loss': 0.0002, 'grad_norm': 0.0016465788939967752, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.96it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.51it/s]                                               {'loss': 0.0001, 'grad_norm': 0.0008574960520491004, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.51it/s] 97%|█████████▋| 29/30 [00:06<00:00,  4.22it/s]                                               {'loss': 0.0002, 'grad_norm': 0.0016204359708353877, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  4.22it/s]100%|██████████| 30/30 [00:06<00:00,  4.72it/s]                                               {'loss': 0.0002, 'grad_norm': 0.0022591198794543743, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.72it/s]                                               {'train_runtime': 6.3618, 'train_samples_per_second': 66.805, 'train_steps_per_second': 4.716, 'train_loss': 0.030456396170969433, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.72it/s]100%|██████████| 30/30 [00:06<00:00,  4.72it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:38
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.01it/s]                                              {'loss': 0.6628, 'grad_norm': 1.6266162395477295, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.01it/s]                                              {'loss': 0.7359, 'grad_norm': 2.622626781463623, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.01it/s] 10%|█         | 3/30 [00:00<00:03,  8.57it/s]                                              {'loss': 0.6768, 'grad_norm': 2.934596538543701, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.57it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.63it/s]                                              {'loss': 0.6149, 'grad_norm': 1.3630201816558838, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.63it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.89it/s]                                              {'loss': 0.6953, 'grad_norm': 1.2504438161849976, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.89it/s]                                              {'loss': 0.7802, 'grad_norm': 5.412813186645508, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.89it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.64it/s]                                              {'loss': 0.6589, 'grad_norm': 1.4283301830291748, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.64it/s] 27%|██▋       | 8/30 [00:00<00:02,  8.14it/s]                                              {'loss': 0.7119, 'grad_norm': 0.7709286212921143, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  8.14it/s] 30%|███       | 9/30 [00:01<00:02,  8.24it/s]                                              {'loss': 0.7116, 'grad_norm': 1.4166021347045898, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  8.24it/s] 33%|███▎      | 10/30 [00:01<00:02,  8.02it/s]                                               {'loss': 0.6749, 'grad_norm': 1.7135976552963257, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.02it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.94it/s]                                               {'loss': 0.6272, 'grad_norm': 1.4651895761489868, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.94it/s]                                               {'loss': 0.6229, 'grad_norm': 2.3823421001434326, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.94it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.25it/s]                                               {'loss': 0.6236, 'grad_norm': 1.103926181793213, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.25it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.84it/s]                                               {'loss': 0.7372, 'grad_norm': 2.1643147468566895, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.84it/s]                                               {'loss': 0.6416, 'grad_norm': 2.0296528339385986, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  7.84it/s] 53%|█████▎    | 16/30 [00:01<00:01,  8.67it/s]                                               {'loss': 0.672, 'grad_norm': 1.704984188079834, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  8.67it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.94it/s]                                               {'loss': 0.5903, 'grad_norm': 1.2959833145141602, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.94it/s]                                               {'loss': 0.8416, 'grad_norm': 2.32509446144104, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.94it/s] 63%|██████▎   | 19/30 [00:02<00:01,  7.99it/s]                                               {'loss': 0.6264, 'grad_norm': 1.2379668951034546, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.99it/s] 67%|██████▋   | 20/30 [00:02<00:01,  7.87it/s]                                               {'loss': 0.7166, 'grad_norm': 1.2728434801101685, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  7.87it/s] 70%|███████   | 21/30 [00:02<00:01,  7.84it/s]                                               {'loss': 0.6471, 'grad_norm': 1.5257354974746704, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  7.84it/s] 73%|███████▎  | 22/30 [00:02<00:01,  7.60it/s]                                               {'loss': 0.6442, 'grad_norm': 0.8100319504737854, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  7.60it/s] 77%|███████▋  | 23/30 [00:02<00:00,  7.58it/s]                                               {'loss': 0.7211, 'grad_norm': 1.3532555103302002, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  7.58it/s] 80%|████████  | 24/30 [00:02<00:00,  7.87it/s]                                               {'loss': 0.7102, 'grad_norm': 2.270033359527588, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.87it/s] 83%|████████▎ | 25/30 [00:03<00:00,  6.43it/s]                                               {'loss': 0.5866, 'grad_norm': 1.026068091392517, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  6.43it/s] 87%|████████▋ | 26/30 [00:03<00:00,  5.37it/s]                                               {'loss': 0.6619, 'grad_norm': 0.7312092781066895, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  5.37it/s] 90%|█████████ | 27/30 [00:03<00:00,  4.91it/s]                                               {'loss': 0.7057, 'grad_norm': 0.8659038543701172, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  4.91it/s] 93%|█████████▎| 28/30 [00:03<00:00,  4.76it/s]                                               {'loss': 0.6952, 'grad_norm': 1.1265357732772827, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  4.76it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.18it/s]                                               {'loss': 0.6636, 'grad_norm': 0.9840327501296997, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.18it/s]                                               {'loss': 0.5153, 'grad_norm': 1.8675472736358643, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.18it/s]                                               {'train_runtime': 4.2952, 'train_samples_per_second': 98.948, 'train_steps_per_second': 6.985, 'train_loss': 0.6724454383055369, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.18it/s]100%|██████████| 30/30 [00:04<00:00,  6.99it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:11
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:10,  2.81it/s]                                              {'loss': 0.7753, 'grad_norm': 3.8890221118927, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:10,  2.81it/s]  7%|▋         | 2/30 [00:00<00:10,  2.72it/s]                                              {'loss': 0.7474, 'grad_norm': 1.725856065750122, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:10,  2.72it/s] 10%|█         | 3/30 [00:01<00:10,  2.69it/s]                                              {'loss': 0.7218, 'grad_norm': 1.1341753005981445, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:10,  2.69it/s] 13%|█▎        | 4/30 [00:01<00:09,  2.77it/s]                                              {'loss': 0.6556, 'grad_norm': 0.9529055953025818, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:09,  2.77it/s] 17%|█▋        | 5/30 [00:01<00:08,  2.85it/s]                                              {'loss': 0.7184, 'grad_norm': 0.9326117038726807, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:08,  2.85it/s] 20%|██        | 6/30 [00:01<00:06,  3.46it/s]                                              {'loss': 0.8587, 'grad_norm': 3.68475079536438, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.46it/s] 23%|██▎       | 7/30 [00:02<00:07,  3.14it/s]                                              {'loss': 0.6757, 'grad_norm': 0.8358294367790222, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:07,  3.14it/s] 27%|██▋       | 8/30 [00:02<00:07,  3.03it/s]                                              {'loss': 0.6697, 'grad_norm': 0.3488454520702362, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:07,  3.03it/s] 30%|███       | 9/30 [00:03<00:07,  2.96it/s]                                              {'loss': 0.683, 'grad_norm': 0.7958292365074158, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:03<00:07,  2.96it/s] 33%|███▎      | 10/30 [00:03<00:06,  2.91it/s]                                               {'loss': 0.6722, 'grad_norm': 0.41034772992134094, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:06,  2.91it/s] 37%|███▋      | 11/30 [00:03<00:06,  2.94it/s]                                               {'loss': 0.6673, 'grad_norm': 0.3432651162147522, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:06,  2.94it/s] 40%|████      | 12/30 [00:03<00:05,  3.57it/s]                                               {'loss': 0.6055, 'grad_norm': 2.192432165145874, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:05,  3.57it/s] 43%|████▎     | 13/30 [00:04<00:05,  3.27it/s]                                               {'loss': 0.6333, 'grad_norm': 0.4548262357711792, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:04<00:05,  3.27it/s] 47%|████▋     | 14/30 [00:04<00:05,  3.11it/s]                                               {'loss': 0.685, 'grad_norm': 0.7269174456596375, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:05,  3.11it/s] 50%|█████     | 15/30 [00:04<00:04,  3.02it/s]                                               {'loss': 0.674, 'grad_norm': 0.8886294960975647, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:04,  3.02it/s] 53%|█████▎    | 16/30 [00:05<00:04,  2.95it/s]                                               {'loss': 0.6613, 'grad_norm': 1.0947644710540771, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:05<00:04,  2.95it/s] 57%|█████▋    | 17/30 [00:05<00:04,  3.00it/s]                                               {'loss': 0.6593, 'grad_norm': 0.6428350806236267, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:05<00:04,  3.00it/s] 60%|██████    | 18/30 [00:05<00:03,  3.73it/s]                                               {'loss': 0.6573, 'grad_norm': 1.185820460319519, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:03,  3.73it/s] 63%|██████▎   | 19/30 [00:06<00:03,  3.56it/s]                                               {'loss': 0.6629, 'grad_norm': 1.0700947046279907, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:06<00:03,  3.56it/s] 67%|██████▋   | 20/30 [00:06<00:02,  3.45it/s]                                               {'loss': 0.6562, 'grad_norm': 0.5532847046852112, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:06<00:02,  3.45it/s] 70%|███████   | 21/30 [00:06<00:02,  3.44it/s]                                               {'loss': 0.6237, 'grad_norm': 0.633324146270752, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:06<00:02,  3.44it/s] 73%|███████▎  | 22/30 [00:06<00:02,  3.43it/s]                                               {'loss': 0.6305, 'grad_norm': 0.8192123174667358, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:06<00:02,  3.43it/s] 77%|███████▋  | 23/30 [00:07<00:02,  3.41it/s]                                               {'loss': 0.6666, 'grad_norm': 0.9518867135047913, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:07<00:02,  3.41it/s]                                               {'loss': 0.626, 'grad_norm': 1.0481387376785278, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:07<00:01,  3.41it/s] 83%|████████▎ | 25/30 [00:07<00:01,  4.07it/s]                                               {'loss': 0.6049, 'grad_norm': 0.9299135208129883, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:07<00:01,  4.07it/s] 87%|████████▋ | 26/30 [00:07<00:01,  3.91it/s]                                               {'loss': 0.5796, 'grad_norm': 0.7947665452957153, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:07<00:01,  3.91it/s] 90%|█████████ | 27/30 [00:08<00:00,  3.74it/s]                                               {'loss': 0.6599, 'grad_norm': 0.8063861727714539, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:08<00:00,  3.74it/s] 93%|█████████▎| 28/30 [00:08<00:00,  3.61it/s]                                               {'loss': 0.598, 'grad_norm': 0.8638578057289124, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:08<00:00,  3.61it/s] 97%|█████████▋| 29/30 [00:08<00:00,  3.70it/s]                                               {'loss': 0.5728, 'grad_norm': 1.2666789293289185, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:08<00:00,  3.70it/s]                                               {'loss': 0.6112, 'grad_norm': 1.6347155570983887, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  3.70it/s]                                               {'train_runtime': 9.1252, 'train_samples_per_second': 46.575, 'train_steps_per_second': 3.288, 'train_loss': 0.6637758235136668, 'epoch': 5.0}
100%|██████████| 30/30 [00:09<00:00,  3.70it/s]100%|██████████| 30/30 [00:09<00:00,  3.29it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:40
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  7.50it/s]                                              {'loss': 0.6486, 'grad_norm': 3.7002389430999756, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  7.50it/s]                                              {'loss': 0.1516, 'grad_norm': 1.8608275651931763, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.50it/s] 10%|█         | 3/30 [00:00<00:03,  7.85it/s]                                              {'loss': 0.3019, 'grad_norm': 1.246002435684204, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.85it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.53it/s]                                              {'loss': 0.2274, 'grad_norm': 2.3273701667785645, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.53it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.52it/s]                                              {'loss': 0.0228, 'grad_norm': 0.3077329695224762, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.52it/s]                                              {'loss': 0.0224, 'grad_norm': 0.2862630784511566, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.52it/s] 23%|██▎       | 7/30 [00:00<00:02,  7.87it/s]                                              {'loss': 0.3274, 'grad_norm': 0.8429961204528809, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  7.87it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.72it/s]                                              {'loss': 0.0152, 'grad_norm': 0.20597213506698608, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.72it/s] 30%|███       | 9/30 [00:01<00:02,  7.43it/s]                                              {'loss': 0.2873, 'grad_norm': 1.1595944166183472, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.43it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.20it/s]                                               {'loss': 0.2152, 'grad_norm': 1.058454155921936, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.20it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.27it/s]                                               {'loss': 0.0226, 'grad_norm': 0.30745717883110046, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.27it/s]                                               {'loss': 0.0169, 'grad_norm': 0.32404860854148865, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.27it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.20it/s]                                               {'loss': 0.316, 'grad_norm': 1.1701093912124634, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.20it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.66it/s]                                               {'loss': 0.0169, 'grad_norm': 0.8947438597679138, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.66it/s] 50%|█████     | 15/30 [00:01<00:02,  7.42it/s]                                               {'loss': 0.2434, 'grad_norm': 1.1302090883255005, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:02,  7.42it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.87it/s]                                               {'loss': 0.2407, 'grad_norm': 1.571005940437317, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.87it/s]                                               {'loss': 0.0196, 'grad_norm': 0.2835649251937866, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.87it/s] 60%|██████    | 18/30 [00:02<00:01, 10.27it/s]                                               {'loss': 0.0199, 'grad_norm': 0.5312439799308777, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 10.27it/s]                                               {'loss': 0.017, 'grad_norm': 0.24396958947181702, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01, 10.27it/s] 67%|██████▋   | 20/30 [00:02<00:00, 10.92it/s]                                               {'loss': 0.0162, 'grad_norm': 0.2657073140144348, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 10.92it/s]                                               {'loss': 0.017, 'grad_norm': 0.25246015191078186, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 10.92it/s] 73%|███████▎  | 22/30 [00:02<00:00, 11.50it/s]                                               {'loss': 0.0248, 'grad_norm': 0.4523357152938843, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.50it/s]                                               {'loss': 0.5161, 'grad_norm': 2.1681346893310547, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 11.50it/s] 80%|████████  | 24/30 [00:02<00:00, 11.14it/s]                                               {'loss': 0.7724, 'grad_norm': 4.282301902770996, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 11.14it/s]                                               {'loss': 0.0275, 'grad_norm': 0.4210781157016754, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 11.14it/s] 87%|████████▋ | 26/30 [00:03<00:00,  8.93it/s]                                               {'loss': 0.5144, 'grad_norm': 2.5025863647460938, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  8.93it/s] 90%|█████████ | 27/30 [00:03<00:00,  7.84it/s]                                               {'loss': 0.0148, 'grad_norm': 0.194507896900177, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  7.84it/s] 93%|█████████▎| 28/30 [00:03<00:00,  7.81it/s]                                               {'loss': 0.2267, 'grad_norm': 1.418521761894226, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  7.81it/s] 97%|█████████▋| 29/30 [00:03<00:00,  7.68it/s]                                               {'loss': 0.0194, 'grad_norm': 0.31254494190216064, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  7.68it/s]                                               {'loss': 0.0232, 'grad_norm': 0.4238283038139343, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  7.68it/s]                                               {'train_runtime': 3.7281, 'train_samples_per_second': 114.0, 'train_steps_per_second': 8.047, 'train_loss': 0.1768377239195009, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  7.68it/s]100%|██████████| 30/30 [00:03<00:00,  8.05it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:45
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.66it/s]                                              {'loss': 0.7302, 'grad_norm': 3.1911096572875977, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.66it/s]                                              {'loss': 0.743, 'grad_norm': 1.5360221862792969, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.66it/s] 10%|█         | 3/30 [00:00<00:03,  8.55it/s]                                              {'loss': 0.7003, 'grad_norm': 1.0762358903884888, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.55it/s]                                              {'loss': 0.6764, 'grad_norm': 1.1214240789413452, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  8.55it/s] 17%|█▋        | 5/30 [00:00<00:02,  8.72it/s]                                              {'loss': 0.7094, 'grad_norm': 1.625993013381958, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  8.72it/s]                                              {'loss': 0.8399, 'grad_norm': 3.763120651245117, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02,  8.72it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.53it/s]                                              {'loss': 0.6804, 'grad_norm': 0.6578401923179626, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  9.53it/s] 27%|██▋       | 8/30 [00:00<00:02,  8.77it/s]                                              {'loss': 0.6863, 'grad_norm': 1.1009632349014282, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  8.77it/s]                                              {'loss': 0.6763, 'grad_norm': 0.8190426230430603, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  8.77it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.15it/s]                                               {'loss': 0.6793, 'grad_norm': 0.9186115860939026, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  9.15it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.20it/s]                                               {'loss': 0.6843, 'grad_norm': 0.5986480712890625, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  9.20it/s]                                               {'loss': 0.6983, 'grad_norm': 2.7368268966674805, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01,  9.20it/s] 43%|████▎     | 13/30 [00:01<00:01, 10.25it/s]                                               {'loss': 0.6922, 'grad_norm': 0.7053619027137756, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 10.25it/s]                                               {'loss': 0.7017, 'grad_norm': 0.7421881556510925, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 10.25it/s] 50%|█████     | 15/30 [00:01<00:01,  9.74it/s]                                               {'loss': 0.6382, 'grad_norm': 2.7224109172821045, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  9.74it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.26it/s]                                               {'loss': 0.6634, 'grad_norm': 1.199055790901184, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  9.26it/s]                                               {'loss': 0.7082, 'grad_norm': 0.7756233811378479, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01,  9.26it/s] 60%|██████    | 18/30 [00:01<00:01, 10.84it/s]                                               {'loss': 0.6767, 'grad_norm': 1.5717551708221436, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 10.84it/s]                                               {'loss': 0.7004, 'grad_norm': 0.899451732635498, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01, 10.84it/s] 67%|██████▋   | 20/30 [00:02<00:00, 10.45it/s]                                               {'loss': 0.706, 'grad_norm': 0.678922712802887, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 10.45it/s]                                               {'loss': 0.7192, 'grad_norm': 0.5150127410888672, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 10.45it/s] 73%|███████▎  | 22/30 [00:02<00:00, 10.46it/s]                                               {'loss': 0.6983, 'grad_norm': 0.6184557676315308, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 10.46it/s]                                               {'loss': 0.7308, 'grad_norm': 2.0064358711242676, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 10.46it/s] 80%|████████  | 24/30 [00:02<00:00, 10.92it/s]                                               {'loss': 0.6656, 'grad_norm': 0.7888154983520508, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 10.92it/s]                                               {'loss': 0.6924, 'grad_norm': 0.7094329595565796, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 10.92it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.72it/s]                                               {'loss': 0.6763, 'grad_norm': 0.8683675527572632, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00,  9.72it/s]                                               {'loss': 0.7084, 'grad_norm': 1.044372320175171, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00,  9.72it/s] 93%|█████████▎| 28/30 [00:02<00:00,  9.21it/s]                                               {'loss': 0.6967, 'grad_norm': 0.33494579792022705, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00,  9.21it/s]                                               {'loss': 0.6669, 'grad_norm': 0.9041972756385803, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  9.21it/s]100%|██████████| 30/30 [00:03<00:00,  9.98it/s]                                               {'loss': 0.6741, 'grad_norm': 0.7224465608596802, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  9.98it/s]                                               {'train_runtime': 3.262, 'train_samples_per_second': 130.287, 'train_steps_per_second': 9.197, 'train_loss': 0.6973234613736471, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  9.98it/s]100%|██████████| 30/30 [00:03<00:00,  9.23it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:39
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:01<00:29,  1.03s/it]                                              {'loss': 0.6139, 'grad_norm': 1.0454076528549194, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:01<00:29,  1.03s/it]                                              {'loss': 0.5549, 'grad_norm': 1.9666147232055664, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:28,  1.03s/it] 10%|█         | 3/30 [00:01<00:08,  3.02it/s]                                              {'loss': 0.5474, 'grad_norm': 1.3289661407470703, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:08,  3.02it/s]                                              {'loss': 0.4308, 'grad_norm': 3.400377035140991, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  3.02it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.91it/s]                                              {'loss': 0.7297, 'grad_norm': 2.7553205490112305, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.91it/s]                                              {'loss': 0.5144, 'grad_norm': 2.9802491664886475, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.91it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.94it/s]                                              {'loss': 0.5642, 'grad_norm': 2.2863028049468994, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.94it/s]                                              {'loss': 0.5634, 'grad_norm': 1.0811429023742676, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.94it/s] 30%|███       | 9/30 [00:01<00:02,  8.33it/s]                                              {'loss': 0.6082, 'grad_norm': 1.0932915210723877, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  8.33it/s]                                              {'loss': 0.4603, 'grad_norm': 1.7435232400894165, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.33it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.44it/s]                                               {'loss': 0.4814, 'grad_norm': 1.4604440927505493, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  9.44it/s]                                               {'loss': 0.38, 'grad_norm': 1.4197380542755127, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01,  9.44it/s] 43%|████▎     | 13/30 [00:01<00:01, 11.25it/s]                                               {'loss': 0.5908, 'grad_norm': 1.9329123497009277, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 11.25it/s]                                               {'loss': 0.4398, 'grad_norm': 1.0975024700164795, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01, 11.25it/s] 50%|█████     | 15/30 [00:02<00:01, 11.78it/s]                                               {'loss': 0.7215, 'grad_norm': 3.101788282394409, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01, 11.78it/s]                                               {'loss': 0.4648, 'grad_norm': 1.4906785488128662, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01, 11.78it/s] 57%|█████▋    | 17/30 [00:02<00:01, 12.01it/s]                                               {'loss': 0.4795, 'grad_norm': 1.4426997900009155, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01, 12.01it/s]                                               {'loss': 0.2515, 'grad_norm': 2.5989272594451904, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:00, 12.01it/s] 63%|██████▎   | 19/30 [00:02<00:00, 12.66it/s]                                               {'loss': 0.4186, 'grad_norm': 1.5871517658233643, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 12.66it/s]                                               {'loss': 0.5793, 'grad_norm': 1.7227739095687866, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 12.66it/s] 70%|███████   | 21/30 [00:02<00:00, 12.34it/s]                                               {'loss': 0.5301, 'grad_norm': 1.6137213706970215, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 12.34it/s]                                               {'loss': 0.4995, 'grad_norm': 1.59736168384552, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 12.34it/s] 77%|███████▋  | 23/30 [00:02<00:00, 11.83it/s]                                               {'loss': 0.5868, 'grad_norm': 1.836676836013794, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 11.83it/s]                                               {'loss': 0.2242, 'grad_norm': 4.1833391189575195, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 11.83it/s] 83%|████████▎ | 25/30 [00:02<00:00, 12.34it/s]                                               {'loss': 0.2759, 'grad_norm': 2.19199800491333, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.34it/s]                                               {'loss': 0.4602, 'grad_norm': 1.7081292867660522, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 12.34it/s] 90%|█████████ | 27/30 [00:03<00:00, 12.02it/s]                                               {'loss': 0.3541, 'grad_norm': 2.263801097869873, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 12.02it/s]                                               {'loss': 0.4875, 'grad_norm': 1.6275426149368286, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 12.02it/s] 97%|█████████▋| 29/30 [00:03<00:00, 12.19it/s]                                               {'loss': 0.5212, 'grad_norm': 2.8687474727630615, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 12.19it/s]                                               {'loss': 0.5517, 'grad_norm': 6.53759765625, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.19it/s]                                               {'train_runtime': 3.3534, 'train_samples_per_second': 126.737, 'train_steps_per_second': 8.946, 'train_loss': 0.49619048784176506, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.19it/s]100%|██████████| 30/30 [00:03<00:00,  8.95it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:62
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.47it/s]                                              {'loss': 0.7344, 'grad_norm': 3.338571310043335, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.47it/s]  7%|▋         | 2/30 [00:00<00:03,  7.83it/s]                                              {'loss': 0.3908, 'grad_norm': 1.0999046564102173, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.83it/s] 10%|█         | 3/30 [00:00<00:03,  8.18it/s]                                              {'loss': 0.2001, 'grad_norm': 1.5028338432312012, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.18it/s] 13%|█▎        | 4/30 [00:00<00:03,  8.39it/s]                                              {'loss': 0.5174, 'grad_norm': 2.4283573627471924, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  8.39it/s] 17%|█▋        | 5/30 [00:00<00:02,  8.85it/s]                                              {'loss': 0.5523, 'grad_norm': 3.2319979667663574, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  8.85it/s]                                              {'loss': 0.6084, 'grad_norm': 2.78922700881958, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02,  8.85it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.17it/s]                                              {'loss': 0.3411, 'grad_norm': 2.4317469596862793, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  9.17it/s]                                              {'loss': 0.2127, 'grad_norm': 3.9856767654418945, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  9.17it/s] 30%|███       | 9/30 [00:00<00:02, 10.07it/s]                                              {'loss': 0.5871, 'grad_norm': 3.691680908203125, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:02, 10.07it/s]                                              {'loss': 0.4714, 'grad_norm': 1.8790063858032227, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:01, 10.07it/s] 37%|███▋      | 11/30 [00:01<00:01, 10.34it/s]                                               {'loss': 0.3105, 'grad_norm': 2.965336561203003, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 10.34it/s]                                               {'loss': 0.4036, 'grad_norm': 3.033966064453125, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 10.34it/s] 43%|████▎     | 13/30 [00:01<00:01, 10.94it/s]                                               {'loss': 0.4752, 'grad_norm': 1.5282511711120605, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 10.94it/s]                                               {'loss': 0.4503, 'grad_norm': 1.385675072669983, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 10.94it/s] 50%|█████     | 15/30 [00:01<00:01, 10.52it/s]                                               {'loss': 0.2136, 'grad_norm': 2.253307819366455, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 10.52it/s]                                               {'loss': 0.5327, 'grad_norm': 2.1567041873931885, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 10.52it/s] 57%|█████▋    | 17/30 [00:01<00:01, 10.35it/s]                                               {'loss': 0.405, 'grad_norm': 2.0172131061553955, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 10.35it/s]                                               {'loss': 0.4562, 'grad_norm': 3.116281270980835, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 10.35it/s] 63%|██████▎   | 19/30 [00:01<00:01, 10.72it/s]                                               {'loss': 0.2501, 'grad_norm': 1.4075932502746582, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:01, 10.72it/s]                                               {'loss': 0.2936, 'grad_norm': 1.052763819694519, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 10.72it/s] 70%|███████   | 21/30 [00:02<00:00, 11.17it/s]                                               {'loss': 0.5299, 'grad_norm': 2.148958921432495, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.17it/s]                                               {'loss': 0.8316, 'grad_norm': 5.145604133605957, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.17it/s] 77%|███████▋  | 23/30 [00:02<00:00, 11.58it/s]                                               {'loss': 0.3061, 'grad_norm': 2.129018545150757, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 11.58it/s]                                               {'loss': 0.1116, 'grad_norm': 1.7467838525772095, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 11.58it/s] 83%|████████▎ | 25/30 [00:02<00:00, 12.09it/s]                                               {'loss': 0.3271, 'grad_norm': 1.1425758600234985, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.09it/s]                                               {'loss': 0.4369, 'grad_norm': 1.5253549814224243, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 12.09it/s] 90%|█████████ | 27/30 [00:02<00:00, 12.17it/s]                                               {'loss': 0.3357, 'grad_norm': 1.5531107187271118, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 12.17it/s]                                               {'loss': 0.26, 'grad_norm': 1.3614304065704346, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.17it/s] 97%|█████████▋| 29/30 [00:02<00:00, 12.03it/s]                                               {'loss': 0.4423, 'grad_norm': 2.6915485858917236, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.03it/s]                                               {'loss': 0.5772, 'grad_norm': 3.4190211296081543, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.03it/s]                                               {'train_runtime': 2.8592, 'train_samples_per_second': 148.642, 'train_steps_per_second': 10.492, 'train_loss': 0.41882403840621313, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.03it/s]100%|██████████| 30/30 [00:02<00:00, 10.50it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
the shape of base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.weight is (768, 768)
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  5%|▍         | 3/66 [00:00<00:02, 23.04it/s]  9%|▉         | 6/66 [00:00<00:04, 14.38it/s] 12%|█▏        | 8/66 [00:00<00:04, 12.41it/s] 15%|█▌        | 10/66 [00:00<00:04, 11.56it/s] 18%|█▊        | 12/66 [00:00<00:04, 11.24it/s] 21%|██        | 14/66 [00:01<00:04, 11.35it/s] 24%|██▍       | 16/66 [00:01<00:04, 11.10it/s] 27%|██▋       | 18/66 [00:01<00:04, 10.97it/s] 30%|███       | 20/66 [00:01<00:04, 10.81it/s] 33%|███▎      | 22/66 [00:01<00:04, 10.70it/s] 36%|███▋      | 24/66 [00:02<00:03, 10.68it/s] 39%|███▉      | 26/66 [00:02<00:03, 10.61it/s] 42%|████▏     | 28/66 [00:02<00:03, 10.33it/s] 45%|████▌     | 30/66 [00:02<00:03, 10.43it/s] 48%|████▊     | 32/66 [00:02<00:03,  9.78it/s] 50%|█████     | 33/66 [00:03<00:03,  9.10it/s] 52%|█████▏    | 34/66 [00:03<00:03,  8.21it/s] 53%|█████▎    | 35/66 [00:03<00:04,  7.68it/s] 55%|█████▍    | 36/66 [00:03<00:04,  7.49it/s] 56%|█████▌    | 37/66 [00:03<00:03,  7.29it/s] 58%|█████▊    | 38/66 [00:03<00:03,  7.01it/s] 59%|█████▉    | 39/66 [00:04<00:04,  6.68it/s] 61%|██████    | 40/66 [00:04<00:03,  6.66it/s] 62%|██████▏   | 41/66 [00:04<00:03,  6.40it/s] 64%|██████▎   | 42/66 [00:04<00:03,  6.64it/s] 65%|██████▌   | 43/66 [00:04<00:03,  7.17it/s] 68%|██████▊   | 45/66 [00:04<00:02,  8.74it/s] 70%|██████▉   | 46/66 [00:04<00:02,  8.99it/s] 71%|███████   | 47/66 [00:05<00:02,  8.18it/s] 74%|███████▍  | 49/66 [00:05<00:02,  8.14it/s] 76%|███████▌  | 50/66 [00:05<00:01,  8.29it/s] 77%|███████▋  | 51/66 [00:05<00:01,  8.00it/s] 79%|███████▉  | 52/66 [00:05<00:01,  7.94it/s] 82%|████████▏ | 54/66 [00:05<00:01,  8.21it/s] 85%|████████▍ | 56/66 [00:06<00:01,  8.13it/s] 86%|████████▋ | 57/66 [00:06<00:01,  8.21it/s] 89%|████████▉ | 59/66 [00:06<00:00,  8.20it/s] 91%|█████████ | 60/66 [00:06<00:00,  8.17it/s] 92%|█████████▏| 61/66 [00:06<00:00,  8.32it/s] 94%|█████████▍| 62/66 [00:06<00:00,  7.39it/s] 95%|█████████▌| 63/66 [00:07<00:00,  7.38it/s] 97%|█████████▋| 64/66 [00:07<00:00,  6.98it/s] 98%|█████████▊| 65/66 [00:07<00:00,  7.45it/s]100%|██████████| 66/66 [00:07<00:00,  8.94it/s]
{'eval_loss': 0.6070977449417114, 'eval_model_preparation_time': 0.0084, 'eval_acc': 0.6912751677852349, 'eval_runtime': 7.4398, 'eval_samples_per_second': 140.191, 'eval_steps_per_second': 8.871}
ROUND:6
CLIENT:69
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.57it/s]                                              {'loss': 0.7935, 'grad_norm': 3.663907289505005, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.57it/s]  7%|▋         | 2/30 [00:00<00:07,  3.59it/s]                                              {'loss': 0.6686, 'grad_norm': 4.599165916442871, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.59it/s] 10%|█         | 3/30 [00:00<00:07,  3.60it/s]                                              {'loss': 0.6486, 'grad_norm': 2.2131550312042236, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.60it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.59it/s]                                              {'loss': 0.7001, 'grad_norm': 1.447966456413269, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.59it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.65it/s]                                              {'loss': 0.6662, 'grad_norm': 0.9554824233055115, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.65it/s] 20%|██        | 6/30 [00:01<00:05,  4.04it/s]                                              {'loss': 0.8308, 'grad_norm': 4.027523040771484, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.04it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.86it/s]                                              {'loss': 0.6783, 'grad_norm': 0.8466221690177917, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.86it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.30it/s]                                              {'loss': 0.6659, 'grad_norm': 0.48384881019592285, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.30it/s] 30%|███       | 9/30 [00:02<00:03,  5.54it/s]                                              {'loss': 0.6574, 'grad_norm': 1.2109190225601196, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:03,  5.54it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.84it/s]                                               {'loss': 0.654, 'grad_norm': 0.7822700142860413, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.84it/s] 37%|███▋      | 11/30 [00:02<00:03,  6.19it/s]                                               {'loss': 0.6579, 'grad_norm': 0.6724523305892944, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  6.19it/s]                                               {'loss': 0.6439, 'grad_norm': 3.0334091186523438, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.19it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.19it/s]                                               {'loss': 0.5882, 'grad_norm': 0.7422844171524048, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.19it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.14it/s]                                               {'loss': 0.6227, 'grad_norm': 1.3327099084854126, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.14it/s] 50%|█████     | 15/30 [00:02<00:02,  7.04it/s]                                               {'loss': 0.8969, 'grad_norm': 3.3407201766967773, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.04it/s] 53%|█████▎    | 16/30 [00:03<00:02,  6.53it/s]                                               {'loss': 0.6482, 'grad_norm': 1.304104208946228, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  6.53it/s] 57%|█████▋    | 17/30 [00:03<00:02,  6.43it/s]                                               {'loss': 0.5252, 'grad_norm': 1.2255514860153198, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  6.43it/s]                                               {'loss': 0.7828, 'grad_norm': 3.227240800857544, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  6.43it/s] 63%|██████▎   | 19/30 [00:03<00:01,  8.34it/s]                                               {'loss': 0.6087, 'grad_norm': 1.2162446975708008, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  8.34it/s]                                               {'loss': 0.6187, 'grad_norm': 0.9750799536705017, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  8.34it/s] 70%|███████   | 21/30 [00:03<00:00,  9.38it/s]                                               {'loss': 0.5566, 'grad_norm': 1.9604014158248901, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:00,  9.38it/s]                                               {'loss': 0.6051, 'grad_norm': 0.9146847724914551, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00,  9.38it/s] 77%|███████▋  | 23/30 [00:03<00:00,  9.94it/s]                                               {'loss': 0.7533, 'grad_norm': 2.439354181289673, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  9.94it/s]                                               {'loss': 0.5972, 'grad_norm': 1.6551027297973633, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  9.94it/s] 83%|████████▎ | 25/30 [00:03<00:00, 11.68it/s]                                               {'loss': 0.6255, 'grad_norm': 1.383710503578186, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 11.68it/s]                                               {'loss': 0.6693, 'grad_norm': 1.2930781841278076, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 11.68it/s] 90%|█████████ | 27/30 [00:03<00:00, 12.12it/s]                                               {'loss': 0.591, 'grad_norm': 1.2870922088623047, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 12.12it/s]                                               {'loss': 0.598, 'grad_norm': 1.3608500957489014, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00, 12.12it/s] 97%|█████████▋| 29/30 [00:04<00:00, 12.31it/s]                                               {'loss': 0.6256, 'grad_norm': 2.024077892303467, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00, 12.31it/s]                                               {'loss': 0.4589, 'grad_norm': 3.2625088691711426, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00, 12.31it/s]                                               {'train_runtime': 4.2171, 'train_samples_per_second': 100.78, 'train_steps_per_second': 7.114, 'train_loss': 0.654569685459137, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00, 12.31it/s]100%|██████████| 30/30 [00:04<00:00,  7.12it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:74
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.4926, 'grad_norm': 2.627521514892578, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 11.11it/s]  7%|▋         | 2/30 [00:00<00:02, 11.84it/s]                                              {'loss': 0.3583, 'grad_norm': 1.0267233848571777, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 11.84it/s]                                              {'loss': 0.9231, 'grad_norm': 5.0350728034973145, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.84it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.11it/s]                                              {'loss': 0.5447, 'grad_norm': 2.3481085300445557, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.11it/s]                                              {'loss': 0.4659, 'grad_norm': 1.1750292778015137, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.11it/s] 20%|██        | 6/30 [00:00<00:01, 13.56it/s]                                              {'loss': 0.7182, 'grad_norm': 4.325682640075684, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 13.56it/s]                                              {'loss': 0.2719, 'grad_norm': 1.753443717956543, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 13.56it/s] 27%|██▋       | 8/30 [00:00<00:01, 11.41it/s]                                              {'loss': 0.7617, 'grad_norm': 3.6541357040405273, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 11.41it/s]                                              {'loss': 0.5681, 'grad_norm': 2.12786602973938, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 11.41it/s] 33%|███▎      | 10/30 [00:00<00:01, 11.79it/s]                                               {'loss': 0.5428, 'grad_norm': 0.8350722193717957, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.79it/s]                                               {'loss': 0.4057, 'grad_norm': 0.8590685129165649, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 11.79it/s] 40%|████      | 12/30 [00:00<00:01, 12.26it/s]                                               {'loss': 0.1634, 'grad_norm': 1.6844098567962646, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 12.26it/s]                                               {'loss': 0.5816, 'grad_norm': 1.0325883626937866, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 12.26it/s] 47%|████▋     | 14/30 [00:01<00:01, 11.70it/s]                                               {'loss': 0.5115, 'grad_norm': 0.6225510239601135, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.70it/s]                                               {'loss': 0.6142, 'grad_norm': 1.5775673389434814, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 11.70it/s] 53%|█████▎    | 16/30 [00:01<00:01, 11.81it/s]                                               {'loss': 0.4328, 'grad_norm': 0.4344155490398407, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.81it/s]                                               {'loss': 0.3284, 'grad_norm': 1.1883649826049805, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.81it/s] 60%|██████    | 18/30 [00:01<00:00, 13.14it/s]                                               {'loss': 0.7306, 'grad_norm': 2.3167355060577393, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.14it/s]                                               {'loss': 0.431, 'grad_norm': 0.5658773183822632, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.14it/s] 67%|██████▋   | 20/30 [00:01<00:00, 12.78it/s]                                               {'loss': 0.5671, 'grad_norm': 0.6968492865562439, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 12.78it/s]                                               {'loss': 0.6102, 'grad_norm': 0.6734687685966492, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 12.78it/s] 73%|███████▎  | 22/30 [00:01<00:00, 12.80it/s]                                               {'loss': 0.4415, 'grad_norm': 0.4869980812072754, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 12.80it/s]                                               {'loss': 0.5499, 'grad_norm': 0.6384914517402649, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 12.80it/s] 80%|████████  | 24/30 [00:01<00:00, 14.02it/s]                                               {'loss': 0.2783, 'grad_norm': 2.2380268573760986, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 14.02it/s]                                               {'loss': 0.2403, 'grad_norm': 1.6695200204849243, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.02it/s] 87%|████████▋ | 26/30 [00:02<00:00, 13.53it/s]                                               {'loss': 0.6583, 'grad_norm': 1.0608689785003662, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.53it/s]                                               {'loss': 0.5966, 'grad_norm': 0.9135005474090576, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.53it/s] 93%|█████████▎| 28/30 [00:02<00:00, 12.61it/s]                                               {'loss': 0.4764, 'grad_norm': 0.4290442168712616, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.61it/s]                                               {'loss': 0.432, 'grad_norm': 0.6635809540748596, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.61it/s]100%|██████████| 30/30 [00:02<00:00, 12.92it/s]                                               {'loss': 0.7024, 'grad_norm': 1.528943657875061, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.92it/s]                                               {'train_runtime': 2.5393, 'train_samples_per_second': 167.367, 'train_steps_per_second': 11.814, 'train_loss': 0.5133135164777438, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.92it/s]100%|██████████| 30/30 [00:02<00:00, 11.82it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:34
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.18it/s]                                              {'loss': 0.566, 'grad_norm': 3.4588990211486816, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.18it/s]  7%|▋         | 2/30 [00:00<00:05,  5.48it/s]                                              {'loss': 0.3577, 'grad_norm': 1.1984953880310059, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.48it/s] 10%|█         | 3/30 [00:00<00:04,  5.66it/s]                                              {'loss': 0.392, 'grad_norm': 1.3535784482955933, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  5.66it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.34it/s]                                              {'loss': 0.7827, 'grad_norm': 3.6679763793945312, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.34it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.06it/s]                                              {'loss': 0.5371, 'grad_norm': 2.897364377975464, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.06it/s] 20%|██        | 6/30 [00:01<00:04,  5.80it/s]                                              {'loss': 1.2316, 'grad_norm': 8.948732376098633, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.80it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.05it/s]                                              {'loss': 0.3758, 'grad_norm': 2.072375535964966, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.05it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.57it/s]                                              {'loss': 0.4333, 'grad_norm': 2.0596392154693604, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.57it/s] 30%|███       | 9/30 [00:01<00:04,  4.59it/s]                                              {'loss': 0.6035, 'grad_norm': 2.191676616668701, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.59it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.66it/s]                                               {'loss': 0.3866, 'grad_norm': 1.7978187799453735, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.66it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.68it/s]                                               {'loss': 0.607, 'grad_norm': 2.36081600189209, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.68it/s] 40%|████      | 12/30 [00:02<00:03,  5.30it/s]                                               {'loss': 0.4073, 'grad_norm': 3.746462345123291, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.30it/s] 43%|████▎     | 13/30 [00:02<00:03,  4.84it/s]                                               {'loss': 0.2699, 'grad_norm': 1.6450636386871338, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  4.84it/s] 47%|████▋     | 14/30 [00:02<00:03,  4.58it/s]                                               {'loss': 0.3306, 'grad_norm': 1.6859259605407715, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  4.58it/s] 50%|█████     | 15/30 [00:03<00:03,  4.68it/s]                                               {'loss': 0.7772, 'grad_norm': 4.223691463470459, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.68it/s] 53%|█████▎    | 16/30 [00:03<00:02,  4.89it/s]                                               {'loss': 0.4653, 'grad_norm': 2.39486026763916, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  4.89it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.22it/s]                                               {'loss': 0.4351, 'grad_norm': 1.5608208179473877, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.22it/s] 60%|██████    | 18/30 [00:03<00:02,  5.68it/s]                                               {'loss': 1.0005, 'grad_norm': 9.723268508911133, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.68it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.90it/s]                                               {'loss': 0.3255, 'grad_norm': 2.1744701862335205, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.90it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.67it/s]                                               {'loss': 0.4951, 'grad_norm': 2.3940117359161377, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.67it/s] 70%|███████   | 21/30 [00:04<00:01,  4.89it/s]                                               {'loss': 0.4548, 'grad_norm': 2.6921401023864746, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  4.89it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.69it/s]                                               {'loss': 0.4727, 'grad_norm': 1.881299614906311, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.69it/s] 77%|███████▋  | 23/30 [00:04<00:01,  5.89it/s]                                               {'loss': 0.3504, 'grad_norm': 2.1825621128082275, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  5.89it/s]                                               {'loss': 0.3994, 'grad_norm': 3.2545113563537598, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  5.89it/s] 83%|████████▎ | 25/30 [00:04<00:00,  6.11it/s]                                               {'loss': 0.6775, 'grad_norm': 2.7608706951141357, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  6.11it/s] 87%|████████▋ | 26/30 [00:04<00:00,  5.98it/s]                                               {'loss': 0.4742, 'grad_norm': 2.595299005508423, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  5.98it/s]                                               {'loss': 0.4089, 'grad_norm': 1.7784948348999023, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  5.98it/s] 93%|█████████▎| 28/30 [00:05<00:00,  6.99it/s]                                               {'loss': 0.3258, 'grad_norm': 2.2633450031280518, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  6.99it/s] 97%|█████████▋| 29/30 [00:05<00:00,  7.46it/s]                                               {'loss': 0.5337, 'grad_norm': 2.922579526901245, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  7.46it/s]                                               {'loss': 0.2118, 'grad_norm': 3.683469772338867, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  7.46it/s]                                               {'train_runtime': 5.4241, 'train_samples_per_second': 78.354, 'train_steps_per_second': 5.531, 'train_loss': 0.5029647285739581, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  7.46it/s]100%|██████████| 30/30 [00:05<00:00,  5.53it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.08it/s]                                              {'loss': 0.6024, 'grad_norm': 1.8599748611450195, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.08it/s]  7%|▋         | 2/30 [00:00<00:03,  7.27it/s]                                              {'loss': 0.693, 'grad_norm': 2.623450756072998, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.27it/s] 10%|█         | 3/30 [00:00<00:03,  6.98it/s]                                              {'loss': 0.6783, 'grad_norm': 1.4345054626464844, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  6.98it/s]                                              {'loss': 0.6226, 'grad_norm': 1.2013970613479614, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.98it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.80it/s]                                              {'loss': 0.7231, 'grad_norm': 1.5211398601531982, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.80it/s] 20%|██        | 6/30 [00:01<00:06,  3.81it/s]                                              {'loss': 0.8637, 'grad_norm': 3.09828782081604, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.81it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.18it/s]                                              {'loss': 0.6997, 'grad_norm': 1.8337374925613403, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.18it/s] 27%|██▋       | 8/30 [00:01<00:05,  4.38it/s]                                              {'loss': 0.6637, 'grad_norm': 0.9758141040802002, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  4.38it/s] 30%|███       | 9/30 [00:01<00:04,  4.56it/s]                                              {'loss': 0.693, 'grad_norm': 1.0970890522003174, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.56it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.68it/s]                                               {'loss': 0.663, 'grad_norm': 2.577956199645996, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.68it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.73it/s]                                               {'loss': 0.6268, 'grad_norm': 0.9576169848442078, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.73it/s]                                               {'loss': 0.6563, 'grad_norm': 2.4110212326049805, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.73it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.62it/s]                                               {'loss': 0.6085, 'grad_norm': 1.5175174474716187, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.62it/s] 47%|████▋     | 14/30 [00:02<00:03,  4.96it/s]                                               {'loss': 0.6618, 'grad_norm': 2.053313970565796, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  4.96it/s] 50%|█████     | 15/30 [00:03<00:03,  4.20it/s]                                               {'loss': 0.7093, 'grad_norm': 2.802001714706421, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.20it/s] 53%|█████▎    | 16/30 [00:03<00:03,  3.78it/s]                                               {'loss': 0.5898, 'grad_norm': 1.9597140550613403, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  3.78it/s] 57%|█████▋    | 17/30 [00:03<00:03,  3.52it/s]                                               {'loss': 0.6034, 'grad_norm': 1.4855259656906128, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  3.52it/s] 60%|██████    | 18/30 [00:03<00:02,  4.10it/s]                                               {'loss': 0.6205, 'grad_norm': 4.293992519378662, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.10it/s] 63%|██████▎   | 19/30 [00:04<00:03,  3.65it/s]                                               {'loss': 0.5891, 'grad_norm': 3.2005653381347656, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:03,  3.65it/s] 67%|██████▋   | 20/30 [00:04<00:02,  3.38it/s]                                               {'loss': 0.6693, 'grad_norm': 2.5021657943725586, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  3.38it/s] 70%|███████   | 21/30 [00:05<00:02,  3.20it/s]                                               {'loss': 0.5864, 'grad_norm': 1.965366244316101, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:02,  3.20it/s] 73%|███████▎  | 22/30 [00:05<00:02,  3.20it/s]                                               {'loss': 0.6193, 'grad_norm': 3.112706422805786, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:02,  3.20it/s] 77%|███████▋  | 23/30 [00:05<00:02,  3.08it/s]                                               {'loss': 0.6131, 'grad_norm': 3.436720132827759, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:02,  3.08it/s] 80%|████████  | 24/30 [00:05<00:01,  3.76it/s]                                               {'loss': 0.5269, 'grad_norm': 2.8860020637512207, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  3.76it/s] 83%|████████▎ | 25/30 [00:06<00:01,  3.40it/s]                                               {'loss': 0.603, 'grad_norm': 2.3921236991882324, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:01,  3.40it/s] 87%|████████▋ | 26/30 [00:06<00:01,  3.19it/s]                                               {'loss': 0.5119, 'grad_norm': 2.290823221206665, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:01,  3.19it/s] 90%|█████████ | 27/30 [00:06<00:00,  3.06it/s]                                               {'loss': 0.5379, 'grad_norm': 2.3017418384552, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  3.06it/s] 93%|█████████▎| 28/30 [00:07<00:00,  2.97it/s]                                               {'loss': 0.5938, 'grad_norm': 2.4131076335906982, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  2.97it/s] 97%|█████████▋| 29/30 [00:07<00:00,  2.92it/s]                                               {'loss': 0.4882, 'grad_norm': 1.8247287273406982, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  2.92it/s]                                               {'loss': 0.5024, 'grad_norm': 3.951552152633667, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  2.92it/s]                                               {'train_runtime': 7.7572, 'train_samples_per_second': 54.788, 'train_steps_per_second': 3.867, 'train_loss': 0.6273388087749481, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  2.92it/s]100%|██████████| 30/30 [00:07<00:00,  3.87it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.20it/s]                                              {'loss': 0.6998, 'grad_norm': 2.908019542694092, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.20it/s]  7%|▋         | 2/30 [00:00<00:04,  6.70it/s]                                              {'loss': 0.7506, 'grad_norm': 1.801977515220642, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.70it/s] 10%|█         | 3/30 [00:00<00:04,  6.13it/s]                                              {'loss': 0.7179, 'grad_norm': 1.1489344835281372, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.13it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.20it/s]                                              {'loss': 0.6661, 'grad_norm': 1.503653645515442, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.20it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.01it/s]                                              {'loss': 0.74, 'grad_norm': 1.307694435119629, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.01it/s]                                              {'loss': 0.4878, 'grad_norm': 2.460859775543213, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.01it/s] 23%|██▎       | 7/30 [00:01<00:03,  5.77it/s]                                              {'loss': 0.6818, 'grad_norm': 0.9902096390724182, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  5.77it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.37it/s]                                              {'loss': 0.8334, 'grad_norm': 2.501801013946533, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.37it/s] 30%|███       | 9/30 [00:01<00:03,  5.83it/s]                                              {'loss': 0.5698, 'grad_norm': 0.7689794898033142, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.83it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.39it/s]                                               {'loss': 0.6767, 'grad_norm': 0.8020069599151611, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.39it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.51it/s]                                               {'loss': 0.6788, 'grad_norm': 0.7079064249992371, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.51it/s]                                               {'loss': 0.5526, 'grad_norm': 1.2978111505508423, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.51it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.53it/s]                                               {'loss': 0.6415, 'grad_norm': 0.46526142954826355, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.53it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.79it/s]                                               {'loss': 0.6364, 'grad_norm': 1.0511482954025269, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.79it/s] 50%|█████     | 15/30 [00:02<00:02,  5.05it/s]                                               {'loss': 0.811, 'grad_norm': 2.647873878479004, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.05it/s] 53%|█████▎    | 16/30 [00:02<00:03,  4.58it/s]                                               {'loss': 0.6909, 'grad_norm': 1.8132116794586182, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:03,  4.58it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.43it/s]                                               {'loss': 0.6137, 'grad_norm': 0.7457672357559204, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.43it/s] 60%|██████    | 18/30 [00:03<00:02,  4.95it/s]                                               {'loss': 0.6872, 'grad_norm': 1.3261855840682983, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.95it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.33it/s]                                               {'loss': 0.6399, 'grad_norm': 0.7403445839881897, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.33it/s] 67%|██████▋   | 20/30 [00:03<00:02,  3.98it/s]                                               {'loss': 0.7105, 'grad_norm': 1.1246898174285889, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:02,  3.98it/s] 70%|███████   | 21/30 [00:04<00:02,  3.72it/s]                                               {'loss': 0.6433, 'grad_norm': 1.0075753927230835, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:02,  3.72it/s] 73%|███████▎  | 22/30 [00:04<00:02,  3.48it/s]                                               {'loss': 0.6722, 'grad_norm': 0.6751427054405212, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:02,  3.48it/s] 77%|███████▋  | 23/30 [00:04<00:02,  3.25it/s]                                               {'loss': 0.6382, 'grad_norm': 0.9570590853691101, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:02,  3.25it/s] 80%|████████  | 24/30 [00:05<00:01,  3.47it/s]                                               {'loss': 0.61, 'grad_norm': 1.6912051439285278, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  3.47it/s] 83%|████████▎ | 25/30 [00:05<00:01,  3.35it/s]                                               {'loss': 0.6458, 'grad_norm': 1.5017879009246826, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  3.35it/s] 87%|████████▋ | 26/30 [00:05<00:01,  3.49it/s]                                               {'loss': 0.6564, 'grad_norm': 0.8395826816558838, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:01,  3.49it/s] 90%|█████████ | 27/30 [00:06<00:00,  3.52it/s]                                               {'loss': 0.6514, 'grad_norm': 0.7572856545448303, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  3.52it/s] 93%|█████████▎| 28/30 [00:06<00:00,  3.64it/s]                                               {'loss': 0.6471, 'grad_norm': 0.6852712035179138, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  3.64it/s] 97%|█████████▋| 29/30 [00:06<00:00,  3.71it/s]                                               {'loss': 0.6493, 'grad_norm': 1.5523302555084229, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  3.71it/s]100%|██████████| 30/30 [00:06<00:00,  4.29it/s]                                               {'loss': 0.6227, 'grad_norm': 2.2959606647491455, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.29it/s]                                               {'train_runtime': 7.0039, 'train_samples_per_second': 60.68, 'train_steps_per_second': 4.283, 'train_loss': 0.6640936255455017, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.29it/s]100%|██████████| 30/30 [00:06<00:00,  4.29it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.34it/s]                                              {'loss': 0.5197, 'grad_norm': 4.665818214416504, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.34it/s]  7%|▋         | 2/30 [00:00<00:06,  4.62it/s]                                              {'loss': 0.1918, 'grad_norm': 1.9153062105178833, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.62it/s] 10%|█         | 3/30 [00:00<00:05,  4.76it/s]                                              {'loss': 0.0366, 'grad_norm': 1.167633056640625, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.76it/s] 13%|█▎        | 4/30 [00:00<00:05,  5.06it/s]                                              {'loss': 0.3207, 'grad_norm': 1.758481502532959, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  5.06it/s] 17%|█▋        | 5/30 [00:01<00:04,  5.15it/s]                                              {'loss': 0.2998, 'grad_norm': 1.522721290588379, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:04,  5.15it/s]                                              {'loss': 0.0191, 'grad_norm': 0.3095857501029968, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.15it/s] 23%|██▎       | 7/30 [00:01<00:03,  5.87it/s]                                              {'loss': 0.2166, 'grad_norm': 0.9194917678833008, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  5.87it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.87it/s]                                              {'loss': 0.0359, 'grad_norm': 0.4249756336212158, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.87it/s] 30%|███       | 9/30 [00:01<00:03,  6.11it/s]                                              {'loss': 0.3262, 'grad_norm': 6.386224746704102, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.11it/s]                                              {'loss': 0.033, 'grad_norm': 0.39001187682151794, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.11it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.54it/s]                                               {'loss': 0.0204, 'grad_norm': 0.35176897048950195, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.54it/s]                                               {'loss': 0.014, 'grad_norm': 0.3046585023403168, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.54it/s] 43%|████▎     | 13/30 [00:01<00:01,  8.64it/s]                                               {'loss': 0.0101, 'grad_norm': 0.18574270606040955, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  8.64it/s] 47%|████▋     | 14/30 [00:02<00:01,  8.25it/s]                                               {'loss': 0.0034, 'grad_norm': 0.11585642397403717, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  8.25it/s] 50%|█████     | 15/30 [00:02<00:01,  8.06it/s]                                               {'loss': 0.4594, 'grad_norm': 1.011675477027893, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.06it/s] 53%|█████▎    | 16/30 [00:02<00:01,  8.14it/s]                                               {'loss': 0.0008, 'grad_norm': 0.011389157734811306, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.14it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.98it/s]                                               {'loss': 0.3656, 'grad_norm': 1.0651832818984985, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.98it/s] 60%|██████    | 18/30 [00:02<00:01,  7.86it/s]                                               {'loss': 0.0013, 'grad_norm': 0.030711982399225235, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.86it/s] 63%|██████▎   | 19/30 [00:02<00:01,  6.78it/s]                                               {'loss': 0.0013, 'grad_norm': 0.018423212692141533, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  6.78it/s] 67%|██████▋   | 20/30 [00:03<00:01,  6.14it/s]                                               {'loss': 0.4281, 'grad_norm': 1.3985074758529663, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  6.14it/s] 70%|███████   | 21/30 [00:03<00:01,  5.90it/s]                                               {'loss': 0.0023, 'grad_norm': 0.044728782027959824, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.90it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.34it/s]                                               {'loss': 0.0029, 'grad_norm': 0.05800412595272064, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.34it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.74it/s]                                               {'loss': 0.3521, 'grad_norm': 1.9092843532562256, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.74it/s] 80%|████████  | 24/30 [00:03<00:01,  5.42it/s]                                               {'loss': 0.0045, 'grad_norm': 0.11740061640739441, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:01,  5.42it/s] 83%|████████▎ | 25/30 [00:04<00:01,  4.96it/s]                                               {'loss': 0.7387, 'grad_norm': 3.1563072204589844, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:01,  4.96it/s] 87%|████████▋ | 26/30 [00:04<00:00,  4.99it/s]                                               {'loss': 0.0105, 'grad_norm': 0.22860462963581085, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  4.99it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.12it/s]                                               {'loss': 0.0127, 'grad_norm': 0.2779586911201477, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.12it/s] 93%|█████████▎| 28/30 [00:04<00:00,  5.19it/s]                                               {'loss': 0.0108, 'grad_norm': 0.24465365707874298, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  5.19it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.38it/s]                                               {'loss': 0.0149, 'grad_norm': 0.27255162596702576, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.38it/s]100%|██████████| 30/30 [00:04<00:00,  6.20it/s]                                               {'loss': 0.0164, 'grad_norm': 0.34357306361198425, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.20it/s]                                               {'train_runtime': 5.0991, 'train_samples_per_second': 83.348, 'train_steps_per_second': 5.883, 'train_loss': 0.14898669923034807, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.20it/s]100%|██████████| 30/30 [00:05<00:00,  5.89it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:32
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.79it/s]                                              {'loss': 0.7448, 'grad_norm': 2.18426513671875, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.79it/s]  7%|▋         | 2/30 [00:00<00:07,  3.85it/s]                                              {'loss': 0.6944, 'grad_norm': 1.3232742547988892, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.85it/s] 10%|█         | 3/30 [00:00<00:07,  3.74it/s]                                              {'loss': 0.6768, 'grad_norm': 1.486061930656433, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.74it/s] 13%|█▎        | 4/30 [00:01<00:06,  3.79it/s]                                              {'loss': 0.6477, 'grad_norm': 0.6779086589813232, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:06,  3.79it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.80it/s]                                              {'loss': 0.6707, 'grad_norm': 0.8510501384735107, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.80it/s]                                              {'loss': 0.7741, 'grad_norm': 1.7842395305633545, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.80it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.51it/s]                                              {'loss': 0.6278, 'grad_norm': 0.7773652672767639, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.51it/s] 27%|██▋       | 8/30 [00:01<00:05,  4.29it/s]                                              {'loss': 0.7457, 'grad_norm': 0.9672666192054749, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  4.29it/s] 30%|███       | 9/30 [00:02<00:05,  4.07it/s]                                              {'loss': 0.6602, 'grad_norm': 0.6502836346626282, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  4.07it/s] 33%|███▎      | 10/30 [00:02<00:05,  3.98it/s]                                               {'loss': 0.6534, 'grad_norm': 0.4505103528499603, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  3.98it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.01it/s]                                               {'loss': 0.6526, 'grad_norm': 0.5542356967926025, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.01it/s] 40%|████      | 12/30 [00:02<00:03,  4.82it/s]                                               {'loss': 0.5771, 'grad_norm': 1.993985891342163, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.82it/s] 43%|████▎     | 13/30 [00:03<00:03,  4.38it/s]                                               {'loss': 0.6071, 'grad_norm': 0.8752509951591492, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  4.38it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.22it/s]                                               {'loss': 0.6248, 'grad_norm': 1.2399234771728516, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.22it/s] 50%|█████     | 15/30 [00:03<00:03,  4.41it/s]                                               {'loss': 0.7513, 'grad_norm': 3.561070442199707, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.41it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.25it/s]                                               {'loss': 0.6668, 'grad_norm': 0.7473921179771423, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.25it/s] 57%|█████▋    | 17/30 [00:04<00:03,  4.16it/s]                                               {'loss': 0.5977, 'grad_norm': 1.2399864196777344, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:03,  4.16it/s] 60%|██████    | 18/30 [00:04<00:02,  4.74it/s]                                               {'loss': 0.7571, 'grad_norm': 1.5165495872497559, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.74it/s] 63%|██████▎   | 19/30 [00:04<00:02,  4.63it/s]                                               {'loss': 0.6137, 'grad_norm': 0.9752834439277649, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  4.63it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.77it/s]                                               {'loss': 0.6808, 'grad_norm': 0.6019803881645203, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.77it/s] 70%|███████   | 21/30 [00:04<00:01,  4.74it/s]                                               {'loss': 0.634, 'grad_norm': 0.6789559125900269, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  4.74it/s] 73%|███████▎  | 22/30 [00:05<00:01,  4.77it/s]                                               {'loss': 0.6275, 'grad_norm': 0.7576795816421509, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:01,  4.77it/s] 77%|███████▋  | 23/30 [00:05<00:01,  4.78it/s]                                               {'loss': 0.6485, 'grad_norm': 1.1462031602859497, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  4.78it/s] 80%|████████  | 24/30 [00:05<00:01,  5.53it/s]                                               {'loss': 0.6177, 'grad_norm': 1.2297992706298828, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.53it/s] 83%|████████▎ | 25/30 [00:05<00:01,  4.69it/s]                                               {'loss': 0.6314, 'grad_norm': 0.9337401390075684, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  4.69it/s] 87%|████████▋ | 26/30 [00:05<00:00,  4.24it/s]                                               {'loss': 0.6048, 'grad_norm': 0.6657608151435852, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  4.24it/s] 90%|█████████ | 27/30 [00:06<00:00,  3.98it/s]                                               {'loss': 0.6162, 'grad_norm': 0.9916669130325317, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  3.98it/s] 93%|█████████▎| 28/30 [00:06<00:00,  3.78it/s]                                               {'loss': 0.6273, 'grad_norm': 0.7185538411140442, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  3.78it/s] 97%|█████████▋| 29/30 [00:06<00:00,  3.86it/s]                                               {'loss': 0.5612, 'grad_norm': 1.0179325342178345, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  3.86it/s]                                               {'loss': 0.5756, 'grad_norm': 1.4412239789962769, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  3.86it/s]                                               {'train_runtime': 6.9549, 'train_samples_per_second': 61.108, 'train_steps_per_second': 4.314, 'train_loss': 0.6522953848044077, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  3.86it/s]100%|██████████| 30/30 [00:06<00:00,  4.32it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.76it/s]                                              {'loss': 0.4574, 'grad_norm': 3.2338740825653076, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.76it/s]  7%|▋         | 2/30 [00:00<00:04,  6.12it/s]                                              {'loss': 0.2896, 'grad_norm': 1.2668718099594116, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.12it/s] 10%|█         | 3/30 [00:00<00:04,  6.20it/s]                                              {'loss': 0.4186, 'grad_norm': 1.56094491481781, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.20it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.40it/s]                                              {'loss': 0.2659, 'grad_norm': 1.2956510782241821, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.40it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.73it/s]                                              {'loss': 0.6838, 'grad_norm': 2.2880027294158936, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.73it/s]                                              {'loss': 0.0649, 'grad_norm': 0.8383931517601013, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.73it/s] 23%|██▎       | 7/30 [00:00<00:02,  7.87it/s]                                              {'loss': 0.1537, 'grad_norm': 0.7563726902008057, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  7.87it/s] 27%|██▋       | 8/30 [00:01<00:03,  7.22it/s]                                              {'loss': 0.4712, 'grad_norm': 3.0929062366485596, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  7.22it/s] 30%|███       | 9/30 [00:01<00:02,  7.08it/s]                                              {'loss': 0.5447, 'grad_norm': 2.538677930831909, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.08it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.02it/s]                                               {'loss': 0.5759, 'grad_norm': 2.5581676959991455, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.02it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.97it/s]                                               {'loss': 0.2372, 'grad_norm': 1.3498384952545166, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.97it/s]                                               {'loss': 0.5181, 'grad_norm': 3.117523193359375, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.97it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.65it/s]                                               {'loss': 0.3791, 'grad_norm': 1.1485600471496582, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.65it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.33it/s]                                               {'loss': 0.3524, 'grad_norm': 1.8938672542572021, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.33it/s] 50%|█████     | 15/30 [00:02<00:02,  6.46it/s]                                               {'loss': 0.5391, 'grad_norm': 2.1090214252471924, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.46it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.09it/s]                                               {'loss': 0.4203, 'grad_norm': 2.0443294048309326, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.09it/s] 57%|█████▋    | 17/30 [00:02<00:02,  5.67it/s]                                               {'loss': 0.2035, 'grad_norm': 1.9358675479888916, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  5.67it/s] 60%|██████    | 18/30 [00:02<00:01,  6.14it/s]                                               {'loss': 0.613, 'grad_norm': 4.953054904937744, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  6.14it/s] 63%|██████▎   | 19/30 [00:02<00:02,  5.28it/s]                                               {'loss': 0.3381, 'grad_norm': 2.3408448696136475, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:02,  5.28it/s] 67%|██████▋   | 20/30 [00:03<00:02,  4.77it/s]                                               {'loss': 0.4611, 'grad_norm': 2.5093462467193604, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:02,  4.77it/s] 70%|███████   | 21/30 [00:03<00:01,  4.72it/s]                                               {'loss': 0.4412, 'grad_norm': 2.8100857734680176, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  4.72it/s] 73%|███████▎  | 22/30 [00:03<00:01,  4.90it/s]                                               {'loss': 0.4705, 'grad_norm': 2.674360513687134, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  4.90it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.01it/s]                                               {'loss': 0.1615, 'grad_norm': 2.272855043411255, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.01it/s]                                               {'loss': 0.4321, 'grad_norm': 4.246249675750732, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:01,  5.01it/s] 83%|████████▎ | 25/30 [00:04<00:00,  5.56it/s]                                               {'loss': 0.4178, 'grad_norm': 2.8535661697387695, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  5.56it/s] 87%|████████▋ | 26/30 [00:04<00:00,  5.26it/s]                                               {'loss': 0.4512, 'grad_norm': 3.0460519790649414, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  5.26it/s] 90%|█████████ | 27/30 [00:04<00:00,  4.94it/s]                                               {'loss': 0.5549, 'grad_norm': 7.942321300506592, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  4.94it/s] 93%|█████████▎| 28/30 [00:04<00:00,  5.10it/s]                                               {'loss': 0.1065, 'grad_norm': 1.8855172395706177, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  5.10it/s] 97%|█████████▋| 29/30 [00:04<00:00,  4.99it/s]                                               {'loss': 0.3217, 'grad_norm': 2.971041202545166, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  4.99it/s]                                               {'loss': 0.7018, 'grad_norm': 7.50330924987793, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.99it/s]                                               {'train_runtime': 5.1197, 'train_samples_per_second': 83.013, 'train_steps_per_second': 5.86, 'train_loss': 0.4015655102829138, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.99it/s]100%|██████████| 30/30 [00:05<00:00,  5.86it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:10,  2.89it/s]                                              {'loss': 0.6094, 'grad_norm': 4.264180660247803, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:10,  2.89it/s]  7%|▋         | 2/30 [00:00<00:09,  2.83it/s]                                              {'loss': 0.1962, 'grad_norm': 1.7748533487319946, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:09,  2.83it/s] 10%|█         | 3/30 [00:01<00:09,  2.79it/s]                                              {'loss': 0.0489, 'grad_norm': 0.8968688249588013, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:09,  2.79it/s] 13%|█▎        | 4/30 [00:01<00:09,  2.78it/s]                                              {'loss': 0.3076, 'grad_norm': 1.2197856903076172, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:09,  2.78it/s] 17%|█▋        | 5/30 [00:01<00:08,  2.95it/s]                                              {'loss': 0.0073, 'grad_norm': 0.09449756890535355, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:08,  2.95it/s] 20%|██        | 6/30 [00:02<00:12,  1.96it/s]                                              {'loss': 0.0095, 'grad_norm': 0.16877886652946472, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:12,  1.96it/s] 23%|██▎       | 7/30 [00:02<00:10,  2.15it/s]                                              {'loss': 0.0072, 'grad_norm': 0.12088451534509659, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:10,  2.15it/s] 27%|██▋       | 8/30 [00:03<00:09,  2.30it/s]                                              {'loss': 0.0044, 'grad_norm': 0.07957276701927185, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:03<00:09,  2.30it/s] 30%|███       | 9/30 [00:03<00:08,  2.43it/s]                                              {'loss': 0.0053, 'grad_norm': 0.09116199612617493, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:03<00:08,  2.43it/s] 33%|███▎      | 10/30 [00:04<00:07,  2.53it/s]                                               {'loss': 0.2318, 'grad_norm': 4.59003210067749, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:04<00:07,  2.53it/s] 37%|███▋      | 11/30 [00:04<00:07,  2.59it/s]                                               {'loss': 0.0024, 'grad_norm': 0.0646781176328659, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:04<00:07,  2.59it/s] 40%|████      | 12/30 [00:04<00:05,  3.30it/s]                                               {'loss': 0.0031, 'grad_norm': 0.09865433722734451, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:04<00:05,  3.30it/s] 43%|████▎     | 13/30 [00:04<00:05,  3.10it/s]                                               {'loss': 0.0024, 'grad_norm': 0.07107766717672348, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:04<00:05,  3.10it/s] 47%|████▋     | 14/30 [00:05<00:05,  2.94it/s]                                               {'loss': 0.004, 'grad_norm': 0.07352637499570847, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:05<00:05,  2.94it/s] 50%|█████     | 15/30 [00:05<00:05,  2.87it/s]                                               {'loss': 0.4492, 'grad_norm': 4.895946025848389, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:05<00:05,  2.87it/s] 53%|█████▎    | 16/30 [00:05<00:04,  2.84it/s]                                               {'loss': 0.0027, 'grad_norm': 0.07828618586063385, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:05<00:04,  2.84it/s] 57%|█████▋    | 17/30 [00:06<00:04,  2.80it/s]                                               {'loss': 0.0078, 'grad_norm': 0.18139798939228058, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:06<00:04,  2.80it/s] 60%|██████    | 18/30 [00:06<00:03,  3.41it/s]                                               {'loss': 0.0028, 'grad_norm': 0.12292436510324478, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:06<00:03,  3.41it/s] 63%|██████▎   | 19/30 [00:06<00:03,  3.21it/s]                                               {'loss': 0.0023, 'grad_norm': 0.046235162764787674, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:06<00:03,  3.21it/s] 67%|██████▋   | 20/30 [00:07<00:03,  3.07it/s]                                               {'loss': 0.004, 'grad_norm': 0.09294530749320984, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:07<00:03,  3.07it/s] 70%|███████   | 21/30 [00:07<00:03,  2.98it/s]                                               {'loss': 0.003, 'grad_norm': 0.08025859296321869, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:07<00:03,  2.98it/s] 73%|███████▎  | 22/30 [00:07<00:02,  2.89it/s]                                               {'loss': 0.0024, 'grad_norm': 0.05095573887228966, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:07<00:02,  2.89it/s] 77%|███████▋  | 23/30 [00:08<00:02,  2.84it/s]                                               {'loss': 0.4893, 'grad_norm': 1.498813509941101, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:08<00:02,  2.84it/s] 80%|████████  | 24/30 [00:08<00:01,  3.47it/s]                                               {'loss': 0.0016, 'grad_norm': 0.038038015365600586, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:08<00:01,  3.47it/s] 83%|████████▎ | 25/30 [00:08<00:01,  3.22it/s]                                               {'loss': 0.0046, 'grad_norm': 0.11169453710317612, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:08<00:01,  3.22it/s] 87%|████████▋ | 26/30 [00:09<00:01,  3.08it/s]                                               {'loss': 0.0034, 'grad_norm': 0.07490803301334381, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:09<00:01,  3.08it/s] 90%|█████████ | 27/30 [00:09<00:01,  2.98it/s]                                               {'loss': 0.3444, 'grad_norm': 1.2724803686141968, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:09<00:01,  2.98it/s] 93%|█████████▎| 28/30 [00:09<00:00,  2.92it/s]                                               {'loss': 0.0019, 'grad_norm': 0.048076629638671875, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:09<00:00,  2.92it/s] 97%|█████████▋| 29/30 [00:10<00:00,  2.89it/s]                                               {'loss': 0.0033, 'grad_norm': 0.0724216178059578, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:10<00:00,  2.89it/s]100%|██████████| 30/30 [00:10<00:00,  3.64it/s]                                               {'loss': 0.003, 'grad_norm': 0.06128641963005066, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:10<00:00,  3.64it/s]                                               {'train_runtime': 10.4455, 'train_samples_per_second': 40.688, 'train_steps_per_second': 2.872, 'train_loss': 0.09217306996773307, 'epoch': 5.0}
100%|██████████| 30/30 [00:10<00:00,  3.64it/s]100%|██████████| 30/30 [00:10<00:00,  2.87it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:01<00:33,  1.17s/it]                                              {'loss': 0.7775, 'grad_norm': 2.5585691928863525, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:01<00:33,  1.17s/it]  7%|▋         | 2/30 [00:01<00:16,  1.65it/s]                                              {'loss': 0.7055, 'grad_norm': 1.3197466135025024, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:16,  1.65it/s] 10%|█         | 3/30 [00:01<00:12,  2.18it/s]                                              {'loss': 0.6438, 'grad_norm': 1.6042720079421997, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:12,  2.18it/s] 13%|█▎        | 4/30 [00:01<00:09,  2.60it/s]                                              {'loss': 0.622, 'grad_norm': 1.1875537633895874, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:09,  2.60it/s] 17%|█▋        | 5/30 [00:02<00:08,  2.89it/s]                                              {'loss': 0.7815, 'grad_norm': 1.4563496112823486, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:02<00:08,  2.89it/s]                                              {'loss': 0.7737, 'grad_norm': 2.68798565864563, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:08,  2.89it/s] 23%|██▎       | 7/30 [00:02<00:06,  3.74it/s]                                              {'loss': 0.621, 'grad_norm': 0.6898695826530457, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:06,  3.74it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.62it/s]                                              {'loss': 0.6993, 'grad_norm': 0.5756934285163879, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.62it/s] 30%|███       | 9/30 [00:03<00:05,  3.88it/s]                                              {'loss': 0.6845, 'grad_norm': 0.770555317401886, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:03<00:05,  3.88it/s] 33%|███▎      | 10/30 [00:03<00:04,  4.08it/s]                                               {'loss': 0.7092, 'grad_norm': 0.6898161172866821, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:04,  4.08it/s] 37%|███▋      | 11/30 [00:03<00:04,  4.24it/s]                                               {'loss': 0.6558, 'grad_norm': 0.42348065972328186, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:04,  4.24it/s]                                               {'loss': 0.6522, 'grad_norm': 3.3238325119018555, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:04,  4.24it/s] 43%|████▎     | 13/30 [00:03<00:03,  5.18it/s]                                               {'loss': 0.6065, 'grad_norm': 0.5920071601867676, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  5.18it/s] 47%|████▋     | 14/30 [00:04<00:03,  4.89it/s]                                               {'loss': 0.679, 'grad_norm': 0.7092788815498352, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:03,  4.89it/s] 50%|█████     | 15/30 [00:04<00:02,  5.14it/s]                                               {'loss': 0.6889, 'grad_norm': 0.8918770551681519, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:02,  5.14it/s]                                               {'loss': 0.6226, 'grad_norm': 0.7280734777450562, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:02,  5.14it/s] 57%|█████▋    | 17/30 [00:04<00:02,  6.15it/s]                                               {'loss': 0.5813, 'grad_norm': 1.0494356155395508, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:02,  6.15it/s]                                               {'loss': 0.5563, 'grad_norm': 1.5367515087127686, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:01,  6.15it/s] 63%|██████▎   | 19/30 [00:04<00:01,  7.44it/s]                                               {'loss': 0.5978, 'grad_norm': 1.2802952527999878, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:01,  7.44it/s] 67%|██████▋   | 20/30 [00:04<00:01,  7.52it/s]                                               {'loss': 0.7261, 'grad_norm': 0.9965215921401978, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  7.52it/s] 70%|███████   | 21/30 [00:04<00:01,  7.61it/s]                                               {'loss': 0.6863, 'grad_norm': 1.327384352684021, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  7.61it/s]                                               {'loss': 0.5892, 'grad_norm': 1.4161627292633057, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  7.61it/s] 77%|███████▋  | 23/30 [00:05<00:00,  8.90it/s]                                               {'loss': 0.671, 'grad_norm': 1.333915114402771, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:00,  8.90it/s]                                               {'loss': 0.6811, 'grad_norm': 5.4425177574157715, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:00,  8.90it/s] 83%|████████▎ | 25/30 [00:05<00:00, 10.55it/s]                                               {'loss': 0.671, 'grad_norm': 1.436536192893982, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00, 10.55it/s]                                               {'loss': 0.7154, 'grad_norm': 1.7735023498535156, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00, 10.55it/s] 90%|█████████ | 27/30 [00:05<00:00, 11.05it/s]                                               {'loss': 0.5969, 'grad_norm': 1.265010118484497, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00, 11.05it/s]                                               {'loss': 0.6262, 'grad_norm': 1.5267150402069092, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00, 11.05it/s] 97%|█████████▋| 29/30 [00:05<00:00, 11.50it/s]                                               {'loss': 0.6041, 'grad_norm': 1.7635254859924316, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00, 11.50it/s]                                               {'loss': 0.5887, 'grad_norm': 2.1947858333587646, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00, 11.50it/s]                                               {'train_runtime': 5.6508, 'train_samples_per_second': 75.211, 'train_steps_per_second': 5.309, 'train_loss': 0.6604719519615173, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00, 11.50it/s]100%|██████████| 30/30 [00:05<00:00,  5.31it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
the shape of base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.weight is (768, 768)
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  5%|▍         | 3/66 [00:00<00:03, 17.18it/s]  8%|▊         | 5/66 [00:00<00:03, 16.24it/s] 11%|█         | 7/66 [00:00<00:03, 16.12it/s] 14%|█▎        | 9/66 [00:00<00:04, 13.02it/s] 17%|█▋        | 11/66 [00:00<00:04, 13.56it/s] 21%|██        | 14/66 [00:00<00:03, 15.67it/s] 24%|██▍       | 16/66 [00:01<00:03, 15.49it/s] 27%|██▋       | 18/66 [00:01<00:03, 15.05it/s] 30%|███       | 20/66 [00:01<00:03, 15.28it/s] 33%|███▎      | 22/66 [00:01<00:02, 15.15it/s] 36%|███▋      | 24/66 [00:01<00:02, 16.00it/s] 41%|████      | 27/66 [00:01<00:02, 18.41it/s] 45%|████▌     | 30/66 [00:01<00:01, 20.16it/s] 50%|█████     | 33/66 [00:01<00:01, 22.05it/s] 55%|█████▍    | 36/66 [00:02<00:01, 23.69it/s] 59%|█████▉    | 39/66 [00:02<00:01, 24.80it/s] 64%|██████▎   | 42/66 [00:02<00:00, 26.10it/s] 68%|██████▊   | 45/66 [00:02<00:00, 27.11it/s] 73%|███████▎  | 48/66 [00:02<00:00, 27.39it/s] 77%|███████▋  | 51/66 [00:02<00:00, 27.67it/s] 82%|████████▏ | 54/66 [00:02<00:00, 27.83it/s] 86%|████████▋ | 57/66 [00:02<00:00, 28.08it/s] 91%|█████████ | 60/66 [00:02<00:00, 28.28it/s] 95%|█████████▌| 63/66 [00:02<00:00, 28.27it/s]100%|██████████| 66/66 [00:03<00:00, 21.61it/s]
{'eval_loss': 0.6071798801422119, 'eval_model_preparation_time': 0.0088, 'eval_acc': 0.6912751677852349, 'eval_runtime': 3.1721, 'eval_samples_per_second': 328.799, 'eval_steps_per_second': 20.806}
ROUND:7
CLIENT:71
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.19it/s]                                              {'loss': 0.7619, 'grad_norm': 2.36525559425354, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.19it/s]  7%|▋         | 2/30 [00:00<00:07,  3.78it/s]                                              {'loss': 0.571, 'grad_norm': 2.02716064453125, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.78it/s] 10%|█         | 3/30 [00:00<00:07,  3.60it/s]                                              {'loss': 0.4624, 'grad_norm': 1.083452820777893, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.60it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.54it/s]                                              {'loss': 0.8241, 'grad_norm': 3.479137659072876, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.54it/s] 17%|█▋        | 5/30 [00:01<00:07,  3.55it/s]                                              {'loss': 0.7415, 'grad_norm': 2.088078498840332, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:07,  3.55it/s] 20%|██        | 6/30 [00:02<00:12,  1.91it/s]                                              {'loss': 1.1072, 'grad_norm': 4.629857540130615, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:12,  1.91it/s] 23%|██▎       | 7/30 [00:02<00:10,  2.22it/s]                                              {'loss': 0.5498, 'grad_norm': 0.9755723476409912, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:10,  2.22it/s] 27%|██▋       | 8/30 [00:02<00:08,  2.51it/s]                                              {'loss': 0.6358, 'grad_norm': 0.9160223603248596, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:08,  2.51it/s] 30%|███       | 9/30 [00:03<00:07,  2.75it/s]                                              {'loss': 0.6383, 'grad_norm': 1.1375994682312012, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:03<00:07,  2.75it/s] 33%|███▎      | 10/30 [00:03<00:06,  2.93it/s]                                               {'loss': 0.6207, 'grad_norm': 0.9112566113471985, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:06,  2.93it/s] 37%|███▋      | 11/30 [00:03<00:06,  3.09it/s]                                               {'loss': 0.616, 'grad_norm': 1.103882908821106, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:06,  3.09it/s]                                               {'loss': 0.5519, 'grad_norm': 1.961775779724121, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:05,  3.09it/s] 43%|████▎     | 13/30 [00:04<00:04,  4.11it/s]                                               {'loss': 0.6203, 'grad_norm': 1.6200774908065796, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:04<00:04,  4.11it/s] 47%|████▋     | 14/30 [00:04<00:03,  4.19it/s]                                               {'loss': 0.5366, 'grad_norm': 1.4094206094741821, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:03,  4.19it/s] 50%|█████     | 15/30 [00:04<00:03,  4.29it/s]                                               {'loss': 0.7205, 'grad_norm': 2.0167999267578125, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:03,  4.29it/s] 53%|█████▎    | 16/30 [00:04<00:03,  4.35it/s]                                               {'loss': 0.513, 'grad_norm': 1.5163142681121826, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:03,  4.35it/s] 57%|█████▋    | 17/30 [00:05<00:02,  4.46it/s]                                               {'loss': 0.5282, 'grad_norm': 1.245385766029358, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:05<00:02,  4.46it/s]                                               {'loss': 0.642, 'grad_norm': 2.361948013305664, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:02,  4.46it/s] 63%|██████▎   | 19/30 [00:05<00:02,  5.18it/s]                                               {'loss': 0.5259, 'grad_norm': 3.1796274185180664, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  5.18it/s] 67%|██████▋   | 20/30 [00:05<00:02,  5.00it/s]                                               {'loss': 0.5469, 'grad_norm': 1.011027455329895, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  5.00it/s] 70%|███████   | 21/30 [00:05<00:01,  4.89it/s]                                               {'loss': 0.5246, 'grad_norm': 1.141217589378357, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:01,  4.89it/s] 73%|███████▎  | 22/30 [00:05<00:01,  4.77it/s]                                               {'loss': 0.53, 'grad_norm': 1.444788932800293, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:01,  4.77it/s] 77%|███████▋  | 23/30 [00:06<00:01,  4.60it/s]                                               {'loss': 0.5396, 'grad_norm': 1.247963547706604, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  4.60it/s]                                               {'loss': 0.4658, 'grad_norm': 2.1460373401641846, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  4.60it/s] 83%|████████▎ | 25/30 [00:06<00:00,  5.17it/s]                                               {'loss': 0.5252, 'grad_norm': 1.2483936548233032, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:00,  5.17it/s] 87%|████████▋ | 26/30 [00:06<00:00,  4.98it/s]                                               {'loss': 0.3976, 'grad_norm': 1.8380292654037476, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:00,  4.98it/s] 90%|█████████ | 27/30 [00:06<00:00,  5.02it/s]                                               {'loss': 0.5324, 'grad_norm': 1.4274734258651733, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  5.02it/s] 93%|█████████▎| 28/30 [00:07<00:00,  4.76it/s]                                               {'loss': 0.4257, 'grad_norm': 1.3417590856552124, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  4.76it/s] 97%|█████████▋| 29/30 [00:07<00:00,  4.68it/s]                                               {'loss': 0.4597, 'grad_norm': 1.5177885293960571, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  4.68it/s]                                               {'loss': 0.2719, 'grad_norm': 2.6760716438293457, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  4.68it/s]                                               {'train_runtime': 7.558, 'train_samples_per_second': 56.232, 'train_steps_per_second': 3.969, 'train_loss': 0.5795555661122004, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  4.68it/s]100%|██████████| 30/30 [00:07<00:00,  3.97it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.23it/s]                                              {'loss': 0.9423, 'grad_norm': 4.470630645751953, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.23it/s]  7%|▋         | 2/30 [00:00<00:08,  3.49it/s]                                              {'loss': 0.6472, 'grad_norm': 1.3612669706344604, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:08,  3.49it/s] 10%|█         | 3/30 [00:00<00:07,  3.55it/s]                                              {'loss': 0.6925, 'grad_norm': 1.318703055381775, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.55it/s] 13%|█▎        | 4/30 [00:01<00:06,  4.05it/s]                                              {'loss': 0.6912, 'grad_norm': 1.597355604171753, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:06,  4.05it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.33it/s]                                              {'loss': 0.7176, 'grad_norm': 1.3724428415298462, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.33it/s] 20%|██        | 6/30 [00:01<00:05,  4.03it/s]                                              {'loss': 0.6794, 'grad_norm': 2.445690393447876, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.03it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.31it/s]                                              {'loss': 0.7014, 'grad_norm': 0.9205868244171143, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.31it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.91it/s]                                              {'loss': 0.6434, 'grad_norm': 0.625150740146637, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.91it/s] 30%|███       | 9/30 [00:02<00:03,  5.44it/s]                                              {'loss': 0.6465, 'grad_norm': 1.2975544929504395, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:03,  5.44it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.87it/s]                                               {'loss': 0.7305, 'grad_norm': 1.3362088203430176, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.87it/s] 37%|███▋      | 11/30 [00:02<00:03,  6.22it/s]                                               {'loss': 0.7083, 'grad_norm': 0.6938316226005554, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  6.22it/s]                                               {'loss': 0.8924, 'grad_norm': 3.144594669342041, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.22it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.29it/s]                                               {'loss': 0.7183, 'grad_norm': 0.9478100538253784, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.29it/s]                                               {'loss': 0.7183, 'grad_norm': 1.2105447053909302, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.29it/s] 50%|█████     | 15/30 [00:02<00:01,  8.68it/s]                                               {'loss': 0.6093, 'grad_norm': 2.314948081970215, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.68it/s] 53%|█████▎    | 16/30 [00:02<00:01,  8.86it/s]                                               {'loss': 0.6676, 'grad_norm': 0.9005050659179688, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.86it/s]                                               {'loss': 0.7025, 'grad_norm': 0.9790237545967102, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  8.86it/s] 60%|██████    | 18/30 [00:02<00:01, 10.05it/s]                                               {'loss': 0.6901, 'grad_norm': 1.9825525283813477, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 10.05it/s]                                               {'loss': 0.6721, 'grad_norm': 1.056515097618103, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01, 10.05it/s] 67%|██████▋   | 20/30 [00:03<00:00, 10.39it/s]                                               {'loss': 0.6828, 'grad_norm': 0.9564453363418579, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:00, 10.39it/s]                                               {'loss': 0.6893, 'grad_norm': 0.8139787912368774, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:00, 10.39it/s] 73%|███████▎  | 22/30 [00:03<00:00, 10.52it/s]                                               {'loss': 0.7347, 'grad_norm': 1.0801260471343994, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00, 10.52it/s]                                               {'loss': 0.7081, 'grad_norm': 1.7543801069259644, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00, 10.52it/s] 80%|████████  | 24/30 [00:03<00:00, 12.37it/s]                                               {'loss': 0.617, 'grad_norm': 2.2303411960601807, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00, 12.37it/s]                                               {'loss': 0.6881, 'grad_norm': 0.929363489151001, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 12.37it/s] 87%|████████▋ | 26/30 [00:03<00:00, 12.61it/s]                                               {'loss': 0.6852, 'grad_norm': 0.848518431186676, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 12.61it/s]                                               {'loss': 0.6869, 'grad_norm': 0.7471578121185303, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 12.61it/s] 93%|█████████▎| 28/30 [00:03<00:00, 12.75it/s]                                               {'loss': 0.7114, 'grad_norm': 1.1608585119247437, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 12.75it/s]                                               {'loss': 0.6493, 'grad_norm': 1.1581138372421265, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 12.75it/s]100%|██████████| 30/30 [00:03<00:00, 13.87it/s]                                               {'loss': 0.6918, 'grad_norm': 2.3790674209594727, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.87it/s]                                               {'train_runtime': 3.9331, 'train_samples_per_second': 108.058, 'train_steps_per_second': 7.628, 'train_loss': 0.7005060811837515, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.87it/s]100%|██████████| 30/30 [00:03<00:00,  7.63it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.63it/s]                                              {'loss': 0.6583, 'grad_norm': 3.1656150817871094, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.63it/s]  7%|▋         | 2/30 [00:00<00:06,  4.51it/s]                                              {'loss': 0.2948, 'grad_norm': 0.9782791137695312, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.51it/s] 10%|█         | 3/30 [00:00<00:06,  4.35it/s]                                              {'loss': 1.0039, 'grad_norm': 5.718059062957764, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.35it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.38it/s]                                              {'loss': 0.7411, 'grad_norm': 3.313490629196167, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.38it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.50it/s]                                              {'loss': 0.8254, 'grad_norm': 3.9576385021209717, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.50it/s] 20%|██        | 6/30 [00:01<00:05,  4.18it/s]                                              {'loss': 1.3649, 'grad_norm': 5.508334159851074, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.18it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.37it/s]                                              {'loss': 0.3732, 'grad_norm': 1.5396759510040283, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.37it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.52it/s]                                              {'loss': 0.5485, 'grad_norm': 0.37510061264038086, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.52it/s] 30%|███       | 9/30 [00:02<00:04,  4.58it/s]                                              {'loss': 0.7393, 'grad_norm': 1.0039054155349731, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.58it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.71it/s]                                               {'loss': 0.6096, 'grad_norm': 0.3299542963504791, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.71it/s] 37%|███▋      | 11/30 [00:02<00:03,  4.88it/s]                                               {'loss': 0.6398, 'grad_norm': 0.4039180874824524, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  4.88it/s] 40%|████      | 12/30 [00:02<00:03,  5.58it/s]                                               {'loss': 0.4682, 'grad_norm': 1.094553828239441, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.58it/s] 43%|████▎     | 13/30 [00:02<00:03,  4.79it/s]                                               {'loss': 0.5703, 'grad_norm': 0.6167722940444946, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  4.79it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.27it/s]                                               {'loss': 0.6078, 'grad_norm': 0.33811068534851074, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.27it/s] 50%|█████     | 15/30 [00:03<00:03,  3.82it/s]                                               {'loss': 0.6612, 'grad_norm': 0.8120842576026917, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  3.82it/s] 53%|█████▎    | 16/30 [00:03<00:03,  3.62it/s]                                               {'loss': 0.5777, 'grad_norm': 0.5421897172927856, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  3.62it/s] 57%|█████▋    | 17/30 [00:04<00:03,  3.53it/s]                                               {'loss': 0.4998, 'grad_norm': 1.1598891019821167, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:03,  3.53it/s]                                               {'loss': 0.6416, 'grad_norm': 0.8203799724578857, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:03,  3.53it/s] 63%|██████▎   | 19/30 [00:04<00:02,  4.52it/s]                                               {'loss': 0.565, 'grad_norm': 0.42288267612457275, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  4.52it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.88it/s]                                               {'loss': 0.4342, 'grad_norm': 1.2490493059158325, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.88it/s] 70%|███████   | 21/30 [00:04<00:01,  4.84it/s]                                               {'loss': 0.658, 'grad_norm': 0.5689719915390015, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  4.84it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.02it/s]                                               {'loss': 0.642, 'grad_norm': 0.6947522163391113, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.02it/s] 77%|███████▋  | 23/30 [00:05<00:01,  5.51it/s]                                               {'loss': 0.7331, 'grad_norm': 1.2037359476089478, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  5.51it/s]                                               {'loss': 0.5467, 'grad_norm': 0.8894939422607422, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.51it/s] 83%|████████▎ | 25/30 [00:05<00:00,  5.84it/s]                                               {'loss': 0.4304, 'grad_norm': 1.090983510017395, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  5.84it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.66it/s]                                               {'loss': 0.6257, 'grad_norm': 0.6801514625549316, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.66it/s] 90%|█████████ | 27/30 [00:05<00:00,  5.96it/s]                                               {'loss': 0.7562, 'grad_norm': 1.408115267753601, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  5.96it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.91it/s]                                               {'loss': 0.5916, 'grad_norm': 0.3542623221874237, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.91it/s] 97%|█████████▋| 29/30 [00:06<00:00,  4.73it/s]                                               {'loss': 0.6627, 'grad_norm': 1.083182454109192, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  4.73it/s]                                               {'loss': 0.3575, 'grad_norm': 2.1201488971710205, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.73it/s]                                               {'train_runtime': 6.3721, 'train_samples_per_second': 66.697, 'train_steps_per_second': 4.708, 'train_loss': 0.6276161988576253, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.73it/s]100%|██████████| 30/30 [00:06<00:00,  4.71it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:10
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  5.86it/s]                                              {'loss': 0.7821, 'grad_norm': 3.4352831840515137, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  5.86it/s]  7%|▋         | 2/30 [00:00<00:05,  5.03it/s]                                              {'loss': 0.7049, 'grad_norm': 2.246521472930908, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.03it/s] 10%|█         | 3/30 [00:00<00:05,  5.37it/s]                                              {'loss': 0.6495, 'grad_norm': 0.6367340683937073, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  5.37it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.69it/s]                                              {'loss': 0.6501, 'grad_norm': 1.552268385887146, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.69it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.58it/s]                                              {'loss': 0.6701, 'grad_norm': 0.6418418288230896, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.58it/s] 20%|██        | 6/30 [00:01<00:07,  3.09it/s]                                              {'loss': 0.4625, 'grad_norm': 1.3487144708633423, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:07,  3.09it/s] 23%|██▎       | 7/30 [00:01<00:06,  3.41it/s]                                              {'loss': 0.6656, 'grad_norm': 0.7975987195968628, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:06,  3.41it/s] 27%|██▋       | 8/30 [00:01<00:05,  4.00it/s]                                              {'loss': 0.765, 'grad_norm': 1.4694733619689941, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  4.00it/s] 30%|███       | 9/30 [00:02<00:04,  4.38it/s]                                              {'loss': 0.6056, 'grad_norm': 1.092840552330017, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.38it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.65it/s]                                               {'loss': 0.5394, 'grad_norm': 0.5741345882415771, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.65it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.01it/s]                                               {'loss': 0.6738, 'grad_norm': 0.7236560583114624, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.01it/s] 40%|████      | 12/30 [00:02<00:03,  5.66it/s]                                               {'loss': 0.4707, 'grad_norm': 0.8223473429679871, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.66it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.61it/s]                                               {'loss': 0.5803, 'grad_norm': 0.9308875203132629, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.61it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.59it/s]                                               {'loss': 0.516, 'grad_norm': 0.7540621161460876, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.59it/s] 50%|█████     | 15/30 [00:03<00:02,  5.44it/s]                                               {'loss': 0.8648, 'grad_norm': 2.279841899871826, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.44it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.61it/s]                                               {'loss': 0.6455, 'grad_norm': 1.5753456354141235, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.61it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.70it/s]                                               {'loss': 0.5011, 'grad_norm': 1.157975196838379, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.70it/s]                                               {'loss': 0.7364, 'grad_norm': 1.7652206420898438, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.70it/s] 63%|██████▎   | 19/30 [00:03<00:01,  6.28it/s]                                               {'loss': 0.5453, 'grad_norm': 1.015188217163086, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  6.28it/s] 67%|██████▋   | 20/30 [00:03<00:01,  6.13it/s]                                               {'loss': 0.6153, 'grad_norm': 0.7384447455406189, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  6.13it/s] 70%|███████   | 21/30 [00:04<00:01,  6.13it/s]                                               {'loss': 0.552, 'grad_norm': 1.1875393390655518, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  6.13it/s] 73%|███████▎  | 22/30 [00:04<00:01,  6.00it/s]                                               {'loss': 0.5657, 'grad_norm': 2.0284290313720703, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  6.00it/s] 77%|███████▋  | 23/30 [00:04<00:01,  6.01it/s]                                               {'loss': 0.6379, 'grad_norm': 1.2465839385986328, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  6.01it/s]                                               {'loss': 0.5134, 'grad_norm': 2.0862019062042236, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  6.01it/s] 83%|████████▎ | 25/30 [00:04<00:00,  7.21it/s]                                               {'loss': 0.6384, 'grad_norm': 1.2683310508728027, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  7.21it/s] 87%|████████▋ | 26/30 [00:04<00:00,  7.56it/s]                                               {'loss': 0.5491, 'grad_norm': 0.9380474090576172, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  7.56it/s] 90%|█████████ | 27/30 [00:04<00:00,  7.62it/s]                                               {'loss': 0.5334, 'grad_norm': 1.5505213737487793, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  7.62it/s] 93%|█████████▎| 28/30 [00:05<00:00,  7.37it/s]                                               {'loss': 0.5713, 'grad_norm': 0.9459845423698425, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  7.37it/s] 97%|█████████▋| 29/30 [00:05<00:00,  6.38it/s]                                               {'loss': 0.5585, 'grad_norm': 1.851830244064331, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  6.38it/s]                                               {'loss': 0.6213, 'grad_norm': 2.3655242919921875, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.38it/s]                                               {'train_runtime': 5.3957, 'train_samples_per_second': 78.767, 'train_steps_per_second': 5.56, 'train_loss': 0.6128293186426162, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.38it/s]100%|██████████| 30/30 [00:05<00:00,  5.56it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:44
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:28,  1.03it/s]                                              {'loss': 0.6474, 'grad_norm': 3.6901979446411133, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:28,  1.03it/s]  7%|▋         | 2/30 [00:01<00:14,  2.00it/s]                                              {'loss': 0.2316, 'grad_norm': 1.3801984786987305, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:14,  2.00it/s] 10%|█         | 3/30 [00:01<00:09,  2.90it/s]                                              {'loss': 0.2014, 'grad_norm': 1.4399936199188232, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:09,  2.90it/s]                                              {'loss': 0.0251, 'grad_norm': 0.277682363986969, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  2.90it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.93it/s]                                              {'loss': 0.0094, 'grad_norm': 0.11327622830867767, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.93it/s] 20%|██        | 6/30 [00:01<00:04,  5.60it/s]                                              {'loss': 0.0041, 'grad_norm': 0.06980001926422119, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.60it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.66it/s]                                              {'loss': 0.2146, 'grad_norm': 1.0830367803573608, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.66it/s]                                              {'loss': 0.0026, 'grad_norm': 0.04555530473589897, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.66it/s] 30%|███       | 9/30 [00:02<00:03,  6.58it/s]                                              {'loss': 0.3363, 'grad_norm': 1.3512495756149292, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:03,  6.58it/s]                                              {'loss': 0.7631, 'grad_norm': 3.4454431533813477, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  6.58it/s] 37%|███▋      | 11/30 [00:02<00:02,  7.33it/s]                                               {'loss': 0.3367, 'grad_norm': 1.9628281593322754, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:02,  7.33it/s]                                               {'loss': 0.0062, 'grad_norm': 0.18750499188899994, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  7.33it/s] 43%|████▎     | 13/30 [00:02<00:02,  8.31it/s]                                               {'loss': 0.3065, 'grad_norm': 1.7877672910690308, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  8.31it/s]                                               {'loss': 0.0132, 'grad_norm': 0.18107616901397705, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  8.31it/s] 50%|█████     | 15/30 [00:02<00:01,  9.13it/s]                                               {'loss': 0.6075, 'grad_norm': 2.0175247192382812, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  9.13it/s]                                               {'loss': 0.0267, 'grad_norm': 0.372699111700058, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  9.13it/s] 57%|█████▋    | 17/30 [00:02<00:01,  9.92it/s]                                               {'loss': 0.5487, 'grad_norm': 3.5000500679016113, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  9.92it/s]                                               {'loss': 0.0306, 'grad_norm': 0.536278247833252, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  9.92it/s] 63%|██████▎   | 19/30 [00:02<00:00, 11.25it/s]                                               {'loss': 0.362, 'grad_norm': 2.46996808052063, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 11.25it/s]                                               {'loss': 0.0472, 'grad_norm': 0.5632582902908325, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 11.25it/s] 70%|███████   | 21/30 [00:03<00:00, 11.31it/s]                                               {'loss': 0.053, 'grad_norm': 0.6717534065246582, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:00, 11.31it/s]                                               {'loss': 0.5848, 'grad_norm': 2.467681407928467, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00, 11.31it/s] 77%|███████▋  | 23/30 [00:03<00:00, 11.80it/s]                                               {'loss': 0.0648, 'grad_norm': 0.8164186477661133, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00, 11.80it/s]                                               {'loss': 0.0571, 'grad_norm': 0.7276726961135864, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00, 11.80it/s] 83%|████████▎ | 25/30 [00:03<00:00, 13.03it/s]                                               {'loss': 0.0526, 'grad_norm': 0.5034893155097961, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 13.03it/s]                                               {'loss': 0.2591, 'grad_norm': 1.3589035272598267, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 13.03it/s] 90%|█████████ | 27/30 [00:03<00:00, 13.06it/s]                                               {'loss': 0.2496, 'grad_norm': 1.702860713005066, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 13.06it/s]                                               {'loss': 0.2353, 'grad_norm': 1.0704255104064941, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 13.06it/s] 97%|█████████▋| 29/30 [00:03<00:00, 12.67it/s]                                               {'loss': 0.2335, 'grad_norm': 1.826409101486206, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 12.67it/s]                                               {'loss': 0.941, 'grad_norm': 5.691425800323486, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.67it/s]                                               {'train_runtime': 3.8079, 'train_samples_per_second': 111.609, 'train_steps_per_second': 7.878, 'train_loss': 0.24840214878010253, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.67it/s]100%|██████████| 30/30 [00:03<00:00,  7.90it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:34
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.39it/s]                                              {'loss': 0.5306, 'grad_norm': 3.416905641555786, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.39it/s]  7%|▋         | 2/30 [00:00<00:05,  5.01it/s]                                              {'loss': 0.3436, 'grad_norm': 11.085214614868164, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.01it/s] 10%|█         | 3/30 [00:00<00:05,  4.72it/s]                                              {'loss': 0.4203, 'grad_norm': 1.7551652193069458, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.72it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.70it/s]                                              {'loss': 0.58, 'grad_norm': 3.3643670082092285, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.70it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.65it/s]                                              {'loss': 0.5289, 'grad_norm': 1.272463321685791, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.65it/s]                                              {'loss': 1.1031, 'grad_norm': 3.3925845623016357, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.65it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.44it/s]                                              {'loss': 0.4165, 'grad_norm': 1.243403673171997, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.44it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.89it/s]                                              {'loss': 0.4681, 'grad_norm': 1.2581846714019775, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.89it/s] 30%|███       | 9/30 [00:01<00:03,  5.46it/s]                                              {'loss': 0.6534, 'grad_norm': 1.3901063203811646, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.46it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.18it/s]                                               {'loss': 0.4551, 'grad_norm': 1.4020965099334717, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.18it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.05it/s]                                               {'loss': 0.5725, 'grad_norm': 0.8364741802215576, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.05it/s]                                               {'loss': 0.411, 'grad_norm': 1.3855674266815186, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.05it/s] 43%|████▎     | 13/30 [00:02<00:02,  5.87it/s]                                               {'loss': 0.3177, 'grad_norm': 1.2203857898712158, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  5.87it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.47it/s]                                               {'loss': 0.3958, 'grad_norm': 0.8976049423217773, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.47it/s] 50%|█████     | 15/30 [00:02<00:02,  5.13it/s]                                               {'loss': 0.7177, 'grad_norm': 2.0233139991760254, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.13it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.20it/s]                                               {'loss': 0.4194, 'grad_norm': 1.3147177696228027, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.20it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.05it/s]                                               {'loss': 0.4556, 'grad_norm': 0.6760671138763428, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.05it/s] 60%|██████    | 18/30 [00:03<00:02,  5.30it/s]                                               {'loss': 0.8823, 'grad_norm': 6.260988712310791, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.30it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.67it/s]                                               {'loss': 0.3877, 'grad_norm': 0.710830807685852, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.67it/s] 67%|██████▋   | 20/30 [00:03<00:02,  4.31it/s]                                               {'loss': 0.4688, 'grad_norm': 0.9318873882293701, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:02,  4.31it/s] 70%|███████   | 21/30 [00:04<00:02,  4.08it/s]                                               {'loss': 0.3759, 'grad_norm': 0.955733597278595, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:02,  4.08it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.09it/s]                                               {'loss': 0.4647, 'grad_norm': 1.158805251121521, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.09it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.07it/s]                                               {'loss': 0.3515, 'grad_norm': 1.406646966934204, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.07it/s] 80%|████████  | 24/30 [00:04<00:01,  4.66it/s]                                               {'loss': 0.3991, 'grad_norm': 1.2348896265029907, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  4.66it/s] 83%|████████▎ | 25/30 [00:05<00:01,  4.50it/s]                                               {'loss': 0.6906, 'grad_norm': 2.064816474914551, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  4.50it/s] 87%|████████▋ | 26/30 [00:05<00:00,  4.67it/s]                                               {'loss': 0.4548, 'grad_norm': 1.388100504875183, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  4.67it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.70it/s]                                               {'loss': 0.3446, 'grad_norm': 1.181801438331604, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.70it/s] 93%|█████████▎| 28/30 [00:05<00:00,  5.04it/s]                                               {'loss': 0.2585, 'grad_norm': 1.6217002868652344, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  5.04it/s] 97%|█████████▋| 29/30 [00:05<00:00,  5.43it/s]                                               {'loss': 0.5547, 'grad_norm': 1.937926173210144, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  5.43it/s]                                               {'loss': 0.2223, 'grad_norm': 1.585519552230835, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.43it/s]                                               {'train_runtime': 6.0197, 'train_samples_per_second': 70.601, 'train_steps_per_second': 4.984, 'train_loss': 0.48816301971673964, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.43it/s]100%|██████████| 30/30 [00:06<00:00,  4.99it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:24
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.52it/s]                                              {'loss': 0.6552, 'grad_norm': 3.2351207733154297, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.52it/s]  7%|▋         | 2/30 [00:00<00:05,  4.72it/s]                                              {'loss': 0.2612, 'grad_norm': 1.489620327949524, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  4.72it/s] 10%|█         | 3/30 [00:00<00:05,  4.89it/s]                                              {'loss': 0.6231, 'grad_norm': 3.82677960395813, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.89it/s] 13%|█▎        | 4/30 [00:00<00:05,  5.08it/s]                                              {'loss': 0.1635, 'grad_norm': 0.9432464838027954, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  5.08it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.86it/s]                                              {'loss': 0.712, 'grad_norm': 4.163399696350098, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.86it/s]                                              {'loss': 2.156, 'grad_norm': 8.698460578918457, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.86it/s] 23%|██▎       | 7/30 [00:01<00:03,  5.80it/s]                                              {'loss': 0.101, 'grad_norm': 1.0104833841323853, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  5.80it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.31it/s]                                              {'loss': 0.7242, 'grad_norm': 3.1916449069976807, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.31it/s] 30%|███       | 9/30 [00:01<00:04,  4.97it/s]                                              {'loss': 0.3318, 'grad_norm': 0.748802125453949, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.97it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.73it/s]                                               {'loss': 0.2538, 'grad_norm': 1.393985629081726, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.73it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.66it/s]                                               {'loss': 0.7249, 'grad_norm': 2.367020606994629, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.66it/s]                                               {'loss': 0.1175, 'grad_norm': 1.1327152252197266, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.66it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.43it/s]                                               {'loss': 0.3127, 'grad_norm': 3.345271587371826, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.43it/s] 47%|████▋     | 14/30 [00:02<00:03,  5.14it/s]                                               {'loss': 0.2201, 'grad_norm': 1.0058298110961914, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  5.14it/s] 50%|█████     | 15/30 [00:02<00:03,  4.95it/s]                                               {'loss': 0.5592, 'grad_norm': 2.2640504837036133, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:03,  4.95it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.54it/s]                                               {'loss': 0.4866, 'grad_norm': 4.655364036560059, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.54it/s] 57%|█████▋    | 17/30 [00:03<00:03,  4.21it/s]                                               {'loss': 0.4092, 'grad_norm': 1.573280930519104, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  4.21it/s] 60%|██████    | 18/30 [00:03<00:02,  4.94it/s]                                               {'loss': 0.7177, 'grad_norm': 5.165616035461426, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.94it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.79it/s]                                               {'loss': 0.343, 'grad_norm': 2.1623172760009766, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.79it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.71it/s]                                               {'loss': 0.315, 'grad_norm': 1.8221250772476196, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.71it/s] 70%|███████   | 21/30 [00:04<00:01,  5.07it/s]                                               {'loss': 0.5082, 'grad_norm': 6.031396865844727, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.07it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.69it/s]                                               {'loss': 0.466, 'grad_norm': 3.3885746002197266, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.69it/s] 77%|███████▋  | 23/30 [00:04<00:01,  6.32it/s]                                               {'loss': 0.4619, 'grad_norm': 3.2582390308380127, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  6.32it/s] 80%|████████  | 24/30 [00:04<00:00,  6.87it/s]                                               {'loss': 0.5682, 'grad_norm': 9.017878532409668, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  6.87it/s] 83%|████████▎ | 25/30 [00:04<00:00,  6.18it/s]                                               {'loss': 0.3751, 'grad_norm': 3.0422911643981934, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  6.18it/s] 87%|████████▋ | 26/30 [00:04<00:00,  5.89it/s]                                               {'loss': 0.4776, 'grad_norm': 3.0255537033081055, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.89it/s] 90%|█████████ | 27/30 [00:05<00:00,  5.65it/s]                                               {'loss': 0.3132, 'grad_norm': 1.4000964164733887, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  5.65it/s] 93%|█████████▎| 28/30 [00:05<00:00,  5.50it/s]                                               {'loss': 0.4676, 'grad_norm': 1.678062081336975, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  5.50it/s] 97%|█████████▋| 29/30 [00:05<00:00,  5.39it/s]                                               {'loss': 0.3724, 'grad_norm': 1.862626552581787, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  5.39it/s]                                               {'loss': 0.171, 'grad_norm': 3.146994113922119, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.39it/s]                                               {'train_runtime': 5.6998, 'train_samples_per_second': 74.564, 'train_steps_per_second': 5.263, 'train_loss': 0.4789673425257206, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.39it/s]100%|██████████| 30/30 [00:05<00:00,  5.27it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:98
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  7.85it/s]                                              {'loss': 0.5615, 'grad_norm': 3.360701084136963, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  7.85it/s]  7%|▋         | 2/30 [00:00<00:03,  8.15it/s]                                              {'loss': 0.4729, 'grad_norm': 2.0113909244537354, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.15it/s] 10%|█         | 3/30 [00:00<00:03,  8.17it/s]                                              {'loss': 0.3897, 'grad_norm': 1.3255709409713745, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.17it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.85it/s]                                              {'loss': 0.2732, 'grad_norm': 0.8603308200836182, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.85it/s]                                              {'loss': 0.3639, 'grad_norm': 0.8246063590049744, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.85it/s] 20%|██        | 6/30 [00:01<00:04,  4.83it/s]                                              {'loss': 1.3221, 'grad_norm': 6.243898868560791, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.83it/s]                                              {'loss': 0.232, 'grad_norm': 1.0346726179122925, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.83it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.54it/s]                                              {'loss': 0.6257, 'grad_norm': 1.9706902503967285, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.54it/s]                                              {'loss': 0.3506, 'grad_norm': 0.5801355242729187, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.54it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.97it/s]                                               {'loss': 0.313, 'grad_norm': 0.8299602270126343, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.97it/s]                                               {'loss': 0.3775, 'grad_norm': 0.9554824233055115, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.97it/s] 40%|████      | 12/30 [00:01<00:01,  9.29it/s]                                               {'loss': 0.4389, 'grad_norm': 1.7082668542861938, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01,  9.29it/s]                                               {'loss': 0.1247, 'grad_norm': 1.2857189178466797, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  9.29it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.98it/s]                                               {'loss': 0.5101, 'grad_norm': 1.066400408744812, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.98it/s]                                               {'loss': 0.5076, 'grad_norm': 1.2657960653305054, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  8.98it/s] 53%|█████▎    | 16/30 [00:01<00:01,  8.95it/s]                                               {'loss': 0.4199, 'grad_norm': 1.0383599996566772, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  8.95it/s]                                               {'loss': 0.2428, 'grad_norm': 0.8335309028625488, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  8.95it/s] 60%|██████    | 18/30 [00:02<00:01, 10.47it/s]                                               {'loss': 0.936, 'grad_norm': 3.457379102706909, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 10.47it/s]                                               {'loss': 0.3574, 'grad_norm': 0.6779215931892395, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01, 10.47it/s] 67%|██████▋   | 20/30 [00:02<00:00, 10.94it/s]                                               {'loss': 0.3388, 'grad_norm': 0.9036755561828613, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 10.94it/s]                                               {'loss': 0.3293, 'grad_norm': 0.8909600377082825, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 10.94it/s] 73%|███████▎  | 22/30 [00:02<00:00, 10.89it/s]                                               {'loss': 0.4391, 'grad_norm': 0.6825124025344849, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 10.89it/s]                                               {'loss': 0.3558, 'grad_norm': 0.7279179096221924, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 10.89it/s] 80%|████████  | 24/30 [00:02<00:00, 12.06it/s]                                               {'loss': 0.4443, 'grad_norm': 1.0475159883499146, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 12.06it/s]                                               {'loss': 0.249, 'grad_norm': 0.9384374022483826, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.06it/s] 87%|████████▋ | 26/30 [00:02<00:00, 11.45it/s]                                               {'loss': 0.551, 'grad_norm': 2.297020673751831, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 11.45it/s]                                               {'loss': 0.4943, 'grad_norm': 1.117316484451294, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 11.45it/s] 93%|█████████▎| 28/30 [00:02<00:00, 11.55it/s]                                               {'loss': 0.2352, 'grad_norm': 1.2784781455993652, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 11.55it/s]                                               {'loss': 0.3688, 'grad_norm': 1.0947864055633545, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 11.55it/s]100%|██████████| 30/30 [00:03<00:00, 11.88it/s]                                               {'loss': 0.6625, 'grad_norm': 5.505191802978516, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.88it/s]                                               {'train_runtime': 3.2423, 'train_samples_per_second': 131.08, 'train_steps_per_second': 9.253, 'train_loss': 0.4429169329504172, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.88it/s]100%|██████████| 30/30 [00:03<00:00,  9.25it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.79it/s]                                              {'loss': 0.7514, 'grad_norm': 3.124185085296631, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.79it/s]  7%|▋         | 2/30 [00:00<00:03,  7.42it/s]                                              {'loss': 0.7148, 'grad_norm': 1.8945631980895996, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.42it/s]                                              {'loss': 0.6847, 'grad_norm': 1.1508897542953491, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.42it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.26it/s]                                              {'loss': 0.6911, 'grad_norm': 1.328366994857788, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  9.26it/s]                                              {'loss': 0.7346, 'grad_norm': 1.0361567735671997, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  9.26it/s] 20%|██        | 6/30 [00:00<00:02, 11.81it/s]                                              {'loss': 1.0845, 'grad_norm': 4.878647327423096, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.81it/s]                                              {'loss': 0.6235, 'grad_norm': 0.8569057583808899, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 11.81it/s] 27%|██▋       | 8/30 [00:00<00:01, 11.81it/s]                                              {'loss': 0.7727, 'grad_norm': 1.6086761951446533, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 11.81it/s]                                              {'loss': 0.7067, 'grad_norm': 1.0790889263153076, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 11.81it/s] 33%|███▎      | 10/30 [00:00<00:01, 11.74it/s]                                               {'loss': 0.6417, 'grad_norm': 0.7561100721359253, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.74it/s]                                               {'loss': 0.663, 'grad_norm': 0.4760935604572296, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 11.74it/s] 40%|████      | 12/30 [00:01<00:01, 12.31it/s]                                               {'loss': 0.5879, 'grad_norm': 2.623840093612671, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 12.31it/s]                                               {'loss': 0.6498, 'grad_norm': 0.993796169757843, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 12.31it/s] 47%|████▋     | 14/30 [00:01<00:01, 12.00it/s]                                               {'loss': 0.6326, 'grad_norm': 0.9632153511047363, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 12.00it/s]                                               {'loss': 0.7352, 'grad_norm': 1.7924085855484009, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 12.00it/s] 53%|█████▎    | 16/30 [00:01<00:01, 11.86it/s]                                               {'loss': 0.628, 'grad_norm': 0.5954277515411377, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.86it/s]                                               {'loss': 0.6234, 'grad_norm': 0.7260450720787048, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.86it/s] 60%|██████    | 18/30 [00:01<00:00, 13.17it/s]                                               {'loss': 0.697, 'grad_norm': 1.3631606101989746, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.17it/s]                                               {'loss': 0.6514, 'grad_norm': 1.2993800640106201, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.17it/s] 67%|██████▋   | 20/30 [00:01<00:00, 13.06it/s]                                               {'loss': 0.6582, 'grad_norm': 0.5613088011741638, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.06it/s]                                               {'loss': 0.6239, 'grad_norm': 1.668312907218933, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.06it/s] 73%|███████▎  | 22/30 [00:01<00:00, 12.91it/s]                                               {'loss': 0.6545, 'grad_norm': 1.2205250263214111, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 12.91it/s]                                               {'loss': 0.7, 'grad_norm': 6.595496654510498, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 12.91it/s] 80%|████████  | 24/30 [00:01<00:00, 14.32it/s]                                               {'loss': 0.6237, 'grad_norm': 1.148512601852417, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 14.32it/s]                                               {'loss': 0.6468, 'grad_norm': 1.4630630016326904, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 14.32it/s] 87%|████████▋ | 26/30 [00:02<00:00, 13.74it/s]                                               {'loss': 0.6259, 'grad_norm': 0.9424317479133606, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.74it/s]                                               {'loss': 0.5817, 'grad_norm': 1.366603970527649, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.74it/s] 93%|█████████▎| 28/30 [00:02<00:00, 12.61it/s]                                               {'loss': 0.6286, 'grad_norm': 0.7937675714492798, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.61it/s]                                               {'loss': 0.5806, 'grad_norm': 1.5579370260238647, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.61it/s]100%|██████████| 30/30 [00:02<00:00, 12.84it/s]                                               {'loss': 0.5628, 'grad_norm': 1.9409865140914917, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.84it/s]                                               {'train_runtime': 2.5353, 'train_samples_per_second': 167.63, 'train_steps_per_second': 11.833, 'train_loss': 0.6720252017180125, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.84it/s]100%|██████████| 30/30 [00:02<00:00, 11.84it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:95
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  4.10it/s]                                              {'loss': 0.833, 'grad_norm': 4.17824649810791, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  4.10it/s]  7%|▋         | 2/30 [00:00<00:07,  3.76it/s]                                              {'loss': 0.7241, 'grad_norm': 1.9190587997436523, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.76it/s] 10%|█         | 3/30 [00:00<00:07,  3.66it/s]                                              {'loss': 0.666, 'grad_norm': 1.3019152879714966, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.66it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.58it/s]                                              {'loss': 0.6323, 'grad_norm': 1.8898578882217407, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.58it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.71it/s]                                              {'loss': 0.7394, 'grad_norm': 1.0853397846221924, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.71it/s] 20%|██        | 6/30 [00:01<00:05,  4.62it/s]                                              {'loss': 0.8115, 'grad_norm': 2.2357425689697266, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.62it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.17it/s]                                              {'loss': 0.619, 'grad_norm': 0.9104564189910889, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.17it/s] 27%|██▋       | 8/30 [00:02<00:05,  3.94it/s]                                              {'loss': 0.734, 'grad_norm': 1.0429128408432007, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:05,  3.94it/s] 30%|███       | 9/30 [00:02<00:04,  4.53it/s]                                              {'loss': 0.6641, 'grad_norm': 0.640981137752533, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.53it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.85it/s]                                               {'loss': 0.6241, 'grad_norm': 0.6299244165420532, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.85it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.66it/s]                                               {'loss': 0.6829, 'grad_norm': 0.5120312571525574, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.66it/s]                                               {'loss': 0.4579, 'grad_norm': 0.7169919013977051, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.66it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.66it/s]                                               {'loss': 0.6803, 'grad_norm': 1.0386295318603516, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.66it/s] 47%|████▋     | 14/30 [00:03<00:02,  5.72it/s]                                               {'loss': 0.6232, 'grad_norm': 0.7521944642066956, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:02,  5.72it/s] 50%|█████     | 15/30 [00:03<00:02,  5.61it/s]                                               {'loss': 0.7451, 'grad_norm': 1.8042042255401611, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.61it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.39it/s]                                               {'loss': 0.6415, 'grad_norm': 1.1571379899978638, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.39it/s]                                               {'loss': 0.6469, 'grad_norm': 0.708638072013855, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.39it/s] 60%|██████    | 18/30 [00:03<00:01,  7.03it/s]                                               {'loss': 0.6995, 'grad_norm': 0.8750015497207642, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  7.03it/s] 63%|██████▎   | 19/30 [00:03<00:01,  6.79it/s]                                               {'loss': 0.6135, 'grad_norm': 0.9721684455871582, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  6.79it/s] 67%|██████▋   | 20/30 [00:03<00:01,  6.30it/s]                                               {'loss': 0.6626, 'grad_norm': 0.43844425678253174, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  6.30it/s] 70%|███████   | 21/30 [00:04<00:01,  5.91it/s]                                               {'loss': 0.6643, 'grad_norm': 0.6625112295150757, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.91it/s] 73%|███████▎  | 22/30 [00:04<00:01,  6.40it/s]                                               {'loss': 0.6461, 'grad_norm': 0.7996324300765991, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  6.40it/s] 77%|███████▋  | 23/30 [00:04<00:01,  6.68it/s]                                               {'loss': 0.6345, 'grad_norm': 0.48760244250297546, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  6.68it/s]                                               {'loss': 0.6139, 'grad_norm': 1.5808385610580444, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  6.68it/s] 83%|████████▎ | 25/30 [00:04<00:00,  7.40it/s]                                               {'loss': 0.7631, 'grad_norm': 1.2611961364746094, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  7.40it/s] 87%|████████▋ | 26/30 [00:04<00:00,  7.03it/s]                                               {'loss': 0.5688, 'grad_norm': 0.7868162393569946, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  7.03it/s] 90%|█████████ | 27/30 [00:05<00:00,  6.12it/s]                                               {'loss': 0.6198, 'grad_norm': 1.5729562044143677, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  6.12it/s] 93%|█████████▎| 28/30 [00:05<00:00,  5.78it/s]                                               {'loss': 0.619, 'grad_norm': 0.6838087439537048, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  5.78it/s] 97%|█████████▋| 29/30 [00:05<00:00,  5.65it/s]                                               {'loss': 0.6239, 'grad_norm': 1.578323245048523, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  5.65it/s]                                               {'loss': 0.5009, 'grad_norm': 1.2834961414337158, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.65it/s]                                               {'train_runtime': 5.5924, 'train_samples_per_second': 75.995, 'train_steps_per_second': 5.364, 'train_loss': 0.6585111091534297, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.65it/s]100%|██████████| 30/30 [00:05<00:00,  5.37it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
the shape of base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.weight is (768, 768)
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  8%|▊         | 5/66 [00:00<00:01, 38.40it/s] 14%|█▎        | 9/66 [00:00<00:01, 33.18it/s] 20%|█▉        | 13/66 [00:00<00:01, 31.73it/s] 26%|██▌       | 17/66 [00:00<00:01, 31.03it/s] 32%|███▏      | 21/66 [00:00<00:01, 30.23it/s] 38%|███▊      | 25/66 [00:00<00:01, 28.36it/s] 42%|████▏     | 28/66 [00:00<00:01, 27.54it/s] 47%|████▋     | 31/66 [00:01<00:01, 25.77it/s] 52%|█████▏    | 34/66 [00:01<00:01, 25.55it/s] 56%|█████▌    | 37/66 [00:01<00:01, 24.87it/s] 61%|██████    | 40/66 [00:01<00:01, 23.86it/s] 65%|██████▌   | 43/66 [00:01<00:00, 24.46it/s] 70%|██████▉   | 46/66 [00:01<00:01, 19.50it/s] 74%|███████▍  | 49/66 [00:02<00:01, 15.14it/s] 77%|███████▋  | 51/66 [00:02<00:01, 14.28it/s] 80%|████████  | 53/66 [00:02<00:00, 14.42it/s] 83%|████████▎ | 55/66 [00:02<00:00, 13.10it/s] 86%|████████▋ | 57/66 [00:02<00:00, 14.10it/s] 89%|████████▉ | 59/66 [00:02<00:00, 13.16it/s] 92%|█████████▏| 61/66 [00:03<00:00, 12.86it/s] 95%|█████████▌| 63/66 [00:03<00:00, 14.10it/s] 98%|█████████▊| 65/66 [00:03<00:00, 14.26it/s]100%|██████████| 66/66 [00:03<00:00, 19.87it/s]
{'eval_loss': 0.6095807552337646, 'eval_model_preparation_time': 0.0058, 'eval_acc': 0.6912751677852349, 'eval_runtime': 3.3572, 'eval_samples_per_second': 310.672, 'eval_steps_per_second': 19.659}
ROUND:8
CLIENT:2
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.61it/s]                                              {'loss': 0.8337, 'grad_norm': 4.015848159790039, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.61it/s]  7%|▋         | 2/30 [00:00<00:14,  1.89it/s]                                              {'loss': 0.6878, 'grad_norm': 1.4835972785949707, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:14,  1.89it/s] 10%|█         | 3/30 [00:01<00:10,  2.61it/s]                                              {'loss': 0.6372, 'grad_norm': 0.7995061874389648, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:10,  2.61it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.34it/s]                                              {'loss': 0.6114, 'grad_norm': 0.8736823797225952, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.34it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.93it/s]                                              {'loss': 0.7227, 'grad_norm': 1.0448493957519531, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.93it/s]                                              {'loss': 0.981, 'grad_norm': 4.702847003936768, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.93it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.10it/s]                                              {'loss': 0.7038, 'grad_norm': 1.0347429513931274, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.10it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.63it/s]                                              {'loss': 0.7614, 'grad_norm': 1.1568294763565063, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.63it/s] 30%|███       | 9/30 [00:02<00:03,  5.94it/s]                                              {'loss': 0.7577, 'grad_norm': 1.1253525018692017, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:03,  5.94it/s] 33%|███▎      | 10/30 [00:02<00:03,  6.31it/s]                                               {'loss': 0.6565, 'grad_norm': 0.544350802898407, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  6.31it/s] 37%|███▋      | 11/30 [00:02<00:02,  6.60it/s]                                               {'loss': 0.6613, 'grad_norm': 0.931984007358551, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:02,  6.60it/s]                                               {'loss': 0.5896, 'grad_norm': 1.544044852256775, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.60it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.35it/s]                                               {'loss': 0.6735, 'grad_norm': 0.8654647469520569, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.35it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.67it/s]                                               {'loss': 0.6193, 'grad_norm': 2.735438823699951, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.67it/s] 50%|█████     | 15/30 [00:02<00:01,  7.72it/s]                                               {'loss': 0.8228, 'grad_norm': 2.887596845626831, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.72it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.50it/s]                                               {'loss': 0.6547, 'grad_norm': 1.039569616317749, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.50it/s] 57%|█████▋    | 17/30 [00:03<00:01,  7.32it/s]                                               {'loss': 0.6093, 'grad_norm': 1.1163153648376465, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:01,  7.32it/s]                                               {'loss': 0.7453, 'grad_norm': 3.3038649559020996, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  7.32it/s] 63%|██████▎   | 19/30 [00:03<00:01,  8.35it/s]                                               {'loss': 0.5397, 'grad_norm': 1.8620704412460327, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  8.35it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.93it/s]                                               {'loss': 0.6308, 'grad_norm': 0.7829866409301758, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.93it/s] 70%|███████   | 21/30 [00:03<00:01,  7.58it/s]                                               {'loss': 0.6641, 'grad_norm': 0.9455035924911499, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.58it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.34it/s]                                               {'loss': 0.6419, 'grad_norm': 1.356427788734436, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.34it/s] 77%|███████▋  | 23/30 [00:03<00:00,  7.30it/s]                                               {'loss': 0.6898, 'grad_norm': 1.8114898204803467, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.30it/s]                                               {'loss': 0.6408, 'grad_norm': 2.366798162460327, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.30it/s] 83%|████████▎ | 25/30 [00:04<00:00,  8.23it/s]                                               {'loss': 0.5963, 'grad_norm': 1.3685662746429443, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  8.23it/s] 87%|████████▋ | 26/30 [00:04<00:00,  8.07it/s]                                               {'loss': 0.6729, 'grad_norm': 1.186583399772644, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  8.07it/s] 90%|█████████ | 27/30 [00:04<00:00,  7.84it/s]                                               {'loss': 0.6775, 'grad_norm': 1.124658226966858, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  7.84it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.47it/s]                                               {'loss': 0.5804, 'grad_norm': 2.09379506111145, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.47it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.80it/s]                                               {'loss': 0.5717, 'grad_norm': 1.8285142183303833, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.80it/s]                                               {'loss': 0.5425, 'grad_norm': 2.7222535610198975, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.80it/s]                                               {'train_runtime': 4.9764, 'train_samples_per_second': 85.404, 'train_steps_per_second': 6.029, 'train_loss': 0.6725835045178731, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.80it/s]100%|██████████| 30/30 [00:04<00:00,  6.03it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:49
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  5.87it/s]                                              {'loss': 0.5257, 'grad_norm': 3.990780830383301, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  5.87it/s]  7%|▋         | 2/30 [00:00<00:04,  6.32it/s]                                              {'loss': 0.2547, 'grad_norm': 1.0645761489868164, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.32it/s] 10%|█         | 3/30 [00:00<00:04,  6.21it/s]                                              {'loss': 0.26, 'grad_norm': 1.379166841506958, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.21it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.66it/s]                                              {'loss': 0.4857, 'grad_norm': 2.8524107933044434, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.66it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.43it/s]                                              {'loss': 0.2733, 'grad_norm': 0.8870909810066223, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.43it/s] 20%|██        | 6/30 [00:00<00:03,  6.42it/s]                                              {'loss': 0.0681, 'grad_norm': 1.091550588607788, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.42it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.35it/s]                                              {'loss': 0.3581, 'grad_norm': 1.4911831617355347, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.35it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.07it/s]                                              {'loss': 0.2748, 'grad_norm': 0.8316903114318848, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.07it/s] 30%|███       | 9/30 [00:01<00:04,  4.75it/s]                                              {'loss': 0.5191, 'grad_norm': 1.6805862188339233, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.75it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.16it/s]                                               {'loss': 0.3368, 'grad_norm': 1.401962161064148, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.16it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.53it/s]                                               {'loss': 0.1017, 'grad_norm': 1.299401044845581, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.53it/s]                                               {'loss': 0.0764, 'grad_norm': 0.9634892344474792, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.53it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.63it/s]                                               {'loss': 0.2318, 'grad_norm': 1.1761995553970337, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.63it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.87it/s]                                               {'loss': 0.137, 'grad_norm': 0.692552387714386, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.87it/s] 50%|█████     | 15/30 [00:02<00:03,  4.91it/s]                                               {'loss': 0.7521, 'grad_norm': 5.476747989654541, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:03,  4.91it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.32it/s]                                               {'loss': 0.2519, 'grad_norm': 1.028657078742981, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.32it/s] 57%|█████▋    | 17/30 [00:03<00:03,  3.97it/s]                                               {'loss': 0.1837, 'grad_norm': 0.6782219409942627, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  3.97it/s] 60%|██████    | 18/30 [00:03<00:02,  4.54it/s]                                               {'loss': 0.0449, 'grad_norm': 0.7284401655197144, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.54it/s] 63%|██████▎   | 19/30 [00:03<00:02,  3.98it/s]                                               {'loss': 0.0393, 'grad_norm': 0.5369894504547119, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  3.98it/s] 67%|██████▋   | 20/30 [00:04<00:02,  3.95it/s]                                               {'loss': 0.7249, 'grad_norm': 4.6855902671813965, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  3.95it/s] 70%|███████   | 21/30 [00:04<00:02,  3.83it/s]                                               {'loss': 0.2381, 'grad_norm': 1.2836369276046753, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:02,  3.83it/s] 73%|███████▎  | 22/30 [00:04<00:02,  3.74it/s]                                               {'loss': 0.0426, 'grad_norm': 0.5048117637634277, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:02,  3.74it/s] 77%|███████▋  | 23/30 [00:04<00:01,  3.59it/s]                                               {'loss': 0.3147, 'grad_norm': 2.504655361175537, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  3.59it/s] 80%|████████  | 24/30 [00:05<00:01,  4.21it/s]                                               {'loss': 0.0612, 'grad_norm': 1.1115477085113525, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  4.21it/s] 83%|████████▎ | 25/30 [00:05<00:01,  3.89it/s]                                               {'loss': 0.4213, 'grad_norm': 1.658823013305664, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  3.89it/s] 87%|████████▋ | 26/30 [00:05<00:01,  3.90it/s]                                               {'loss': 0.0498, 'grad_norm': 0.8105342984199524, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:01,  3.90it/s] 90%|█████████ | 27/30 [00:05<00:00,  3.66it/s]                                               {'loss': 0.1883, 'grad_norm': 1.0277771949768066, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  3.66it/s] 93%|█████████▎| 28/30 [00:06<00:00,  3.54it/s]                                               {'loss': 0.1528, 'grad_norm': 0.9038745164871216, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  3.54it/s] 97%|█████████▋| 29/30 [00:06<00:00,  3.64it/s]                                               {'loss': 0.2719, 'grad_norm': 1.477503776550293, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  3.64it/s]100%|██████████| 30/30 [00:06<00:00,  4.27it/s]                                               {'loss': 0.3118, 'grad_norm': 2.6208503246307373, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.27it/s]                                               {'train_runtime': 6.7779, 'train_samples_per_second': 62.704, 'train_steps_per_second': 4.426, 'train_loss': 0.26508499396344026, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.27it/s]100%|██████████| 30/30 [00:06<00:00,  4.43it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:19,  1.48it/s]                                              {'loss': 0.8311, 'grad_norm': 2.805222272872925, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:19,  1.48it/s]  7%|▋         | 2/30 [00:01<00:14,  1.92it/s]                                              {'loss': 0.5924, 'grad_norm': 1.112922191619873, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:14,  1.92it/s]                                              {'loss': 0.5067, 'grad_norm': 0.8007904887199402, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:14,  1.92it/s] 13%|█▎        | 4/30 [00:01<00:06,  3.99it/s]                                              {'loss': 0.5351, 'grad_norm': 1.2573784589767456, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:06,  3.99it/s]                                              {'loss': 0.9015, 'grad_norm': 2.8765742778778076, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.99it/s] 20%|██        | 6/30 [00:01<00:03,  6.25it/s]                                              {'loss': 0.8988, 'grad_norm': 2.8529675006866455, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:03,  6.25it/s]                                              {'loss': 0.505, 'grad_norm': 3.834026336669922, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.25it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.77it/s]                                              {'loss': 0.6827, 'grad_norm': 2.3491079807281494, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.77it/s]                                              {'loss': 0.5688, 'grad_norm': 1.6067768335342407, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.77it/s] 33%|███▎      | 10/30 [00:01<00:02,  8.14it/s]                                               {'loss': 0.5875, 'grad_norm': 2.6629600524902344, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.14it/s]                                               {'loss': 0.677, 'grad_norm': 5.605266571044922, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.14it/s] 40%|████      | 12/30 [00:01<00:02,  8.81it/s]                                               {'loss': 0.5358, 'grad_norm': 3.761403799057007, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.81it/s]                                               {'loss': 0.5993, 'grad_norm': 4.167245388031006, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:01,  8.81it/s] 47%|████▋     | 14/30 [00:02<00:01,  8.09it/s]                                               {'loss': 0.5569, 'grad_norm': 3.130380630493164, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  8.09it/s] 50%|█████     | 15/30 [00:02<00:01,  7.76it/s]                                               {'loss': 0.7699, 'grad_norm': 2.034590244293213, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.76it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.24it/s]                                               {'loss': 0.6834, 'grad_norm': 2.6038119792938232, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.24it/s] 57%|█████▋    | 17/30 [00:02<00:01,  6.98it/s]                                               {'loss': 0.5741, 'grad_norm': 1.4349305629730225, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  6.98it/s]                                               {'loss': 0.7146, 'grad_norm': 5.8532395362854, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  6.98it/s] 63%|██████▎   | 19/30 [00:03<00:01,  6.88it/s]                                               {'loss': 0.634, 'grad_norm': 4.045010089874268, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  6.88it/s] 67%|██████▋   | 20/30 [00:03<00:01,  6.38it/s]                                               {'loss': 0.5048, 'grad_norm': 2.7044105529785156, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  6.38it/s] 70%|███████   | 21/30 [00:03<00:01,  5.94it/s]                                               {'loss': 0.7323, 'grad_norm': 3.294480323791504, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.94it/s] 73%|███████▎  | 22/30 [00:03<00:01,  5.57it/s]                                               {'loss': 0.6777, 'grad_norm': 6.1104044914245605, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  5.57it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.44it/s]                                               {'loss': 0.6646, 'grad_norm': 5.154428482055664, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.44it/s]                                               {'loss': 0.6253, 'grad_norm': 7.112404823303223, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:01,  5.44it/s] 83%|████████▎ | 25/30 [00:04<00:00,  6.70it/s]                                               {'loss': 0.6668, 'grad_norm': 3.4193177223205566, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  6.70it/s] 87%|████████▋ | 26/30 [00:04<00:00,  6.48it/s]                                               {'loss': 0.6857, 'grad_norm': 3.3659274578094482, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  6.48it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.24it/s]                                               {'loss': 0.5657, 'grad_norm': 4.81423807144165, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.24it/s] 93%|█████████▎| 28/30 [00:04<00:00,  4.67it/s]                                               {'loss': 0.5195, 'grad_norm': 5.903503894805908, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  4.67it/s] 97%|█████████▋| 29/30 [00:05<00:00,  4.33it/s]                                               {'loss': 0.7437, 'grad_norm': 7.022158622741699, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  4.33it/s]100%|██████████| 30/30 [00:05<00:00,  4.94it/s]                                               {'loss': 0.6474, 'grad_norm': 10.014998435974121, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.94it/s]                                               {'train_runtime': 5.2986, 'train_samples_per_second': 80.209, 'train_steps_per_second': 5.662, 'train_loss': 0.6462616662184397, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.94it/s]100%|██████████| 30/30 [00:05<00:00,  5.66it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  9.60it/s]                                              {'loss': 0.7146, 'grad_norm': 7.212106227874756, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  9.60it/s]                                              {'loss': 0.5173, 'grad_norm': 2.422196626663208, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02,  9.60it/s] 10%|█         | 3/30 [00:00<00:02, 11.31it/s]                                              {'loss': 0.5142, 'grad_norm': 0.6913542747497559, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.31it/s]                                              {'loss': 0.4345, 'grad_norm': 4.3107590675354, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.31it/s] 17%|█▋        | 5/30 [00:00<00:02, 12.07it/s]                                              {'loss': 0.7426, 'grad_norm': 2.968518018722534, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.07it/s]                                              {'loss': 1.011, 'grad_norm': 5.407961368560791, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 12.07it/s] 23%|██▎       | 7/30 [00:00<00:01, 13.64it/s]                                              {'loss': 0.4451, 'grad_norm': 0.5909246206283569, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 13.64it/s]                                              {'loss': 0.5351, 'grad_norm': 0.4679737985134125, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.64it/s] 30%|███       | 9/30 [00:00<00:01, 13.14it/s]                                              {'loss': 0.6355, 'grad_norm': 0.7173400521278381, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.14it/s]                                              {'loss': 0.5576, 'grad_norm': 0.38372451066970825, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.14it/s] 37%|███▋      | 11/30 [00:00<00:01, 12.56it/s]                                               {'loss': 0.7279, 'grad_norm': 1.0774093866348267, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.56it/s]                                               {'loss': 0.4233, 'grad_norm': 0.7201289534568787, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 12.56it/s] 43%|████▎     | 13/30 [00:01<00:01, 13.32it/s]                                               {'loss': 0.5953, 'grad_norm': 0.6811990737915039, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 13.32it/s]                                               {'loss': 0.465, 'grad_norm': 1.252535343170166, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.32it/s] 50%|█████     | 15/30 [00:01<00:01, 13.29it/s]                                               {'loss': 0.5786, 'grad_norm': 0.39535894989967346, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.29it/s]                                               {'loss': 0.6355, 'grad_norm': 0.5901329517364502, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.29it/s] 57%|█████▋    | 17/30 [00:01<00:01, 12.81it/s]                                               {'loss': 0.5123, 'grad_norm': 0.8703351616859436, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.81it/s]                                               {'loss': 0.9265, 'grad_norm': 2.3751862049102783, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 12.81it/s] 63%|██████▎   | 19/30 [00:01<00:00, 12.46it/s]                                               {'loss': 0.5261, 'grad_norm': 0.45598337054252625, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 12.46it/s]                                               {'loss': 0.5467, 'grad_norm': 0.44972294569015503, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 12.46it/s] 70%|███████   | 21/30 [00:01<00:00, 11.91it/s]                                               {'loss': 0.5642, 'grad_norm': 0.42214715480804443, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 11.91it/s]                                               {'loss': 0.8098, 'grad_norm': 1.7481491565704346, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 11.91it/s] 77%|███████▋  | 23/30 [00:01<00:00, 11.66it/s]                                               {'loss': 0.4283, 'grad_norm': 0.9958584308624268, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 11.66it/s]                                               {'loss': 0.7036, 'grad_norm': 1.0628584623336792, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 11.66it/s] 83%|████████▎ | 25/30 [00:02<00:00, 12.24it/s]                                               {'loss': 0.5334, 'grad_norm': 0.5726620554924011, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.24it/s]                                               {'loss': 0.7101, 'grad_norm': 1.0186680555343628, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 12.24it/s] 90%|█████████ | 27/30 [00:02<00:00, 12.63it/s]                                               {'loss': 0.5159, 'grad_norm': 0.42820224165916443, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 12.63it/s]                                               {'loss': 0.5496, 'grad_norm': 0.3622157573699951, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.63it/s] 97%|█████████▋| 29/30 [00:02<00:00, 12.82it/s]                                               {'loss': 0.7248, 'grad_norm': 1.3434821367263794, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.82it/s]                                               {'loss': 0.3556, 'grad_norm': 1.9261971712112427, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.82it/s]                                               {'train_runtime': 2.4429, 'train_samples_per_second': 173.971, 'train_steps_per_second': 12.28, 'train_loss': 0.5980045090119044, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.82it/s]100%|██████████| 30/30 [00:02<00:00, 12.28it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  9.49it/s]                                              {'loss': 0.6417, 'grad_norm': 3.0611183643341064, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  9.49it/s]  7%|▋         | 2/30 [00:00<00:02,  9.35it/s]                                              {'loss': 0.2908, 'grad_norm': 0.949614405632019, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02,  9.35it/s]                                              {'loss': 0.9763, 'grad_norm': 10.722784042358398, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02,  9.35it/s] 13%|█▎        | 4/30 [00:00<00:02, 10.75it/s]                                              {'loss': 0.7616, 'grad_norm': 3.679170846939087, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.75it/s]                                              {'loss': 0.8493, 'grad_norm': 3.00123929977417, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.75it/s] 20%|██        | 6/30 [00:01<00:07,  3.25it/s]                                              {'loss': 1.427, 'grad_norm': 5.462165355682373, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:07,  3.25it/s] 23%|██▎       | 7/30 [00:01<00:06,  3.71it/s]                                              {'loss': 0.3568, 'grad_norm': 1.4576038122177124, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:06,  3.71it/s] 27%|██▋       | 8/30 [00:01<00:05,  4.08it/s]                                              {'loss': 0.5456, 'grad_norm': 0.3184845447540283, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  4.08it/s] 30%|███       | 9/30 [00:01<00:04,  4.36it/s]                                              {'loss': 0.7423, 'grad_norm': 1.0354276895523071, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.36it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.80it/s]                                               {'loss': 0.6098, 'grad_norm': 0.32855990529060364, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.80it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.20it/s]                                               {'loss': 0.6362, 'grad_norm': 0.3899697959423065, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.20it/s]                                               {'loss': 0.471, 'grad_norm': 1.1181302070617676, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.20it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.20it/s]                                               {'loss': 0.5726, 'grad_norm': 0.6220173835754395, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.20it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.50it/s]                                               {'loss': 0.6062, 'grad_norm': 0.3217962384223938, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.50it/s] 50%|█████     | 15/30 [00:02<00:02,  6.43it/s]                                               {'loss': 0.6576, 'grad_norm': 0.7579273581504822, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.43it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.53it/s]                                               {'loss': 0.5774, 'grad_norm': 0.5498294234275818, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.53it/s] 57%|█████▋    | 17/30 [00:03<00:01,  6.67it/s]                                               {'loss': 0.5025, 'grad_norm': 1.15560781955719, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:01,  6.67it/s]                                               {'loss': 0.6386, 'grad_norm': 0.7447132468223572, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  6.67it/s] 63%|██████▎   | 19/30 [00:03<00:01,  7.63it/s]                                               {'loss': 0.5681, 'grad_norm': 0.4346870183944702, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  7.63it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.10it/s]                                               {'loss': 0.4368, 'grad_norm': 1.2367860078811646, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.10it/s] 70%|███████   | 21/30 [00:03<00:01,  6.91it/s]                                               {'loss': 0.658, 'grad_norm': 0.5434107780456543, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  6.91it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.48it/s]                                               {'loss': 0.6418, 'grad_norm': 0.6677917838096619, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.48it/s] 77%|███████▋  | 23/30 [00:04<00:01,  6.56it/s]                                               {'loss': 0.7335, 'grad_norm': 1.185558795928955, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  6.56it/s]                                               {'loss': 0.5455, 'grad_norm': 0.8704039454460144, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  6.56it/s] 83%|████████▎ | 25/30 [00:04<00:00,  7.74it/s]                                               {'loss': 0.4292, 'grad_norm': 1.0617637634277344, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  7.74it/s] 87%|████████▋ | 26/30 [00:04<00:00,  7.51it/s]                                               {'loss': 0.6231, 'grad_norm': 0.6522064805030823, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  7.51it/s] 90%|█████████ | 27/30 [00:04<00:00,  7.33it/s]                                               {'loss': 0.7538, 'grad_norm': 1.3863892555236816, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  7.33it/s] 93%|█████████▎| 28/30 [00:04<00:00,  7.48it/s]                                               {'loss': 0.5932, 'grad_norm': 0.34006205201148987, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  7.48it/s] 97%|█████████▋| 29/30 [00:04<00:00,  7.38it/s]                                               {'loss': 0.6701, 'grad_norm': 1.115702509880066, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  7.38it/s]                                               {'loss': 0.3554, 'grad_norm': 2.0843708515167236, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.38it/s]                                               {'train_runtime': 5.0107, 'train_samples_per_second': 84.819, 'train_steps_per_second': 5.987, 'train_loss': 0.6290500740210215, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  7.38it/s]100%|██████████| 30/30 [00:05<00:00,  5.99it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  4.00it/s]                                              {'loss': 0.7406, 'grad_norm': 3.2963364124298096, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  4.00it/s]  7%|▋         | 2/30 [00:00<00:07,  3.92it/s]                                              {'loss': 0.6814, 'grad_norm': 1.35120689868927, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.92it/s] 10%|█         | 3/30 [00:00<00:06,  4.00it/s]                                              {'loss': 0.693, 'grad_norm': 0.8500257134437561, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.00it/s] 13%|█▎        | 4/30 [00:00<00:06,  4.22it/s]                                              {'loss': 0.6331, 'grad_norm': 1.0679972171783447, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:06,  4.22it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.23it/s]                                              {'loss': 0.7028, 'grad_norm': 1.0921059846878052, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.23it/s] 20%|██        | 6/30 [00:01<00:04,  4.83it/s]                                              {'loss': 0.8431, 'grad_norm': 3.58429217338562, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.83it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.56it/s]                                              {'loss': 0.6565, 'grad_norm': 0.8063234090805054, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.56it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.55it/s]                                              {'loss': 0.6671, 'grad_norm': 0.7162461876869202, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.55it/s] 30%|███       | 9/30 [00:02<00:04,  4.52it/s]                                              {'loss': 0.6472, 'grad_norm': 0.9315902590751648, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.52it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.53it/s]                                               {'loss': 0.653, 'grad_norm': 2.5518980026245117, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.53it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.52it/s]                                               {'loss': 0.6765, 'grad_norm': 0.8895376920700073, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.52it/s]                                               {'loss': 0.6179, 'grad_norm': 2.840094566345215, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.52it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.00it/s]                                               {'loss': 0.67, 'grad_norm': 2.0598912239074707, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.00it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.05it/s]                                               {'loss': 0.6262, 'grad_norm': 1.201311469078064, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.05it/s] 50%|█████     | 15/30 [00:02<00:02,  6.38it/s]                                               {'loss': 0.6431, 'grad_norm': 2.1880431175231934, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.38it/s] 53%|█████▎    | 16/30 [00:03<00:02,  6.42it/s]                                               {'loss': 0.5705, 'grad_norm': 1.7115938663482666, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  6.42it/s] 57%|█████▋    | 17/30 [00:03<00:02,  6.46it/s]                                               {'loss': 0.6094, 'grad_norm': 24.423616409301758, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  6.46it/s]                                               {'loss': 0.7466, 'grad_norm': 2.2183220386505127, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  6.46it/s] 63%|██████▎   | 19/30 [00:03<00:01,  7.68it/s]                                               {'loss': 0.5333, 'grad_norm': 1.7712770700454712, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  7.68it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.47it/s]                                               {'loss': 0.6238, 'grad_norm': 1.0458507537841797, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.47it/s] 70%|███████   | 21/30 [00:03<00:01,  7.51it/s]                                               {'loss': 0.5794, 'grad_norm': 1.4449347257614136, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.51it/s] 73%|███████▎  | 22/30 [00:03<00:00,  8.03it/s]                                               {'loss': 0.5677, 'grad_norm': 10.53309440612793, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00,  8.03it/s] 77%|███████▋  | 23/30 [00:03<00:00,  8.43it/s]                                               {'loss': 0.5598, 'grad_norm': 1.670100450515747, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  8.43it/s]                                               {'loss': 0.6031, 'grad_norm': 3.24088978767395, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  8.43it/s] 83%|████████▎ | 25/30 [00:04<00:00,  9.14it/s]                                               {'loss': 0.6239, 'grad_norm': 2.1461143493652344, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  9.14it/s]                                               {'loss': 0.5221, 'grad_norm': 1.7334420680999756, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  9.14it/s] 90%|█████████ | 27/30 [00:04<00:00, 10.22it/s]                                               {'loss': 0.5802, 'grad_norm': 2.0752346515655518, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00, 10.22it/s]                                               {'loss': 0.5368, 'grad_norm': 1.4767502546310425, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00, 10.22it/s] 97%|█████████▋| 29/30 [00:04<00:00, 10.40it/s]                                               {'loss': 0.4529, 'grad_norm': 1.9614118337631226, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00, 10.40it/s]                                               {'loss': 0.5632, 'grad_norm': 4.154594421386719, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00, 10.40it/s]                                               {'train_runtime': 4.7242, 'train_samples_per_second': 89.961, 'train_steps_per_second': 6.35, 'train_loss': 0.6274719764788945, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00, 10.40it/s]100%|██████████| 30/30 [00:04<00:00,  6.35it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:87
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.16it/s]                                              {'loss': 0.7615, 'grad_norm': 3.7079427242279053, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.16it/s]  7%|▋         | 2/30 [00:00<00:04,  6.25it/s]                                              {'loss': 0.6849, 'grad_norm': 1.7746860980987549, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.25it/s] 10%|█         | 3/30 [00:00<00:04,  6.59it/s]                                              {'loss': 0.6521, 'grad_norm': 0.985842227935791, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.59it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.62it/s]                                              {'loss': 0.6228, 'grad_norm': 0.9890574812889099, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.62it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.64it/s]                                              {'loss': 0.6836, 'grad_norm': 0.9263002276420593, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.64it/s]                                              {'loss': 0.9113, 'grad_norm': 3.7728400230407715, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.64it/s] 23%|██▎       | 7/30 [00:00<00:02,  7.82it/s]                                              {'loss': 0.6375, 'grad_norm': 0.4892851412296295, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  7.82it/s]                                              {'loss': 0.7516, 'grad_norm': 1.1779471635818481, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.82it/s] 30%|███       | 9/30 [00:01<00:02,  9.31it/s]                                              {'loss': 0.679, 'grad_norm': 0.6444187164306641, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  9.31it/s]                                              {'loss': 0.6364, 'grad_norm': 0.5713638067245483, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  9.31it/s] 37%|███▋      | 11/30 [00:01<00:01, 10.29it/s]                                               {'loss': 0.6318, 'grad_norm': 0.47643667459487915, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 10.29it/s]                                               {'loss': 0.6424, 'grad_norm': 1.8866170644760132, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 10.29it/s] 43%|████▎     | 13/30 [00:01<00:01, 11.27it/s]                                               {'loss': 0.6328, 'grad_norm': 0.8724815845489502, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 11.27it/s]                                               {'loss': 0.6458, 'grad_norm': 0.7255743145942688, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.27it/s] 50%|█████     | 15/30 [00:01<00:01,  9.59it/s]                                               {'loss': 0.7518, 'grad_norm': 1.626525640487671, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  9.59it/s]                                               {'loss': 0.5912, 'grad_norm': 0.7169164419174194, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  9.59it/s] 57%|█████▋    | 17/30 [00:02<00:01,  8.23it/s]                                               {'loss': 0.5778, 'grad_norm': 0.9655529260635376, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  8.23it/s]                                               {'loss': 0.8102, 'grad_norm': 2.082951068878174, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  8.23it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.73it/s]                                               {'loss': 0.6015, 'grad_norm': 1.2526811361312866, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  9.73it/s]                                               {'loss': 0.6323, 'grad_norm': 0.7075701355934143, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  9.73it/s] 70%|███████   | 21/30 [00:02<00:00, 10.38it/s]                                               {'loss': 0.615, 'grad_norm': 1.0386614799499512, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 10.38it/s]                                               {'loss': 0.5824, 'grad_norm': 1.3143795728683472, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 10.38it/s] 77%|███████▋  | 23/30 [00:02<00:00, 11.04it/s]                                               {'loss': 0.6543, 'grad_norm': 1.7587119340896606, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 11.04it/s]                                               {'loss': 0.5099, 'grad_norm': 2.2936785221099854, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 11.04it/s] 83%|████████▎ | 25/30 [00:02<00:00, 12.55it/s]                                               {'loss': 0.6167, 'grad_norm': 1.5566985607147217, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.55it/s]                                               {'loss': 0.5955, 'grad_norm': 1.2352585792541504, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 12.55it/s] 90%|█████████ | 27/30 [00:02<00:00, 10.89it/s]                                               {'loss': 0.575, 'grad_norm': 1.211543083190918, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 10.89it/s]                                               {'loss': 0.5469, 'grad_norm': 1.2646995782852173, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 10.89it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.77it/s]                                               {'loss': 0.558, 'grad_norm': 1.6377464532852173, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  9.77it/s]                                               {'loss': 0.5167, 'grad_norm': 3.0138535499572754, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  9.77it/s]                                               {'train_runtime': 3.2173, 'train_samples_per_second': 132.099, 'train_steps_per_second': 9.325, 'train_loss': 0.6436223228772481, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  9.77it/s]100%|██████████| 30/30 [00:03<00:00,  9.33it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.71it/s]                                              {'loss': 0.7765, 'grad_norm': 4.076582431793213, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.71it/s]  7%|▋         | 2/30 [00:00<00:04,  6.13it/s]                                              {'loss': 0.7239, 'grad_norm': 2.05442476272583, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.13it/s] 10%|█         | 3/30 [00:00<00:04,  6.25it/s]                                              {'loss': 0.688, 'grad_norm': 1.1912680864334106, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.25it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.30it/s]                                              {'loss': 0.6718, 'grad_norm': 1.2610678672790527, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.30it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.92it/s]                                              {'loss': 0.7366, 'grad_norm': 1.0525846481323242, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.92it/s]                                              {'loss': 0.4512, 'grad_norm': 2.2816691398620605, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.92it/s] 23%|██▎       | 7/30 [00:00<00:03,  7.63it/s]                                              {'loss': 0.6951, 'grad_norm': 1.1195647716522217, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  7.63it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.44it/s]                                              {'loss': 0.8496, 'grad_norm': 2.1639339923858643, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.44it/s] 30%|███       | 9/30 [00:01<00:02,  7.25it/s]                                              {'loss': 0.5341, 'grad_norm': 0.7725148797035217, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.25it/s] 33%|███▎      | 10/30 [00:01<00:02,  6.80it/s]                                               {'loss': 0.7091, 'grad_norm': 0.7553526163101196, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  6.80it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.96it/s]                                               {'loss': 0.6558, 'grad_norm': 0.7079002261161804, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.96it/s]                                               {'loss': 0.5322, 'grad_norm': 1.0505032539367676, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.96it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.19it/s]                                               {'loss': 0.6326, 'grad_norm': 1.3828250169754028, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.19it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.88it/s]                                               {'loss': 0.6033, 'grad_norm': 1.0384509563446045, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.88it/s] 50%|█████     | 15/30 [00:02<00:02,  6.80it/s]                                               {'loss': 0.768, 'grad_norm': 2.0807130336761475, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.80it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.90it/s]                                               {'loss': 0.7058, 'grad_norm': 1.602040410041809, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.90it/s] 57%|█████▋    | 17/30 [00:02<00:01,  6.58it/s]                                               {'loss': 0.5899, 'grad_norm': 1.0599465370178223, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  6.58it/s]                                               {'loss': 0.6467, 'grad_norm': 1.7358423471450806, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  6.58it/s] 63%|██████▎   | 19/30 [00:02<00:01,  6.87it/s]                                               {'loss': 0.5778, 'grad_norm': 1.0295943021774292, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  6.87it/s] 67%|██████▋   | 20/30 [00:02<00:01,  6.59it/s]                                               {'loss': 0.6647, 'grad_norm': 1.1205463409423828, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  6.59it/s] 70%|███████   | 21/30 [00:03<00:01,  7.21it/s]                                               {'loss': 0.6641, 'grad_norm': 1.4633996486663818, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.21it/s]                                               {'loss': 0.6442, 'grad_norm': 1.6001460552215576, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.21it/s] 77%|███████▋  | 23/30 [00:03<00:00,  8.47it/s]                                               {'loss': 0.5994, 'grad_norm': 1.1111602783203125, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  8.47it/s]                                               {'loss': 0.5714, 'grad_norm': 1.9528149366378784, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  8.47it/s] 83%|████████▎ | 25/30 [00:03<00:00,  9.66it/s]                                               {'loss': 0.6121, 'grad_norm': 1.254101037979126, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  9.66it/s] 87%|████████▋ | 26/30 [00:03<00:00,  9.50it/s]                                               {'loss': 0.6249, 'grad_norm': 1.701353907585144, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  9.50it/s] 90%|█████████ | 27/30 [00:03<00:00,  8.76it/s]                                               {'loss': 0.5901, 'grad_norm': 1.209605097770691, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  8.76it/s] 93%|█████████▎| 28/30 [00:03<00:00,  8.66it/s]                                               {'loss': 0.5496, 'grad_norm': 1.2800877094268799, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  8.66it/s]                                               {'loss': 0.5676, 'grad_norm': 2.7105162143707275, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  8.66it/s]100%|██████████| 30/30 [00:03<00:00, 11.00it/s]                                               {'loss': 0.5825, 'grad_norm': 2.1977386474609375, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.00it/s]                                               {'train_runtime': 3.953, 'train_samples_per_second': 107.512, 'train_steps_per_second': 7.589, 'train_loss': 0.6406159768501918, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.00it/s]100%|██████████| 30/30 [00:03<00:00,  7.59it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:99
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:05,  7.91it/s]                                              {'loss': 0.5357, 'grad_norm': 1.3522597551345825, 'learning_rate': 0.001, 'epoch': 0.11}
  2%|▏         | 1/45 [00:00<00:05,  7.91it/s]  4%|▍         | 2/45 [00:00<00:04,  8.99it/s]                                              {'loss': 0.4743, 'grad_norm': 1.284505844116211, 'learning_rate': 0.0009777777777777777, 'epoch': 0.22}
  4%|▍         | 2/45 [00:00<00:04,  8.99it/s]  7%|▋         | 3/45 [00:00<00:04,  9.06it/s]                                              {'loss': 0.3094, 'grad_norm': 0.7521519660949707, 'learning_rate': 0.0009555555555555556, 'epoch': 0.33}
  7%|▋         | 3/45 [00:00<00:04,  9.06it/s]                                              {'loss': 0.7292, 'grad_norm': 3.59395432472229, 'learning_rate': 0.0009333333333333333, 'epoch': 0.44}
  9%|▉         | 4/45 [00:00<00:04,  9.06it/s] 11%|█         | 5/45 [00:00<00:04,  9.39it/s]                                              {'loss': 0.3936, 'grad_norm': 1.904903531074524, 'learning_rate': 0.0009111111111111111, 'epoch': 0.56}
 11%|█         | 5/45 [00:00<00:04,  9.39it/s]                                              {'loss': 0.6311, 'grad_norm': 5.043588638305664, 'learning_rate': 0.0008888888888888888, 'epoch': 0.67}
 13%|█▎        | 6/45 [00:00<00:04,  9.39it/s] 16%|█▌        | 7/45 [00:00<00:03, 10.77it/s]                                              {'loss': 0.368, 'grad_norm': 1.2400966882705688, 'learning_rate': 0.0008666666666666667, 'epoch': 0.78}
 16%|█▌        | 7/45 [00:00<00:03, 10.77it/s]                                              {'loss': 0.6424, 'grad_norm': 2.9365897178649902, 'learning_rate': 0.0008444444444444444, 'epoch': 0.89}
 18%|█▊        | 8/45 [00:00<00:03, 10.77it/s] 20%|██        | 9/45 [00:00<00:02, 12.60it/s]                                              {'loss': 0.3659, 'grad_norm': 1.6553559303283691, 'learning_rate': 0.0008222222222222222, 'epoch': 1.0}
 20%|██        | 9/45 [00:00<00:02, 12.60it/s]                                              {'loss': 0.4594, 'grad_norm': 7.949873924255371, 'learning_rate': 0.0008, 'epoch': 1.11}
 22%|██▏       | 10/45 [00:00<00:02, 12.60it/s] 24%|██▍       | 11/45 [00:00<00:02, 12.44it/s]                                               {'loss': 0.4676, 'grad_norm': 1.9500608444213867, 'learning_rate': 0.0007777777777777778, 'epoch': 1.22}
 24%|██▍       | 11/45 [00:00<00:02, 12.44it/s]                                               {'loss': 0.4552, 'grad_norm': 3.7701237201690674, 'learning_rate': 0.0007555555555555555, 'epoch': 1.33}
 27%|██▋       | 12/45 [00:01<00:02, 12.44it/s] 29%|██▉       | 13/45 [00:01<00:02, 12.42it/s]                                               {'loss': 0.3346, 'grad_norm': 2.8730013370513916, 'learning_rate': 0.0007333333333333333, 'epoch': 1.44}
 29%|██▉       | 13/45 [00:01<00:02, 12.42it/s]                                               {'loss': 0.6237, 'grad_norm': 2.3558356761932373, 'learning_rate': 0.0007111111111111111, 'epoch': 1.56}
 31%|███       | 14/45 [00:01<00:02, 12.42it/s] 33%|███▎      | 15/45 [00:01<00:02, 12.08it/s]                                               {'loss': 0.6245, 'grad_norm': 2.1380279064178467, 'learning_rate': 0.000688888888888889, 'epoch': 1.67}
 33%|███▎      | 15/45 [00:01<00:02, 12.08it/s]                                               {'loss': 0.4986, 'grad_norm': 0.969040036201477, 'learning_rate': 0.0006666666666666666, 'epoch': 1.78}
 36%|███▌      | 16/45 [00:01<00:02, 12.08it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.27it/s]                                               {'loss': 0.3893, 'grad_norm': 3.252943515777588, 'learning_rate': 0.0006444444444444444, 'epoch': 1.89}
 38%|███▊      | 17/45 [00:01<00:02, 12.27it/s]                                               {'loss': 0.7262, 'grad_norm': 4.136804103851318, 'learning_rate': 0.0006222222222222223, 'epoch': 2.0}
 40%|████      | 18/45 [00:01<00:02, 12.27it/s] 42%|████▏     | 19/45 [00:01<00:02, 12.66it/s]                                               {'loss': 0.6019, 'grad_norm': 1.6424065828323364, 'learning_rate': 0.0006, 'epoch': 2.11}
 42%|████▏     | 19/45 [00:01<00:02, 12.66it/s]                                               {'loss': 0.4969, 'grad_norm': 1.6649131774902344, 'learning_rate': 0.0005777777777777778, 'epoch': 2.22}
 44%|████▍     | 20/45 [00:01<00:01, 12.66it/s] 47%|████▋     | 21/45 [00:01<00:01, 12.32it/s]                                               {'loss': 0.5796, 'grad_norm': 1.1255018711090088, 'learning_rate': 0.0005555555555555556, 'epoch': 2.33}
 47%|████▋     | 21/45 [00:01<00:01, 12.32it/s]                                               {'loss': 0.3577, 'grad_norm': 1.650808334350586, 'learning_rate': 0.0005333333333333334, 'epoch': 2.44}
 49%|████▉     | 22/45 [00:01<00:01, 12.32it/s] 51%|█████     | 23/45 [00:01<00:01, 12.28it/s]                                               {'loss': 0.4321, 'grad_norm': 1.4398940801620483, 'learning_rate': 0.0005111111111111111, 'epoch': 2.56}
 51%|█████     | 23/45 [00:01<00:01, 12.28it/s]                                               {'loss': 0.4741, 'grad_norm': 0.7998513579368591, 'learning_rate': 0.0004888888888888889, 'epoch': 2.67}
 53%|█████▎    | 24/45 [00:02<00:01, 12.28it/s] 56%|█████▌    | 25/45 [00:02<00:01, 12.49it/s]                                               {'loss': 0.3157, 'grad_norm': 1.2519817352294922, 'learning_rate': 0.00046666666666666666, 'epoch': 2.78}
 56%|█████▌    | 25/45 [00:02<00:01, 12.49it/s]                                               {'loss': 0.5343, 'grad_norm': 0.7891993522644043, 'learning_rate': 0.0004444444444444444, 'epoch': 2.89}
 58%|█████▊    | 26/45 [00:02<00:01, 12.49it/s] 60%|██████    | 27/45 [00:02<00:01, 13.62it/s]                                               {'loss': 0.549, 'grad_norm': 1.3149497509002686, 'learning_rate': 0.0004222222222222222, 'epoch': 3.0}
 60%|██████    | 27/45 [00:02<00:01, 13.62it/s]                                               {'loss': 0.1929, 'grad_norm': 1.4386259317398071, 'learning_rate': 0.0004, 'epoch': 3.11}
 62%|██████▏   | 28/45 [00:02<00:01, 13.62it/s] 64%|██████▍   | 29/45 [00:02<00:01, 12.31it/s]                                               {'loss': 0.5523, 'grad_norm': 0.6582173109054565, 'learning_rate': 0.00037777777777777777, 'epoch': 3.22}
 64%|██████▍   | 29/45 [00:02<00:01, 12.31it/s]                                               {'loss': 0.6606, 'grad_norm': 1.4376931190490723, 'learning_rate': 0.00035555555555555557, 'epoch': 3.33}
 67%|██████▋   | 30/45 [00:02<00:01, 12.31it/s] 69%|██████▉   | 31/45 [00:02<00:01, 12.10it/s]                                               {'loss': 0.2711, 'grad_norm': 1.0648530721664429, 'learning_rate': 0.0003333333333333333, 'epoch': 3.44}
 69%|██████▉   | 31/45 [00:02<00:01, 12.10it/s]                                               {'loss': 0.4681, 'grad_norm': 0.3987027108669281, 'learning_rate': 0.0003111111111111111, 'epoch': 3.56}
 71%|███████   | 32/45 [00:02<00:01, 12.10it/s] 73%|███████▎  | 33/45 [00:02<00:01, 10.86it/s]                                               {'loss': 0.5486, 'grad_norm': 0.9914970993995667, 'learning_rate': 0.0002888888888888889, 'epoch': 3.67}
 73%|███████▎  | 33/45 [00:02<00:01, 10.86it/s]                                               {'loss': 0.7257, 'grad_norm': 2.2209842205047607, 'learning_rate': 0.0002666666666666667, 'epoch': 3.78}
 76%|███████▌  | 34/45 [00:02<00:01, 10.86it/s] 78%|███████▊  | 35/45 [00:03<00:01,  9.86it/s]                                               {'loss': 0.6138, 'grad_norm': 1.1093957424163818, 'learning_rate': 0.00024444444444444443, 'epoch': 3.89}
 78%|███████▊  | 35/45 [00:03<00:01,  9.86it/s]                                               {'loss': 0.1826, 'grad_norm': 1.322060227394104, 'learning_rate': 0.0002222222222222222, 'epoch': 4.0}
 80%|████████  | 36/45 [00:03<00:00,  9.86it/s] 82%|████████▏ | 37/45 [00:03<00:00,  8.87it/s]                                               {'loss': 0.4496, 'grad_norm': 0.3480571210384369, 'learning_rate': 0.0002, 'epoch': 4.11}
 82%|████████▏ | 37/45 [00:03<00:00,  8.87it/s] 84%|████████▍ | 38/45 [00:03<00:00,  8.98it/s]                                               {'loss': 0.3816, 'grad_norm': 0.46989572048187256, 'learning_rate': 0.00017777777777777779, 'epoch': 4.22}
 84%|████████▍ | 38/45 [00:03<00:00,  8.98it/s]                                               {'loss': 0.6237, 'grad_norm': 1.0000444650650024, 'learning_rate': 0.00015555555555555556, 'epoch': 4.33}
 87%|████████▋ | 39/45 [00:03<00:00,  8.98it/s] 89%|████████▉ | 40/45 [00:03<00:00,  9.83it/s]                                               {'loss': 0.2915, 'grad_norm': 0.7879151701927185, 'learning_rate': 0.00013333333333333334, 'epoch': 4.44}
 89%|████████▉ | 40/45 [00:03<00:00,  9.83it/s]                                               {'loss': 0.3693, 'grad_norm': 0.5306981801986694, 'learning_rate': 0.0001111111111111111, 'epoch': 4.56}
 91%|█████████ | 41/45 [00:03<00:00,  9.83it/s] 93%|█████████▎| 42/45 [00:03<00:00, 10.36it/s]                                               {'loss': 0.5651, 'grad_norm': 1.2644578218460083, 'learning_rate': 8.888888888888889e-05, 'epoch': 4.67}
 93%|█████████▎| 42/45 [00:03<00:00, 10.36it/s]                                               {'loss': 0.5631, 'grad_norm': 1.4342659711837769, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.78}
 96%|█████████▌| 43/45 [00:03<00:00, 10.36it/s] 98%|█████████▊| 44/45 [00:03<00:00, 10.77it/s]                                               {'loss': 0.5442, 'grad_norm': 0.6982690095901489, 'learning_rate': 4.4444444444444447e-05, 'epoch': 4.89}
 98%|█████████▊| 44/45 [00:03<00:00, 10.77it/s]                                               {'loss': 0.5279, 'grad_norm': 0.6056510210037231, 'learning_rate': 2.2222222222222223e-05, 'epoch': 5.0}
100%|██████████| 45/45 [00:03<00:00, 10.77it/s]                                               {'train_runtime': 4.0895, 'train_samples_per_second': 166.282, 'train_steps_per_second': 11.004, 'train_loss': 0.48670076694753434, 'epoch': 5.0}
100%|██████████| 45/45 [00:04<00:00, 10.77it/s]100%|██████████| 45/45 [00:04<00:00, 11.01it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7297, 'grad_norm': 4.44276237487793, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 11.45it/s]  7%|▋         | 2/30 [00:00<00:02, 12.02it/s]                                              {'loss': 0.6869, 'grad_norm': 1.8168227672576904, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 12.02it/s]                                              {'loss': 0.7004, 'grad_norm': 1.2060949802398682, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 12.02it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.37it/s]                                              {'loss': 0.6359, 'grad_norm': 1.2742156982421875, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.37it/s]                                              {'loss': 0.7699, 'grad_norm': 1.8492960929870605, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.37it/s] 20%|██        | 6/30 [00:01<00:06,  3.77it/s]                                              {'loss': 0.7663, 'grad_norm': 1.8770487308502197, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.77it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.45it/s]                                              {'loss': 0.6471, 'grad_norm': 0.8703821301460266, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.45it/s]                                              {'loss': 0.7031, 'grad_norm': 1.3516052961349487, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.45it/s] 30%|███       | 9/30 [00:01<00:03,  6.08it/s]                                              {'loss': 0.664, 'grad_norm': 0.9100519418716431, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.08it/s]                                              {'loss': 0.6622, 'grad_norm': 1.951316475868225, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.08it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.06it/s]                                               {'loss': 0.6728, 'grad_norm': 0.8001130223274231, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.06it/s]                                               {'loss': 0.673, 'grad_norm': 2.5194435119628906, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.06it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.83it/s]                                               {'loss': 0.6747, 'grad_norm': 1.0858290195465088, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.83it/s]                                               {'loss': 0.6396, 'grad_norm': 0.8404264450073242, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.83it/s] 50%|█████     | 15/30 [00:02<00:01,  8.82it/s]                                               {'loss': 0.6307, 'grad_norm': 1.9879547357559204, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.82it/s]                                               {'loss': 0.6136, 'grad_norm': 0.9777010679244995, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.82it/s] 57%|█████▋    | 17/30 [00:02<00:01,  9.26it/s]                                               {'loss': 0.6569, 'grad_norm': 0.8687483668327332, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  9.26it/s]                                               {'loss': 0.6435, 'grad_norm': 1.6534113883972168, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  9.26it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.85it/s]                                               {'loss': 0.5916, 'grad_norm': 1.1785056591033936, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  9.85it/s]                                               {'loss': 0.6563, 'grad_norm': 0.9990614652633667, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  9.85it/s] 70%|███████   | 21/30 [00:02<00:00,  9.96it/s]                                               {'loss': 0.6851, 'grad_norm': 1.7397316694259644, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00,  9.96it/s]                                               {'loss': 0.6458, 'grad_norm': 1.0500361919403076, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  9.96it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.61it/s]                                               {'loss': 0.6553, 'grad_norm': 1.6068238019943237, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  9.61it/s]                                               {'loss': 0.6091, 'grad_norm': 1.2079520225524902, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  9.61it/s] 83%|████████▎ | 25/30 [00:03<00:00, 10.31it/s]                                               {'loss': 0.6281, 'grad_norm': 1.9614711999893188, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 10.31it/s]                                               {'loss': 0.6254, 'grad_norm': 1.0893760919570923, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 10.31it/s] 90%|█████████ | 27/30 [00:03<00:00, 10.65it/s]                                               {'loss': 0.618, 'grad_norm': 6.080894470214844, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 10.65it/s]                                               {'loss': 0.6152, 'grad_norm': 0.7240368723869324, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 10.65it/s] 97%|█████████▋| 29/30 [00:03<00:00, 10.91it/s]                                               {'loss': 0.5428, 'grad_norm': 0.9941161274909973, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 10.91it/s]                                               {'loss': 0.5879, 'grad_norm': 1.3179033994674683, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 10.91it/s]                                               {'train_runtime': 3.5537, 'train_samples_per_second': 119.595, 'train_steps_per_second': 8.442, 'train_loss': 0.6543655395507812, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 10.91it/s]100%|██████████| 30/30 [00:03<00:00,  8.44it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
the shape of base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.weight is (768, 768)
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  6%|▌         | 4/66 [00:00<00:01, 35.87it/s] 12%|█▏        | 8/66 [00:00<00:02, 28.81it/s] 17%|█▋        | 11/66 [00:00<00:02, 27.48it/s] 23%|██▎       | 15/66 [00:00<00:01, 28.45it/s] 27%|██▋       | 18/66 [00:00<00:01, 28.47it/s] 32%|███▏      | 21/66 [00:00<00:01, 28.49it/s] 36%|███▋      | 24/66 [00:00<00:01, 28.21it/s] 41%|████      | 27/66 [00:00<00:01, 27.32it/s] 45%|████▌     | 30/66 [00:01<00:01, 27.77it/s] 50%|█████     | 33/66 [00:01<00:01, 28.19it/s] 55%|█████▍    | 36/66 [00:01<00:01, 28.25it/s] 59%|█████▉    | 39/66 [00:01<00:00, 28.38it/s] 64%|██████▎   | 42/66 [00:01<00:00, 28.48it/s] 68%|██████▊   | 45/66 [00:01<00:00, 28.68it/s] 73%|███████▎  | 48/66 [00:01<00:00, 28.91it/s] 77%|███████▋  | 51/66 [00:01<00:00, 29.20it/s] 83%|████████▎ | 55/66 [00:01<00:00, 29.55it/s] 88%|████████▊ | 58/66 [00:02<00:00, 29.33it/s] 92%|█████████▏| 61/66 [00:02<00:00, 29.31it/s] 97%|█████████▋| 64/66 [00:02<00:00, 29.43it/s]100%|██████████| 66/66 [00:02<00:00, 29.16it/s]
{'eval_loss': 0.609501302242279, 'eval_model_preparation_time': 0.006, 'eval_acc': 0.6912751677852349, 'eval_runtime': 2.3019, 'eval_samples_per_second': 453.109, 'eval_steps_per_second': 28.672}
ROUND:9
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.41it/s]                                              {'loss': 0.507, 'grad_norm': 3.9891412258148193, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.41it/s]  7%|▋         | 2/30 [00:00<00:04,  6.96it/s]                                              {'loss': 0.1519, 'grad_norm': 1.475510597229004, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.96it/s] 10%|█         | 3/30 [00:00<00:03,  6.93it/s]                                              {'loss': 0.0365, 'grad_norm': 0.5098827481269836, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  6.93it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.05it/s]                                              {'loss': 0.3252, 'grad_norm': 13.879095077514648, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.05it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.15it/s]                                              {'loss': 0.0126, 'grad_norm': 0.6195440888404846, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.15it/s]                                              {'loss': 0.0074, 'grad_norm': 0.26380303502082825, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.15it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.12it/s]                                              {'loss': 0.0071, 'grad_norm': 0.12027700990438461, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.12it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.98it/s]                                              {'loss': 0.0044, 'grad_norm': 0.09175986796617508, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.98it/s] 30%|███       | 9/30 [00:01<00:02,  7.61it/s]                                              {'loss': 0.0025, 'grad_norm': 0.04965715855360031, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.61it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.36it/s]                                               {'loss': 0.4086, 'grad_norm': 1.127558708190918, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.36it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.19it/s]                                               {'loss': 0.0009, 'grad_norm': 0.01735527440905571, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.19it/s]                                               {'loss': 0.0009, 'grad_norm': 0.019662955775856972, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.19it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.34it/s]                                               {'loss': 0.0005, 'grad_norm': 0.00915599800646305, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.34it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.95it/s]                                               {'loss': 0.0009, 'grad_norm': 0.013927236199378967, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.95it/s] 50%|█████     | 15/30 [00:01<00:01,  7.84it/s]                                               {'loss': 0.4334, 'grad_norm': 1.4808142185211182, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  7.84it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.44it/s]                                               {'loss': 0.001, 'grad_norm': 0.01618822105228901, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.44it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.38it/s]                                               {'loss': 0.002, 'grad_norm': 0.03287192061543465, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.38it/s]                                               {'loss': 0.0011, 'grad_norm': 0.021728109568357468, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.38it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.58it/s]                                               {'loss': 0.0011, 'grad_norm': 0.017594555392861366, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.58it/s] 67%|██████▋   | 20/30 [00:02<00:01,  7.92it/s]                                               {'loss': 0.0019, 'grad_norm': 0.0333780013024807, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  7.92it/s] 70%|███████   | 21/30 [00:02<00:01,  7.45it/s]                                               {'loss': 0.002, 'grad_norm': 0.02931961417198181, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  7.45it/s] 73%|███████▎  | 22/30 [00:02<00:01,  6.96it/s]                                               {'loss': 0.002, 'grad_norm': 0.035552408546209335, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  6.96it/s] 77%|███████▋  | 23/30 [00:03<00:00,  7.20it/s]                                               {'loss': 0.4815, 'grad_norm': 1.6827102899551392, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.20it/s]                                               {'loss': 0.0019, 'grad_norm': 0.033474624156951904, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.20it/s] 83%|████████▎ | 25/30 [00:03<00:00,  7.74it/s]                                               {'loss': 0.0039, 'grad_norm': 0.059363238513469696, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  7.74it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.26it/s]                                               {'loss': 0.0029, 'grad_norm': 0.04820925369858742, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.26it/s] 90%|█████████ | 27/30 [00:03<00:00,  7.11it/s]                                               {'loss': 0.3353, 'grad_norm': 1.3576925992965698, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  7.11it/s] 93%|█████████▎| 28/30 [00:03<00:00,  7.03it/s]                                               {'loss': 0.003, 'grad_norm': 0.04739060997962952, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  7.03it/s] 97%|█████████▋| 29/30 [00:03<00:00,  6.97it/s]                                               {'loss': 0.0031, 'grad_norm': 0.05370897054672241, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  6.97it/s]                                               {'loss': 0.0034, 'grad_norm': 0.05897214263677597, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  6.97it/s]                                               {'train_runtime': 4.0211, 'train_samples_per_second': 105.693, 'train_steps_per_second': 7.461, 'train_loss': 0.09152288187566834, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.97it/s]100%|██████████| 30/30 [00:04<00:00,  7.46it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.76it/s]                                              {'loss': 0.8246, 'grad_norm': 3.7704339027404785, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.76it/s]                                              {'loss': 0.6709, 'grad_norm': 1.563786506652832, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.76it/s] 10%|█         | 3/30 [00:00<00:02, 10.17it/s]                                              {'loss': 0.6718, 'grad_norm': 0.8578183054924011, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.17it/s]                                              {'loss': 0.6392, 'grad_norm': 1.524886131286621, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.17it/s] 17%|█▋        | 5/30 [00:00<00:02, 10.71it/s]                                              {'loss': 0.757, 'grad_norm': 1.5089443922042847, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.71it/s]                                              {'loss': 0.811, 'grad_norm': 3.350903034210205, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 10.71it/s] 23%|██▎       | 7/30 [00:00<00:02, 11.47it/s]                                              {'loss': 0.6778, 'grad_norm': 0.6530582308769226, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02, 11.47it/s]                                              {'loss': 0.6973, 'grad_norm': 1.2919293642044067, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 11.47it/s] 30%|███       | 9/30 [00:00<00:01, 11.33it/s]                                              {'loss': 0.6571, 'grad_norm': 0.8378063440322876, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 11.33it/s]                                              {'loss': 0.6848, 'grad_norm': 0.8458769917488098, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.33it/s] 37%|███▋      | 11/30 [00:00<00:01, 11.79it/s]                                               {'loss': 0.7023, 'grad_norm': 1.2305564880371094, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 11.79it/s]                                               {'loss': 0.8198, 'grad_norm': 3.608919858932495, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 11.79it/s] 43%|████▎     | 13/30 [00:01<00:01, 13.43it/s]                                               {'loss': 0.6834, 'grad_norm': 0.925139844417572, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 13.43it/s]                                               {'loss': 0.6735, 'grad_norm': 0.8659092783927917, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.43it/s] 50%|█████     | 15/30 [00:01<00:01, 12.91it/s]                                               {'loss': 0.5633, 'grad_norm': 1.5163394212722778, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 12.91it/s]                                               {'loss': 0.7059, 'grad_norm': 1.2966513633728027, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 12.91it/s] 57%|█████▋    | 17/30 [00:01<00:01, 12.65it/s]                                               {'loss': 0.6813, 'grad_norm': 1.0133963823318481, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.65it/s]                                               {'loss': 0.6089, 'grad_norm': 1.1856110095977783, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 12.65it/s] 63%|██████▎   | 19/30 [00:01<00:00, 14.21it/s]                                               {'loss': 0.7008, 'grad_norm': 1.2973078489303589, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.21it/s]                                               {'loss': 0.6206, 'grad_norm': 0.5254446864128113, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.21it/s] 70%|███████   | 21/30 [00:01<00:00, 13.17it/s]                                               {'loss': 0.6439, 'grad_norm': 0.6253211498260498, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.17it/s]                                               {'loss': 0.6402, 'grad_norm': 0.6603777408599854, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.17it/s] 77%|███████▋  | 23/30 [00:01<00:00, 12.89it/s]                                               {'loss': 0.6347, 'grad_norm': 1.157475471496582, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 12.89it/s]                                               {'loss': 0.5559, 'grad_norm': 1.1034505367279053, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 12.89it/s] 83%|████████▎ | 25/30 [00:01<00:00, 13.69it/s]                                               {'loss': 0.6806, 'grad_norm': 0.9713310599327087, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 13.69it/s]                                               {'loss': 0.6718, 'grad_norm': 2.9290263652801514, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.69it/s] 90%|█████████ | 27/30 [00:02<00:00, 12.97it/s]                                               {'loss': 0.6577, 'grad_norm': 0.9225131869316101, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 12.97it/s]                                               {'loss': 0.6572, 'grad_norm': 0.8503237366676331, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.97it/s] 97%|█████████▋| 29/30 [00:02<00:00, 12.60it/s]                                               {'loss': 0.539, 'grad_norm': 0.8801746964454651, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.60it/s]                                               {'loss': 0.6105, 'grad_norm': 1.8416069746017456, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.60it/s]                                               {'train_runtime': 2.4349, 'train_samples_per_second': 174.543, 'train_steps_per_second': 12.321, 'train_loss': 0.671424514055252, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.60it/s]100%|██████████| 30/30 [00:02<00:00, 12.32it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.4704, 'grad_norm': 2.123840093612671, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 10.16it/s]  7%|▋         | 2/30 [00:00<00:02, 10.86it/s]                                              {'loss': 0.1666, 'grad_norm': 1.4968607425689697, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 10.86it/s]                                              {'loss': 0.4141, 'grad_norm': 2.916050672531128, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.86it/s] 13%|█▎        | 4/30 [00:00<00:02, 10.68it/s]                                              {'loss': 0.0211, 'grad_norm': 0.2339072972536087, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.68it/s]                                              {'loss': 0.5266, 'grad_norm': 1.8780540227890015, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.68it/s] 20%|██        | 6/30 [00:00<00:01, 12.24it/s]                                              {'loss': 0.0089, 'grad_norm': 0.15441104769706726, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 12.24it/s]                                              {'loss': 0.248, 'grad_norm': 1.163610816001892, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 12.24it/s] 27%|██▋       | 8/30 [00:00<00:01, 11.88it/s]                                              {'loss': 1.2787, 'grad_norm': 3.7964253425598145, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 11.88it/s]                                              {'loss': 0.0281, 'grad_norm': 0.28576400876045227, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 11.88it/s] 33%|███▎      | 10/30 [00:00<00:01, 12.23it/s]                                               {'loss': 0.2175, 'grad_norm': 0.9424423575401306, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 12.23it/s]                                               {'loss': 0.0329, 'grad_norm': 0.38370412588119507, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.23it/s] 40%|████      | 12/30 [00:00<00:01, 13.91it/s]                                               {'loss': 0.0299, 'grad_norm': 0.4663289785385132, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 13.91it/s]                                               {'loss': 0.4329, 'grad_norm': 3.7351622581481934, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 13.91it/s] 47%|████▋     | 14/30 [00:01<00:01, 13.07it/s]                                               {'loss': 0.0328, 'grad_norm': 0.425383597612381, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.07it/s]                                               {'loss': 0.592, 'grad_norm': 2.4866864681243896, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.07it/s] 53%|█████▎    | 16/30 [00:01<00:01, 12.66it/s]                                               {'loss': 0.2609, 'grad_norm': 1.0873299837112427, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 12.66it/s]                                               {'loss': 0.0375, 'grad_norm': 0.381796270608902, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.66it/s] 60%|██████    | 18/30 [00:01<00:00, 12.37it/s]                                               {'loss': 0.6432, 'grad_norm': 14.08414363861084, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 12.37it/s]                                               {'loss': 0.0348, 'grad_norm': 0.45510435104370117, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 12.37it/s] 67%|██████▋   | 20/30 [00:01<00:00, 11.22it/s]                                               {'loss': 0.8034, 'grad_norm': 6.449179172515869, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 11.22it/s]                                               {'loss': 0.2055, 'grad_norm': 0.8364331126213074, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 11.22it/s] 73%|███████▎  | 22/30 [00:01<00:00,  9.90it/s]                                               {'loss': 0.2434, 'grad_norm': 0.8911405205726624, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00,  9.90it/s]                                               {'loss': 0.2468, 'grad_norm': 1.4717140197753906, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  9.90it/s] 80%|████████  | 24/30 [00:02<00:00,  9.84it/s]                                               {'loss': 0.0504, 'grad_norm': 0.7161269187927246, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  9.84it/s]                                               {'loss': 0.2037, 'grad_norm': 1.0315104722976685, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00,  9.84it/s] 87%|████████▋ | 26/30 [00:02<00:00, 10.18it/s]                                               {'loss': 0.0595, 'grad_norm': 0.6994717717170715, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 10.18it/s]                                               {'loss': 0.1776, 'grad_norm': 0.9591479897499084, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 10.18it/s] 93%|█████████▎| 28/30 [00:02<00:00, 10.22it/s]                                               {'loss': 0.4167, 'grad_norm': 1.6342880725860596, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 10.22it/s]                                               {'loss': 0.2083, 'grad_norm': 0.8935116529464722, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 10.22it/s]100%|██████████| 30/30 [00:02<00:00, 11.08it/s]                                               {'loss': 0.4823, 'grad_norm': 3.1156413555145264, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 11.08it/s]                                               {'train_runtime': 2.8703, 'train_samples_per_second': 148.068, 'train_steps_per_second': 10.452, 'train_loss': 0.28581571703155834, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 11.08it/s]100%|██████████| 30/30 [00:02<00:00, 10.43it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:5
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.33it/s]                                              {'loss': 0.917, 'grad_norm': 4.273609638214111, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.33it/s]  7%|▋         | 2/30 [00:00<00:04,  6.61it/s]                                              {'loss': 0.6777, 'grad_norm': 1.2326345443725586, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.61it/s] 10%|█         | 3/30 [00:00<00:03,  6.76it/s]                                              {'loss': 0.6841, 'grad_norm': 2.1143605709075928, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  6.76it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.98it/s]                                              {'loss': 0.6517, 'grad_norm': 1.503443956375122, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.98it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.26it/s]                                              {'loss': 0.7428, 'grad_norm': 1.2685129642486572, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.26it/s]                                              {'loss': 0.8113, 'grad_norm': 4.017758846282959, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.26it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.08it/s]                                              {'loss': 0.6545, 'grad_norm': 0.5695626139640808, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.08it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.70it/s]                                              {'loss': 0.6782, 'grad_norm': 1.0158997774124146, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.70it/s] 30%|███       | 9/30 [00:01<00:02,  7.36it/s]                                              {'loss': 0.6559, 'grad_norm': 0.7437462210655212, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.36it/s] 33%|███▎      | 10/30 [00:01<00:02,  6.81it/s]                                               {'loss': 0.679, 'grad_norm': 0.3489011526107788, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  6.81it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.69it/s]                                               {'loss': 0.6698, 'grad_norm': 0.46021363139152527, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.69it/s]                                               {'loss': 0.8922, 'grad_norm': 3.2944538593292236, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.69it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.22it/s]                                               {'loss': 0.6694, 'grad_norm': 1.0828028917312622, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.22it/s] 47%|████▋     | 14/30 [00:01<00:02,  6.99it/s]                                               {'loss': 0.7222, 'grad_norm': 0.8434445261955261, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  6.99it/s] 50%|█████     | 15/30 [00:02<00:02,  6.75it/s]                                               {'loss': 0.5329, 'grad_norm': 1.0365170240402222, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.75it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.42it/s]                                               {'loss': 0.6961, 'grad_norm': 0.9228349328041077, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.42it/s] 57%|█████▋    | 17/30 [00:02<00:02,  6.26it/s]                                               {'loss': 0.7377, 'grad_norm': 0.9976726770401001, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  6.26it/s]                                               {'loss': 0.6487, 'grad_norm': 1.0794448852539062, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  6.26it/s] 63%|██████▎   | 19/30 [00:02<00:01,  6.77it/s]                                               {'loss': 0.7796, 'grad_norm': 1.4459524154663086, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  6.77it/s] 67%|██████▋   | 20/30 [00:02<00:01,  6.79it/s]                                               {'loss': 0.6483, 'grad_norm': 0.3910471498966217, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  6.79it/s] 70%|███████   | 21/30 [00:03<00:01,  6.64it/s]                                               {'loss': 0.6796, 'grad_norm': 0.4246794879436493, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  6.64it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.48it/s]                                               {'loss': 0.6699, 'grad_norm': 0.44971081614494324, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.48it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.48it/s]                                               {'loss': 0.6231, 'grad_norm': 0.5614054203033447, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.48it/s]                                               {'loss': 0.6436, 'grad_norm': 0.6953988075256348, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.48it/s] 83%|████████▎ | 25/30 [00:03<00:00,  7.01it/s]                                               {'loss': 0.6753, 'grad_norm': 0.5481220483779907, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  7.01it/s] 87%|████████▋ | 26/30 [00:03<00:00,  6.54it/s]                                               {'loss': 0.6348, 'grad_norm': 0.4622378945350647, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  6.54it/s] 90%|█████████ | 27/30 [00:03<00:00,  6.26it/s]                                               {'loss': 0.6725, 'grad_norm': 0.41254138946533203, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  6.26it/s] 93%|█████████▎| 28/30 [00:04<00:00,  5.94it/s]                                               {'loss': 0.6761, 'grad_norm': 0.43898192048072815, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  5.94it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.72it/s]                                               {'loss': 0.6, 'grad_norm': 0.7173361778259277, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.72it/s]100%|██████████| 30/30 [00:04<00:00,  6.03it/s]                                               {'loss': 0.5943, 'grad_norm': 0.7880855798721313, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.03it/s]                                               {'train_runtime': 4.7522, 'train_samples_per_second': 89.432, 'train_steps_per_second': 6.313, 'train_loss': 0.6872768660386404, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.03it/s]100%|██████████| 30/30 [00:04<00:00,  6.32it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:52
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.72it/s]                                              {'loss': 0.7996, 'grad_norm': 3.5276150703430176, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.72it/s]  7%|▋         | 2/30 [00:00<00:04,  6.94it/s]                                              {'loss': 0.6672, 'grad_norm': 1.3010555505752563, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.94it/s] 10%|█         | 3/30 [00:00<00:03,  7.02it/s]                                              {'loss': 0.6625, 'grad_norm': 0.6232459545135498, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.02it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.94it/s]                                              {'loss': 0.6518, 'grad_norm': 1.8680416345596313, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.94it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.96it/s]                                              {'loss': 0.7277, 'grad_norm': 1.4302427768707275, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.96it/s]                                              {'loss': 0.7586, 'grad_norm': 3.4201838970184326, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.96it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.28it/s]                                              {'loss': 0.6821, 'grad_norm': 0.6755086183547974, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  9.28it/s]                                              {'loss': 0.666, 'grad_norm': 1.226976990699768, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  9.28it/s] 30%|███       | 9/30 [00:01<00:01, 10.55it/s]                                              {'loss': 0.6449, 'grad_norm': 0.8116396069526672, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:01, 10.55it/s]                                              {'loss': 0.6975, 'grad_norm': 0.8172234892845154, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:01, 10.55it/s] 37%|███▋      | 11/30 [00:01<00:01, 11.35it/s]                                               {'loss': 0.6753, 'grad_norm': 0.5495786070823669, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 11.35it/s]                                               {'loss': 0.7869, 'grad_norm': 3.5285727977752686, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 11.35it/s] 43%|████▎     | 13/30 [00:01<00:01, 13.33it/s]                                               {'loss': 0.6574, 'grad_norm': 0.9857786297798157, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 13.33it/s]                                               {'loss': 0.6664, 'grad_norm': 0.6968281269073486, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.33it/s] 50%|█████     | 15/30 [00:01<00:01, 12.80it/s]                                               {'loss': 0.5503, 'grad_norm': 1.4367281198501587, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 12.80it/s]                                               {'loss': 0.6375, 'grad_norm': 1.1677078008651733, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 12.80it/s] 57%|█████▋    | 17/30 [00:01<00:01, 12.44it/s]                                               {'loss': 0.6677, 'grad_norm': 0.8567036390304565, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.44it/s]                                               {'loss': 0.6395, 'grad_norm': 1.138385534286499, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 12.44it/s] 63%|██████▎   | 19/30 [00:01<00:00, 13.86it/s]                                               {'loss': 0.6895, 'grad_norm': 1.8100043535232544, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.86it/s]                                               {'loss': 0.6403, 'grad_norm': 0.7294973134994507, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.86it/s] 70%|███████   | 21/30 [00:01<00:00, 13.30it/s]                                               {'loss': 0.6658, 'grad_norm': 0.5771335363388062, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.30it/s]                                               {'loss': 0.6126, 'grad_norm': 0.6931584477424622, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.30it/s] 77%|███████▋  | 23/30 [00:02<00:00, 13.30it/s]                                               {'loss': 0.6026, 'grad_norm': 1.0727554559707642, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 13.30it/s]                                               {'loss': 0.5685, 'grad_norm': 1.1259759664535522, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 13.30it/s] 83%|████████▎ | 25/30 [00:02<00:00, 14.49it/s]                                               {'loss': 0.654, 'grad_norm': 1.0890015363693237, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 14.49it/s]                                               {'loss': 0.5941, 'grad_norm': 0.881619930267334, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 14.49it/s] 90%|█████████ | 27/30 [00:02<00:00, 13.94it/s]                                               {'loss': 0.6232, 'grad_norm': 0.9495009183883667, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.94it/s]                                               {'loss': 0.5839, 'grad_norm': 0.8428390026092529, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.94it/s] 97%|█████████▋| 29/30 [00:02<00:00, 13.64it/s]                                               {'loss': 0.5349, 'grad_norm': 1.2997349500656128, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.64it/s]                                               {'loss': 0.5938, 'grad_norm': 1.9846088886260986, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.64it/s]                                               {'train_runtime': 2.586, 'train_samples_per_second': 164.346, 'train_steps_per_second': 11.601, 'train_loss': 0.6534018576145172, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.64it/s]100%|██████████| 30/30 [00:02<00:00, 11.61it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:41
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.94it/s]                                              {'loss': 0.6067, 'grad_norm': 1.1457551717758179, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.94it/s]  7%|▋         | 2/30 [00:00<00:04,  6.97it/s]                                              {'loss': 0.5248, 'grad_norm': 1.2263559103012085, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.97it/s] 10%|█         | 3/30 [00:00<00:03,  6.93it/s]                                              {'loss': 0.4464, 'grad_norm': 0.8539949655532837, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  6.93it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.92it/s]                                              {'loss': 0.7325, 'grad_norm': 2.7170989513397217, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.92it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.96it/s]                                              {'loss': 0.6764, 'grad_norm': 2.847273111343384, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.96it/s]                                              {'loss': 0.877, 'grad_norm': 9.409085273742676, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.96it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.54it/s]                                              {'loss': 0.5821, 'grad_norm': 1.2073675394058228, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.54it/s] 27%|██▋       | 8/30 [00:01<00:02,  8.03it/s]                                              {'loss': 0.8666, 'grad_norm': 2.202211856842041, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  8.03it/s] 30%|███       | 9/30 [00:01<00:02,  7.63it/s]                                              {'loss': 0.5595, 'grad_norm': 0.5716524124145508, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.63it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.37it/s]                                               {'loss': 0.494, 'grad_norm': 0.9984210729598999, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.37it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.18it/s]                                               {'loss': 0.5446, 'grad_norm': 0.7225468754768372, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.18it/s]                                               {'loss': 0.4095, 'grad_norm': 0.7182890772819519, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.18it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.38it/s]                                               {'loss': 0.4496, 'grad_norm': 1.7590744495391846, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.38it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.03it/s]                                               {'loss': 0.6075, 'grad_norm': 0.7902395129203796, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.03it/s] 50%|█████     | 15/30 [00:01<00:01,  7.64it/s]                                               {'loss': 1.0848, 'grad_norm': 3.6482126712799072, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  7.64it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.30it/s]                                               {'loss': 0.5051, 'grad_norm': 0.6175792813301086, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.30it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.11it/s]                                               {'loss': 0.5238, 'grad_norm': 1.0388206243515015, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.11it/s]                                               {'loss': 0.4819, 'grad_norm': 1.2462886571884155, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.11it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.24it/s]                                               {'loss': 0.5177, 'grad_norm': 1.6731520891189575, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.24it/s] 67%|██████▋   | 20/30 [00:02<00:01,  7.95it/s]                                               {'loss': 0.6474, 'grad_norm': 1.747337818145752, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  7.95it/s] 70%|███████   | 21/30 [00:02<00:01,  7.65it/s]                                               {'loss': 0.648, 'grad_norm': 2.745823383331299, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  7.65it/s] 73%|███████▎  | 22/30 [00:02<00:01,  7.83it/s]                                               {'loss': 0.6349, 'grad_norm': 2.22253155708313, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  7.83it/s]                                               {'loss': 0.6772, 'grad_norm': 7.154611110687256, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  7.83it/s] 80%|████████  | 24/30 [00:02<00:00, 10.50it/s]                                               {'loss': 0.3779, 'grad_norm': 3.1679112911224365, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 10.50it/s]                                               {'loss': 0.3492, 'grad_norm': 2.1407010555267334, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 10.50it/s] 87%|████████▋ | 26/30 [00:03<00:00, 11.19it/s]                                               {'loss': 0.5, 'grad_norm': 1.6617505550384521, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 11.19it/s]                                               {'loss': 0.6484, 'grad_norm': 2.7149205207824707, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 11.19it/s] 93%|█████████▎| 28/30 [00:03<00:00, 11.81it/s]                                               {'loss': 0.8524, 'grad_norm': 3.5502188205718994, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 11.81it/s]                                               {'loss': 0.648, 'grad_norm': 3.811859607696533, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 11.81it/s]100%|██████████| 30/30 [00:03<00:00, 13.32it/s]                                               {'loss': 0.7798, 'grad_norm': 10.580509185791016, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.32it/s]                                               {'train_runtime': 3.4984, 'train_samples_per_second': 121.484, 'train_steps_per_second': 8.575, 'train_loss': 0.6084627439578374, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.32it/s]100%|██████████| 30/30 [00:03<00:00,  8.58it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.38it/s]                                              {'loss': 0.4162, 'grad_norm': 4.299258232116699, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.38it/s]                                              {'loss': 0.1471, 'grad_norm': 1.5739169120788574, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.38it/s] 10%|█         | 3/30 [00:00<00:02, 10.61it/s]                                              {'loss': 0.0305, 'grad_norm': 1.2888307571411133, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.61it/s]                                              {'loss': 0.3003, 'grad_norm': 1.5379360914230347, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.61it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.08it/s]                                              {'loss': 0.296, 'grad_norm': 1.7231841087341309, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.08it/s]                                              {'loss': 0.0212, 'grad_norm': 0.39114910364151, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.08it/s] 23%|██▎       | 7/30 [00:00<00:01, 12.90it/s]                                              {'loss': 0.3212, 'grad_norm': 20.838180541992188, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 12.90it/s]                                              {'loss': 0.0308, 'grad_norm': 0.4489138424396515, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 12.90it/s] 30%|███       | 9/30 [00:00<00:01, 10.76it/s]                                              {'loss': 0.2946, 'grad_norm': 4.824594974517822, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 10.76it/s]                                              {'loss': 0.0202, 'grad_norm': 0.3357027471065521, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 10.76it/s] 37%|███▋      | 11/30 [00:01<00:02,  8.80it/s]                                               {'loss': 0.0092, 'grad_norm': 0.41948941349983215, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.80it/s]                                               {'loss': 0.0021, 'grad_norm': 0.1536383032798767, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.80it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.40it/s]                                               {'loss': 0.0015, 'grad_norm': 0.0586189366877079, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  9.40it/s]                                               {'loss': 0.0007, 'grad_norm': 0.013179962523281574, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  9.40it/s] 50%|█████     | 15/30 [00:01<00:01,  8.36it/s]                                               {'loss': 0.4858, 'grad_norm': 1.2536457777023315, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  8.36it/s] 53%|█████▎    | 16/30 [00:01<00:01,  8.31it/s]                                               {'loss': 0.0007, 'grad_norm': 0.010955920442938805, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  8.31it/s]                                               {'loss': 0.3768, 'grad_norm': 1.4121906757354736, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01,  8.31it/s]                                               {'loss': 0.0012, 'grad_norm': 0.02477041445672512, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01,  8.31it/s] 63%|██████▎   | 19/30 [00:01<00:01, 10.83it/s]                                               {'loss': 0.0012, 'grad_norm': 0.02283661998808384, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:01, 10.83it/s]                                               {'loss': 0.4107, 'grad_norm': 1.6010874509811401, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 10.83it/s] 70%|███████   | 21/30 [00:02<00:00, 11.40it/s]                                               {'loss': 0.0027, 'grad_norm': 0.05412007123231888, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.40it/s]                                               {'loss': 0.0041, 'grad_norm': 0.0823761597275734, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.40it/s] 77%|███████▋  | 23/30 [00:02<00:00, 10.26it/s]                                               {'loss': 0.347, 'grad_norm': 1.4882965087890625, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 10.26it/s]                                               {'loss': 0.0047, 'grad_norm': 0.09399818629026413, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 10.26it/s] 83%|████████▎ | 25/30 [00:02<00:00, 10.32it/s]                                               {'loss': 0.7104, 'grad_norm': 3.815272569656372, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 10.32it/s]                                               {'loss': 0.0135, 'grad_norm': 0.26475873589515686, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 10.32it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.06it/s]                                               {'loss': 0.0145, 'grad_norm': 0.2894047796726227, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00,  9.06it/s] 93%|█████████▎| 28/30 [00:02<00:00,  8.59it/s]                                               {'loss': 0.0152, 'grad_norm': 0.3046991229057312, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00,  8.59it/s] 97%|█████████▋| 29/30 [00:03<00:00,  8.25it/s]                                               {'loss': 0.0143, 'grad_norm': 0.26138386130332947, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  8.25it/s]                                               {'loss': 0.0187, 'grad_norm': 0.3474922776222229, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.25it/s]                                               {'train_runtime': 3.1855, 'train_samples_per_second': 133.417, 'train_steps_per_second': 9.418, 'train_loss': 0.1437597076408565, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.25it/s]100%|██████████| 30/30 [00:03<00:00,  9.42it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7787, 'grad_norm': 3.295365333557129, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 12.03it/s]  7%|▋         | 2/30 [00:00<00:02, 12.68it/s]                                              {'loss': 0.6932, 'grad_norm': 1.6120456457138062, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 12.68it/s]                                              {'loss': 0.6635, 'grad_norm': 1.0425840616226196, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 12.68it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.52it/s]                                              {'loss': 0.6723, 'grad_norm': 1.2736300230026245, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.52it/s]                                              {'loss': 0.7446, 'grad_norm': 1.1707974672317505, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:01, 12.52it/s] 20%|██        | 6/30 [00:00<00:02, 11.81it/s]                                              {'loss': 1.1275, 'grad_norm': 4.615381717681885, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.81it/s]                                              {'loss': 0.6156, 'grad_norm': 0.7989616990089417, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 11.81it/s] 27%|██▋       | 8/30 [00:00<00:01, 11.07it/s]                                              {'loss': 0.7869, 'grad_norm': 1.674837350845337, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 11.07it/s]                                              {'loss': 0.6883, 'grad_norm': 1.0195178985595703, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 11.07it/s] 33%|███▎      | 10/30 [00:00<00:01, 11.55it/s]                                               {'loss': 0.6343, 'grad_norm': 0.7276806235313416, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.55it/s]                                               {'loss': 0.6549, 'grad_norm': 0.5177300572395325, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 11.55it/s] 40%|████      | 12/30 [00:00<00:01, 13.08it/s]                                               {'loss': 0.5797, 'grad_norm': 2.1515355110168457, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 13.08it/s]                                               {'loss': 0.6308, 'grad_norm': 0.828461229801178, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 13.08it/s] 47%|████▋     | 14/30 [00:01<00:01, 12.73it/s]                                               {'loss': 0.6269, 'grad_norm': 0.8670888543128967, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 12.73it/s]                                               {'loss': 0.7249, 'grad_norm': 1.8570613861083984, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 12.73it/s] 53%|█████▎    | 16/30 [00:01<00:01, 12.65it/s]                                               {'loss': 0.6219, 'grad_norm': 0.7073853611946106, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 12.65it/s]                                               {'loss': 0.6041, 'grad_norm': 0.8230574727058411, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.65it/s] 60%|██████    | 18/30 [00:01<00:00, 13.88it/s]                                               {'loss': 0.6949, 'grad_norm': 1.2956644296646118, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.88it/s]                                               {'loss': 0.6269, 'grad_norm': 1.3357588052749634, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.88it/s] 67%|██████▋   | 20/30 [00:01<00:00, 12.54it/s]                                               {'loss': 0.6574, 'grad_norm': 0.6281595826148987, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 12.54it/s]                                               {'loss': 0.6096, 'grad_norm': 0.9088265895843506, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 12.54it/s] 73%|███████▎  | 22/30 [00:01<00:00, 12.13it/s]                                               {'loss': 0.6274, 'grad_norm': 1.166266918182373, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 12.13it/s]                                               {'loss': 0.6544, 'grad_norm': 1.720332384109497, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 12.13it/s]                                               {'loss': 0.5805, 'grad_norm': 1.4104979038238525, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 12.13it/s] 83%|████████▎ | 25/30 [00:01<00:00, 13.60it/s]                                               {'loss': 0.568, 'grad_norm': 0.9768986105918884, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 13.60it/s]                                               {'loss': 0.572, 'grad_norm': 0.9086267352104187, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.60it/s] 90%|█████████ | 27/30 [00:02<00:00, 13.41it/s]                                               {'loss': 0.5448, 'grad_norm': 1.7149028778076172, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.41it/s]                                               {'loss': 0.5822, 'grad_norm': 0.9769107699394226, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.41it/s] 97%|█████████▋| 29/30 [00:02<00:00, 13.20it/s]                                               {'loss': 0.5393, 'grad_norm': 1.5716665983200073, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.20it/s]                                               {'loss': 0.5559, 'grad_norm': 2.689847469329834, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.20it/s]                                               {'train_runtime': 2.3815, 'train_samples_per_second': 178.455, 'train_steps_per_second': 12.597, 'train_loss': 0.6553808232148488, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.20it/s]100%|██████████| 30/30 [00:02<00:00, 12.60it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.4231, 'grad_norm': 4.2230448722839355, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 10.05it/s]  7%|▋         | 2/30 [00:00<00:02, 11.23it/s]                                              {'loss': 0.2751, 'grad_norm': 1.13868248462677, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 11.23it/s]                                              {'loss': 0.4066, 'grad_norm': 1.395704984664917, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.23it/s] 13%|█▎        | 4/30 [00:00<00:02, 11.94it/s]                                              {'loss': 0.2597, 'grad_norm': 1.2181612253189087, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.94it/s]                                              {'loss': 0.637, 'grad_norm': 2.2659335136413574, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.94it/s] 20%|██        | 6/30 [00:00<00:01, 14.07it/s]                                              {'loss': 0.0778, 'grad_norm': 1.0217366218566895, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 14.07it/s]                                              {'loss': 0.1582, 'grad_norm': 1.180284023284912, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.07it/s] 27%|██▋       | 8/30 [00:00<00:01, 13.21it/s]                                              {'loss': 0.4666, 'grad_norm': 2.133316993713379, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.21it/s]                                              {'loss': 0.5588, 'grad_norm': 2.080181121826172, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.21it/s] 33%|███▎      | 10/30 [00:00<00:01, 12.79it/s]                                               {'loss': 0.5745, 'grad_norm': 2.1928164958953857, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 12.79it/s]                                               {'loss': 0.2359, 'grad_norm': 0.8366436958312988, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.79it/s] 40%|████      | 12/30 [00:00<00:01, 14.38it/s]                                               {'loss': 0.5123, 'grad_norm': 4.235128879547119, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 14.38it/s]                                               {'loss': 0.3558, 'grad_norm': 1.4416011571884155, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 14.38it/s] 47%|████▋     | 14/30 [00:01<00:01, 13.94it/s]                                               {'loss': 0.4323, 'grad_norm': 4.611496925354004, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.94it/s]                                               {'loss': 0.4743, 'grad_norm': 2.954918622970581, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.94it/s] 53%|█████▎    | 16/30 [00:01<00:01, 13.49it/s]                                               {'loss': 0.4385, 'grad_norm': 3.9087677001953125, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.49it/s]                                               {'loss': 0.1398, 'grad_norm': 2.1593241691589355, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.49it/s] 60%|██████    | 18/30 [00:01<00:00, 14.96it/s]                                               {'loss': 0.5955, 'grad_norm': 5.828754901885986, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 14.96it/s]                                               {'loss': 0.4437, 'grad_norm': 4.506309986114502, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.96it/s] 67%|██████▋   | 20/30 [00:01<00:00, 14.55it/s]                                               {'loss': 0.529, 'grad_norm': 5.666923999786377, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.55it/s]                                               {'loss': 0.4453, 'grad_norm': 4.227579593658447, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.55it/s] 73%|███████▎  | 22/30 [00:01<00:00, 14.09it/s]                                               {'loss': 0.4383, 'grad_norm': 2.868673324584961, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 14.09it/s]                                               {'loss': 0.2087, 'grad_norm': 3.2157933712005615, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 14.09it/s] 80%|████████  | 24/30 [00:01<00:00, 15.44it/s]                                               {'loss': 0.4788, 'grad_norm': 11.789007186889648, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 15.44it/s]                                               {'loss': 0.3643, 'grad_norm': 1.9125850200653076, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 15.44it/s] 87%|████████▋ | 26/30 [00:01<00:00, 14.60it/s]                                               {'loss': 0.477, 'grad_norm': 4.771981239318848, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 14.60it/s]                                               {'loss': 0.4553, 'grad_norm': 2.1093969345092773, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 14.60it/s] 93%|█████████▎| 28/30 [00:02<00:00, 14.07it/s]                                               {'loss': 0.1515, 'grad_norm': 2.405945301055908, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 14.07it/s]                                               {'loss': 0.2907, 'grad_norm': 1.9178270101547241, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 14.07it/s]100%|██████████| 30/30 [00:02<00:00, 15.16it/s]                                               {'loss': 0.5107, 'grad_norm': 8.801920890808105, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 15.16it/s]                                               {'train_runtime': 2.1859, 'train_samples_per_second': 194.431, 'train_steps_per_second': 13.725, 'train_loss': 0.39383159453670186, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 15.16it/s]100%|██████████| 30/30 [00:02<00:00, 13.73it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.6547, 'grad_norm': 2.4003021717071533, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 12.10it/s]  7%|▋         | 2/30 [00:00<00:02, 12.20it/s]                                              {'loss': 0.6601, 'grad_norm': 2.465622663497925, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 12.20it/s]                                              {'loss': 0.6506, 'grad_norm': 1.1942185163497925, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 12.20it/s] 13%|█▎        | 4/30 [00:00<00:02, 12.40it/s]                                              {'loss': 0.6156, 'grad_norm': 1.0364879369735718, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 12.40it/s]                                              {'loss': 0.7081, 'grad_norm': 4.713631629943848, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.40it/s] 20%|██        | 6/30 [00:00<00:01, 14.35it/s]                                              {'loss': 0.8696, 'grad_norm': 2.788496971130371, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 14.35it/s]                                              {'loss': 0.6876, 'grad_norm': 1.4029120206832886, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.35it/s] 27%|██▋       | 8/30 [00:00<00:02, 10.28it/s]                                              {'loss': 0.6743, 'grad_norm': 1.212317705154419, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02, 10.28it/s]                                              {'loss': 0.6869, 'grad_norm': 1.6091121435165405, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:02, 10.28it/s] 33%|███▎      | 10/30 [00:01<00:02,  8.64it/s]                                               {'loss': 0.6329, 'grad_norm': 1.6902307271957397, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.64it/s] 37%|███▋      | 11/30 [00:01<00:02,  8.22it/s]                                               {'loss': 0.6232, 'grad_norm': 1.5438662767410278, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.22it/s]                                               {'loss': 0.5776, 'grad_norm': 2.388014078140259, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.22it/s] 43%|████▎     | 13/30 [00:01<00:01,  8.93it/s]                                               {'loss': 0.6341, 'grad_norm': 2.3294808864593506, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  8.93it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.69it/s]                                               {'loss': 0.5858, 'grad_norm': 2.4544260501861572, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.69it/s]                                               {'loss': 0.9882, 'grad_norm': 8.53405475616455, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  8.69it/s] 53%|█████▎    | 16/30 [00:01<00:01, 10.01it/s]                                               {'loss': 0.5713, 'grad_norm': 4.847829818725586, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 10.01it/s]                                               {'loss': 0.5469, 'grad_norm': 2.6709887981414795, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 10.01it/s] 60%|██████    | 18/30 [00:01<00:00, 12.08it/s]                                               {'loss': 0.6415, 'grad_norm': 5.60573673248291, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 12.08it/s]                                               {'loss': 0.5815, 'grad_norm': 3.5240941047668457, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 12.08it/s] 67%|██████▋   | 20/30 [00:01<00:00, 12.39it/s]                                               {'loss': 0.6627, 'grad_norm': 4.401692867279053, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 12.39it/s]                                               {'loss': 0.6234, 'grad_norm': 4.677763938903809, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 12.39it/s] 73%|███████▎  | 22/30 [00:02<00:00, 12.54it/s]                                               {'loss': 0.661, 'grad_norm': 3.3474090099334717, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 12.54it/s]                                               {'loss': 0.692, 'grad_norm': 3.6811749935150146, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 12.54it/s] 80%|████████  | 24/30 [00:02<00:00, 12.09it/s]                                               {'loss': 0.656, 'grad_norm': 5.034669876098633, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 12.09it/s]                                               {'loss': 0.6949, 'grad_norm': 3.1219897270202637, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.09it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.66it/s]                                               {'loss': 0.569, 'grad_norm': 3.926924228668213, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00,  9.66it/s]                                               {'loss': 0.5481, 'grad_norm': 3.072737455368042, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00,  9.66it/s] 93%|█████████▎| 28/30 [00:02<00:00,  8.55it/s]                                               {'loss': 0.6463, 'grad_norm': 3.4570674896240234, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00,  8.55it/s] 97%|█████████▋| 29/30 [00:02<00:00,  8.37it/s]                                               {'loss': 0.5862, 'grad_norm': 3.227393865585327, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00,  8.37it/s]100%|██████████| 30/30 [00:03<00:00,  8.57it/s]                                               {'loss': 0.5894, 'grad_norm': 6.664636611938477, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.57it/s]                                               {'train_runtime': 3.1566, 'train_samples_per_second': 134.639, 'train_steps_per_second': 9.504, 'train_loss': 0.6506514350573221, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.57it/s]100%|██████████| 30/30 [00:03<00:00,  9.51it/s]
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
rowB and colA are  76 76
the shape and nnz of result are  torch.Size([589824]) tensor(5776)
the number of uploaded param is  218
the shape of loraA and loraB are  torch.Size([16, 768]) torch.Size([768, 16])
the shape of base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.weight is (768, 768)
the shape of base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.weight is (768, 768)
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  8%|▊         | 5/66 [00:00<00:01, 38.81it/s] 14%|█▎        | 9/66 [00:00<00:01, 32.64it/s] 20%|█▉        | 13/66 [00:00<00:01, 31.88it/s] 26%|██▌       | 17/66 [00:00<00:01, 30.99it/s] 32%|███▏      | 21/66 [00:00<00:01, 30.61it/s] 38%|███▊      | 25/66 [00:00<00:01, 30.39it/s] 44%|████▍     | 29/66 [00:00<00:01, 29.87it/s] 50%|█████     | 33/66 [00:01<00:01, 30.01it/s] 56%|█████▌    | 37/66 [00:01<00:00, 30.39it/s] 62%|██████▏   | 41/66 [00:01<00:00, 25.91it/s] 67%|██████▋   | 44/66 [00:01<00:00, 22.90it/s] 71%|███████   | 47/66 [00:01<00:00, 20.39it/s] 76%|███████▌  | 50/66 [00:01<00:00, 18.65it/s] 79%|███████▉  | 52/66 [00:02<00:00, 18.05it/s] 82%|████████▏ | 54/66 [00:02<00:00, 17.39it/s] 85%|████████▍ | 56/66 [00:02<00:00, 17.80it/s] 91%|█████████ | 60/66 [00:02<00:00, 21.77it/s] 97%|█████████▋| 64/66 [00:02<00:00, 24.48it/s]100%|██████████| 66/66 [00:02<00:00, 25.18it/s]
{'eval_loss': 0.6156653165817261, 'eval_model_preparation_time': 0.006, 'eval_acc': 0.6912751677852349, 'eval_runtime': 2.6584, 'eval_samples_per_second': 392.346, 'eval_steps_per_second': 24.827}

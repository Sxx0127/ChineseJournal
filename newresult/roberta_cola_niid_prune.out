nohup: ignoring input
/home/suxiaoxin/.conda/envs/sxx/lib/python3.10/site-packages/datasets/load.py:929: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at ./data/glue/glue.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./model/models--roberta-base/ and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
trainable params: 589,824 || all params: 125,236,994 || trainable%: 0.4710
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): RobertaForSequenceClassification(
      (roberta): RobertaModel(
        (embeddings): RobertaEmbeddings(
          (word_embeddings): Embedding(50265, 768, padding_idx=1)
          (position_embeddings): Embedding(514, 768, padding_idx=1)
          (token_type_embeddings): Embedding(1, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): RobertaEncoder(
          (layer): ModuleList(
            (0-11): 12 x RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSdpaSelfAttention(
                  (query): lora.Linear(
                    (base_layer): Linear(in_features=768, out_features=768, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.05, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=768, out_features=16, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=16, out_features=768, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): lora.Linear(
                    (base_layer): Linear(in_features=768, out_features=768, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.05, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=768, out_features=16, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=16, out_features=768, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                    (lora_magnitude_vector): ModuleDict()
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (classifier): RobertaClassificationHead(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (out_proj): Linear(in_features=768, out_features=2, bias=True)
      )
    )
  )
)
ROUND:0
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:01<00:44,  1.55s/it]                                              {'loss': 0.6388, 'grad_norm': 1.4005168676376343, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:01<00:44,  1.55s/it]  7%|▋         | 2/30 [00:01<00:21,  1.31it/s]                                              {'loss': 1.0447, 'grad_norm': 6.367481231689453, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:21,  1.31it/s] 10%|█         | 3/30 [00:02<00:14,  1.86it/s]                                              {'loss': 1.4145, 'grad_norm': 5.909910202026367, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:02<00:14,  1.86it/s] 13%|█▎        | 4/30 [00:02<00:10,  2.37it/s]                                              {'loss': 1.6263, 'grad_norm': 5.317267894744873, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:02<00:10,  2.37it/s] 17%|█▋        | 5/30 [00:02<00:09,  2.76it/s]                                              {'loss': 0.9472, 'grad_norm': 3.1480088233947754, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:02<00:09,  2.76it/s] 20%|██        | 6/30 [00:02<00:06,  3.46it/s]                                              {'loss': 1.0693, 'grad_norm': 5.5416388511657715, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:06,  3.46it/s] 23%|██▎       | 7/30 [00:02<00:06,  3.65it/s]                                              {'loss': 0.7133, 'grad_norm': 1.850606083869934, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:06,  3.65it/s] 27%|██▋       | 8/30 [00:03<00:05,  3.97it/s]                                              {'loss': 0.8409, 'grad_norm': 5.16675329208374, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:03<00:05,  3.97it/s] 30%|███       | 9/30 [00:03<00:05,  3.90it/s]                                              {'loss': 0.7365, 'grad_norm': 2.135141372680664, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:03<00:05,  3.90it/s] 33%|███▎      | 10/30 [00:03<00:05,  3.80it/s]                                               {'loss': 0.7032, 'grad_norm': 0.6126570701599121, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:05,  3.80it/s] 37%|███▋      | 11/30 [00:03<00:05,  3.74it/s]                                               {'loss': 0.716, 'grad_norm': 0.8783379793167114, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:05,  3.74it/s]                                               {'loss': 0.5712, 'grad_norm': 2.3020858764648438, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:04<00:04,  3.74it/s] 43%|████▎     | 13/30 [00:04<00:03,  4.66it/s]                                               {'loss': 0.8319, 'grad_norm': 1.6218489408493042, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:04<00:03,  4.66it/s] 47%|████▋     | 14/30 [00:04<00:03,  4.38it/s]                                               {'loss': 0.656, 'grad_norm': 1.144134759902954, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:03,  4.38it/s] 50%|█████     | 15/30 [00:04<00:03,  4.15it/s]                                               {'loss': 0.7155, 'grad_norm': 5.533949851989746, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:03,  4.15it/s] 53%|█████▎    | 16/30 [00:05<00:03,  4.03it/s]                                               {'loss': 0.7041, 'grad_norm': 1.0108829736709595, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:05<00:03,  4.03it/s] 57%|█████▋    | 17/30 [00:05<00:03,  3.97it/s]                                               {'loss': 0.6973, 'grad_norm': 1.9846556186676025, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:05<00:03,  3.97it/s]                                               {'loss': 0.7075, 'grad_norm': 1.420720100402832, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:03,  3.97it/s] 63%|██████▎   | 19/30 [00:05<00:02,  4.57it/s]                                               {'loss': 0.6819, 'grad_norm': 1.0436923503875732, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  4.57it/s] 67%|██████▋   | 20/30 [00:05<00:02,  4.31it/s]                                               {'loss': 0.6773, 'grad_norm': 0.6120210886001587, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  4.31it/s] 70%|███████   | 21/30 [00:06<00:02,  4.11it/s]                                               {'loss': 0.7204, 'grad_norm': 0.9211769104003906, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:06<00:02,  4.11it/s] 73%|███████▎  | 22/30 [00:06<00:02,  3.97it/s]                                               {'loss': 0.7136, 'grad_norm': 1.3112200498580933, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:06<00:02,  3.97it/s] 77%|███████▋  | 23/30 [00:06<00:01,  3.86it/s]                                               {'loss': 0.69, 'grad_norm': 1.8725273609161377, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  3.86it/s] 80%|████████  | 24/30 [00:06<00:01,  4.66it/s]                                               {'loss': 0.6584, 'grad_norm': 1.2874832153320312, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  4.66it/s] 83%|████████▎ | 25/30 [00:07<00:01,  4.47it/s]                                               {'loss': 0.6622, 'grad_norm': 1.0944267511367798, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:07<00:01,  4.47it/s] 87%|████████▋ | 26/30 [00:07<00:00,  4.20it/s]                                               {'loss': 0.6514, 'grad_norm': 1.177304983139038, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:07<00:00,  4.20it/s] 90%|█████████ | 27/30 [00:07<00:00,  4.07it/s]                                               {'loss': 0.6678, 'grad_norm': 0.9974465370178223, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:07<00:00,  4.07it/s] 93%|█████████▎| 28/30 [00:07<00:00,  3.94it/s]                                               {'loss': 0.6799, 'grad_norm': 0.6460588574409485, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  3.94it/s] 97%|█████████▋| 29/30 [00:08<00:00,  4.13it/s]                                               {'loss': 0.6138, 'grad_norm': 1.343395709991455, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:08<00:00,  4.13it/s]                                               {'loss': 0.6637, 'grad_norm': 1.2779037952423096, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.13it/s]                                               {'train_runtime': 8.3561, 'train_samples_per_second': 50.861, 'train_steps_per_second': 3.59, 'train_loss': 0.7804885586102803, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.13it/s]100%|██████████| 30/30 [00:08<00:00,  3.59it/s]
CLIENT:64
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.59it/s]                                              {'loss': 0.9727, 'grad_norm': 1.871934175491333, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.59it/s]  7%|▋         | 2/30 [00:00<00:04,  6.22it/s]                                              {'loss': 0.5338, 'grad_norm': 1.2208521366119385, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.22it/s] 10%|█         | 3/30 [00:00<00:04,  6.27it/s]                                              {'loss': 0.3484, 'grad_norm': 1.727776050567627, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.27it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.42it/s]                                              {'loss': 0.1922, 'grad_norm': 1.5756406784057617, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.42it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.69it/s]                                              {'loss': 0.1555, 'grad_norm': 0.5358352661132812, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.69it/s]                                              {'loss': 0.0228, 'grad_norm': 0.4462833106517792, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.69it/s] 23%|██▎       | 7/30 [00:01<00:03,  7.14it/s]                                              {'loss': 0.0153, 'grad_norm': 0.3370838761329651, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  7.14it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.63it/s]                                              {'loss': 0.0035, 'grad_norm': 0.09431260079145432, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.63it/s] 30%|███       | 9/30 [00:01<00:03,  6.14it/s]                                              {'loss': 0.0093, 'grad_norm': 0.4377182722091675, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.14it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.59it/s]                                               {'loss': 0.011, 'grad_norm': 0.4709473252296448, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.59it/s] 37%|███▋      | 11/30 [00:01<00:03,  5.35it/s]                                               {'loss': 0.4549, 'grad_norm': 1.5269911289215088, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  5.35it/s]                                               {'loss': 0.0007, 'grad_norm': 0.027298256754875183, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:03,  5.35it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.51it/s]                                               {'loss': 0.0006, 'grad_norm': 0.011512666940689087, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.51it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.65it/s]                                               {'loss': 0.4722, 'grad_norm': 2.5536413192749023, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.65it/s] 50%|█████     | 15/30 [00:02<00:02,  6.65it/s]                                               {'loss': 0.0009, 'grad_norm': 0.01757085509598255, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.65it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.89it/s]                                               {'loss': 0.0016, 'grad_norm': 0.04468359798192978, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.89it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.13it/s]                                               {'loss': 0.0023, 'grad_norm': 0.054326381534338, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.13it/s]                                               {'loss': 0.0019, 'grad_norm': 0.06037186458706856, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.13it/s] 63%|██████▎   | 19/30 [00:02<00:01,  7.36it/s]                                               {'loss': 0.4224, 'grad_norm': 1.783085823059082, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.36it/s] 67%|██████▋   | 20/30 [00:03<00:01,  6.48it/s]                                               {'loss': 0.0024, 'grad_norm': 0.07571293413639069, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  6.48it/s] 70%|███████   | 21/30 [00:03<00:01,  5.98it/s]                                               {'loss': 0.0043, 'grad_norm': 0.13900451362133026, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.98it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.09it/s]                                               {'loss': 0.007, 'grad_norm': 0.21014520525932312, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.09it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.21it/s]                                               {'loss': 0.0065, 'grad_norm': 0.193483367562294, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.21it/s]                                               {'loss': 0.0023, 'grad_norm': 0.06673794984817505, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.21it/s] 83%|████████▎ | 25/30 [00:03<00:00,  7.51it/s]                                               {'loss': 0.4436, 'grad_norm': 5.388617038726807, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  7.51it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.49it/s]                                               {'loss': 0.0054, 'grad_norm': 0.143761545419693, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.49it/s] 90%|█████████ | 27/30 [00:04<00:00,  7.04it/s]                                               {'loss': 0.0055, 'grad_norm': 0.12900234758853912, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  7.04it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.98it/s]                                               {'loss': 0.0044, 'grad_norm': 0.13673634827136993, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.98it/s] 97%|█████████▋| 29/30 [00:04<00:00,  6.36it/s]                                               {'loss': 0.0036, 'grad_norm': 0.09153585880994797, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  6.36it/s]100%|██████████| 30/30 [00:04<00:00,  6.90it/s]                                               {'loss': 0.0033, 'grad_norm': 0.09267550706863403, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.90it/s]                                               {'train_runtime': 4.6332, 'train_samples_per_second': 91.729, 'train_steps_per_second': 6.475, 'train_loss': 0.1370069779048208, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.90it/s]100%|██████████| 30/30 [00:04<00:00,  6.48it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.39it/s]                                              {'loss': 0.59, 'grad_norm': 0.25399214029312134, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.39it/s]  7%|▋         | 2/30 [00:00<00:06,  4.55it/s]                                              {'loss': 0.6675, 'grad_norm': 0.6621816754341125, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.55it/s] 10%|█         | 3/30 [00:00<00:05,  4.59it/s]                                              {'loss': 0.7623, 'grad_norm': 0.5680248141288757, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.59it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.57it/s]                                              {'loss': 0.651, 'grad_norm': 1.1885638236999512, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.57it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.83it/s]                                              {'loss': 0.6632, 'grad_norm': 0.4213665723800659, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.83it/s]                                              {'loss': 0.7468, 'grad_norm': 1.7116076946258545, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.83it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.55it/s]                                              {'loss': 0.6151, 'grad_norm': 0.5424310564994812, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.55it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.26it/s]                                              {'loss': 0.7354, 'grad_norm': 0.9516140818595886, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.26it/s] 30%|███       | 9/30 [00:01<00:04,  5.00it/s]                                              {'loss': 0.6729, 'grad_norm': 0.4524596035480499, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  5.00it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.88it/s]                                               {'loss': 0.5883, 'grad_norm': 0.9332521557807922, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.88it/s] 37%|███▋      | 11/30 [00:02<00:03,  4.85it/s]                                               {'loss': 0.6355, 'grad_norm': 0.3359020948410034, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  4.85it/s]                                               {'loss': 0.4847, 'grad_norm': 0.9527598023414612, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.85it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.26it/s]                                               {'loss': 0.5497, 'grad_norm': 1.1038286685943604, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.26it/s] 47%|████▋     | 14/30 [00:02<00:03,  4.74it/s]                                               {'loss': 0.5589, 'grad_norm': 0.56981360912323, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  4.74it/s] 50%|█████     | 15/30 [00:03<00:03,  4.38it/s]                                               {'loss': 0.8235, 'grad_norm': 2.836682081222534, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.38it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.17it/s]                                               {'loss': 0.6076, 'grad_norm': 0.8298165202140808, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.17it/s] 57%|█████▋    | 17/30 [00:03<00:03,  4.27it/s]                                               {'loss': 0.4804, 'grad_norm': 1.5088376998901367, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  4.27it/s]                                               {'loss': 0.7358, 'grad_norm': 1.1862049102783203, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.27it/s] 63%|██████▎   | 19/30 [00:03<00:02,  5.26it/s]                                               {'loss': 0.5274, 'grad_norm': 0.9665462374687195, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  5.26it/s] 67%|██████▋   | 20/30 [00:04<00:01,  5.29it/s]                                               {'loss': 0.6681, 'grad_norm': 0.8185516595840454, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  5.29it/s] 70%|███████   | 21/30 [00:04<00:01,  5.38it/s]                                               {'loss': 0.5942, 'grad_norm': 0.5948647260665894, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.38it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.47it/s]                                               {'loss': 0.545, 'grad_norm': 0.8182544708251953, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.47it/s] 77%|███████▋  | 23/30 [00:04<00:01,  5.53it/s]                                               {'loss': 0.8632, 'grad_norm': 2.707021474838257, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  5.53it/s] 80%|████████  | 24/30 [00:04<00:00,  6.23it/s]                                               {'loss': 0.4697, 'grad_norm': 1.7301082611083984, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  6.23it/s] 83%|████████▎ | 25/30 [00:04<00:00,  6.41it/s]                                               {'loss': 0.5562, 'grad_norm': 0.5269181132316589, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  6.41it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.79it/s]                                               {'loss': 0.6396, 'grad_norm': 0.901508629322052, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.79it/s] 90%|█████████ | 27/30 [00:05<00:00,  5.38it/s]                                               {'loss': 0.5227, 'grad_norm': 0.933239221572876, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  5.38it/s] 93%|█████████▎| 28/30 [00:05<00:00,  5.00it/s]                                               {'loss': 0.66, 'grad_norm': 1.0919649600982666, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  5.00it/s] 97%|█████████▋| 29/30 [00:05<00:00,  4.95it/s]                                               {'loss': 0.5645, 'grad_norm': 1.2086607217788696, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  4.95it/s]                                               {'loss': 0.4147, 'grad_norm': 1.5233778953552246, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.95it/s]                                               {'train_runtime': 5.9957, 'train_samples_per_second': 70.885, 'train_steps_per_second': 5.004, 'train_loss': 0.6198077897230784, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.95it/s]100%|██████████| 30/30 [00:05<00:00,  5.01it/s]
CLIENT:80
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.20it/s]                                              {'loss': 0.6277, 'grad_norm': 0.7007676362991333, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.20it/s]  7%|▋         | 2/30 [00:00<00:04,  6.61it/s]                                              {'loss': 0.5536, 'grad_norm': 1.015824556350708, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.61it/s] 10%|█         | 3/30 [00:00<00:03,  7.02it/s]                                              {'loss': 0.3641, 'grad_norm': 1.416274070739746, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.02it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.38it/s]                                              {'loss': 0.2786, 'grad_norm': 0.8449075222015381, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.38it/s] 17%|█▋        | 5/30 [00:00<00:04,  6.09it/s]                                              {'loss': 0.2612, 'grad_norm': 1.018212080001831, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  6.09it/s] 20%|██        | 6/30 [00:00<00:03,  6.02it/s]                                              {'loss': 0.8928, 'grad_norm': 3.7048964500427246, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.02it/s] 23%|██▎       | 7/30 [00:01<00:03,  5.99it/s]                                              {'loss': 0.2274, 'grad_norm': 2.389568328857422, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  5.99it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.19it/s]                                              {'loss': 0.8015, 'grad_norm': 4.00150203704834, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.19it/s] 30%|███       | 9/30 [00:01<00:03,  6.31it/s]                                              {'loss': 0.2178, 'grad_norm': 4.267513275146484, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.31it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.46it/s]                                               {'loss': 0.4352, 'grad_norm': 6.086329936981201, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.46it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.47it/s]                                               {'loss': 0.0828, 'grad_norm': 3.9781007766723633, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.47it/s] 40%|████      | 12/30 [00:01<00:02,  6.53it/s]                                               {'loss': 0.6602, 'grad_norm': 14.941079139709473, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.53it/s] 43%|████▎     | 13/30 [00:02<00:02,  5.99it/s]                                               {'loss': 0.3862, 'grad_norm': 1.2051881551742554, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  5.99it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.68it/s]                                               {'loss': 0.2161, 'grad_norm': 4.052003860473633, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.68it/s] 50%|█████     | 15/30 [00:02<00:02,  5.41it/s]                                               {'loss': 0.9072, 'grad_norm': 7.204794883728027, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.41it/s] 53%|█████▎    | 16/30 [00:02<00:02,  5.29it/s]                                               {'loss': 0.455, 'grad_norm': 6.618232727050781, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  5.29it/s] 57%|█████▋    | 17/30 [00:02<00:02,  5.11it/s]                                               {'loss': 0.1594, 'grad_norm': 2.712836742401123, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  5.11it/s] 60%|██████    | 18/30 [00:03<00:02,  5.29it/s]                                               {'loss': 0.6931, 'grad_norm': 2.087141513824463, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.29it/s] 63%|██████▎   | 19/30 [00:03<00:02,  5.34it/s]                                               {'loss': 0.1078, 'grad_norm': 2.0118799209594727, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  5.34it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.61it/s]                                               {'loss': 0.525, 'grad_norm': 1.7062792778015137, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.61it/s] 70%|███████   | 21/30 [00:03<00:01,  5.53it/s]                                               {'loss': 0.2763, 'grad_norm': 1.7016469240188599, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.53it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.03it/s]                                               {'loss': 0.5817, 'grad_norm': 1.7789480686187744, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.03it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.46it/s]                                               {'loss': 0.2396, 'grad_norm': 1.0779695510864258, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.46it/s]                                               {'loss': 0.4799, 'grad_norm': 1.272999882698059, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.46it/s] 83%|████████▎ | 25/30 [00:04<00:00,  6.93it/s]                                               {'loss': 0.2148, 'grad_norm': 1.6460269689559937, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  6.93it/s] 87%|████████▋ | 26/30 [00:04<00:00,  6.38it/s]                                               {'loss': 0.3584, 'grad_norm': 0.7227937579154968, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  6.38it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.97it/s]                                               {'loss': 0.3704, 'grad_norm': 1.1229984760284424, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.97it/s] 93%|█████████▎| 28/30 [00:04<00:00,  5.27it/s]                                               {'loss': 0.411, 'grad_norm': 2.4662933349609375, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  5.27it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.34it/s]                                               {'loss': 0.2289, 'grad_norm': 0.7166114449501038, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.34it/s]100%|██████████| 30/30 [00:05<00:00,  5.41it/s]                                               {'loss': 0.5063, 'grad_norm': 4.354124546051025, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.41it/s]                                               {'train_runtime': 5.6697, 'train_samples_per_second': 74.959, 'train_steps_per_second': 5.291, 'train_loss': 0.41733457098404564, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.41it/s]100%|██████████| 30/30 [00:05<00:00,  5.29it/s]
CLIENT:6
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:28,  1.02it/s]                                              {'loss': 0.6888, 'grad_norm': 0.8841153979301453, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:28,  1.02it/s]                                              {'loss': 0.7561, 'grad_norm': 0.8362323045730591, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:27,  1.02it/s] 10%|█         | 3/30 [00:01<00:09,  2.96it/s]                                              {'loss': 0.7057, 'grad_norm': 0.4058668613433838, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:09,  2.96it/s]                                              {'loss': 0.6521, 'grad_norm': 0.9173121452331543, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  2.96it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.86it/s]                                              {'loss': 0.7248, 'grad_norm': 0.6597058773040771, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.86it/s]                                              {'loss': 0.7029, 'grad_norm': 0.5055615901947021, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.86it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.71it/s]                                              {'loss': 0.6853, 'grad_norm': 0.3144643306732178, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.71it/s]                                              {'loss': 0.7102, 'grad_norm': 0.41733574867248535, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.71it/s] 30%|███       | 9/30 [00:01<00:02,  7.17it/s]                                              {'loss': 0.7267, 'grad_norm': 0.3539634644985199, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.17it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.61it/s]                                               {'loss': 0.6781, 'grad_norm': 0.195914626121521, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.61it/s] 37%|███▋      | 11/30 [00:02<00:02,  6.52it/s]                                               {'loss': 0.6694, 'grad_norm': 0.5187716484069824, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:02,  6.52it/s] 40%|████      | 12/30 [00:02<00:02,  6.50it/s]                                               {'loss': 0.5155, 'grad_norm': 0.9829460978507996, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.50it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.43it/s]                                               {'loss': 0.6774, 'grad_norm': 0.5203406810760498, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.43it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.02it/s]                                               {'loss': 0.6286, 'grad_norm': 0.3958926796913147, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.02it/s] 50%|█████     | 15/30 [00:02<00:02,  5.78it/s]                                               {'loss': 0.7465, 'grad_norm': 0.9490768313407898, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.78it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.36it/s]                                               {'loss': 0.668, 'grad_norm': 0.4988964796066284, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.36it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.41it/s]                                               {'loss': 0.6746, 'grad_norm': 0.46701744198799133, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.41it/s] 60%|██████    | 18/30 [00:03<00:02,  5.76it/s]                                               {'loss': 0.6553, 'grad_norm': 0.49170705676078796, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.76it/s] 63%|██████▎   | 19/30 [00:03<00:01,  6.13it/s]                                               {'loss': 0.7057, 'grad_norm': 0.6182738542556763, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  6.13it/s] 67%|██████▋   | 20/30 [00:03<00:01,  6.19it/s]                                               {'loss': 0.6934, 'grad_norm': 0.5554694533348083, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  6.19it/s] 70%|███████   | 21/30 [00:03<00:01,  6.80it/s]                                               {'loss': 0.676, 'grad_norm': 0.39144715666770935, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  6.80it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.18it/s]                                               {'loss': 0.6942, 'grad_norm': 0.5146713256835938, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.18it/s] 77%|███████▋  | 23/30 [00:04<00:00,  7.32it/s]                                               {'loss': 0.6587, 'grad_norm': 0.4391045868396759, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:00,  7.32it/s] 80%|████████  | 24/30 [00:04<00:00,  7.13it/s]                                               {'loss': 0.6495, 'grad_norm': 0.46995019912719727, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  7.13it/s] 83%|████████▎ | 25/30 [00:04<00:00,  6.70it/s]                                               {'loss': 0.7057, 'grad_norm': 0.38282984495162964, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  6.70it/s] 87%|████████▋ | 26/30 [00:04<00:00,  6.51it/s]                                               {'loss': 0.6687, 'grad_norm': 0.36368563771247864, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  6.51it/s]                                               {'loss': 0.6472, 'grad_norm': 0.478702574968338, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  6.51it/s] 93%|█████████▎| 28/30 [00:04<00:00,  7.07it/s]                                               {'loss': 0.6454, 'grad_norm': 0.20705491304397583, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  7.07it/s] 97%|█████████▋| 29/30 [00:04<00:00,  7.16it/s]                                               {'loss': 0.7093, 'grad_norm': 0.44281482696533203, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  7.16it/s]                                               {'loss': 0.6511, 'grad_norm': 0.4042463004589081, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.16it/s]                                               {'train_runtime': 5.2888, 'train_samples_per_second': 80.359, 'train_steps_per_second': 5.672, 'train_loss': 0.679032051563263, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  7.16it/s]100%|██████████| 30/30 [00:05<00:00,  5.70it/s]
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.06it/s]                                              {'loss': 0.5429, 'grad_norm': 0.3323134481906891, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.06it/s]  7%|▋         | 2/30 [00:00<00:05,  4.87it/s]                                              {'loss': 0.7396, 'grad_norm': 0.6940767168998718, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  4.87it/s] 10%|█         | 3/30 [00:00<00:05,  4.78it/s]                                              {'loss': 0.7259, 'grad_norm': 0.5771657824516296, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.78it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.32it/s]                                              {'loss': 0.6381, 'grad_norm': 0.5914069414138794, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.32it/s] 17%|█▋        | 5/30 [00:00<00:04,  6.01it/s]                                              {'loss': 0.6791, 'grad_norm': 0.46004533767700195, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  6.01it/s]                                              {'loss': 0.7871, 'grad_norm': 1.5754685401916504, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.01it/s] 23%|██▎       | 7/30 [00:01<00:03,  7.39it/s]                                              {'loss': 0.6424, 'grad_norm': 0.5827187895774841, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  7.39it/s] 27%|██▋       | 8/30 [00:01<00:03,  7.11it/s]                                              {'loss': 0.6931, 'grad_norm': 0.4361380338668823, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  7.11it/s] 30%|███       | 9/30 [00:01<00:02,  7.10it/s]                                              {'loss': 0.6313, 'grad_norm': 0.38450488448143005, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.10it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.03it/s]                                               {'loss': 0.6692, 'grad_norm': 0.22819282114505768, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.03it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.23it/s]                                               {'loss': 0.6681, 'grad_norm': 0.20492105185985565, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.23it/s]                                               {'loss': 0.6355, 'grad_norm': 1.2692853212356567, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.23it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.23it/s]                                               {'loss': 0.6644, 'grad_norm': 0.8052757382392883, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.23it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.45it/s]                                               {'loss': 0.6718, 'grad_norm': 0.42716261744499207, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.45it/s] 50%|█████     | 15/30 [00:02<00:02,  7.10it/s]                                               {'loss': 0.6654, 'grad_norm': 0.8588471412658691, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.10it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.08it/s]                                               {'loss': 0.6002, 'grad_norm': 0.5135666131973267, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.08it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.36it/s]                                               {'loss': 0.6503, 'grad_norm': 0.38684868812561035, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.36it/s]                                               {'loss': 0.637, 'grad_norm': 0.5888269543647766, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.36it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.47it/s]                                               {'loss': 0.6334, 'grad_norm': 0.8219261765480042, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.47it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.23it/s]                                               {'loss': 0.6345, 'grad_norm': 0.3489810824394226, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.23it/s] 70%|███████   | 21/30 [00:02<00:01,  8.08it/s]                                               {'loss': 0.6299, 'grad_norm': 0.4063757061958313, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.08it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.78it/s]                                               {'loss': 0.6179, 'grad_norm': 0.5095772743225098, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.78it/s] 77%|███████▋  | 23/30 [00:03<00:00,  7.89it/s]                                               {'loss': 0.581, 'grad_norm': 0.480997771024704, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.89it/s]                                               {'loss': 0.5314, 'grad_norm': 0.7003899812698364, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.89it/s] 83%|████████▎ | 25/30 [00:03<00:00,  9.01it/s]                                               {'loss': 0.7148, 'grad_norm': 0.7760977149009705, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  9.01it/s] 87%|████████▋ | 26/30 [00:03<00:00,  8.66it/s]                                               {'loss': 0.5833, 'grad_norm': 0.6849678158760071, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  8.66it/s] 90%|█████████ | 27/30 [00:03<00:00,  8.34it/s]                                               {'loss': 0.6109, 'grad_norm': 0.5320348739624023, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  8.34it/s] 93%|█████████▎| 28/30 [00:03<00:00,  8.17it/s]                                               {'loss': 0.5921, 'grad_norm': 0.45830807089805603, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  8.17it/s] 97%|█████████▋| 29/30 [00:03<00:00,  8.01it/s]                                               {'loss': 0.5245, 'grad_norm': 0.8357928991317749, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  8.01it/s]                                               {'loss': 0.5786, 'grad_norm': 0.7819607257843018, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.01it/s]                                               {'train_runtime': 4.0899, 'train_samples_per_second': 103.915, 'train_steps_per_second': 7.335, 'train_loss': 0.6391143759091695, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  8.01it/s]100%|██████████| 30/30 [00:04<00:00,  7.34it/s]
CLIENT:43
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.44it/s]                                              {'loss': 0.5379, 'grad_norm': 0.4597685933113098, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.44it/s]  7%|▋         | 2/30 [00:00<00:06,  4.38it/s]                                              {'loss': 0.7718, 'grad_norm': 1.219869613647461, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.38it/s] 10%|█         | 3/30 [00:00<00:06,  4.25it/s]                                              {'loss': 0.7299, 'grad_norm': 0.5379438400268555, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.25it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.47it/s]                                              {'loss': 0.7469, 'grad_norm': 1.0939141511917114, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.47it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.95it/s]                                              {'loss': 0.6736, 'grad_norm': 0.3037467300891876, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.95it/s]                                              {'loss': 0.7669, 'grad_norm': 1.3577905893325806, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.95it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.82it/s]                                              {'loss': 0.6884, 'grad_norm': 0.4107373356819153, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.82it/s]                                              {'loss': 0.6863, 'grad_norm': 0.236302450299263, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.82it/s] 30%|███       | 9/30 [00:01<00:02,  7.29it/s]                                              {'loss': 0.662, 'grad_norm': 0.27035653591156006, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.29it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.09it/s]                                               {'loss': 0.6636, 'grad_norm': 0.28594592213630676, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.09it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.13it/s]                                               {'loss': 0.6748, 'grad_norm': 0.18145623803138733, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.13it/s]                                               {'loss': 0.6376, 'grad_norm': 1.3635226488113403, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.13it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.08it/s]                                               {'loss': 0.6503, 'grad_norm': 0.3772719204425812, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.08it/s] 47%|████▋     | 14/30 [00:02<00:01,  8.02it/s]                                               {'loss': 0.6161, 'grad_norm': 0.40015411376953125, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  8.02it/s]                                               {'loss': 0.692, 'grad_norm': 0.7727009057998657, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.02it/s] 53%|█████▎    | 16/30 [00:02<00:01,  9.10it/s]                                               {'loss': 0.6458, 'grad_norm': 0.33605024218559265, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  9.10it/s]                                               {'loss': 0.6614, 'grad_norm': 0.29993003606796265, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  9.10it/s] 60%|██████    | 18/30 [00:02<00:01, 11.11it/s]                                               {'loss': 0.7145, 'grad_norm': 0.6329452991485596, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 11.11it/s]                                               {'loss': 0.6859, 'grad_norm': 0.8080011606216431, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 11.11it/s] 67%|██████▋   | 20/30 [00:02<00:00, 11.41it/s]                                               {'loss': 0.6565, 'grad_norm': 0.1768110692501068, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 11.41it/s]                                               {'loss': 0.685, 'grad_norm': 0.26819634437561035, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.41it/s] 73%|███████▎  | 22/30 [00:02<00:00, 11.66it/s]                                               {'loss': 0.6575, 'grad_norm': 0.4043060839176178, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.66it/s]                                               {'loss': 0.6535, 'grad_norm': 0.3793719708919525, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 11.66it/s] 80%|████████  | 24/30 [00:02<00:00, 13.30it/s]                                               {'loss': 0.6583, 'grad_norm': 0.3663519024848938, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 13.30it/s]                                               {'loss': 0.7424, 'grad_norm': 0.7347246408462524, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 13.30it/s] 87%|████████▋ | 26/30 [00:03<00:00, 12.26it/s]                                               {'loss': 0.6313, 'grad_norm': 0.3131698668003082, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00, 12.26it/s]                                               {'loss': 0.6505, 'grad_norm': 0.43941211700439453, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 12.26it/s] 93%|█████████▎| 28/30 [00:03<00:00, 10.03it/s]                                               {'loss': 0.6594, 'grad_norm': 0.33283117413520813, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 10.03it/s]                                               {'loss': 0.619, 'grad_norm': 0.5138232707977295, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 10.03it/s]100%|██████████| 30/30 [00:03<00:00,  9.17it/s]                                               {'loss': 0.5985, 'grad_norm': 0.40348848700523376, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  9.17it/s]                                               {'train_runtime': 3.8403, 'train_samples_per_second': 110.669, 'train_steps_per_second': 7.812, 'train_loss': 0.6705928206443786, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  9.17it/s]100%|██████████| 30/30 [00:03<00:00,  7.83it/s]
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.73it/s]                                              {'loss': 0.682, 'grad_norm': 0.6487892270088196, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.73it/s]  7%|▋         | 2/30 [00:00<00:13,  2.02it/s]                                              {'loss': 0.7365, 'grad_norm': 0.8148269057273865, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:13,  2.02it/s] 10%|█         | 3/30 [00:01<00:09,  2.86it/s]                                              {'loss': 0.7168, 'grad_norm': 0.4599253237247467, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:09,  2.86it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.58it/s]                                              {'loss': 0.6823, 'grad_norm': 0.5245240926742554, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.58it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.98it/s]                                              {'loss': 0.701, 'grad_norm': 0.39455899596214294, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.98it/s]                                              {'loss': 0.7474, 'grad_norm': 1.4640787839889526, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.98it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.03it/s]                                              {'loss': 0.6218, 'grad_norm': 0.36953848600387573, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.03it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.29it/s]                                              {'loss': 0.6929, 'grad_norm': 0.3168694078922272, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.29it/s] 30%|███       | 9/30 [00:02<00:03,  5.79it/s]                                              {'loss': 0.7516, 'grad_norm': 0.4121895134449005, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:03,  5.79it/s] 33%|███▎      | 10/30 [00:02<00:03,  6.26it/s]                                               {'loss': 0.6586, 'grad_norm': 0.17572355270385742, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  6.26it/s] 37%|███▋      | 11/30 [00:02<00:03,  6.09it/s]                                               {'loss': 0.6775, 'grad_norm': 0.21055461466312408, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  6.09it/s]                                               {'loss': 0.4848, 'grad_norm': 1.1582809686660767, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.09it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.26it/s]                                               {'loss': 0.6576, 'grad_norm': 0.9787609577178955, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.26it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.80it/s]                                               {'loss': 0.61, 'grad_norm': 1.306656837463379, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.80it/s] 50%|█████     | 15/30 [00:03<00:02,  5.47it/s]                                               {'loss': 0.7153, 'grad_norm': 1.8279019594192505, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.47it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.61it/s]                                               {'loss': 0.6314, 'grad_norm': 0.4232509136199951, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.61it/s]                                               {'loss': 0.6398, 'grad_norm': 0.488411545753479, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.61it/s] 60%|██████    | 18/30 [00:03<00:01,  7.72it/s]                                               {'loss': 0.6263, 'grad_norm': 0.5973438024520874, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  7.72it/s]                                               {'loss': 0.5714, 'grad_norm': 1.715735673904419, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  7.72it/s] 67%|██████▋   | 20/30 [00:03<00:01,  9.13it/s]                                               {'loss': 0.7247, 'grad_norm': 1.135233998298645, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  9.13it/s]                                               {'loss': 0.7104, 'grad_norm': 0.603685736656189, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:00,  9.13it/s] 73%|███████▎  | 22/30 [00:03<00:00,  9.83it/s]                                               {'loss': 0.6184, 'grad_norm': 0.7819553017616272, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00,  9.83it/s]                                               {'loss': 0.7521, 'grad_norm': 1.5122252702713013, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  9.83it/s] 80%|████████  | 24/30 [00:03<00:00, 11.24it/s]                                               {'loss': 0.5679, 'grad_norm': 2.509436845779419, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00, 11.24it/s]                                               {'loss': 0.6922, 'grad_norm': 0.5544222593307495, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 11.24it/s] 87%|████████▋ | 26/30 [00:04<00:00, 11.10it/s]                                               {'loss': 0.595, 'grad_norm': 0.6705347895622253, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00, 11.10it/s]                                               {'loss': 0.6718, 'grad_norm': 0.5806758403778076, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00, 11.10it/s] 93%|█████████▎| 28/30 [00:04<00:00,  9.92it/s]                                               {'loss': 0.7137, 'grad_norm': 0.9033098816871643, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  9.92it/s]                                               {'loss': 0.6112, 'grad_norm': 0.6382206678390503, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  9.92it/s]100%|██████████| 30/30 [00:04<00:00,  9.52it/s]                                               {'loss': 0.607, 'grad_norm': 0.6388480067253113, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  9.52it/s]                                               {'train_runtime': 4.6396, 'train_samples_per_second': 91.602, 'train_steps_per_second': 6.466, 'train_loss': 0.6623150169849396, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  9.52it/s]100%|██████████| 30/30 [00:04<00:00,  6.47it/s]
CLIENT:7
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:01<00:29,  1.03s/it]                                              {'loss': 0.6482, 'grad_norm': 0.48440036177635193, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:01<00:29,  1.03s/it]                                              {'loss': 0.67, 'grad_norm': 0.734569251537323, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:28,  1.03s/it] 10%|█         | 3/30 [00:01<00:08,  3.00it/s]                                              {'loss': 0.6706, 'grad_norm': 0.8394435048103333, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:08,  3.00it/s]                                              {'loss': 0.6089, 'grad_norm': 0.8721417188644409, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  3.00it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.92it/s]                                              {'loss': 0.705, 'grad_norm': 1.4170769453048706, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.92it/s]                                              {'loss': 0.961, 'grad_norm': 1.9929624795913696, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.92it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.98it/s]                                              {'loss': 0.5588, 'grad_norm': 0.6798190474510193, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.98it/s]                                              {'loss': 0.6662, 'grad_norm': 5.735291957855225, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.98it/s] 30%|███       | 9/30 [00:01<00:02,  8.42it/s]                                              {'loss': 0.7647, 'grad_norm': 1.6427652835845947, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  8.42it/s]                                              {'loss': 0.5653, 'grad_norm': 0.701953113079071, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.42it/s] 37%|███▋      | 11/30 [00:01<00:02,  8.76it/s]                                               {'loss': 0.4908, 'grad_norm': 1.2453519105911255, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.76it/s]                                               {'loss': 0.5102, 'grad_norm': 1.149135708808899, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.76it/s] 43%|████▎     | 13/30 [00:02<00:01,  9.34it/s]                                               {'loss': 0.7596, 'grad_norm': 0.9438140392303467, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:01,  9.34it/s]                                               {'loss': 0.5697, 'grad_norm': 0.7509909272193909, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  9.34it/s] 50%|█████     | 15/30 [00:02<00:01,  8.77it/s]                                               {'loss': 0.6497, 'grad_norm': 1.0618948936462402, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.77it/s]                                               {'loss': 0.5963, 'grad_norm': 0.7650233507156372, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.77it/s] 57%|█████▋    | 17/30 [00:02<00:01,  8.17it/s]                                               {'loss': 0.6025, 'grad_norm': 1.32370126247406, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  8.17it/s] 60%|██████    | 18/30 [00:02<00:01,  8.32it/s]                                               {'loss': 0.6839, 'grad_norm': 1.0355395078659058, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  8.32it/s] 63%|██████▎   | 19/30 [00:02<00:01,  7.16it/s]                                               {'loss': 0.4932, 'grad_norm': 1.595774531364441, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.16it/s] 67%|██████▋   | 20/30 [00:03<00:01,  6.94it/s]                                               {'loss': 0.6627, 'grad_norm': 0.5598867535591125, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  6.94it/s] 70%|███████   | 21/30 [00:03<00:01,  6.82it/s]                                               {'loss': 0.6292, 'grad_norm': 1.1532946825027466, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  6.82it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.16it/s]                                               {'loss': 0.6275, 'grad_norm': 0.712239682674408, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.16it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.74it/s]                                               {'loss': 0.5654, 'grad_norm': 0.8759034276008606, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.74it/s] 80%|████████  | 24/30 [00:03<00:00,  7.11it/s]                                               {'loss': 0.702, 'grad_norm': 0.9447341561317444, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.11it/s] 83%|████████▎ | 25/30 [00:03<00:00,  6.41it/s]                                               {'loss': 0.6196, 'grad_norm': 0.5370454788208008, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  6.41it/s] 87%|████████▋ | 26/30 [00:03<00:00,  6.50it/s]                                               {'loss': 0.5739, 'grad_norm': 0.6895399689674377, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  6.50it/s] 90%|█████████ | 27/30 [00:04<00:00,  6.58it/s]                                               {'loss': 0.5488, 'grad_norm': 1.306820273399353, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  6.58it/s] 93%|█████████▎| 28/30 [00:04<00:00,  7.03it/s]                                               {'loss': 0.5786, 'grad_norm': 0.805790364742279, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  7.03it/s] 97%|█████████▋| 29/30 [00:04<00:00,  6.85it/s]                                               {'loss': 0.5987, 'grad_norm': 0.88437819480896, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  6.85it/s]                                               {'loss': 0.6319, 'grad_norm': 1.2930991649627686, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.85it/s]                                               {'train_runtime': 4.5796, 'train_samples_per_second': 92.802, 'train_steps_per_second': 6.551, 'train_loss': 0.6304266969362895, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.85it/s]100%|██████████| 30/30 [00:04<00:00,  6.55it/s]
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.21it/s]                                              {'loss': 0.9842, 'grad_norm': 2.0161242485046387, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.21it/s]  7%|▋         | 2/30 [00:00<00:04,  6.46it/s]                                              {'loss': 0.5864, 'grad_norm': 1.085045337677002, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.46it/s] 10%|█         | 3/30 [00:00<00:04,  6.58it/s]                                              {'loss': 0.42, 'grad_norm': 1.5830849409103394, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.58it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.70it/s]                                              {'loss': 0.3752, 'grad_norm': 1.2272568941116333, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.70it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.15it/s]                                              {'loss': 0.1316, 'grad_norm': 1.176522970199585, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.15it/s]                                              {'loss': 0.0376, 'grad_norm': 0.6250765323638916, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.15it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.07it/s]                                              {'loss': 0.0509, 'grad_norm': 0.7619750499725342, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.07it/s] 27%|██▋       | 8/30 [00:01<00:02,  8.28it/s]                                              {'loss': 0.0136, 'grad_norm': 0.35880786180496216, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  8.28it/s] 30%|███       | 9/30 [00:01<00:02,  7.89it/s]                                              {'loss': 0.0118, 'grad_norm': 0.6542733311653137, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.89it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.72it/s]                                               {'loss': 0.237, 'grad_norm': 1.6744003295898438, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.72it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.76it/s]                                               {'loss': 0.0016, 'grad_norm': 0.02368435636162758, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.76it/s]                                               {'loss': 0.0023, 'grad_norm': 0.05467021092772484, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.76it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.17it/s]                                               {'loss': 0.0013, 'grad_norm': 0.017307355999946594, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.17it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.06it/s]                                               {'loss': 0.0017, 'grad_norm': 0.03558754920959473, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.06it/s] 50%|█████     | 15/30 [00:01<00:01,  7.75it/s]                                               {'loss': 0.4562, 'grad_norm': 1.4138628244400024, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  7.75it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.67it/s]                                               {'loss': 0.0016, 'grad_norm': 0.02592976577579975, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.67it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.95it/s]                                               {'loss': 0.0031, 'grad_norm': 0.0501333512365818, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.95it/s]                                               {'loss': 0.0016, 'grad_norm': 0.030667103826999664, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.95it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.14it/s]                                               {'loss': 0.0019, 'grad_norm': 0.02694862335920334, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  9.14it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.67it/s]                                               {'loss': 0.0018, 'grad_norm': 0.027213377878069878, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.67it/s] 70%|███████   | 21/30 [00:02<00:01,  8.42it/s]                                               {'loss': 0.0023, 'grad_norm': 0.032192692160606384, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.42it/s] 73%|███████▎  | 22/30 [00:02<00:00,  8.20it/s]                                               {'loss': 0.0027, 'grad_norm': 0.04815196245908737, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  8.20it/s] 77%|███████▋  | 23/30 [00:02<00:00,  8.44it/s]                                               {'loss': 0.4442, 'grad_norm': 2.3112761974334717, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  8.44it/s]                                               {'loss': 0.0034, 'grad_norm': 0.05784108117222786, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  8.44it/s] 83%|████████▎ | 25/30 [00:03<00:00,  9.05it/s]                                               {'loss': 0.0039, 'grad_norm': 0.052641939371824265, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  9.05it/s] 87%|████████▋ | 26/30 [00:03<00:00,  8.43it/s]                                               {'loss': 0.0038, 'grad_norm': 0.05567127838730812, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  8.43it/s] 90%|█████████ | 27/30 [00:03<00:00,  8.19it/s]                                               {'loss': 0.3172, 'grad_norm': 1.3147056102752686, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  8.19it/s] 93%|█████████▎| 28/30 [00:03<00:00,  7.81it/s]                                               {'loss': 0.0033, 'grad_norm': 0.04964581876993179, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  7.81it/s] 97%|█████████▋| 29/30 [00:03<00:00,  7.83it/s]                                               {'loss': 0.0029, 'grad_norm': 0.05199353024363518, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  7.83it/s]                                               {'loss': 0.0043, 'grad_norm': 0.07717021554708481, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  7.83it/s]                                               {'train_runtime': 3.7944, 'train_samples_per_second': 112.009, 'train_steps_per_second': 7.906, 'train_loss': 0.13698408410418778, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  7.83it/s]100%|██████████| 30/30 [00:03<00:00,  7.91it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  5%|▍         | 3/66 [00:00<00:02, 25.51it/s]  9%|▉         | 6/66 [00:00<00:03, 19.69it/s] 14%|█▎        | 9/66 [00:00<00:03, 18.21it/s] 18%|█▊        | 12/66 [00:00<00:02, 18.43it/s] 21%|██        | 14/66 [00:00<00:02, 18.10it/s] 24%|██▍       | 16/66 [00:00<00:02, 17.48it/s] 27%|██▋       | 18/66 [00:00<00:02, 17.15it/s] 30%|███       | 20/66 [00:01<00:02, 16.89it/s] 33%|███▎      | 22/66 [00:01<00:02, 16.51it/s] 38%|███▊      | 25/66 [00:01<00:02, 16.94it/s] 41%|████      | 27/66 [00:01<00:02, 15.50it/s] 44%|████▍     | 29/66 [00:01<00:02, 14.28it/s] 47%|████▋     | 31/66 [00:01<00:02, 13.72it/s] 50%|█████     | 33/66 [00:02<00:02, 14.34it/s] 55%|█████▍    | 36/66 [00:02<00:01, 16.45it/s] 59%|█████▉    | 39/66 [00:02<00:01, 18.64it/s] 64%|██████▎   | 42/66 [00:02<00:01, 20.13it/s] 68%|██████▊   | 45/66 [00:02<00:00, 21.73it/s] 73%|███████▎  | 48/66 [00:02<00:00, 23.64it/s] 77%|███████▋  | 51/66 [00:02<00:00, 24.97it/s] 82%|████████▏ | 54/66 [00:02<00:00, 24.41it/s] 86%|████████▋ | 57/66 [00:03<00:00, 22.22it/s] 91%|█████████ | 60/66 [00:03<00:00, 20.71it/s] 95%|█████████▌| 63/66 [00:03<00:00, 20.37it/s]100%|██████████| 66/66 [00:03<00:00, 20.74it/s]100%|██████████| 66/66 [00:03<00:00, 18.92it/s]
{'eval_loss': 0.6682265996932983, 'eval_model_preparation_time': 0.0066, 'eval_acc': 0.6692233940556088, 'eval_runtime': 3.5494, 'eval_samples_per_second': 293.851, 'eval_steps_per_second': 18.595}
ROUND:1
CLIENT:26
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.78it/s]                                              {'loss': 0.8522, 'grad_norm': 0.9977433085441589, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.78it/s]                                              {'loss': 0.5979, 'grad_norm': 0.9672289490699768, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.78it/s] 10%|█         | 3/30 [00:00<00:02,  9.76it/s]                                              {'loss': 0.6826, 'grad_norm': 0.688420832157135, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02,  9.76it/s]                                              {'loss': 0.9541, 'grad_norm': 2.0393896102905273, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  9.76it/s] 17%|█▋        | 5/30 [00:00<00:02, 10.80it/s]                                              {'loss': 0.7679, 'grad_norm': 1.281693696975708, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.80it/s]                                              {'loss': 1.409, 'grad_norm': 4.316424369812012, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 10.80it/s] 23%|██▎       | 7/30 [00:00<00:02, 10.07it/s]                                              {'loss': 0.5752, 'grad_norm': 0.5120654702186584, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02, 10.07it/s]                                              {'loss': 0.7956, 'grad_norm': 1.2838308811187744, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02, 10.07it/s] 30%|███       | 9/30 [00:00<00:01, 10.83it/s]                                              {'loss': 0.6878, 'grad_norm': 0.8781350255012512, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 10.83it/s]                                              {'loss': 0.6205, 'grad_norm': 0.6312770843505859, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 10.83it/s] 37%|███▋      | 11/30 [00:01<00:01, 11.32it/s]                                               {'loss': 0.5739, 'grad_norm': 1.0599523782730103, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 11.32it/s]                                               {'loss': 0.5153, 'grad_norm': 0.9401610493659973, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 11.32it/s] 43%|████▎     | 13/30 [00:01<00:01, 11.97it/s]                                               {'loss': 0.6553, 'grad_norm': 0.8195424675941467, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 11.97it/s]                                               {'loss': 0.5935, 'grad_norm': 0.9721787571907043, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.97it/s] 50%|█████     | 15/30 [00:01<00:01, 11.81it/s]                                               {'loss': 0.7811, 'grad_norm': 1.4366493225097656, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 11.81it/s]                                               {'loss': 0.5741, 'grad_norm': 0.9236602187156677, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.81it/s] 57%|█████▋    | 17/30 [00:01<00:01, 12.07it/s]                                               {'loss': 0.6417, 'grad_norm': 0.617387592792511, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.07it/s]                                               {'loss': 0.7274, 'grad_norm': 1.1096614599227905, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 12.07it/s] 63%|██████▎   | 19/30 [00:01<00:00, 13.06it/s]                                               {'loss': 0.6532, 'grad_norm': 0.71625816822052, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.06it/s]                                               {'loss': 0.6136, 'grad_norm': 0.5424209833145142, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.06it/s] 70%|███████   | 21/30 [00:01<00:00, 12.94it/s]                                               {'loss': 0.58, 'grad_norm': 1.0884742736816406, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 12.94it/s]                                               {'loss': 0.6086, 'grad_norm': 0.5668920874595642, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 12.94it/s] 77%|███████▋  | 23/30 [00:01<00:00, 12.81it/s]                                               {'loss': 0.575, 'grad_norm': 0.7411285638809204, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 12.81it/s]                                               {'loss': 0.5892, 'grad_norm': 1.3543407917022705, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 12.81it/s] 83%|████████▎ | 25/30 [00:02<00:00, 13.72it/s]                                               {'loss': 0.578, 'grad_norm': 0.7781530618667603, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 13.72it/s]                                               {'loss': 0.5633, 'grad_norm': 0.5820488333702087, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.72it/s] 90%|█████████ | 27/30 [00:02<00:00, 13.02it/s]                                               {'loss': 0.5985, 'grad_norm': 0.727379322052002, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.02it/s]                                               {'loss': 0.5836, 'grad_norm': 0.8615560531616211, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.02it/s] 97%|█████████▋| 29/30 [00:02<00:00, 12.42it/s]                                               {'loss': 0.7203, 'grad_norm': 1.8054261207580566, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.42it/s]                                               {'loss': 0.439, 'grad_norm': 1.7817214727401733, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.42it/s]                                               {'train_runtime': 2.5592, 'train_samples_per_second': 166.067, 'train_steps_per_second': 11.722, 'train_loss': 0.6702476739883423, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 12.42it/s]100%|██████████| 30/30 [00:02<00:00, 11.73it/s]
CLIENT:80
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  7.73it/s]                                              {'loss': 0.6219, 'grad_norm': 0.6960463523864746, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  7.73it/s]                                              {'loss': 0.5312, 'grad_norm': 1.084844946861267, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.73it/s] 10%|█         | 3/30 [00:00<00:02,  9.87it/s]                                              {'loss': 0.3291, 'grad_norm': 1.1878819465637207, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02,  9.87it/s]                                              {'loss': 0.2588, 'grad_norm': 0.7677838206291199, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  9.87it/s] 17%|█▋        | 5/30 [00:00<00:02, 10.68it/s]                                              {'loss': 0.2845, 'grad_norm': 1.4100487232208252, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.68it/s]                                              {'loss': 0.9236, 'grad_norm': 3.707639694213867, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 10.68it/s] 23%|██▎       | 7/30 [00:00<00:02, 10.58it/s]                                              {'loss': 0.1863, 'grad_norm': 0.9545712471008301, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02, 10.58it/s]                                              {'loss': 0.8592, 'grad_norm': 6.932370662689209, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02, 10.58it/s] 30%|███       | 9/30 [00:00<00:01, 11.11it/s]                                              {'loss': 0.2573, 'grad_norm': 2.1420986652374268, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 11.11it/s]                                              {'loss': 0.4854, 'grad_norm': 5.0523786544799805, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.11it/s] 37%|███▋      | 11/30 [00:01<00:01, 11.07it/s]                                               {'loss': 0.0378, 'grad_norm': 0.8522530794143677, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 11.07it/s]                                               {'loss': 0.8086, 'grad_norm': 6.809023857116699, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 11.07it/s] 43%|████▎     | 13/30 [00:01<00:01, 10.53it/s]                                               {'loss': 0.3347, 'grad_norm': 1.2854899168014526, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 10.53it/s]                                               {'loss': 0.2165, 'grad_norm': 5.070559024810791, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 10.53it/s] 50%|█████     | 15/30 [00:01<00:01, 10.87it/s]                                               {'loss': 0.7762, 'grad_norm': 10.581507682800293, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 10.87it/s]                                               {'loss': 0.3492, 'grad_norm': 3.135200262069702, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 10.87it/s] 57%|█████▋    | 17/30 [00:01<00:01, 11.22it/s]                                               {'loss': 0.0944, 'grad_norm': 1.521089792251587, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.22it/s]                                               {'loss': 0.8193, 'grad_norm': 3.732208013534546, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 11.22it/s] 63%|██████▎   | 19/30 [00:01<00:00, 11.84it/s]                                               {'loss': 0.0853, 'grad_norm': 1.8061891794204712, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 11.84it/s]                                               {'loss': 0.6737, 'grad_norm': 5.808830738067627, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 11.84it/s] 70%|███████   | 21/30 [00:01<00:00, 12.23it/s]                                               {'loss': 0.3099, 'grad_norm': 4.058012962341309, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 12.23it/s]                                               {'loss': 0.5858, 'grad_norm': 8.70361042022705, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 12.23it/s] 77%|███████▋  | 23/30 [00:02<00:00, 12.56it/s]                                               {'loss': 0.2891, 'grad_norm': 4.782007694244385, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 12.56it/s]                                               {'loss': 0.4141, 'grad_norm': 2.889125347137451, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 12.56it/s] 83%|████████▎ | 25/30 [00:02<00:00, 12.10it/s]                                               {'loss': 0.2086, 'grad_norm': 1.2055411338806152, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.10it/s]                                               {'loss': 0.3134, 'grad_norm': 3.737553119659424, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 12.10it/s] 90%|█████████ | 27/30 [00:02<00:00, 10.28it/s]                                               {'loss': 0.3655, 'grad_norm': 3.253913402557373, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 10.28it/s]                                               {'loss': 0.4824, 'grad_norm': 2.7231757640838623, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 10.28it/s] 97%|█████████▋| 29/30 [00:02<00:00,  9.81it/s]                                               {'loss': 0.2271, 'grad_norm': 1.4478538036346436, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00,  9.81it/s]                                               {'loss': 0.3569, 'grad_norm': 3.2242138385772705, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00,  9.81it/s]                                               {'train_runtime': 2.869, 'train_samples_per_second': 148.134, 'train_steps_per_second': 10.457, 'train_loss': 0.41619514959553877, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00,  9.81it/s]100%|██████████| 30/30 [00:02<00:00, 10.46it/s]
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:26,  1.09it/s]                                              {'loss': 0.8531, 'grad_norm': 1.1110761165618896, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:26,  1.09it/s]  7%|▋         | 2/30 [00:01<00:13,  2.07it/s]                                              {'loss': 0.645, 'grad_norm': 0.5050839781761169, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:13,  2.07it/s] 10%|█         | 3/30 [00:01<00:10,  2.70it/s]                                              {'loss': 0.5491, 'grad_norm': 0.4833380877971649, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:10,  2.70it/s] 13%|█▎        | 4/30 [00:01<00:08,  3.10it/s]                                              {'loss': 0.5435, 'grad_norm': 0.8416712284088135, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  3.10it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.98it/s]                                              {'loss': 0.8379, 'grad_norm': 1.927648901939392, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.98it/s] 20%|██        | 6/30 [00:01<00:05,  4.79it/s]                                              {'loss': 0.9865, 'grad_norm': 2.0752410888671875, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.79it/s] 23%|██▎       | 7/30 [00:02<00:04,  4.95it/s]                                              {'loss': 0.5066, 'grad_norm': 0.547290563583374, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:04,  4.95it/s] 27%|██▋       | 8/30 [00:02<00:03,  5.73it/s]                                              {'loss': 0.6987, 'grad_norm': 1.242823839187622, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:03,  5.73it/s] 30%|███       | 9/30 [00:02<00:03,  6.18it/s]                                              {'loss': 0.5535, 'grad_norm': 2.3129687309265137, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:03,  6.18it/s] 33%|███▎      | 10/30 [00:02<00:02,  6.86it/s]                                               {'loss': 0.581, 'grad_norm': 3.224174976348877, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:02,  6.86it/s] 37%|███▋      | 11/30 [00:02<00:02,  7.27it/s]                                               {'loss': 0.7344, 'grad_norm': 1.7194758653640747, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:02,  7.27it/s] 40%|████      | 12/30 [00:02<00:02,  7.36it/s]                                               {'loss': 0.4958, 'grad_norm': 1.4646120071411133, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  7.36it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.51it/s]                                               {'loss': 0.5776, 'grad_norm': 1.4757543802261353, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.51it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.76it/s]                                               {'loss': 0.5755, 'grad_norm': 1.1018438339233398, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.76it/s] 50%|█████     | 15/30 [00:02<00:01,  8.23it/s]                                               {'loss': 0.6932, 'grad_norm': 1.7528588771820068, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.23it/s]                                               {'loss': 0.6964, 'grad_norm': 1.7076892852783203, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:01,  8.23it/s] 57%|█████▋    | 17/30 [00:03<00:01,  9.06it/s]                                               {'loss': 0.5924, 'grad_norm': 0.9579766392707825, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:01,  9.06it/s]                                               {'loss': 0.6961, 'grad_norm': 1.742466688156128, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  9.06it/s] 63%|██████▎   | 19/30 [00:03<00:01,  9.15it/s]                                               {'loss': 0.6871, 'grad_norm': 2.9746854305267334, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  9.15it/s] 67%|██████▋   | 20/30 [00:03<00:01,  9.13it/s]                                               {'loss': 0.5293, 'grad_norm': 1.5641597509384155, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  9.13it/s] 70%|███████   | 21/30 [00:03<00:01,  8.42it/s]                                               {'loss': 0.7313, 'grad_norm': 2.1395843029022217, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  8.42it/s] 73%|███████▎  | 22/30 [00:03<00:00,  8.25it/s]                                               {'loss': 0.7151, 'grad_norm': 2.348513126373291, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00,  8.25it/s] 77%|███████▋  | 23/30 [00:03<00:00,  7.97it/s]                                               {'loss': 0.5869, 'grad_norm': 4.009302139282227, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.97it/s] 80%|████████  | 24/30 [00:04<00:00,  8.02it/s]                                               {'loss': 0.5714, 'grad_norm': 2.4056687355041504, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  8.02it/s] 83%|████████▎ | 25/30 [00:04<00:00,  7.62it/s]                                               {'loss': 0.5956, 'grad_norm': 3.883814573287964, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  7.62it/s] 87%|████████▋ | 26/30 [00:04<00:00,  7.43it/s]                                               {'loss': 0.637, 'grad_norm': 1.1361379623413086, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  7.43it/s] 90%|█████████ | 27/30 [00:04<00:00,  7.20it/s]                                               {'loss': 0.5542, 'grad_norm': 0.8120385408401489, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  7.20it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.98it/s]                                               {'loss': 0.4675, 'grad_norm': 1.779260516166687, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.98it/s] 97%|█████████▋| 29/30 [00:04<00:00,  6.88it/s]                                               {'loss': 0.7962, 'grad_norm': 2.31933331489563, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  6.88it/s]100%|██████████| 30/30 [00:04<00:00,  7.19it/s]                                               {'loss': 0.5962, 'grad_norm': 1.5971710681915283, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.19it/s]                                               {'train_runtime': 5.0804, 'train_samples_per_second': 83.655, 'train_steps_per_second': 5.905, 'train_loss': 0.6428028444449106, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  7.19it/s]100%|██████████| 30/30 [00:05<00:00,  5.91it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.57it/s]                                              {'loss': 0.5312, 'grad_norm': 0.267039954662323, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.57it/s]                                              {'loss': 0.673, 'grad_norm': 1.2590681314468384, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.57it/s] 10%|█         | 3/30 [00:00<00:02,  9.19it/s]                                              {'loss': 0.6842, 'grad_norm': 0.694593071937561, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02,  9.19it/s]                                              {'loss': 0.787, 'grad_norm': 1.4241260290145874, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  9.19it/s] 17%|█▋        | 5/30 [00:00<00:02, 10.19it/s]                                              {'loss': 0.7009, 'grad_norm': 14.17005729675293, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.19it/s]                                              {'loss': 0.8792, 'grad_norm': 2.0430753231048584, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 10.19it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.34it/s]                                              {'loss': 0.6193, 'grad_norm': 1.1194249391555786, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.34it/s] 27%|██▋       | 8/30 [00:00<00:02,  7.89it/s]                                              {'loss': 0.7054, 'grad_norm': 0.9024761915206909, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  7.89it/s] 30%|███       | 9/30 [00:01<00:02,  7.37it/s]                                              {'loss': 0.712, 'grad_norm': 0.872695803642273, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.37it/s] 33%|███▎      | 10/30 [00:01<00:02,  6.77it/s]                                               {'loss': 0.6324, 'grad_norm': 0.6297807693481445, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  6.77it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.77it/s]                                               {'loss': 0.6715, 'grad_norm': 0.7164122462272644, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.77it/s] 40%|████      | 12/30 [00:01<00:02,  7.15it/s]                                               {'loss': 0.6682, 'grad_norm': 1.0180755853652954, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.15it/s] 43%|████▎     | 13/30 [00:01<00:02,  6.91it/s]                                               {'loss': 0.6089, 'grad_norm': 0.6754820942878723, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  6.91it/s] 47%|████▋     | 14/30 [00:01<00:02,  6.16it/s]                                               {'loss': 0.6619, 'grad_norm': 0.6964568495750427, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  6.16it/s] 50%|█████     | 15/30 [00:02<00:02,  6.64it/s]                                               {'loss': 0.691, 'grad_norm': 1.9519315958023071, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.64it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.69it/s]                                               {'loss': 0.6121, 'grad_norm': 0.6498947143554688, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.69it/s] 57%|█████▋    | 17/30 [00:02<00:01,  6.91it/s]                                               {'loss': 0.6291, 'grad_norm': 0.6004760265350342, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  6.91it/s]                                               {'loss': 0.6454, 'grad_norm': 0.7471316456794739, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  6.91it/s] 63%|██████▎   | 19/30 [00:02<00:01,  7.36it/s]                                               {'loss': 0.6197, 'grad_norm': 1.0401862859725952, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.36it/s] 67%|██████▋   | 20/30 [00:02<00:01,  7.84it/s]                                               {'loss': 0.6509, 'grad_norm': 0.3684883117675781, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  7.84it/s] 70%|███████   | 21/30 [00:02<00:01,  7.80it/s]                                               {'loss': 0.6671, 'grad_norm': 0.47451722621917725, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  7.80it/s] 73%|███████▎  | 22/30 [00:02<00:01,  7.38it/s]                                               {'loss': 0.628, 'grad_norm': 0.871775209903717, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  7.38it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.68it/s]                                               {'loss': 0.6477, 'grad_norm': 0.7127006649971008, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.68it/s]                                               {'loss': 0.5578, 'grad_norm': 0.8706015944480896, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.68it/s] 83%|████████▎ | 25/30 [00:03<00:00,  7.75it/s]                                               {'loss': 0.6499, 'grad_norm': 1.2311198711395264, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  7.75it/s]                                               {'loss': 0.573, 'grad_norm': 0.931512713432312, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.75it/s] 90%|█████████ | 27/30 [00:03<00:00,  8.62it/s]                                               {'loss': 0.6124, 'grad_norm': 0.9624993205070496, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  8.62it/s] 93%|█████████▎| 28/30 [00:03<00:00,  8.33it/s]                                               {'loss': 0.664, 'grad_norm': 0.6859174370765686, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  8.33it/s] 97%|█████████▋| 29/30 [00:03<00:00,  8.41it/s]                                               {'loss': 0.558, 'grad_norm': 0.8192422986030579, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  8.41it/s]100%|██████████| 30/30 [00:03<00:00,  8.46it/s]                                               {'loss': 0.514, 'grad_norm': 1.795903205871582, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.46it/s]                                               {'train_runtime': 4.3629, 'train_samples_per_second': 97.413, 'train_steps_per_second': 6.876, 'train_loss': 0.6485148807366689, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  8.46it/s]100%|██████████| 30/30 [00:04<00:00,  6.88it/s]
CLIENT:77
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.72it/s]                                              {'loss': 0.8946, 'grad_norm': 1.4689151048660278, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.72it/s]                                              {'loss': 0.584, 'grad_norm': 1.1254985332489014, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  4.72it/s] 10%|█         | 3/30 [00:00<00:03,  8.94it/s]                                              {'loss': 0.5558, 'grad_norm': 0.6623116731643677, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.94it/s]                                              {'loss': 0.3557, 'grad_norm': 0.7476919889450073, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  8.94it/s] 17%|█▋        | 5/30 [00:00<00:02, 10.17it/s]                                              {'loss': 0.3473, 'grad_norm': 3.253899097442627, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.17it/s]                                              {'loss': 0.4156, 'grad_norm': 1.5578516721725464, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 10.17it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.29it/s]                                              {'loss': 0.6057, 'grad_norm': 2.4012291431427, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  9.29it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.17it/s]                                              {'loss': 0.4491, 'grad_norm': 2.0306124687194824, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  9.17it/s] 30%|███       | 9/30 [00:01<00:02,  8.64it/s]                                              {'loss': 0.3635, 'grad_norm': 1.0790627002716064, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  8.64it/s] 33%|███▎      | 10/30 [00:01<00:02,  8.86it/s]                                               {'loss': 0.2995, 'grad_norm': 0.9184361100196838, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.86it/s]                                               {'loss': 0.4644, 'grad_norm': 0.8373351693153381, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.86it/s]                                               {'loss': 0.1134, 'grad_norm': 1.3062554597854614, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.86it/s] 43%|████▎     | 13/30 [00:01<00:01, 11.83it/s]                                               {'loss': 0.5355, 'grad_norm': 2.199728488922119, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 11.83it/s]                                               {'loss': 0.2979, 'grad_norm': 0.7106029987335205, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.83it/s] 50%|█████     | 15/30 [00:01<00:01, 11.14it/s]                                               {'loss': 0.8175, 'grad_norm': 4.344465732574463, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 11.14it/s]                                               {'loss': 0.4719, 'grad_norm': 1.0354032516479492, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.14it/s] 57%|█████▋    | 17/30 [00:01<00:01, 10.97it/s]                                               {'loss': 0.2596, 'grad_norm': 1.0278109312057495, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 10.97it/s]                                               {'loss': 0.1152, 'grad_norm': 1.539732813835144, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 10.97it/s] 63%|██████▎   | 19/30 [00:01<00:00, 11.47it/s]                                               {'loss': 0.302, 'grad_norm': 0.7965251207351685, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 11.47it/s]                                               {'loss': 0.3843, 'grad_norm': 1.233297348022461, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 11.47it/s] 70%|███████   | 21/30 [00:02<00:01,  8.74it/s]                                               {'loss': 0.4975, 'grad_norm': 1.9287160634994507, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.74it/s] 73%|███████▎  | 22/30 [00:02<00:01,  7.89it/s]                                               {'loss': 0.092, 'grad_norm': 1.2074164152145386, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  7.89it/s] 77%|███████▋  | 23/30 [00:02<00:00,  7.04it/s]                                               {'loss': 0.4636, 'grad_norm': 1.5938360691070557, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  7.04it/s] 80%|████████  | 24/30 [00:02<00:00,  7.15it/s]                                               {'loss': 0.1032, 'grad_norm': 1.416977047920227, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  7.15it/s] 83%|████████▎ | 25/30 [00:02<00:00,  6.22it/s]                                               {'loss': 0.666, 'grad_norm': 3.8676140308380127, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00,  6.22it/s] 87%|████████▋ | 26/30 [00:03<00:00,  6.24it/s]                                               {'loss': 0.3899, 'grad_norm': 1.195425033569336, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  6.24it/s] 90%|█████████ | 27/30 [00:03<00:00,  6.34it/s]                                               {'loss': 0.3287, 'grad_norm': 1.3058866262435913, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  6.34it/s] 93%|█████████▎| 28/30 [00:03<00:00,  6.38it/s]                                               {'loss': 0.4016, 'grad_norm': 3.043322801589966, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  6.38it/s] 97%|█████████▋| 29/30 [00:03<00:00,  6.09it/s]                                               {'loss': 0.2267, 'grad_norm': 0.9099024534225464, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  6.09it/s]100%|██████████| 30/30 [00:03<00:00,  6.81it/s]                                               {'loss': 0.138, 'grad_norm': 1.9819071292877197, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  6.81it/s]                                               {'train_runtime': 3.931, 'train_samples_per_second': 108.114, 'train_steps_per_second': 7.632, 'train_loss': 0.39799144541223846, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  6.81it/s]100%|██████████| 30/30 [00:03<00:00,  7.63it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.68it/s]                                              {'loss': 0.9306, 'grad_norm': 1.5283756256103516, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.68it/s]  7%|▋         | 2/30 [00:00<00:06,  4.43it/s]                                              {'loss': 0.5646, 'grad_norm': 1.030111312866211, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.43it/s] 10%|█         | 3/30 [00:00<00:06,  4.12it/s]                                              {'loss': 0.7337, 'grad_norm': 0.8138622641563416, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.12it/s] 13%|█▎        | 4/30 [00:00<00:06,  4.24it/s]                                              {'loss': 0.7293, 'grad_norm': 1.3715260028839111, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:06,  4.24it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.36it/s]                                              {'loss': 0.6711, 'grad_norm': 29.10222816467285, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.36it/s] 20%|██        | 6/30 [00:01<00:04,  4.81it/s]                                              {'loss': 1.2358, 'grad_norm': 3.332108497619629, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.81it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.14it/s]                                              {'loss': 0.3196, 'grad_norm': 1.6279398202896118, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.14it/s] 27%|██▋       | 8/30 [00:01<00:05,  3.88it/s]                                              {'loss': 0.5058, 'grad_norm': 2.4505012035369873, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  3.88it/s] 30%|███       | 9/30 [00:02<00:05,  3.79it/s]                                              {'loss': 0.7103, 'grad_norm': 1.5552424192428589, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  3.79it/s] 33%|███▎      | 10/30 [00:02<00:05,  3.76it/s]                                               {'loss': 0.6059, 'grad_norm': 0.8532806634902954, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  3.76it/s] 37%|███▋      | 11/30 [00:02<00:05,  3.74it/s]                                               {'loss': 0.7242, 'grad_norm': 1.3874452114105225, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:05,  3.74it/s]                                               {'loss': 0.431, 'grad_norm': 1.4421616792678833, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:04,  3.74it/s] 43%|████▎     | 13/30 [00:03<00:03,  4.35it/s]                                               {'loss': 0.4561, 'grad_norm': 0.6652114987373352, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  4.35it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.12it/s]                                               {'loss': 0.5523, 'grad_norm': 0.6977866291999817, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.12it/s] 50%|█████     | 15/30 [00:03<00:03,  3.93it/s]                                               {'loss': 0.5974, 'grad_norm': 1.1911808252334595, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  3.93it/s] 53%|█████▎    | 16/30 [00:03<00:03,  3.93it/s]                                               {'loss': 0.4974, 'grad_norm': 1.0188161134719849, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  3.93it/s] 57%|█████▋    | 17/30 [00:04<00:03,  4.08it/s]                                               {'loss': 0.5832, 'grad_norm': 1.5729962587356567, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:03,  4.08it/s] 60%|██████    | 18/30 [00:04<00:02,  4.86it/s]                                               {'loss': 0.6524, 'grad_norm': 2.1557717323303223, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.86it/s] 63%|██████▎   | 19/30 [00:04<00:02,  4.56it/s]                                               {'loss': 0.6002, 'grad_norm': 1.3391525745391846, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  4.56it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.43it/s]                                               {'loss': 0.4561, 'grad_norm': 1.9231305122375488, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.43it/s] 70%|███████   | 21/30 [00:04<00:01,  4.61it/s]                                               {'loss': 0.5711, 'grad_norm': 1.0496249198913574, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  4.61it/s] 73%|███████▎  | 22/30 [00:05<00:01,  4.67it/s]                                               {'loss': 0.5847, 'grad_norm': 0.7480852603912354, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:01,  4.67it/s] 77%|███████▋  | 23/30 [00:05<00:01,  4.84it/s]                                               {'loss': 0.702, 'grad_norm': 1.1756649017333984, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  4.84it/s] 80%|████████  | 24/30 [00:05<00:01,  5.57it/s]                                               {'loss': 0.3895, 'grad_norm': 1.6968660354614258, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.57it/s] 83%|████████▎ | 25/30 [00:05<00:00,  5.53it/s]                                               {'loss': 0.4614, 'grad_norm': 2.0641465187072754, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  5.53it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.90it/s]                                               {'loss': 0.5185, 'grad_norm': 1.118944525718689, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.90it/s] 90%|█████████ | 27/30 [00:05<00:00,  6.51it/s]                                               {'loss': 0.551, 'grad_norm': 0.8960281014442444, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  6.51it/s] 93%|█████████▎| 28/30 [00:06<00:00,  6.59it/s]                                               {'loss': 0.5576, 'grad_norm': 1.0438193082809448, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  6.59it/s] 97%|█████████▋| 29/30 [00:06<00:00,  6.28it/s]                                               {'loss': 0.5663, 'grad_norm': 1.1578706502914429, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  6.28it/s]                                               {'loss': 0.3277, 'grad_norm': 2.9994678497314453, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  6.28it/s]                                               {'train_runtime': 6.4639, 'train_samples_per_second': 65.75, 'train_steps_per_second': 4.641, 'train_loss': 0.592893385887146, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  6.28it/s]100%|██████████| 30/30 [00:06<00:00,  4.64it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.34it/s]                                              {'loss': 0.7011, 'grad_norm': 0.610747218132019, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.34it/s]  7%|▋         | 2/30 [00:00<00:03,  7.13it/s]                                              {'loss': 0.7115, 'grad_norm': 0.5440027713775635, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.13it/s] 10%|█         | 3/30 [00:00<00:03,  7.71it/s]                                              {'loss': 0.6784, 'grad_norm': 0.7453930377960205, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.71it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.96it/s]                                              {'loss': 0.7128, 'grad_norm': 1.0574231147766113, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.96it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.62it/s]                                              {'loss': 0.7503, 'grad_norm': 0.7331153750419617, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.62it/s] 20%|██        | 6/30 [00:00<00:03,  6.25it/s]                                              {'loss': 0.7309, 'grad_norm': 1.092682957649231, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.25it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.24it/s]                                              {'loss': 0.6438, 'grad_norm': 0.5026525855064392, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.24it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.61it/s]                                              {'loss': 0.6931, 'grad_norm': 0.692438542842865, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.61it/s] 30%|███       | 9/30 [00:01<00:03,  5.72it/s]                                              {'loss': 0.6987, 'grad_norm': 0.4611794948577881, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.72it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.59it/s]                                               {'loss': 0.6905, 'grad_norm': 0.3777894079685211, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.59it/s] 37%|███▋      | 11/30 [00:01<00:03,  5.40it/s]                                               {'loss': 0.644, 'grad_norm': 0.4379493296146393, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  5.40it/s] 40%|████      | 12/30 [00:02<00:03,  5.79it/s]                                               {'loss': 0.524, 'grad_norm': 1.904787540435791, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.79it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.23it/s]                                               {'loss': 0.6491, 'grad_norm': 0.3197914958000183, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.23it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.73it/s]                                               {'loss': 0.6389, 'grad_norm': 0.3614250123500824, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.73it/s] 50%|█████     | 15/30 [00:02<00:02,  5.46it/s]                                               {'loss': 0.7122, 'grad_norm': 1.0986894369125366, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.46it/s] 53%|█████▎    | 16/30 [00:02<00:02,  5.37it/s]                                               {'loss': 0.6209, 'grad_norm': 0.7592336535453796, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  5.37it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.98it/s]                                               {'loss': 0.5588, 'grad_norm': 1.2405742406845093, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.98it/s] 60%|██████    | 18/30 [00:03<00:02,  5.73it/s]                                               {'loss': 0.5505, 'grad_norm': 1.415632724761963, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.73it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.66it/s]                                               {'loss': 0.5989, 'grad_norm': 0.7691746950149536, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.66it/s] 67%|██████▋   | 20/30 [00:03<00:02,  4.57it/s]                                               {'loss': 0.7121, 'grad_norm': 1.0534805059432983, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:02,  4.57it/s] 70%|███████   | 21/30 [00:03<00:01,  4.52it/s]                                               {'loss': 0.7074, 'grad_norm': 1.14398992061615, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  4.52it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.62it/s]                                               {'loss': 0.5487, 'grad_norm': 0.6862573623657227, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.62it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.54it/s]                                               {'loss': 0.6708, 'grad_norm': 0.9273571372032166, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.54it/s] 80%|████████  | 24/30 [00:04<00:01,  5.04it/s]                                               {'loss': 0.7057, 'grad_norm': 2.321502923965454, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  5.04it/s] 83%|████████▎ | 25/30 [00:04<00:01,  4.63it/s]                                               {'loss': 0.6571, 'grad_norm': 0.9223608374595642, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:01,  4.63it/s] 87%|████████▋ | 26/30 [00:05<00:00,  4.32it/s]                                               {'loss': 0.6801, 'grad_norm': 1.055946946144104, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  4.32it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.48it/s]                                               {'loss': 0.568, 'grad_norm': 0.9541072249412537, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.48it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.77it/s]                                               {'loss': 0.6197, 'grad_norm': 1.016827940940857, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.77it/s] 97%|█████████▋| 29/30 [00:05<00:00,  5.09it/s]                                               {'loss': 0.5788, 'grad_norm': 0.82127845287323, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  5.09it/s]100%|██████████| 30/30 [00:05<00:00,  5.76it/s]                                               {'loss': 0.5885, 'grad_norm': 1.6307183504104614, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.76it/s]                                               {'train_runtime': 6.0093, 'train_samples_per_second': 70.724, 'train_steps_per_second': 4.992, 'train_loss': 0.6515072345733642, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.76it/s]100%|██████████| 30/30 [00:06<00:00,  5.00it/s]
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.33it/s]                                              {'loss': 0.5889, 'grad_norm': 0.4671902358531952, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.33it/s]  7%|▋         | 2/30 [00:00<00:04,  5.76it/s]                                              {'loss': 0.7518, 'grad_norm': 0.7531952261924744, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  5.76it/s] 10%|█         | 3/30 [00:00<00:04,  6.47it/s]                                              {'loss': 0.712, 'grad_norm': 0.4818051755428314, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.47it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.97it/s]                                              {'loss': 0.7093, 'grad_norm': 1.0164743661880493, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.97it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.76it/s]                                              {'loss': 0.6728, 'grad_norm': 0.48716622591018677, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.76it/s] 20%|██        | 6/30 [00:01<00:09,  2.65it/s]                                              {'loss': 0.833, 'grad_norm': 1.503730058670044, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:09,  2.65it/s] 23%|██▎       | 7/30 [00:01<00:07,  3.27it/s]                                              {'loss': 0.7031, 'grad_norm': 0.3259085416793823, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:07,  3.27it/s] 27%|██▋       | 8/30 [00:01<00:05,  3.95it/s]                                              {'loss': 0.6921, 'grad_norm': 0.49815526604652405, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  3.95it/s] 30%|███       | 9/30 [00:01<00:04,  4.68it/s]                                              {'loss': 0.6921, 'grad_norm': 0.3484652638435364, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.68it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.15it/s]                                               {'loss': 0.6766, 'grad_norm': 0.2624552547931671, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.15it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.82it/s]                                               {'loss': 0.6753, 'grad_norm': 0.3087126910686493, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.82it/s] 40%|████      | 12/30 [00:02<00:02,  6.59it/s]                                               {'loss': 0.6143, 'grad_norm': 0.8256700038909912, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.59it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.80it/s]                                               {'loss': 0.6504, 'grad_norm': 0.3526773452758789, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.80it/s]                                               {'loss': 0.6897, 'grad_norm': 0.6604633927345276, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.80it/s] 50%|█████     | 15/30 [00:02<00:01,  7.92it/s]                                               {'loss': 0.6026, 'grad_norm': 0.7274197340011597, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.92it/s] 53%|█████▎    | 16/30 [00:02<00:01,  8.11it/s]                                               {'loss': 0.6678, 'grad_norm': 0.6720496416091919, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.11it/s] 57%|█████▋    | 17/30 [00:02<00:01,  8.54it/s]                                               {'loss': 0.6821, 'grad_norm': 0.45801329612731934, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  8.54it/s] 60%|██████    | 18/30 [00:03<00:01,  8.87it/s]                                               {'loss': 0.652, 'grad_norm': 0.4539148807525635, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  8.87it/s] 63%|██████▎   | 19/30 [00:03<00:01,  7.69it/s]                                               {'loss': 0.7493, 'grad_norm': 1.3620587587356567, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  7.69it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.08it/s]                                               {'loss': 0.6653, 'grad_norm': 0.22723816335201263, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.08it/s] 70%|███████   | 21/30 [00:03<00:01,  6.98it/s]                                               {'loss': 0.6594, 'grad_norm': 0.2824554145336151, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  6.98it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.84it/s]                                               {'loss': 0.6733, 'grad_norm': 0.29357215762138367, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.84it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.40it/s]                                               {'loss': 0.6885, 'grad_norm': 0.981829047203064, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.40it/s] 80%|████████  | 24/30 [00:04<00:01,  3.49it/s]                                               {'loss': 0.6431, 'grad_norm': 0.39365801215171814, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  3.49it/s] 83%|████████▎ | 25/30 [00:04<00:01,  4.31it/s]                                               {'loss': 0.691, 'grad_norm': 0.5759313106536865, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:01,  4.31it/s] 87%|████████▋ | 26/30 [00:04<00:00,  4.79it/s]                                               {'loss': 0.6326, 'grad_norm': 0.45754337310791016, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  4.79it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.02it/s]                                               {'loss': 0.6302, 'grad_norm': 0.4461436867713928, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.02it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.64it/s]                                               {'loss': 0.6709, 'grad_norm': 0.3359817862510681, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.64it/s] 97%|█████████▋| 29/30 [00:05<00:00,  4.29it/s]                                               {'loss': 0.6338, 'grad_norm': 0.5189266800880432, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  4.29it/s]100%|██████████| 30/30 [00:05<00:00,  4.89it/s]                                               {'loss': 0.6271, 'grad_norm': 0.44066205620765686, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.89it/s]                                               {'train_runtime': 5.9002, 'train_samples_per_second': 72.032, 'train_steps_per_second': 5.085, 'train_loss': 0.6743474483489991, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.89it/s]100%|██████████| 30/30 [00:05<00:00,  5.08it/s]
CLIENT:20
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.56it/s]                                              {'loss': 0.5445, 'grad_norm': 0.5612521767616272, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.56it/s]  7%|▋         | 2/30 [00:00<00:04,  6.06it/s]                                              {'loss': 0.7657, 'grad_norm': 0.9567024111747742, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.06it/s] 10%|█         | 3/30 [00:00<00:04,  6.24it/s]                                              {'loss': 0.7453, 'grad_norm': 0.5376878976821899, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.24it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.38it/s]                                              {'loss': 0.6807, 'grad_norm': 0.44167786836624146, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.38it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.52it/s]                                              {'loss': 0.6424, 'grad_norm': 0.3897433876991272, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.52it/s] 20%|██        | 6/30 [00:00<00:03,  6.78it/s]                                              {'loss': 0.7919, 'grad_norm': 1.4344892501831055, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.78it/s] 23%|██▎       | 7/30 [00:01<00:03,  7.23it/s]                                              {'loss': 0.6852, 'grad_norm': 0.3112466633319855, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  7.23it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.53it/s]                                              {'loss': 0.6765, 'grad_norm': 0.46459442377090454, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.53it/s] 30%|███       | 9/30 [00:01<00:02,  7.01it/s]                                              {'loss': 0.6561, 'grad_norm': 0.381818950176239, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.01it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.54it/s]                                               {'loss': 0.658, 'grad_norm': 0.18385827541351318, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.54it/s] 37%|███▋      | 11/30 [00:01<00:03,  6.24it/s]                                               {'loss': 0.6875, 'grad_norm': 0.1510739028453827, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  6.24it/s] 40%|████      | 12/30 [00:01<00:02,  6.57it/s]                                               {'loss': 0.6676, 'grad_norm': 1.9544090032577515, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.57it/s] 43%|████▎     | 13/30 [00:02<00:02,  5.85it/s]                                               {'loss': 0.6734, 'grad_norm': 0.3591102957725525, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  5.85it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.42it/s]                                               {'loss': 0.7017, 'grad_norm': 0.5638182163238525, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.42it/s] 50%|█████     | 15/30 [00:02<00:02,  5.31it/s]                                               {'loss': 0.5801, 'grad_norm': 0.88524329662323, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.31it/s] 53%|█████▎    | 16/30 [00:02<00:02,  4.81it/s]                                               {'loss': 0.6555, 'grad_norm': 0.4241161346435547, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  4.81it/s] 57%|█████▋    | 17/30 [00:02<00:03,  4.25it/s]                                               {'loss': 0.7116, 'grad_norm': 0.496773362159729, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  4.25it/s] 60%|██████    | 18/30 [00:03<00:03,  3.98it/s]                                               {'loss': 0.6239, 'grad_norm': 0.3663153350353241, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:03,  3.98it/s] 63%|██████▎   | 19/30 [00:03<00:02,  3.74it/s]                                               {'loss': 0.7897, 'grad_norm': 1.1627143621444702, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  3.74it/s] 67%|██████▋   | 20/30 [00:03<00:02,  3.95it/s]                                               {'loss': 0.6681, 'grad_norm': 0.21516263484954834, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:02,  3.95it/s] 70%|███████   | 21/30 [00:04<00:02,  3.97it/s]                                               {'loss': 0.6692, 'grad_norm': 0.2012573480606079, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:02,  3.97it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.13it/s]                                               {'loss': 0.6809, 'grad_norm': 0.39256903529167175, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.13it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.56it/s]                                               {'loss': 0.6509, 'grad_norm': 0.5771163105964661, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.56it/s] 80%|████████  | 24/30 [00:04<00:01,  5.05it/s]                                               {'loss': 0.6282, 'grad_norm': 0.3381638824939728, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  5.05it/s] 83%|████████▎ | 25/30 [00:04<00:01,  4.98it/s]                                               {'loss': 0.7116, 'grad_norm': 0.3397778868675232, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:01,  4.98it/s] 87%|████████▋ | 26/30 [00:05<00:00,  4.92it/s]                                               {'loss': 0.6557, 'grad_norm': 0.27368125319480896, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  4.92it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.45it/s]                                               {'loss': 0.6934, 'grad_norm': 0.35235628485679626, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.45it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.68it/s]                                               {'loss': 0.6528, 'grad_norm': 0.16844530403614044, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.68it/s] 97%|█████████▋| 29/30 [00:05<00:00,  4.15it/s]                                               {'loss': 0.6085, 'grad_norm': 0.42296135425567627, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  4.15it/s]100%|██████████| 30/30 [00:05<00:00,  4.25it/s]                                               {'loss': 0.6404, 'grad_norm': 0.3136390149593353, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.25it/s]                                               {'train_runtime': 6.4498, 'train_samples_per_second': 65.893, 'train_steps_per_second': 4.651, 'train_loss': 0.6732330441474914, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.25it/s]100%|██████████| 30/30 [00:06<00:00,  4.65it/s]
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:10,  2.86it/s]                                              {'loss': 0.685, 'grad_norm': 0.656740128993988, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:10,  2.86it/s]  7%|▋         | 2/30 [00:00<00:09,  2.98it/s]                                              {'loss': 0.7253, 'grad_norm': 0.788790225982666, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:09,  2.98it/s] 10%|█         | 3/30 [00:01<00:09,  2.98it/s]                                              {'loss': 0.7108, 'grad_norm': 0.4880189597606659, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:09,  2.98it/s] 13%|█▎        | 4/30 [00:01<00:08,  2.90it/s]                                              {'loss': 0.679, 'grad_norm': 0.49438804388046265, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  2.90it/s] 17%|█▋        | 5/30 [00:01<00:08,  2.83it/s]                                              {'loss': 0.6964, 'grad_norm': 0.3815430700778961, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:08,  2.83it/s] 20%|██        | 6/30 [00:01<00:06,  3.59it/s]                                              {'loss': 0.7588, 'grad_norm': 1.5912744998931885, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.59it/s] 23%|██▎       | 7/30 [00:02<00:06,  3.48it/s]                                              {'loss': 0.6152, 'grad_norm': 0.4089800715446472, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:06,  3.48it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.31it/s]                                              {'loss': 0.6885, 'grad_norm': 0.29536157846450806, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.31it/s] 30%|███       | 9/30 [00:02<00:06,  3.14it/s]                                              {'loss': 0.7492, 'grad_norm': 0.41480734944343567, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:06,  3.14it/s] 33%|███▎      | 10/30 [00:03<00:06,  3.09it/s]                                               {'loss': 0.6571, 'grad_norm': 0.18185752630233765, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:06,  3.09it/s] 37%|███▋      | 11/30 [00:03<00:06,  3.12it/s]                                               {'loss': 0.6773, 'grad_norm': 0.2165171355009079, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:06,  3.12it/s] 40%|████      | 12/30 [00:03<00:04,  3.74it/s]                                               {'loss': 0.4835, 'grad_norm': 1.2641116380691528, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:04,  3.74it/s] 43%|████▎     | 13/30 [00:04<00:04,  3.43it/s]                                               {'loss': 0.6576, 'grad_norm': 0.6646460890769958, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:04<00:04,  3.43it/s] 47%|████▋     | 14/30 [00:04<00:04,  3.21it/s]                                               {'loss': 0.5994, 'grad_norm': 0.5227332711219788, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:04,  3.21it/s] 50%|█████     | 15/30 [00:04<00:04,  3.38it/s]                                               {'loss': 0.7074, 'grad_norm': 1.7298552989959717, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:04,  3.38it/s] 53%|█████▎    | 16/30 [00:04<00:03,  3.73it/s]                                               {'loss': 0.6314, 'grad_norm': 0.4484531283378601, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:03,  3.73it/s] 57%|█████▋    | 17/30 [00:05<00:03,  4.05it/s]                                               {'loss': 0.6353, 'grad_norm': 0.515596866607666, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:05<00:03,  4.05it/s]                                               {'loss': 0.6238, 'grad_norm': 0.6140652894973755, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:02,  4.05it/s] 63%|██████▎   | 19/30 [00:05<00:02,  4.94it/s]                                               {'loss': 0.5631, 'grad_norm': 1.7303686141967773, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  4.94it/s] 67%|██████▋   | 20/30 [00:05<00:02,  4.85it/s]                                               {'loss': 0.7172, 'grad_norm': 0.9519979953765869, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  4.85it/s] 70%|███████   | 21/30 [00:05<00:01,  4.88it/s]                                               {'loss': 0.7004, 'grad_norm': 0.598663866519928, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:01,  4.88it/s] 73%|███████▎  | 22/30 [00:05<00:01,  5.13it/s]                                               {'loss': 0.6127, 'grad_norm': 0.7835019826889038, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:01,  5.13it/s] 77%|███████▋  | 23/30 [00:06<00:01,  5.36it/s]                                               {'loss': 0.7471, 'grad_norm': 1.5026304721832275, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  5.36it/s]                                               {'loss': 0.565, 'grad_norm': 2.560648202896118, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  5.36it/s] 83%|████████▎ | 25/30 [00:06<00:00,  5.85it/s]                                               {'loss': 0.6875, 'grad_norm': 0.5450305342674255, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:00,  5.85it/s] 87%|████████▋ | 26/30 [00:06<00:00,  5.60it/s]                                               {'loss': 0.5885, 'grad_norm': 0.7324162721633911, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:00,  5.60it/s] 90%|█████████ | 27/30 [00:06<00:00,  5.46it/s]                                               {'loss': 0.6672, 'grad_norm': 0.5557147264480591, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  5.46it/s] 93%|█████████▎| 28/30 [00:06<00:00,  5.28it/s]                                               {'loss': 0.7043, 'grad_norm': 0.9050660133361816, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  5.28it/s] 97%|█████████▋| 29/30 [00:07<00:00,  5.33it/s]                                               {'loss': 0.6068, 'grad_norm': 0.6538037061691284, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  5.33it/s]                                               {'loss': 0.5889, 'grad_norm': 0.577743411064148, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  5.33it/s]                                               {'train_runtime': 7.3419, 'train_samples_per_second': 57.887, 'train_steps_per_second': 4.086, 'train_loss': 0.6576569060484568, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  5.33it/s]100%|██████████| 30/30 [00:07<00:00,  4.09it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  6%|▌         | 4/66 [00:00<00:02, 24.12it/s] 11%|█         | 7/66 [00:00<00:03, 18.82it/s] 14%|█▎        | 9/66 [00:00<00:03, 17.03it/s] 17%|█▋        | 11/66 [00:00<00:03, 16.63it/s] 20%|█▉        | 13/66 [00:00<00:03, 16.10it/s] 23%|██▎       | 15/66 [00:00<00:03, 13.32it/s] 26%|██▌       | 17/66 [00:01<00:04, 10.99it/s] 29%|██▉       | 19/66 [00:01<00:04, 10.08it/s] 32%|███▏      | 21/66 [00:01<00:04,  9.92it/s] 35%|███▍      | 23/66 [00:01<00:04,  9.49it/s] 36%|███▋      | 24/66 [00:02<00:04,  9.46it/s] 38%|███▊      | 25/66 [00:02<00:04,  9.21it/s] 39%|███▉      | 26/66 [00:02<00:04,  8.60it/s] 41%|████      | 27/66 [00:02<00:04,  8.88it/s] 42%|████▏     | 28/66 [00:02<00:04,  8.80it/s] 45%|████▌     | 30/66 [00:02<00:03,  9.23it/s] 47%|████▋     | 31/66 [00:02<00:03,  8.75it/s] 48%|████▊     | 32/66 [00:02<00:03,  8.62it/s] 50%|█████     | 33/66 [00:03<00:04,  7.84it/s] 52%|█████▏    | 34/66 [00:03<00:04,  7.37it/s] 53%|█████▎    | 35/66 [00:03<00:04,  7.31it/s] 55%|█████▍    | 36/66 [00:03<00:04,  7.31it/s] 56%|█████▌    | 37/66 [00:03<00:04,  7.18it/s] 58%|█████▊    | 38/66 [00:03<00:04,  6.54it/s] 59%|█████▉    | 39/66 [00:03<00:03,  7.16it/s] 61%|██████    | 40/66 [00:04<00:03,  7.05it/s] 62%|██████▏   | 41/66 [00:04<00:03,  7.29it/s] 64%|██████▎   | 42/66 [00:04<00:03,  7.09it/s] 65%|██████▌   | 43/66 [00:04<00:03,  6.89it/s] 67%|██████▋   | 44/66 [00:04<00:03,  6.18it/s] 68%|██████▊   | 45/66 [00:04<00:03,  6.75it/s] 70%|██████▉   | 46/66 [00:05<00:03,  6.66it/s] 71%|███████   | 47/66 [00:05<00:02,  6.80it/s] 73%|███████▎  | 48/66 [00:05<00:02,  6.38it/s] 76%|███████▌  | 50/66 [00:05<00:02,  6.94it/s] 77%|███████▋  | 51/66 [00:05<00:02,  6.95it/s] 79%|███████▉  | 52/66 [00:05<00:02,  6.85it/s] 80%|████████  | 53/66 [00:06<00:01,  7.05it/s] 83%|████████▎ | 55/66 [00:06<00:01,  7.67it/s] 86%|████████▋ | 57/66 [00:06<00:01,  7.94it/s] 89%|████████▉ | 59/66 [00:06<00:00,  8.41it/s] 92%|█████████▏| 61/66 [00:06<00:00,  9.07it/s] 95%|█████████▌| 63/66 [00:07<00:00, 10.33it/s] 98%|█████████▊| 65/66 [00:07<00:00, 12.19it/s]100%|██████████| 66/66 [00:07<00:00,  9.24it/s]
{'eval_loss': 0.6644527912139893, 'eval_model_preparation_time': 0.0102, 'eval_acc': 0.6663470757430489, 'eval_runtime': 7.1976, 'eval_samples_per_second': 144.909, 'eval_steps_per_second': 9.17}
ROUND:2
CLIENT:75
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:09,  2.91it/s]                                              {'loss': 0.8106, 'grad_norm': 1.1969679594039917, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:09,  2.91it/s]  7%|▋         | 2/30 [00:00<00:07,  3.58it/s]                                              {'loss': 0.5548, 'grad_norm': 0.8409608602523804, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.58it/s] 10%|█         | 3/30 [00:00<00:06,  3.90it/s]                                              {'loss': 0.5326, 'grad_norm': 3.2730672359466553, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  3.90it/s] 13%|█▎        | 4/30 [00:01<00:06,  4.22it/s]                                              {'loss': 0.7894, 'grad_norm': 1.7901197671890259, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:06,  4.22it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.55it/s]                                              {'loss': 1.0053, 'grad_norm': 4.20680046081543, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.55it/s] 20%|██        | 6/30 [00:01<00:04,  4.87it/s]                                              {'loss': 0.9417, 'grad_norm': 5.044264793395996, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.87it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.88it/s]                                              {'loss': 0.4261, 'grad_norm': 1.226569652557373, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.88it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.49it/s]                                              {'loss': 0.7291, 'grad_norm': 5.5160651206970215, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.49it/s] 30%|███       | 9/30 [00:02<00:04,  4.24it/s]                                              {'loss': 0.6925, 'grad_norm': 2.385134220123291, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.24it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.14it/s]                                               {'loss': 0.6461, 'grad_norm': 1.5953199863433838, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.14it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.04it/s]                                               {'loss': 0.6846, 'grad_norm': 1.4924606084823608, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.04it/s] 40%|████      | 12/30 [00:02<00:04,  4.10it/s]                                               {'loss': 0.6373, 'grad_norm': 3.437279224395752, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:04,  4.10it/s] 43%|████▎     | 13/30 [00:03<00:04,  3.92it/s]                                               {'loss': 0.6526, 'grad_norm': 3.03436017036438, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:04,  3.92it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.26it/s]                                               {'loss': 0.6379, 'grad_norm': 1.9182807207107544, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.26it/s] 50%|█████     | 15/30 [00:03<00:03,  4.29it/s]                                               {'loss': 0.7977, 'grad_norm': 5.545811176300049, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.29it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.49it/s]                                               {'loss': 0.5955, 'grad_norm': 2.2329673767089844, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.49it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.51it/s]                                               {'loss': 0.7034, 'grad_norm': 2.9975273609161377, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.51it/s] 60%|██████    | 18/30 [00:04<00:02,  4.92it/s]                                               {'loss': 0.8136, 'grad_norm': 4.0143842697143555, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.92it/s] 63%|██████▎   | 19/30 [00:04<00:02,  5.14it/s]                                               {'loss': 0.5846, 'grad_norm': 3.2481062412261963, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  5.14it/s] 67%|██████▋   | 20/30 [00:04<00:01,  5.06it/s]                                               {'loss': 0.6849, 'grad_norm': 7.286543846130371, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  5.06it/s] 70%|███████   | 21/30 [00:04<00:01,  5.20it/s]                                               {'loss': 0.7266, 'grad_norm': 3.8258159160614014, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.20it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.23it/s]                                               {'loss': 0.5855, 'grad_norm': 2.136831045150757, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.23it/s] 77%|███████▋  | 23/30 [00:05<00:01,  5.40it/s]                                               {'loss': 0.6458, 'grad_norm': 4.510700225830078, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  5.40it/s]                                               {'loss': 0.6261, 'grad_norm': 6.631382942199707, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.40it/s] 83%|████████▎ | 25/30 [00:05<00:00,  6.53it/s]                                               {'loss': 0.7244, 'grad_norm': 1.8931121826171875, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  6.53it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.67it/s]                                               {'loss': 0.5199, 'grad_norm': 2.8607118129730225, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.67it/s] 90%|█████████ | 27/30 [00:05<00:00,  5.23it/s]                                               {'loss': 0.6936, 'grad_norm': 6.846323490142822, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  5.23it/s] 93%|█████████▎| 28/30 [00:06<00:00,  4.80it/s]                                               {'loss': 0.7081, 'grad_norm': 2.1770355701446533, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  4.80it/s] 97%|█████████▋| 29/30 [00:06<00:00,  4.84it/s]                                               {'loss': 0.7415, 'grad_norm': 3.3048887252807617, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  4.84it/s]                                               {'loss': 0.754, 'grad_norm': 20.627864837646484, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.84it/s]                                               {'train_runtime': 6.4184, 'train_samples_per_second': 66.216, 'train_steps_per_second': 4.674, 'train_loss': 0.688184795777003, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.84it/s]100%|██████████| 30/30 [00:06<00:00,  4.68it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:01<00:37,  1.28s/it]                                              {'loss': 0.543, 'grad_norm': 0.41299372911453247, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:01<00:37,  1.28s/it]  7%|▋         | 2/30 [00:01<00:18,  1.52it/s]                                              {'loss': 0.7453, 'grad_norm': 0.7055904865264893, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:18,  1.52it/s] 10%|█         | 3/30 [00:01<00:12,  2.18it/s]                                              {'loss': 0.7556, 'grad_norm': 0.6429972648620605, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:12,  2.18it/s] 13%|█▎        | 4/30 [00:01<00:09,  2.82it/s]                                              {'loss': 0.7207, 'grad_norm': 2.2412989139556885, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:09,  2.82it/s] 17%|█▋        | 5/30 [00:02<00:06,  3.60it/s]                                              {'loss': 0.6888, 'grad_norm': 0.3912934958934784, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:02<00:06,  3.60it/s]                                              {'loss': 0.7213, 'grad_norm': 1.1220273971557617, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:06,  3.60it/s] 23%|██▎       | 7/30 [00:02<00:04,  5.21it/s]                                              {'loss': 0.6621, 'grad_norm': 0.2990509271621704, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:04,  5.21it/s] 27%|██▋       | 8/30 [00:02<00:03,  5.64it/s]                                              {'loss': 0.7656, 'grad_norm': 0.6844037771224976, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:03,  5.64it/s] 30%|███       | 9/30 [00:02<00:03,  5.87it/s]                                              {'loss': 0.6166, 'grad_norm': 0.5038142800331116, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:03,  5.87it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.13it/s]                                               {'loss': 0.6602, 'grad_norm': 2.6029186248779297, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.13it/s] 37%|███▋      | 11/30 [00:03<00:04,  4.71it/s]                                               {'loss': 0.6428, 'grad_norm': 0.3135198950767517, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:04,  4.71it/s] 40%|████      | 12/30 [00:03<00:03,  5.18it/s]                                               {'loss': 0.613, 'grad_norm': 1.0589452981948853, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:03,  5.18it/s] 43%|████▎     | 13/30 [00:03<00:03,  4.86it/s]                                               {'loss': 0.6123, 'grad_norm': 0.41332894563674927, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  4.86it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.66it/s]                                               {'loss': 0.5977, 'grad_norm': 0.40846341848373413, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.66it/s] 50%|█████     | 15/30 [00:03<00:03,  4.47it/s]                                               {'loss': 0.7691, 'grad_norm': 1.3168977499008179, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.47it/s] 53%|█████▎    | 16/30 [00:04<00:02,  4.77it/s]                                               {'loss': 0.7093, 'grad_norm': 1.0964741706848145, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:02,  4.77it/s] 57%|█████▋    | 17/30 [00:04<00:02,  5.09it/s]                                               {'loss': 0.6304, 'grad_norm': 0.4875562787055969, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:02,  5.09it/s] 60%|██████    | 18/30 [00:04<00:02,  5.54it/s]                                               {'loss': 0.7008, 'grad_norm': 0.7965680360794067, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  5.54it/s] 63%|██████▎   | 19/30 [00:04<00:02,  4.93it/s]                                               {'loss': 0.6594, 'grad_norm': 0.5958629846572876, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  4.93it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.52it/s]                                               {'loss': 0.7172, 'grad_norm': 0.8074501752853394, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.52it/s] 70%|███████   | 21/30 [00:05<00:02,  4.26it/s]                                               {'loss': 0.6784, 'grad_norm': 0.3688729703426361, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:02,  4.26it/s] 73%|███████▎  | 22/30 [00:05<00:02,  3.97it/s]                                               {'loss': 0.6525, 'grad_norm': 0.4409002959728241, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:02,  3.97it/s] 77%|███████▋  | 23/30 [00:05<00:01,  3.92it/s]                                               {'loss': 0.6519, 'grad_norm': 0.28302663564682007, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  3.92it/s] 80%|████████  | 24/30 [00:05<00:01,  4.66it/s]                                               {'loss': 0.6205, 'grad_norm': 0.6261743903160095, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  4.66it/s] 83%|████████▎ | 25/30 [00:06<00:01,  4.43it/s]                                               {'loss': 0.6678, 'grad_norm': 0.5109781622886658, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:01,  4.43it/s] 87%|████████▋ | 26/30 [00:06<00:00,  4.32it/s]                                               {'loss': 0.6647, 'grad_norm': 0.5105628967285156, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:00,  4.32it/s] 90%|█████████ | 27/30 [00:06<00:00,  4.19it/s]                                               {'loss': 0.679, 'grad_norm': 0.4531128704547882, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  4.19it/s] 93%|█████████▎| 28/30 [00:06<00:00,  3.94it/s]                                               {'loss': 0.6422, 'grad_norm': 0.45161741971969604, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  3.94it/s] 97%|█████████▋| 29/30 [00:07<00:00,  3.82it/s]                                               {'loss': 0.6437, 'grad_norm': 0.613097608089447, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  3.82it/s]                                               {'loss': 0.6204, 'grad_norm': 0.6627467274665833, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  3.82it/s]                                               {'train_runtime': 7.4451, 'train_samples_per_second': 57.085, 'train_steps_per_second': 4.029, 'train_loss': 0.668415363629659, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  3.82it/s]100%|██████████| 30/30 [00:07<00:00,  4.03it/s]
CLIENT:46
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.44it/s]                                              {'loss': 0.5633, 'grad_norm': 0.5854207277297974, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.44it/s]  7%|▋         | 2/30 [00:00<00:04,  6.19it/s]                                              {'loss': 0.7404, 'grad_norm': 0.6342999935150146, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.19it/s] 10%|█         | 3/30 [00:00<00:03,  7.25it/s]                                              {'loss': 0.7144, 'grad_norm': 0.5990151166915894, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.25it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.52it/s]                                              {'loss': 0.6885, 'grad_norm': 0.6786457896232605, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.52it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.45it/s]                                              {'loss': 0.7452, 'grad_norm': 0.4239652454853058, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.45it/s] 20%|██        | 6/30 [00:01<00:09,  2.44it/s]                                              {'loss': 0.7254, 'grad_norm': 1.3364129066467285, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:09,  2.44it/s] 23%|██▎       | 7/30 [00:01<00:07,  3.22it/s]                                              {'loss': 0.6537, 'grad_norm': 0.34530943632125854, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:07,  3.22it/s] 27%|██▋       | 8/30 [00:01<00:05,  3.90it/s]                                              {'loss': 0.6768, 'grad_norm': 0.3594856858253479, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  3.90it/s] 30%|███       | 9/30 [00:02<00:04,  4.37it/s]                                              {'loss': 0.6599, 'grad_norm': 0.48558303713798523, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.37it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.93it/s]                                               {'loss': 0.6756, 'grad_norm': 0.9808454513549805, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.93it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.61it/s]                                               {'loss': 0.6634, 'grad_norm': 0.20605602860450745, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.61it/s] 40%|████      | 12/30 [00:02<00:02,  6.41it/s]                                               {'loss': 0.6751, 'grad_norm': 2.552639961242676, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.41it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.29it/s]                                               {'loss': 0.6492, 'grad_norm': 0.3531004786491394, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.29it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.70it/s]                                               {'loss': 0.6793, 'grad_norm': 0.4463454782962799, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.70it/s] 50%|█████     | 15/30 [00:02<00:02,  6.58it/s]                                               {'loss': 0.586, 'grad_norm': 0.9918839335441589, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.58it/s] 53%|█████▎    | 16/30 [00:03<00:02,  6.57it/s]                                               {'loss': 0.6671, 'grad_norm': 0.4944559335708618, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  6.57it/s] 57%|█████▋    | 17/30 [00:03<00:01,  7.09it/s]                                               {'loss': 0.7151, 'grad_norm': 0.3300958275794983, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:01,  7.09it/s]                                               {'loss': 0.6564, 'grad_norm': 0.3519454300403595, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  7.09it/s] 63%|██████▎   | 19/30 [00:03<00:01,  7.33it/s]                                               {'loss': 0.7843, 'grad_norm': 0.7992430329322815, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  7.33it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.42it/s]                                               {'loss': 0.6648, 'grad_norm': 0.40983718633651733, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.42it/s] 70%|███████   | 21/30 [00:03<00:01,  7.89it/s]                                               {'loss': 0.6636, 'grad_norm': 0.29183077812194824, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.89it/s] 73%|███████▎  | 22/30 [00:03<00:00,  8.26it/s]                                               {'loss': 0.6664, 'grad_norm': 0.23806747794151306, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00,  8.26it/s] 77%|███████▋  | 23/30 [00:03<00:00,  7.84it/s]                                               {'loss': 0.6139, 'grad_norm': 0.4683302044868469, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.84it/s]                                               {'loss': 0.6302, 'grad_norm': 0.41009843349456787, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.84it/s] 83%|████████▎ | 25/30 [00:04<00:00,  8.67it/s]                                               {'loss': 0.7048, 'grad_norm': 0.4468801021575928, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  8.67it/s] 87%|████████▋ | 26/30 [00:04<00:00,  8.29it/s]                                               {'loss': 0.6433, 'grad_norm': 0.2620813846588135, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  8.29it/s] 90%|█████████ | 27/30 [00:04<00:00,  8.38it/s]                                               {'loss': 0.6792, 'grad_norm': 0.38702574372291565, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  8.38it/s] 93%|█████████▎| 28/30 [00:04<00:00,  8.34it/s]                                               {'loss': 0.6687, 'grad_norm': 0.23268699645996094, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  8.34it/s] 97%|█████████▋| 29/30 [00:04<00:00,  7.94it/s]                                               {'loss': 0.6593, 'grad_norm': 0.6058165431022644, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  7.94it/s]                                               {'loss': 0.6336, 'grad_norm': 0.41173794865608215, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.94it/s]                                               {'train_runtime': 4.8104, 'train_samples_per_second': 88.35, 'train_steps_per_second': 6.236, 'train_loss': 0.6715611199537913, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.94it/s]100%|██████████| 30/30 [00:04<00:00,  6.24it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.73it/s]                                              {'loss': 0.5303, 'grad_norm': 0.27255353331565857, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.73it/s]  7%|▋         | 2/30 [00:00<00:08,  3.38it/s]                                              {'loss': 0.6656, 'grad_norm': 1.1408518552780151, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:08,  3.38it/s] 10%|█         | 3/30 [00:00<00:08,  3.15it/s]                                              {'loss': 0.6779, 'grad_norm': 0.6986417174339294, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:08,  3.15it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.27it/s]                                              {'loss': 0.783, 'grad_norm': 1.481653094291687, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.27it/s] 17%|█▋        | 5/30 [00:01<00:07,  3.20it/s]                                              {'loss': 0.699, 'grad_norm': 1.5328783988952637, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:07,  3.20it/s] 20%|██        | 6/30 [00:01<00:07,  3.20it/s]                                              {'loss': 0.87, 'grad_norm': 1.9706063270568848, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:07,  3.20it/s] 23%|██▎       | 7/30 [00:02<00:06,  3.72it/s]                                              {'loss': 0.6275, 'grad_norm': 1.0736063718795776, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:06,  3.72it/s] 27%|██▋       | 8/30 [00:02<00:04,  4.47it/s]                                              {'loss': 0.6662, 'grad_norm': 0.5295930504798889, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:04,  4.47it/s] 30%|███       | 9/30 [00:02<00:04,  4.97it/s]                                              {'loss': 0.6993, 'grad_norm': 0.6919353008270264, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.97it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.29it/s]                                               {'loss': 0.6373, 'grad_norm': 0.6496529579162598, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.29it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.66it/s]                                               {'loss': 0.6545, 'grad_norm': 0.43096354603767395, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.66it/s]                                               {'loss': 0.6689, 'grad_norm': 0.8822867274284363, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.66it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.16it/s]                                               {'loss': 0.5989, 'grad_norm': 0.6507810354232788, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.16it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.21it/s]                                               {'loss': 0.6591, 'grad_norm': 1.5801807641983032, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.21it/s] 50%|█████     | 15/30 [00:03<00:02,  7.07it/s]                                               {'loss': 0.6286, 'grad_norm': 1.1055065393447876, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  7.07it/s] 53%|█████▎    | 16/30 [00:03<00:01,  7.44it/s]                                               {'loss': 0.6261, 'grad_norm': 0.6858233213424683, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:01,  7.44it/s] 57%|█████▋    | 17/30 [00:03<00:01,  7.75it/s]                                               {'loss': 0.6231, 'grad_norm': 0.6948395371437073, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:01,  7.75it/s]                                               {'loss': 0.6491, 'grad_norm': 0.7687691450119019, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  7.75it/s] 63%|██████▎   | 19/30 [00:03<00:01,  8.00it/s]                                               {'loss': 0.6409, 'grad_norm': 1.285109281539917, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  8.00it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.27it/s]                                               {'loss': 0.6375, 'grad_norm': 0.49457213282585144, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.27it/s] 70%|███████   | 21/30 [00:03<00:01,  6.93it/s]                                               {'loss': 0.6482, 'grad_norm': 0.5442277193069458, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  6.93it/s] 73%|███████▎  | 22/30 [00:04<00:01,  6.60it/s]                                               {'loss': 0.6045, 'grad_norm': 0.8936911821365356, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  6.60it/s] 77%|███████▋  | 23/30 [00:04<00:01,  6.41it/s]                                               {'loss': 0.644, 'grad_norm': 0.7616126537322998, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  6.41it/s]                                               {'loss': 0.561, 'grad_norm': 1.1317299604415894, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  6.41it/s] 83%|████████▎ | 25/30 [00:04<00:00,  6.61it/s]                                               {'loss': 0.6013, 'grad_norm': 1.0344816446304321, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  6.61it/s] 87%|████████▋ | 26/30 [00:04<00:00,  6.29it/s]                                               {'loss': 0.5693, 'grad_norm': 1.112840175628662, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  6.29it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.82it/s]                                               {'loss': 0.6005, 'grad_norm': 1.0629045963287354, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.82it/s] 93%|█████████▎| 28/30 [00:05<00:00,  5.85it/s]                                               {'loss': 0.6475, 'grad_norm': 0.7200037837028503, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  5.85it/s]                                               {'loss': 0.5174, 'grad_norm': 0.8076972365379333, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  5.85it/s]100%|██████████| 30/30 [00:05<00:00,  6.91it/s]                                               {'loss': 0.4847, 'grad_norm': 1.808764934539795, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.91it/s]                                               {'train_runtime': 5.6201, 'train_samples_per_second': 75.622, 'train_steps_per_second': 5.338, 'train_loss': 0.6373660176992416, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.91it/s]100%|██████████| 30/30 [00:05<00:00,  5.34it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.43it/s]                                              {'loss': 0.7015, 'grad_norm': 0.6042771935462952, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.43it/s]  7%|▋         | 2/30 [00:00<00:11,  2.50it/s]                                              {'loss': 0.708, 'grad_norm': 0.5431576371192932, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:11,  2.50it/s] 10%|█         | 3/30 [00:00<00:07,  3.69it/s]                                              {'loss': 0.6733, 'grad_norm': 0.7645735740661621, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.69it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.81it/s]                                              {'loss': 0.7134, 'grad_norm': 1.0900967121124268, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.81it/s]                                              {'loss': 0.7463, 'grad_norm': 0.9976215958595276, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.81it/s] 20%|██        | 6/30 [00:01<00:04,  5.35it/s]                                              {'loss': 0.7346, 'grad_norm': 1.126778483390808, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.35it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.12it/s]                                              {'loss': 0.6433, 'grad_norm': 0.4994463324546814, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.12it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.77it/s]                                              {'loss': 0.6929, 'grad_norm': 0.3405833840370178, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.77it/s] 30%|███       | 9/30 [00:02<00:04,  4.73it/s]                                              {'loss': 0.6966, 'grad_norm': 0.4639275372028351, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.73it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.60it/s]                                               {'loss': 0.6852, 'grad_norm': 0.363766610622406, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.60it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.32it/s]                                               {'loss': 0.6382, 'grad_norm': 0.4679147005081177, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.32it/s]                                               {'loss': 0.5261, 'grad_norm': 1.9294698238372803, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.32it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.65it/s]                                               {'loss': 0.643, 'grad_norm': 0.3364686667919159, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.65it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.14it/s]                                               {'loss': 0.6339, 'grad_norm': 0.39169245958328247, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.14it/s] 50%|█████     | 15/30 [00:03<00:02,  5.32it/s]                                               {'loss': 0.7079, 'grad_norm': 1.110992670059204, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.32it/s] 53%|█████▎    | 16/30 [00:03<00:02,  4.81it/s]                                               {'loss': 0.6143, 'grad_norm': 0.6364904642105103, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  4.81it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.71it/s]                                               {'loss': 0.5515, 'grad_norm': 1.3448714017868042, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.71it/s]                                               {'loss': 0.5393, 'grad_norm': 1.3997244834899902, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.71it/s] 63%|██████▎   | 19/30 [00:03<00:01,  5.92it/s]                                               {'loss': 0.5813, 'grad_norm': 0.9154794812202454, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  5.92it/s] 67%|██████▋   | 20/30 [00:03<00:01,  6.07it/s]                                               {'loss': 0.7219, 'grad_norm': 1.377721905708313, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  6.07it/s] 70%|███████   | 21/30 [00:04<00:01,  6.48it/s]                                               {'loss': 0.7066, 'grad_norm': 1.4011852741241455, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  6.48it/s]                                               {'loss': 0.547, 'grad_norm': 0.8676179647445679, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  6.48it/s] 77%|███████▋  | 23/30 [00:04<00:00,  7.20it/s]                                               {'loss': 0.6713, 'grad_norm': 1.1693899631500244, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:00,  7.20it/s]                                               {'loss': 0.687, 'grad_norm': 2.318146228790283, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  7.20it/s] 83%|████████▎ | 25/30 [00:04<00:00,  6.80it/s]                                               {'loss': 0.6727, 'grad_norm': 1.388673186302185, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  6.80it/s] 87%|████████▋ | 26/30 [00:04<00:00,  5.31it/s]                                               {'loss': 0.6767, 'grad_norm': 1.2096775770187378, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  5.31it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.72it/s]                                               {'loss': 0.5479, 'grad_norm': 1.2385262250900269, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.72it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.46it/s]                                               {'loss': 0.6105, 'grad_norm': 1.054732084274292, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.46it/s] 97%|█████████▋| 29/30 [00:05<00:00,  4.08it/s]                                               {'loss': 0.5421, 'grad_norm': 0.9386875629425049, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  4.08it/s]100%|██████████| 30/30 [00:05<00:00,  4.48it/s]                                               {'loss': 0.6089, 'grad_norm': 2.0444071292877197, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.48it/s]                                               {'train_runtime': 6.2128, 'train_samples_per_second': 68.407, 'train_steps_per_second': 4.829, 'train_loss': 0.6474399248758952, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.48it/s]100%|██████████| 30/30 [00:06<00:00,  4.84it/s]
CLIENT:39
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  5.85it/s]                                              {'loss': 0.6516, 'grad_norm': 0.6349821090698242, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  5.85it/s]  7%|▋         | 2/30 [00:00<00:03,  7.60it/s]                                              {'loss': 0.6313, 'grad_norm': 0.8458375930786133, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.60it/s] 10%|█         | 3/30 [00:00<00:03,  7.93it/s]                                              {'loss': 0.6129, 'grad_norm': 0.7680363655090332, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.93it/s]                                              {'loss': 0.4166, 'grad_norm': 1.034415364265442, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.93it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.91it/s]                                              {'loss': 0.6903, 'grad_norm': 1.3704224824905396, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.91it/s] 20%|██        | 6/30 [00:01<00:05,  4.41it/s]                                              {'loss': 0.5255, 'grad_norm': 1.4181183576583862, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.41it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.83it/s]                                              {'loss': 0.5354, 'grad_norm': 0.804004430770874, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.83it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.94it/s]                                              {'loss': 0.5805, 'grad_norm': 0.9575451612472534, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.94it/s] 30%|███       | 9/30 [00:01<00:03,  5.45it/s]                                              {'loss': 0.6327, 'grad_norm': 1.3174594640731812, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.45it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.12it/s]                                               {'loss': 0.4131, 'grad_norm': 0.9088037610054016, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.12it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.85it/s]                                               {'loss': 0.452, 'grad_norm': 0.60621178150177, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.85it/s]                                               {'loss': 0.3976, 'grad_norm': 0.8924490809440613, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.85it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.53it/s]                                               {'loss': 0.555, 'grad_norm': 0.9206560850143433, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.53it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.13it/s]                                               {'loss': 0.4416, 'grad_norm': 0.7057231068611145, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.13it/s]                                               {'loss': 0.7937, 'grad_norm': 2.7233524322509766, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.13it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.43it/s]                                               {'loss': 0.4352, 'grad_norm': 0.7582395672798157, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.43it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.31it/s]                                               {'loss': 0.4845, 'grad_norm': 0.9871441721916199, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.31it/s] 60%|██████    | 18/30 [00:02<00:01,  7.80it/s]                                               {'loss': 0.266, 'grad_norm': 2.254035711288452, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.80it/s]                                               {'loss': 0.4049, 'grad_norm': 0.9379171133041382, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.80it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.26it/s]                                               {'loss': 0.5606, 'grad_norm': 1.029675841331482, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.26it/s] 70%|███████   | 21/30 [00:03<00:01,  6.82it/s]                                               {'loss': 0.5178, 'grad_norm': 1.1590386629104614, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  6.82it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.89it/s]                                               {'loss': 0.4906, 'grad_norm': 1.3020881414413452, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.89it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.62it/s]                                               {'loss': 0.5825, 'grad_norm': 1.72816002368927, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.62it/s]                                               {'loss': 0.2468, 'grad_norm': 2.7000608444213867, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.62it/s] 83%|████████▎ | 25/30 [00:03<00:00,  9.07it/s]                                               {'loss': 0.2363, 'grad_norm': 1.8146487474441528, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  9.07it/s]                                               {'loss': 0.5199, 'grad_norm': 1.1776381731033325, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  9.07it/s] 90%|█████████ | 27/30 [00:03<00:00, 10.23it/s]                                               {'loss': 0.3652, 'grad_norm': 1.0170990228652954, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 10.23it/s]                                               {'loss': 0.5586, 'grad_norm': 2.5261168479919434, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 10.23it/s] 97%|█████████▋| 29/30 [00:03<00:00, 11.08it/s]                                               {'loss': 0.6012, 'grad_norm': 2.549813747406006, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 11.08it/s]                                               {'loss': 0.6569, 'grad_norm': 4.093917369842529, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.08it/s]                                               {'train_runtime': 4.0459, 'train_samples_per_second': 105.046, 'train_steps_per_second': 7.415, 'train_loss': 0.5085689276456833, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00, 11.08it/s]100%|██████████| 30/30 [00:04<00:00,  7.42it/s]
CLIENT:23
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.27it/s]                                              {'loss': 0.9253, 'grad_norm': 1.508190631866455, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.27it/s]                                              {'loss': 0.5129, 'grad_norm': 1.3071788549423218, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.27it/s] 10%|█         | 3/30 [00:00<00:02, 10.21it/s]                                              {'loss': 0.3928, 'grad_norm': 1.2345068454742432, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.21it/s]                                              {'loss': 0.1686, 'grad_norm': 1.0875375270843506, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.21it/s] 17%|█▋        | 5/30 [00:00<00:02, 10.80it/s]                                              {'loss': 0.3731, 'grad_norm': 2.802138090133667, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.80it/s]                                              {'loss': 0.0657, 'grad_norm': 0.9145628213882446, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 10.80it/s] 23%|██▎       | 7/30 [00:00<00:02, 10.58it/s]                                              {'loss': 0.0369, 'grad_norm': 0.50612473487854, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02, 10.58it/s]                                              {'loss': 0.3761, 'grad_norm': 1.0915247201919556, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02, 10.58it/s] 30%|███       | 9/30 [00:00<00:01, 10.92it/s]                                              {'loss': 0.431, 'grad_norm': 0.8906344175338745, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 10.92it/s]                                              {'loss': 0.3805, 'grad_norm': 3.6165032386779785, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 10.92it/s] 37%|███▋      | 11/30 [00:01<00:01, 11.15it/s]                                               {'loss': 0.6886, 'grad_norm': 2.6704070568084717, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 11.15it/s]                                               {'loss': 0.879, 'grad_norm': 4.696538925170898, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 11.15it/s] 43%|████▎     | 13/30 [00:01<00:01, 11.97it/s]                                               {'loss': 0.4679, 'grad_norm': 1.667542815208435, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 11.97it/s]                                               {'loss': 0.3585, 'grad_norm': 1.4706941843032837, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.97it/s] 50%|█████     | 15/30 [00:01<00:01, 11.77it/s]                                               {'loss': 0.0418, 'grad_norm': 0.34989336133003235, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 11.77it/s]                                               {'loss': 0.2606, 'grad_norm': 1.2947734594345093, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.77it/s] 57%|█████▋    | 17/30 [00:01<00:01, 11.66it/s]                                               {'loss': 0.4686, 'grad_norm': 2.176375150680542, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.66it/s]                                               {'loss': 0.0552, 'grad_norm': 1.0178213119506836, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 11.66it/s] 63%|██████▎   | 19/30 [00:01<00:00, 13.22it/s]                                               {'loss': 0.2254, 'grad_norm': 1.188273549079895, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.22it/s]                                               {'loss': 0.5176, 'grad_norm': 6.135257720947266, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.22it/s] 70%|███████   | 21/30 [00:01<00:00, 12.33it/s]                                               {'loss': 0.3212, 'grad_norm': 2.158946990966797, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 12.33it/s]                                               {'loss': 0.3946, 'grad_norm': 2.2555010318756104, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 12.33it/s] 77%|███████▋  | 23/30 [00:02<00:00, 11.58it/s]                                               {'loss': 0.0587, 'grad_norm': 0.6349114179611206, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 11.58it/s]                                               {'loss': 0.0782, 'grad_norm': 1.0060007572174072, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 11.58it/s] 83%|████████▎ | 25/30 [00:02<00:00, 11.19it/s]                                               {'loss': 0.0532, 'grad_norm': 0.7659337520599365, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 11.19it/s]                                               {'loss': 0.4101, 'grad_norm': 2.9134693145751953, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 11.19it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.98it/s]                                               {'loss': 0.4584, 'grad_norm': 3.1688072681427, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00,  9.98it/s]                                               {'loss': 0.2069, 'grad_norm': 0.8322734832763672, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00,  9.98it/s] 97%|█████████▋| 29/30 [00:02<00:00,  9.47it/s]                                               {'loss': 0.2023, 'grad_norm': 0.8027461767196655, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00,  9.47it/s]                                               {'loss': 0.0343, 'grad_norm': 0.5915707945823669, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00,  9.47it/s]                                               {'train_runtime': 2.9476, 'train_samples_per_second': 144.183, 'train_steps_per_second': 10.178, 'train_loss': 0.32814217420915764, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00,  9.47it/s]100%|██████████| 30/30 [00:02<00:00, 10.18it/s]
CLIENT:20
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:10,  2.73it/s]                                              {'loss': 0.5487, 'grad_norm': 0.5831143856048584, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:10,  2.73it/s]  7%|▋         | 2/30 [00:00<00:09,  2.81it/s]                                              {'loss': 0.757, 'grad_norm': 1.247288703918457, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:09,  2.81it/s] 10%|█         | 3/30 [00:01<00:09,  2.84it/s]                                              {'loss': 0.7437, 'grad_norm': 0.5409297943115234, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:09,  2.84it/s] 13%|█▎        | 4/30 [00:01<00:08,  2.90it/s]                                              {'loss': 0.6768, 'grad_norm': 0.45808321237564087, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  2.90it/s] 17%|█▋        | 5/30 [00:01<00:08,  2.98it/s]                                              {'loss': 0.6372, 'grad_norm': 0.3718552589416504, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:08,  2.98it/s] 20%|██        | 6/30 [00:01<00:06,  3.77it/s]                                              {'loss': 0.7695, 'grad_norm': 1.4867435693740845, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.77it/s] 23%|██▎       | 7/30 [00:02<00:06,  3.58it/s]                                              {'loss': 0.6929, 'grad_norm': 0.3484947979450226, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:06,  3.58it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.50it/s]                                              {'loss': 0.6705, 'grad_norm': 0.5187355279922485, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.50it/s] 30%|███       | 9/30 [00:02<00:06,  3.25it/s]                                              {'loss': 0.6539, 'grad_norm': 0.36793485283851624, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:06,  3.25it/s] 33%|███▎      | 10/30 [00:03<00:06,  3.03it/s]                                               {'loss': 0.6586, 'grad_norm': 0.2163398563861847, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:06,  3.03it/s] 37%|███▋      | 11/30 [00:03<00:06,  3.13it/s]                                               {'loss': 0.6857, 'grad_norm': 0.298843115568161, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:06,  3.13it/s] 40%|████      | 12/30 [00:03<00:04,  3.75it/s]                                               {'loss': 0.6777, 'grad_norm': 2.050280809402466, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:04,  3.75it/s] 43%|████▎     | 13/30 [00:03<00:04,  3.51it/s]                                               {'loss': 0.6849, 'grad_norm': 0.38577455282211304, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:04,  3.51it/s] 47%|████▋     | 14/30 [00:04<00:04,  3.34it/s]                                               {'loss': 0.7102, 'grad_norm': 0.82329922914505, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:04,  3.34it/s] 50%|█████     | 15/30 [00:04<00:04,  3.31it/s]                                               {'loss': 0.5722, 'grad_norm': 0.9355761408805847, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:04,  3.31it/s] 53%|█████▎    | 16/30 [00:04<00:04,  3.41it/s]                                               {'loss': 0.661, 'grad_norm': 0.48253554105758667, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:04,  3.41it/s] 57%|█████▋    | 17/30 [00:05<00:03,  3.53it/s]                                               {'loss': 0.7204, 'grad_norm': 0.5324337482452393, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:05<00:03,  3.53it/s]                                               {'loss': 0.62, 'grad_norm': 0.3807193636894226, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:03,  3.53it/s] 63%|██████▎   | 19/30 [00:05<00:02,  4.45it/s]                                               {'loss': 0.778, 'grad_norm': 1.963172197341919, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  4.45it/s] 67%|██████▋   | 20/30 [00:05<00:02,  4.45it/s]                                               {'loss': 0.6573, 'grad_norm': 0.4407216012477875, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  4.45it/s] 70%|███████   | 21/30 [00:05<00:01,  4.53it/s]                                               {'loss': 0.6603, 'grad_norm': 0.258761465549469, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:01,  4.53it/s] 73%|███████▎  | 22/30 [00:06<00:01,  4.59it/s]                                               {'loss': 0.7197, 'grad_norm': 0.46701744198799133, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:06<00:01,  4.59it/s] 77%|███████▋  | 23/30 [00:06<00:01,  4.67it/s]                                               {'loss': 0.6049, 'grad_norm': 0.3709079921245575, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  4.67it/s] 80%|████████  | 24/30 [00:06<00:01,  5.50it/s]                                               {'loss': 0.6232, 'grad_norm': 0.37061426043510437, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  5.50it/s] 83%|████████▎ | 25/30 [00:06<00:00,  5.05it/s]                                               {'loss': 0.7099, 'grad_norm': 0.5023810863494873, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:00,  5.05it/s] 87%|████████▋ | 26/30 [00:06<00:00,  4.83it/s]                                               {'loss': 0.6543, 'grad_norm': 0.3067324757575989, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:00,  4.83it/s] 90%|█████████ | 27/30 [00:07<00:00,  4.36it/s]                                               {'loss': 0.6839, 'grad_norm': 2.0500600337982178, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:07<00:00,  4.36it/s] 93%|█████████▎| 28/30 [00:07<00:00,  4.04it/s]                                               {'loss': 0.6586, 'grad_norm': 0.17438752949237823, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  4.04it/s] 97%|█████████▋| 29/30 [00:07<00:00,  3.78it/s]                                               {'loss': 0.6246, 'grad_norm': 0.6122817993164062, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  3.78it/s]100%|██████████| 30/30 [00:07<00:00,  4.24it/s]                                               {'loss': 0.6614, 'grad_norm': 0.34588944911956787, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  4.24it/s]                                               {'train_runtime': 8.0608, 'train_samples_per_second': 52.724, 'train_steps_per_second': 3.722, 'train_loss': 0.67257386247317, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.24it/s]100%|██████████| 30/30 [00:08<00:00,  3.73it/s]
CLIENT:70
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.99it/s]                                              {'loss': 0.9552, 'grad_norm': 1.9012408256530762, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.99it/s]  7%|▋         | 2/30 [00:00<00:06,  4.57it/s]                                              {'loss': 0.4969, 'grad_norm': 1.3426625728607178, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.57it/s] 10%|█         | 3/30 [00:00<00:05,  4.79it/s]                                              {'loss': 0.267, 'grad_norm': 1.7391657829284668, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.79it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.91it/s]                                              {'loss': 0.4463, 'grad_norm': 1.3839335441589355, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.91it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.82it/s]                                              {'loss': 0.1099, 'grad_norm': 1.0740886926651, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.82it/s] 20%|██        | 6/30 [00:01<00:05,  4.77it/s]                                              {'loss': 2.2775, 'grad_norm': 10.431520462036133, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.77it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.70it/s]                                              {'loss': 0.2915, 'grad_norm': 2.007134437561035, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.70it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.98it/s]                                              {'loss': 0.0418, 'grad_norm': 0.8942015767097473, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.98it/s] 30%|███       | 9/30 [00:01<00:03,  5.35it/s]                                              {'loss': 0.4256, 'grad_norm': 6.865808486938477, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.35it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.36it/s]                                               {'loss': 0.4202, 'grad_norm': 1.4434237480163574, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.36it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.06it/s]                                               {'loss': 0.1564, 'grad_norm': 0.9648067355155945, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.06it/s] 40%|████      | 12/30 [00:02<00:03,  5.35it/s]                                               {'loss': 0.0355, 'grad_norm': 1.0068196058273315, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.35it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.14it/s]                                               {'loss': 0.0358, 'grad_norm': 0.5795668959617615, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.14it/s] 47%|████▋     | 14/30 [00:02<00:03,  5.31it/s]                                               {'loss': 0.0286, 'grad_norm': 0.4520409107208252, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  5.31it/s] 50%|█████     | 15/30 [00:02<00:02,  5.04it/s]                                               {'loss': 0.5272, 'grad_norm': 12.041385650634766, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.04it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.59it/s]                                               {'loss': 0.2212, 'grad_norm': 0.9012609124183655, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.59it/s] 57%|█████▋    | 17/30 [00:03<00:03,  4.31it/s]                                               {'loss': 0.4689, 'grad_norm': 1.846665859222412, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  4.31it/s] 60%|██████    | 18/30 [00:03<00:02,  4.61it/s]                                               {'loss': 0.9604, 'grad_norm': 4.908602714538574, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.61it/s] 63%|██████▎   | 19/30 [00:04<00:02,  4.08it/s]                                               {'loss': 0.2293, 'grad_norm': 0.7873929738998413, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  4.08it/s] 67%|██████▋   | 20/30 [00:04<00:02,  3.68it/s]                                               {'loss': 0.0504, 'grad_norm': 0.7260047793388367, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  3.68it/s] 70%|███████   | 21/30 [00:04<00:02,  3.52it/s]                                               {'loss': 0.0415, 'grad_norm': 0.6518834233283997, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:02,  3.52it/s] 73%|███████▎  | 22/30 [00:05<00:02,  3.30it/s]                                               {'loss': 0.4015, 'grad_norm': 1.9885119199752808, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:02,  3.30it/s] 77%|███████▋  | 23/30 [00:05<00:02,  3.32it/s]                                               {'loss': 0.6821, 'grad_norm': 3.988610029220581, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:02,  3.32it/s] 80%|████████  | 24/30 [00:05<00:01,  3.59it/s]                                               {'loss': 0.0539, 'grad_norm': 0.974714457988739, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  3.59it/s] 83%|████████▎ | 25/30 [00:05<00:01,  3.47it/s]                                               {'loss': 0.3458, 'grad_norm': 0.8940998911857605, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  3.47it/s] 87%|████████▋ | 26/30 [00:06<00:01,  3.52it/s]                                               {'loss': 0.2238, 'grad_norm': 2.168091297149658, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:01,  3.52it/s] 90%|█████████ | 27/30 [00:06<00:00,  3.37it/s]                                               {'loss': 0.4773, 'grad_norm': 2.4217517375946045, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  3.37it/s] 93%|█████████▎| 28/30 [00:06<00:00,  3.54it/s]                                               {'loss': 0.0576, 'grad_norm': 0.9430259466171265, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  3.54it/s] 97%|█████████▋| 29/30 [00:06<00:00,  3.61it/s]                                               {'loss': 0.4639, 'grad_norm': 1.2455275058746338, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  3.61it/s]100%|██████████| 30/30 [00:07<00:00,  3.84it/s]                                               {'loss': 0.0657, 'grad_norm': 0.9509998559951782, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  3.84it/s]                                               {'train_runtime': 7.3908, 'train_samples_per_second': 57.504, 'train_steps_per_second': 4.059, 'train_loss': 0.3752886970217029, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  3.84it/s]100%|██████████| 30/30 [00:07<00:00,  4.06it/s]
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:11,  2.42it/s]                                              {'loss': 0.5702, 'grad_norm': 0.371577650308609, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:11,  2.42it/s]  7%|▋         | 2/30 [00:00<00:09,  2.94it/s]                                              {'loss': 0.7282, 'grad_norm': 0.7633287906646729, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:09,  2.94it/s] 10%|█         | 3/30 [00:00<00:06,  3.89it/s]                                              {'loss': 0.7141, 'grad_norm': 0.6932321190834045, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  3.89it/s] 13%|█▎        | 4/30 [00:01<00:05,  4.37it/s]                                              {'loss': 0.7235, 'grad_norm': 0.75788414478302, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:05,  4.37it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.67it/s]                                              {'loss': 0.6369, 'grad_norm': 0.3504068851470947, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.67it/s] 20%|██        | 6/30 [00:01<00:07,  3.43it/s]                                              {'loss': 0.9594, 'grad_norm': 1.711085557937622, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:07,  3.43it/s]                                              {'loss': 0.6356, 'grad_norm': 0.5092048645019531, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:06,  3.43it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.18it/s]                                              {'loss': 0.7507, 'grad_norm': 0.7006564736366272, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.18it/s]                                              {'loss': 0.6662, 'grad_norm': 0.6231896877288818, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  5.18it/s] 33%|███▎      | 10/30 [00:02<00:03,  6.52it/s]                                               {'loss': 0.6374, 'grad_norm': 0.3816564679145813, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  6.52it/s]                                               {'loss': 0.6792, 'grad_norm': 0.2290671020746231, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:02,  6.52it/s] 40%|████      | 12/30 [00:02<00:02,  8.08it/s]                                               {'loss': 0.586, 'grad_norm': 1.2323909997940063, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  8.08it/s] 43%|████▎     | 13/30 [00:02<00:02,  8.12it/s]                                               {'loss': 0.6419, 'grad_norm': 0.39655137062072754, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  8.12it/s] 47%|████▋     | 14/30 [00:02<00:01,  8.13it/s]                                               {'loss': 0.632, 'grad_norm': 0.33901265263557434, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  8.13it/s] 50%|█████     | 15/30 [00:02<00:01,  8.21it/s]                                               {'loss': 0.6559, 'grad_norm': 0.6891651153564453, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.21it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.13it/s]                                               {'loss': 0.6671, 'grad_norm': 0.3279907703399658, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.13it/s]                                               {'loss': 0.6433, 'grad_norm': 0.34965044260025024, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  6.13it/s] 60%|██████    | 18/30 [00:03<00:01,  8.16it/s]                                               {'loss': 0.6975, 'grad_norm': 0.46868082880973816, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  8.16it/s]                                               {'loss': 0.7016, 'grad_norm': 0.5516441464424133, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  8.16it/s] 67%|██████▋   | 20/30 [00:03<00:01,  9.06it/s]                                               {'loss': 0.6753, 'grad_norm': 0.26800525188446045, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  9.06it/s] 70%|███████   | 21/30 [00:03<00:00,  9.22it/s]                                               {'loss': 0.6496, 'grad_norm': 0.39775004982948303, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:00,  9.22it/s] 73%|███████▎  | 22/30 [00:03<00:00,  9.34it/s]                                               {'loss': 0.6775, 'grad_norm': 0.6640549898147583, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00,  9.34it/s]                                               {'loss': 0.7239, 'grad_norm': 0.8701005578041077, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  9.34it/s] 80%|████████  | 24/30 [00:03<00:00,  9.60it/s]                                               {'loss': 0.6315, 'grad_norm': 0.5064203143119812, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  9.60it/s] 83%|████████▎ | 25/30 [00:03<00:00,  8.46it/s]                                               {'loss': 0.6319, 'grad_norm': 0.4314761757850647, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  8.46it/s] 87%|████████▋ | 26/30 [00:04<00:00,  4.84it/s]                                               {'loss': 0.6388, 'grad_norm': 0.37332722544670105, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  4.84it/s] 90%|█████████ | 27/30 [00:04<00:00,  4.35it/s]                                               {'loss': 0.6036, 'grad_norm': 0.5791561603546143, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  4.35it/s]                                               {'loss': 0.6645, 'grad_norm': 0.36553245782852173, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  4.35it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.72it/s]                                               {'loss': 0.6764, 'grad_norm': 0.8612161874771118, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.72it/s]                                               {'loss': 0.6379, 'grad_norm': 0.9235072731971741, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.72it/s]                                               {'train_runtime': 5.0244, 'train_samples_per_second': 84.588, 'train_steps_per_second': 5.971, 'train_loss': 0.671248992284139, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.72it/s]100%|██████████| 30/30 [00:05<00:00,  5.98it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  5%|▍         | 3/66 [00:00<00:03, 20.22it/s]  9%|▉         | 6/66 [00:00<00:02, 20.90it/s] 14%|█▎        | 9/66 [00:00<00:02, 22.92it/s] 18%|█▊        | 12/66 [00:00<00:02, 22.00it/s] 23%|██▎       | 15/66 [00:00<00:02, 20.58it/s] 27%|██▋       | 18/66 [00:00<00:02, 20.63it/s] 32%|███▏      | 21/66 [00:01<00:02, 18.75it/s] 35%|███▍      | 23/66 [00:01<00:02, 18.37it/s] 38%|███▊      | 25/66 [00:01<00:02, 18.41it/s] 41%|████      | 27/66 [00:01<00:02, 17.10it/s] 44%|████▍     | 29/66 [00:01<00:02, 17.53it/s] 48%|████▊     | 32/66 [00:01<00:01, 19.80it/s] 53%|█████▎    | 35/66 [00:01<00:01, 20.22it/s] 58%|█████▊    | 38/66 [00:01<00:01, 19.58it/s] 62%|██████▏   | 41/66 [00:02<00:01, 21.48it/s] 67%|██████▋   | 44/66 [00:02<00:01, 20.50it/s] 71%|███████   | 47/66 [00:02<00:00, 19.10it/s] 76%|███████▌  | 50/66 [00:02<00:00, 20.75it/s] 80%|████████  | 53/66 [00:02<00:00, 21.38it/s] 85%|████████▍ | 56/66 [00:02<00:00, 20.97it/s] 89%|████████▉ | 59/66 [00:02<00:00, 19.75it/s] 94%|█████████▍| 62/66 [00:03<00:00, 17.97it/s] 97%|█████████▋| 64/66 [00:03<00:00, 18.28it/s]100%|██████████| 66/66 [00:03<00:00, 19.89it/s]
{'eval_loss': 0.6611771583557129, 'eval_model_preparation_time': 0.0093, 'eval_acc': 0.6673058485139022, 'eval_runtime': 3.4023, 'eval_samples_per_second': 306.557, 'eval_steps_per_second': 19.399}
ROUND:3
CLIENT:17
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.60it/s]                                              {'loss': 0.6905, 'grad_norm': 0.6629777550697327, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.60it/s]                                              {'loss': 0.7138, 'grad_norm': 0.7780011892318726, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.60it/s] 10%|█         | 3/30 [00:00<00:03,  7.47it/s]                                              {'loss': 0.7007, 'grad_norm': 0.4967832565307617, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.47it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.95it/s]                                              {'loss': 0.6765, 'grad_norm': 0.4955689311027527, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.95it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.72it/s]                                              {'loss': 0.6934, 'grad_norm': 0.39206022024154663, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.72it/s]                                              {'loss': 0.759, 'grad_norm': 1.5610402822494507, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.72it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.32it/s]                                              {'loss': 0.6081, 'grad_norm': 0.3547009825706482, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.32it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.53it/s]                                              {'loss': 0.6834, 'grad_norm': 0.29064902663230896, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.53it/s] 30%|███       | 9/30 [00:01<00:02,  7.16it/s]                                              {'loss': 0.7445, 'grad_norm': 0.41737258434295654, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.16it/s] 33%|███▎      | 10/30 [00:01<00:02,  6.99it/s]                                               {'loss': 0.6558, 'grad_norm': 0.19405363500118256, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  6.99it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.02it/s]                                               {'loss': 0.6766, 'grad_norm': 0.2520662844181061, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.02it/s]                                               {'loss': 0.4775, 'grad_norm': 1.166066288948059, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.02it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.34it/s]                                               {'loss': 0.6391, 'grad_norm': 0.6048226356506348, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.34it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.08it/s]                                               {'loss': 0.6036, 'grad_norm': 0.5420138239860535, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.08it/s] 50%|█████     | 15/30 [00:01<00:01,  7.65it/s]                                               {'loss': 0.7121, 'grad_norm': 1.6925572156906128, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  7.65it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.16it/s]                                               {'loss': 0.6311, 'grad_norm': 0.42818352580070496, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.16it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.05it/s]                                               {'loss': 0.6256, 'grad_norm': 0.506276547908783, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.05it/s]                                               {'loss': 0.6165, 'grad_norm': 0.6165218949317932, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.05it/s] 63%|██████▎   | 19/30 [00:02<00:01,  7.42it/s]                                               {'loss': 0.5343, 'grad_norm': 1.6306321620941162, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.42it/s] 67%|██████▋   | 20/30 [00:02<00:01,  7.23it/s]                                               {'loss': 0.7031, 'grad_norm': 0.9176581501960754, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  7.23it/s] 70%|███████   | 21/30 [00:02<00:01,  6.78it/s]                                               {'loss': 0.7191, 'grad_norm': 0.6626479029655457, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  6.78it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.84it/s]                                               {'loss': 0.6074, 'grad_norm': 0.699832558631897, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.84it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.72it/s]                                               {'loss': 0.737, 'grad_norm': 1.4716318845748901, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.72it/s]                                               {'loss': 0.5484, 'grad_norm': 2.575753688812256, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.72it/s] 83%|████████▎ | 25/30 [00:03<00:00,  6.96it/s]                                               {'loss': 0.6779, 'grad_norm': 0.5711781978607178, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  6.96it/s] 87%|████████▋ | 26/30 [00:03<00:00,  6.34it/s]                                               {'loss': 0.5735, 'grad_norm': 0.7545061111450195, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  6.34it/s] 90%|█████████ | 27/30 [00:03<00:00,  5.82it/s]                                               {'loss': 0.6605, 'grad_norm': 0.597010612487793, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  5.82it/s] 93%|█████████▎| 28/30 [00:04<00:00,  5.45it/s]                                               {'loss': 0.6914, 'grad_norm': 0.9469701647758484, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  5.45it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.29it/s]                                               {'loss': 0.595, 'grad_norm': 0.7311157584190369, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.29it/s]                                               {'loss': 0.5931, 'grad_norm': 0.5572068691253662, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.29it/s]                                               {'train_runtime': 4.5351, 'train_samples_per_second': 93.713, 'train_steps_per_second': 6.615, 'train_loss': 0.6516158958276113, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.29it/s]100%|██████████| 30/30 [00:04<00:00,  6.62it/s]
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.59it/s]                                              {'loss': 0.7325, 'grad_norm': 1.6052088737487793, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.59it/s]  7%|▋         | 2/30 [00:00<00:04,  6.85it/s]                                              {'loss': 0.6858, 'grad_norm': 0.5395802855491638, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.85it/s] 10%|█         | 3/30 [00:00<00:03,  6.84it/s]                                              {'loss': 0.7123, 'grad_norm': 0.5809537172317505, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  6.84it/s]                                              {'loss': 0.6713, 'grad_norm': 0.6326733231544495, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.84it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.88it/s]                                              {'loss': 0.7085, 'grad_norm': 0.7456231713294983, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.88it/s] 20%|██        | 6/30 [00:01<00:06,  3.64it/s]                                              {'loss': 0.7632, 'grad_norm': 1.7539509534835815, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.64it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.28it/s]                                              {'loss': 0.6707, 'grad_norm': 0.38737916946411133, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.28it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.92it/s]                                              {'loss': 0.6698, 'grad_norm': 0.40350061655044556, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.92it/s] 30%|███       | 9/30 [00:01<00:03,  5.60it/s]                                              {'loss': 0.6429, 'grad_norm': 0.43204566836357117, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.60it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.08it/s]                                               {'loss': 0.6928, 'grad_norm': 0.4064195156097412, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.08it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.49it/s]                                               {'loss': 0.6889, 'grad_norm': 0.35320886969566345, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.49it/s]                                               {'loss': 0.7617, 'grad_norm': 1.4868648052215576, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.49it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.12it/s]                                               {'loss': 0.7094, 'grad_norm': 0.5554801225662231, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.12it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.72it/s]                                               {'loss': 0.6895, 'grad_norm': 0.4603515863418579, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.72it/s] 50%|█████     | 15/30 [00:02<00:02,  6.21it/s]                                               {'loss': 0.5377, 'grad_norm': 0.783046305179596, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.21it/s] 53%|█████▎    | 16/30 [00:02<00:02,  5.29it/s]                                               {'loss': 0.6776, 'grad_norm': 0.5231314897537231, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  5.29it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.18it/s]                                               {'loss': 0.7098, 'grad_norm': 0.6186106204986572, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.18it/s] 60%|██████    | 18/30 [00:03<00:02,  5.64it/s]                                               {'loss': 0.6413, 'grad_norm': 0.5677390098571777, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.64it/s] 63%|██████▎   | 19/30 [00:03<00:02,  5.29it/s]                                               {'loss': 0.6981, 'grad_norm': 0.5716354250907898, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  5.29it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.24it/s]                                               {'loss': 0.6602, 'grad_norm': 0.29925858974456787, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.24it/s] 70%|███████   | 21/30 [00:03<00:01,  4.77it/s]                                               {'loss': 0.6559, 'grad_norm': 0.3457719385623932, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  4.77it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.70it/s]                                               {'loss': 0.6601, 'grad_norm': 0.4031856656074524, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.70it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.99it/s]                                               {'loss': 0.6162, 'grad_norm': 0.449916273355484, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.99it/s]                                               {'loss': 0.6433, 'grad_norm': 0.5283291339874268, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  4.99it/s] 83%|████████▎ | 25/30 [00:04<00:00,  5.44it/s]                                               {'loss': 0.6829, 'grad_norm': 0.36753302812576294, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  5.44it/s] 87%|████████▋ | 26/30 [00:04<00:00,  5.65it/s]                                               {'loss': 0.6616, 'grad_norm': 0.4156728684902191, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  5.65it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.66it/s]                                               {'loss': 0.6641, 'grad_norm': 0.3569435775279999, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.66it/s] 93%|█████████▎| 28/30 [00:05<00:00,  5.85it/s]                                               {'loss': 0.6443, 'grad_norm': 0.3791729509830475, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  5.85it/s] 97%|█████████▋| 29/30 [00:05<00:00,  6.07it/s]                                               {'loss': 0.5988, 'grad_norm': 0.5125178694725037, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  6.07it/s]                                               {'loss': 0.6738, 'grad_norm': 0.7208172678947449, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.07it/s]                                               {'train_runtime': 5.3762, 'train_samples_per_second': 79.052, 'train_steps_per_second': 5.58, 'train_loss': 0.6741641481717427, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.07it/s]100%|██████████| 30/30 [00:05<00:00,  5.58it/s]
CLIENT:76
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.70it/s]                                              {'loss': 0.7886, 'grad_norm': 0.6995364427566528, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.70it/s]  7%|▋         | 2/30 [00:00<00:04,  6.61it/s]                                              {'loss': 0.6335, 'grad_norm': 0.7950584888458252, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.61it/s] 10%|█         | 3/30 [00:00<00:04,  6.13it/s]                                              {'loss': 0.6573, 'grad_norm': 0.7884071469306946, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.13it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.35it/s]                                              {'loss': 0.7164, 'grad_norm': 1.0558314323425293, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.35it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.90it/s]                                              {'loss': 0.7229, 'grad_norm': 1.25565505027771, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.90it/s] 20%|██        | 6/30 [00:00<00:03,  6.48it/s]                                              {'loss': 0.9319, 'grad_norm': 1.4635181427001953, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.48it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.64it/s]                                              {'loss': 0.5946, 'grad_norm': 0.3857201039791107, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.64it/s]                                              {'loss': 0.7277, 'grad_norm': 0.6705523133277893, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.64it/s] 30%|███       | 9/30 [00:01<00:02,  8.42it/s]                                              {'loss': 0.7157, 'grad_norm': 0.6107671856880188, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  8.42it/s]                                              {'loss': 0.6557, 'grad_norm': 0.5490866899490356, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.42it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.44it/s]                                               {'loss': 0.6793, 'grad_norm': 0.27676552534103394, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  9.44it/s]                                               {'loss': 0.5749, 'grad_norm': 1.1637276411056519, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01,  9.44it/s] 43%|████▎     | 13/30 [00:01<00:01, 11.01it/s]                                               {'loss': 0.6423, 'grad_norm': 0.40650272369384766, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 11.01it/s]                                               {'loss': 0.6275, 'grad_norm': 0.35966190695762634, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.01it/s] 50%|█████     | 15/30 [00:01<00:01, 10.29it/s]                                               {'loss': 0.6703, 'grad_norm': 0.9047024846076965, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 10.29it/s]                                               {'loss': 0.6388, 'grad_norm': 0.5682793259620667, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 10.29it/s] 57%|█████▋    | 17/30 [00:01<00:01, 10.39it/s]                                               {'loss': 0.6613, 'grad_norm': 0.6468319296836853, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 10.39it/s]                                               {'loss': 0.6212, 'grad_norm': 0.7520884871482849, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 10.39it/s] 63%|██████▎   | 19/30 [00:02<00:00, 11.04it/s]                                               {'loss': 0.6771, 'grad_norm': 0.8833714723587036, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 11.04it/s]                                               {'loss': 0.6568, 'grad_norm': 0.3684196174144745, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 11.04it/s] 70%|███████   | 21/30 [00:02<00:00, 10.91it/s]                                               {'loss': 0.6505, 'grad_norm': 0.47656333446502686, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 10.91it/s]                                               {'loss': 0.6367, 'grad_norm': 0.4596748948097229, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 10.91it/s] 77%|███████▋  | 23/30 [00:02<00:00, 11.21it/s]                                               {'loss': 0.6449, 'grad_norm': 0.3731060028076172, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 11.21it/s]                                               {'loss': 0.5745, 'grad_norm': 0.838811457157135, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 11.21it/s] 83%|████████▎ | 25/30 [00:02<00:00, 10.10it/s]                                               {'loss': 0.6353, 'grad_norm': 0.46048325300216675, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 10.10it/s]                                               {'loss': 0.6005, 'grad_norm': 0.6156366467475891, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 10.10it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.82it/s]                                               {'loss': 0.7088, 'grad_norm': 0.6415652632713318, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00,  9.82it/s]                                               {'loss': 0.6331, 'grad_norm': 6.625977039337158, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  9.82it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.66it/s]                                               {'loss': 0.6836, 'grad_norm': 0.6735067963600159, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  9.66it/s]                                               {'loss': 0.5561, 'grad_norm': 1.0219379663467407, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  9.66it/s]                                               {'train_runtime': 3.3554, 'train_samples_per_second': 126.662, 'train_steps_per_second': 8.941, 'train_loss': 0.6639249622821808, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  9.66it/s]100%|██████████| 30/30 [00:03<00:00,  8.95it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  7.26it/s]                                              {'loss': 0.7008, 'grad_norm': 0.5006536841392517, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  7.26it/s]                                              {'loss': 0.706, 'grad_norm': 0.541925311088562, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.26it/s] 10%|█         | 3/30 [00:00<00:02, 10.83it/s]                                              {'loss': 0.6722, 'grad_norm': 0.7679621577262878, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.83it/s]                                              {'loss': 0.7084, 'grad_norm': 1.064225196838379, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.83it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.27it/s]                                              {'loss': 0.7523, 'grad_norm': 0.990536093711853, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.27it/s]                                              {'loss': 0.7295, 'grad_norm': 1.150002121925354, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:02, 11.27it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.77it/s]                                              {'loss': 0.6397, 'grad_norm': 0.5118078589439392, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.77it/s]                                              {'loss': 0.6901, 'grad_norm': 0.33061128854751587, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.77it/s] 30%|███       | 9/30 [00:01<00:03,  5.95it/s]                                              {'loss': 0.6941, 'grad_norm': 0.47900134325027466, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.95it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.77it/s]                                               {'loss': 0.6873, 'grad_norm': 0.40018415451049805, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.77it/s] 37%|███▋      | 11/30 [00:01<00:03,  6.07it/s]                                               {'loss': 0.6273, 'grad_norm': 0.48260200023651123, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  6.07it/s] 40%|████      | 12/30 [00:01<00:02,  6.44it/s]                                               {'loss': 0.5036, 'grad_norm': 1.9174833297729492, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.44it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.22it/s]                                               {'loss': 0.6357, 'grad_norm': 0.3785097301006317, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.22it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.87it/s]                                               {'loss': 0.6306, 'grad_norm': 0.44173020124435425, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.87it/s] 50%|█████     | 15/30 [00:02<00:02,  7.40it/s]                                               {'loss': 0.7065, 'grad_norm': 1.3393625020980835, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.40it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.34it/s]                                               {'loss': 0.6077, 'grad_norm': 0.6719916462898254, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.34it/s]                                               {'loss': 0.5294, 'grad_norm': 1.4804928302764893, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.34it/s] 60%|██████    | 18/30 [00:02<00:01,  8.18it/s]                                               {'loss': 0.5236, 'grad_norm': 1.4367601871490479, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  8.18it/s] 63%|██████▎   | 19/30 [00:02<00:01,  7.86it/s]                                               {'loss': 0.56, 'grad_norm': 0.9777815937995911, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.86it/s]                                               {'loss': 0.7145, 'grad_norm': 1.4779331684112549, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  7.86it/s] 70%|███████   | 21/30 [00:02<00:00,  9.05it/s]                                               {'loss': 0.6839, 'grad_norm': 1.4006528854370117, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00,  9.05it/s]                                               {'loss': 0.5492, 'grad_norm': 1.0647355318069458, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  9.05it/s] 77%|███████▋  | 23/30 [00:03<00:00, 10.13it/s]                                               {'loss': 0.6495, 'grad_norm': 1.2756214141845703, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00, 10.13it/s]                                               {'loss': 0.6799, 'grad_norm': 2.560213088989258, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00, 10.13it/s] 83%|████████▎ | 25/30 [00:03<00:00,  9.58it/s]                                               {'loss': 0.6551, 'grad_norm': 1.2488186359405518, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  9.58it/s] 87%|████████▋ | 26/30 [00:03<00:00,  8.96it/s]                                               {'loss': 0.6534, 'grad_norm': 1.1121448278427124, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  8.96it/s] 90%|█████████ | 27/30 [00:03<00:00,  8.77it/s]                                               {'loss': 0.5321, 'grad_norm': 1.258259654045105, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  8.77it/s]                                               {'loss': 0.5832, 'grad_norm': 1.1068609952926636, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  8.77it/s] 97%|█████████▋| 29/30 [00:03<00:00,  8.78it/s]                                               {'loss': 0.5161, 'grad_norm': 1.1084455251693726, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  8.78it/s]100%|██████████| 30/30 [00:04<00:00,  7.18it/s]                                               {'loss': 0.5651, 'grad_norm': 2.133570432662964, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.18it/s]                                               {'train_runtime': 4.138, 'train_samples_per_second': 102.706, 'train_steps_per_second': 7.25, 'train_loss': 0.6362304906050364, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.18it/s]100%|██████████| 30/30 [00:04<00:00,  7.25it/s]
CLIENT:56
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.24it/s]                                              {'loss': 0.7792, 'grad_norm': 1.5806035995483398, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.24it/s]  7%|▋         | 2/30 [00:00<00:05,  5.54it/s]                                              {'loss': 0.5596, 'grad_norm': 0.9289037585258484, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.54it/s] 10%|█         | 3/30 [00:00<00:03,  6.93it/s]                                              {'loss': 0.6401, 'grad_norm': 0.6959248781204224, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  6.93it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.10it/s]                                              {'loss': 0.9091, 'grad_norm': 2.037144422531128, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.10it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.91it/s]                                              {'loss': 0.9532, 'grad_norm': 2.1467740535736084, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.91it/s]                                              {'loss': 0.8527, 'grad_norm': 2.640935182571411, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.91it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.24it/s]                                              {'loss': 0.4707, 'grad_norm': 1.2748593091964722, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  9.24it/s] 27%|██▋       | 8/30 [00:01<00:02,  8.90it/s]                                              {'loss': 0.5744, 'grad_norm': 0.5593991875648499, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  8.90it/s] 30%|███       | 9/30 [00:01<00:02,  8.25it/s]                                              {'loss': 0.7799, 'grad_norm': 1.8360166549682617, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  8.25it/s] 33%|███▎      | 10/30 [00:01<00:02,  8.48it/s]                                               {'loss': 0.5721, 'grad_norm': 0.4137788414955139, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.48it/s] 37%|███▋      | 11/30 [00:01<00:02,  8.18it/s]                                               {'loss': 0.6832, 'grad_norm': 0.4762425124645233, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.18it/s]                                               {'loss': 0.4111, 'grad_norm': 0.8229554891586304, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.18it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.37it/s]                                               {'loss': 0.5351, 'grad_norm': 0.591249942779541, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.37it/s] 47%|████▋     | 14/30 [00:01<00:02,  5.74it/s]                                               {'loss': 0.482, 'grad_norm': 0.7930567264556885, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  5.74it/s] 50%|█████     | 15/30 [00:02<00:03,  4.63it/s]                                               {'loss': 0.7344, 'grad_norm': 1.0140646696090698, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:03,  4.63it/s] 53%|█████▎    | 16/30 [00:02<00:02,  5.13it/s]                                               {'loss': 0.5138, 'grad_norm': 0.4284135401248932, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  5.13it/s] 57%|█████▋    | 17/30 [00:02<00:02,  5.83it/s]                                               {'loss': 0.5603, 'grad_norm': 0.39481011033058167, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  5.83it/s]                                               {'loss': 0.7424, 'grad_norm': 1.3759915828704834, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:02,  5.83it/s] 63%|██████▎   | 19/30 [00:02<00:01,  6.95it/s]                                               {'loss': 0.5311, 'grad_norm': 0.648942768573761, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  6.95it/s] 67%|██████▋   | 20/30 [00:02<00:01,  6.87it/s]                                               {'loss': 0.5609, 'grad_norm': 0.3324553966522217, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  6.87it/s] 70%|███████   | 21/30 [00:03<00:01,  6.94it/s]                                               {'loss': 0.5428, 'grad_norm': 0.8791160583496094, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  6.94it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.51it/s]                                               {'loss': 0.5995, 'grad_norm': 0.37524664402008057, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.51it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.56it/s]                                               {'loss': 0.6469, 'grad_norm': 0.6510540246963501, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.56it/s]                                               {'loss': 0.5529, 'grad_norm': 0.645368218421936, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.56it/s] 83%|████████▎ | 25/30 [00:03<00:00,  7.57it/s]                                               {'loss': 0.6698, 'grad_norm': 0.7128605246543884, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  7.57it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.21it/s]                                               {'loss': 0.5583, 'grad_norm': 0.5236495137214661, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.21it/s] 90%|█████████ | 27/30 [00:03<00:00,  7.05it/s]                                               {'loss': 0.5409, 'grad_norm': 0.7282918095588684, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  7.05it/s] 93%|█████████▎| 28/30 [00:04<00:00,  7.12it/s]                                               {'loss': 0.6567, 'grad_norm': 0.5946285128593445, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  7.12it/s] 97%|█████████▋| 29/30 [00:04<00:00,  6.95it/s]                                               {'loss': 0.6595, 'grad_norm': 1.1416782140731812, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  6.95it/s]                                               {'loss': 0.5587, 'grad_norm': 0.8023138046264648, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.95it/s]                                               {'train_runtime': 4.3842, 'train_samples_per_second': 96.939, 'train_steps_per_second': 6.843, 'train_loss': 0.6277096678813299, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.95it/s]100%|██████████| 30/30 [00:04<00:00,  6.85it/s]
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.14it/s]                                              {'loss': 0.8795, 'grad_norm': 2.360466957092285, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.14it/s]                                              {'loss': 0.5611, 'grad_norm': 1.2423447370529175, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.14it/s] 10%|█         | 3/30 [00:00<00:02, 11.00it/s]                                              {'loss': 0.328, 'grad_norm': 1.8394379615783691, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.00it/s]                                              {'loss': 0.2946, 'grad_norm': 0.6723200082778931, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.00it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.32it/s]                                              {'loss': 0.2502, 'grad_norm': 0.6904966235160828, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.32it/s]                                              {'loss': 0.0546, 'grad_norm': 0.902276873588562, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.32it/s] 23%|██▎       | 7/30 [00:00<00:02, 11.29it/s]                                              {'loss': 0.2831, 'grad_norm': 1.5753734111785889, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02, 11.29it/s]                                              {'loss': 0.0154, 'grad_norm': 0.3959312438964844, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 11.29it/s] 30%|███       | 9/30 [00:00<00:01, 10.93it/s]                                              {'loss': 0.3364, 'grad_norm': 5.901373863220215, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 10.93it/s]                                              {'loss': 0.0139, 'grad_norm': 0.4194314181804657, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 10.93it/s] 37%|███▋      | 11/30 [00:01<00:01, 10.16it/s]                                               {'loss': 0.0125, 'grad_norm': 0.4347783625125885, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 10.16it/s]                                               {'loss': 0.0076, 'grad_norm': 0.21116124093532562, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 10.16it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.51it/s]                                               {'loss': 0.0034, 'grad_norm': 0.10946371406316757, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  9.51it/s]                                               {'loss': 0.0024, 'grad_norm': 0.058412741869688034, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  9.51it/s] 50%|█████     | 15/30 [00:01<00:01,  9.23it/s]                                               {'loss': 0.4112, 'grad_norm': 1.010235071182251, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  9.23it/s]                                               {'loss': 0.0019, 'grad_norm': 0.027649687603116035, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  9.23it/s] 57%|█████▋    | 17/30 [00:01<00:01, 10.24it/s]                                               {'loss': 0.3496, 'grad_norm': 1.1091228723526, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 10.24it/s]                                               {'loss': 0.0029, 'grad_norm': 0.03734632581472397, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 10.24it/s] 63%|██████▎   | 19/30 [00:01<00:00, 11.90it/s]                                               {'loss': 0.0032, 'grad_norm': 0.04485130310058594, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 11.90it/s]                                               {'loss': 0.3342, 'grad_norm': 0.8983616232872009, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 11.90it/s] 70%|███████   | 21/30 [00:01<00:00, 12.26it/s]                                               {'loss': 0.006, 'grad_norm': 0.1578858345746994, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 12.26it/s]                                               {'loss': 0.0067, 'grad_norm': 0.09858067333698273, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 12.26it/s] 77%|███████▋  | 23/30 [00:02<00:00, 12.41it/s]                                               {'loss': 0.3007, 'grad_norm': 1.1043401956558228, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 12.41it/s]                                               {'loss': 0.0085, 'grad_norm': 0.16604994237422943, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 12.41it/s] 83%|████████▎ | 25/30 [00:02<00:00, 13.48it/s]                                               {'loss': 0.6104, 'grad_norm': 1.883044719696045, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 13.48it/s]                                               {'loss': 0.0121, 'grad_norm': 0.22169269621372223, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.48it/s] 90%|█████████ | 27/30 [00:02<00:00, 13.05it/s]                                               {'loss': 0.015, 'grad_norm': 0.29859426617622375, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.05it/s]                                               {'loss': 0.0148, 'grad_norm': 0.3266502618789673, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.05it/s] 97%|█████████▋| 29/30 [00:02<00:00, 11.54it/s]                                               {'loss': 0.0143, 'grad_norm': 0.2434837967157364, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 11.54it/s]                                               {'loss': 0.0191, 'grad_norm': 0.39195236563682556, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 11.54it/s]                                               {'train_runtime': 2.7611, 'train_samples_per_second': 153.922, 'train_steps_per_second': 10.865, 'train_loss': 0.17178043194580822, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 11.54it/s]100%|██████████| 30/30 [00:02<00:00, 10.87it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.11it/s]                                              {'loss': 0.9082, 'grad_norm': 1.5596051216125488, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.11it/s]  7%|▋         | 2/30 [00:00<00:03,  8.61it/s]                                              {'loss': 0.5421, 'grad_norm': 1.078299641609192, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.61it/s]                                              {'loss': 0.7324, 'grad_norm': 0.8635032773017883, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.61it/s] 13%|█▎        | 4/30 [00:00<00:02, 10.22it/s]                                              {'loss': 0.7094, 'grad_norm': 1.4184162616729736, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.22it/s]                                              {'loss': 0.6698, 'grad_norm': 1.4928646087646484, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.22it/s] 20%|██        | 6/30 [00:00<00:02, 10.88it/s]                                              {'loss': 1.1733, 'grad_norm': 2.8873958587646484, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 10.88it/s]                                              {'loss': 0.3369, 'grad_norm': 1.406373381614685, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02, 10.88it/s] 27%|██▋       | 8/30 [00:00<00:02, 10.82it/s]                                              {'loss': 0.491, 'grad_norm': 0.6477249264717102, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02, 10.82it/s]                                              {'loss': 0.6682, 'grad_norm': 1.325801968574524, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 10.82it/s] 33%|███▎      | 10/30 [00:00<00:01, 11.17it/s]                                               {'loss': 0.5643, 'grad_norm': 2.670628070831299, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.17it/s]                                               {'loss': 0.6898, 'grad_norm': 0.9454085826873779, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:01, 11.17it/s] 40%|████      | 12/30 [00:01<00:01, 11.95it/s]                                               {'loss': 0.4021, 'grad_norm': 1.4415994882583618, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 11.95it/s]                                               {'loss': 0.4387, 'grad_norm': 0.9849807024002075, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 11.95it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.37it/s]                                               {'loss': 0.5439, 'grad_norm': 0.7975897192955017, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.37it/s] 50%|█████     | 15/30 [00:01<00:01,  7.75it/s]                                               {'loss': 0.5891, 'grad_norm': 1.306803584098816, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  7.75it/s] 53%|█████▎    | 16/30 [00:01<00:01,  7.48it/s]                                               {'loss': 0.4874, 'grad_norm': 1.14360773563385, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  7.48it/s]                                               {'loss': 0.5456, 'grad_norm': 1.6656194925308228, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01,  7.48it/s] 60%|██████    | 18/30 [00:01<00:01,  9.44it/s]                                               {'loss': 0.6332, 'grad_norm': 1.7845664024353027, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01,  9.44it/s]                                               {'loss': 0.5867, 'grad_norm': 1.1616365909576416, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:01,  9.44it/s] 67%|██████▋   | 20/30 [00:02<00:00, 10.09it/s]                                               {'loss': 0.4042, 'grad_norm': 1.7271878719329834, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 10.09it/s]                                               {'loss': 0.5735, 'grad_norm': 1.50214684009552, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 10.09it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.71it/s]                                               {'loss': 0.524, 'grad_norm': 1.185718059539795, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  9.71it/s]                                               {'loss': 0.6692, 'grad_norm': 1.5376231670379639, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  9.71it/s] 80%|████████  | 24/30 [00:02<00:00,  9.33it/s]                                               {'loss': 0.2976, 'grad_norm': 1.8766571283340454, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  9.33it/s] 83%|████████▎ | 25/30 [00:02<00:00,  8.45it/s]                                               {'loss': 0.3781, 'grad_norm': 1.487216591835022, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00,  8.45it/s] 87%|████████▋ | 26/30 [00:02<00:00,  7.76it/s]                                               {'loss': 0.4195, 'grad_norm': 1.1882200241088867, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00,  7.76it/s] 90%|█████████ | 27/30 [00:03<00:00,  7.46it/s]                                               {'loss': 0.5569, 'grad_norm': 1.9901645183563232, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  7.46it/s] 93%|█████████▎| 28/30 [00:03<00:00,  6.83it/s]                                               {'loss': 0.4623, 'grad_norm': 1.558159351348877, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  6.83it/s] 97%|█████████▋| 29/30 [00:03<00:00,  6.98it/s]                                               {'loss': 0.4944, 'grad_norm': 1.3340141773223877, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  6.98it/s]                                               {'loss': 0.2423, 'grad_norm': 2.7057077884674072, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  6.98it/s]                                               {'train_runtime': 3.5722, 'train_samples_per_second': 118.975, 'train_steps_per_second': 8.398, 'train_loss': 0.5577914183338483, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  6.98it/s]100%|██████████| 30/30 [00:03<00:00,  8.41it/s]
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:09,  3.07it/s]                                              {'loss': 0.9511, 'grad_norm': 1.9412885904312134, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:09,  3.07it/s]  7%|▋         | 2/30 [00:00<00:08,  3.35it/s]                                              {'loss': 0.5377, 'grad_norm': 1.2775654792785645, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:08,  3.35it/s] 10%|█         | 3/30 [00:00<00:07,  3.44it/s]                                              {'loss': 0.3378, 'grad_norm': 1.6652261018753052, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.44it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.47it/s]                                              {'loss': 0.3322, 'grad_norm': 0.8315862417221069, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.47it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.57it/s]                                              {'loss': 0.0822, 'grad_norm': 0.8469834327697754, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.57it/s] 20%|██        | 6/30 [00:01<00:05,  4.45it/s]                                              {'loss': 0.0172, 'grad_norm': 0.2750099301338196, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.45it/s] 23%|██▎       | 7/30 [00:01<00:05,  3.96it/s]                                              {'loss': 0.0171, 'grad_norm': 0.3450099229812622, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  3.96it/s] 27%|██▋       | 8/30 [00:02<00:05,  4.05it/s]                                              {'loss': 0.0039, 'grad_norm': 0.07066679745912552, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:05,  4.05it/s] 30%|███       | 9/30 [00:02<00:05,  4.19it/s]                                              {'loss': 0.0055, 'grad_norm': 0.10125433653593063, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  4.19it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.28it/s]                                               {'loss': 0.2283, 'grad_norm': 4.8738508224487305, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.28it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.58it/s]                                               {'loss': 0.0049, 'grad_norm': 0.13369031250476837, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.58it/s]                                               {'loss': 0.0054, 'grad_norm': 0.10827679187059402, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.58it/s] 43%|████▎     | 13/30 [00:03<00:03,  5.33it/s]                                               {'loss': 0.0045, 'grad_norm': 0.1122080385684967, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  5.33it/s] 47%|████▋     | 14/30 [00:03<00:02,  5.73it/s]                                               {'loss': 0.0048, 'grad_norm': 0.10392427444458008, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:02,  5.73it/s] 50%|█████     | 15/30 [00:03<00:02,  5.99it/s]                                               {'loss': 0.3897, 'grad_norm': 0.6358796954154968, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.99it/s] 53%|█████▎    | 16/30 [00:03<00:02,  6.51it/s]                                               {'loss': 0.0036, 'grad_norm': 0.05745473504066467, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  6.51it/s] 57%|█████▋    | 17/30 [00:03<00:01,  6.93it/s]                                               {'loss': 0.0076, 'grad_norm': 0.14582663774490356, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:01,  6.93it/s]                                               {'loss': 0.0034, 'grad_norm': 0.03529321402311325, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  6.93it/s] 63%|██████▎   | 19/30 [00:03<00:01,  7.47it/s]                                               {'loss': 0.0038, 'grad_norm': 0.04446402192115784, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  7.47it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.43it/s]                                               {'loss': 0.004, 'grad_norm': 0.04437738656997681, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.43it/s]                                               {'loss': 0.0048, 'grad_norm': 0.05503648892045021, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  7.43it/s] 73%|███████▎  | 22/30 [00:04<00:00,  8.93it/s]                                               {'loss': 0.0055, 'grad_norm': 0.06869882345199585, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:00,  8.93it/s] 77%|███████▋  | 23/30 [00:04<00:00,  9.03it/s]                                               {'loss': 0.4033, 'grad_norm': 1.5556671619415283, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:00,  9.03it/s]                                               {'loss': 0.0057, 'grad_norm': 0.08044680207967758, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  9.03it/s] 83%|████████▎ | 25/30 [00:04<00:00, 10.14it/s]                                               {'loss': 0.0067, 'grad_norm': 0.07408159226179123, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00, 10.14it/s]                                               {'loss': 0.0072, 'grad_norm': 0.09846252202987671, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00, 10.14it/s] 90%|█████████ | 27/30 [00:04<00:00, 10.40it/s]                                               {'loss': 0.2852, 'grad_norm': 0.7582229971885681, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00, 10.40it/s]                                               {'loss': 0.0063, 'grad_norm': 0.084113709628582, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00, 10.40it/s] 97%|█████████▋| 29/30 [00:04<00:00, 10.37it/s]                                               {'loss': 0.0061, 'grad_norm': 0.074362613260746, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00, 10.37it/s]                                               {'loss': 0.0055, 'grad_norm': 0.06376427412033081, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00, 10.37it/s]                                               {'train_runtime': 4.9471, 'train_samples_per_second': 85.908, 'train_steps_per_second': 6.064, 'train_loss': 0.12270288201204191, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00, 10.37it/s]100%|██████████| 30/30 [00:04<00:00,  6.08it/s]
CLIENT:60
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.24it/s]                                              {'loss': 0.963, 'grad_norm': 1.8608754873275757, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.24it/s]  7%|▋         | 2/30 [00:00<00:05,  4.88it/s]                                              {'loss': 0.5278, 'grad_norm': 1.0435084104537964, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  4.88it/s] 10%|█         | 3/30 [00:00<00:04,  5.42it/s]                                              {'loss': 0.3367, 'grad_norm': 1.2122647762298584, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  5.42it/s] 13%|█▎        | 4/30 [00:00<00:05,  5.15it/s]                                              {'loss': 0.6995, 'grad_norm': 2.1116294860839844, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  5.15it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.88it/s]                                              {'loss': 0.6146, 'grad_norm': 2.0874745845794678, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.88it/s] 20%|██        | 6/30 [00:01<00:10,  2.38it/s]                                              {'loss': 0.1081, 'grad_norm': 1.328019380569458, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:10,  2.38it/s] 23%|██▎       | 7/30 [00:02<00:08,  2.67it/s]                                              {'loss': 0.5226, 'grad_norm': 2.846437931060791, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:08,  2.67it/s] 27%|██▋       | 8/30 [00:02<00:07,  2.99it/s]                                              {'loss': 0.5939, 'grad_norm': 2.757517099380493, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:07,  2.99it/s] 30%|███       | 9/30 [00:02<00:06,  3.28it/s]                                              {'loss': 0.3427, 'grad_norm': 1.437077283859253, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:06,  3.28it/s] 33%|███▎      | 10/30 [00:02<00:05,  3.58it/s]                                               {'loss': 0.3019, 'grad_norm': 1.1072585582733154, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  3.58it/s] 37%|███▋      | 11/30 [00:03<00:04,  4.12it/s]                                               {'loss': 0.2181, 'grad_norm': 1.3071544170379639, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:04,  4.12it/s] 40%|████      | 12/30 [00:03<00:03,  4.74it/s]                                               {'loss': 0.0768, 'grad_norm': 1.553608775138855, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:03,  4.74it/s] 43%|████▎     | 13/30 [00:03<00:03,  4.65it/s]                                               {'loss': 0.5593, 'grad_norm': 1.7944834232330322, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  4.65it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.64it/s]                                               {'loss': 0.3248, 'grad_norm': 1.296058177947998, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.64it/s] 50%|█████     | 15/30 [00:03<00:03,  4.81it/s]                                               {'loss': 0.9137, 'grad_norm': 3.2240965366363525, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.81it/s] 53%|█████▎    | 16/30 [00:03<00:02,  4.76it/s]                                               {'loss': 0.2713, 'grad_norm': 0.9574570059776306, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  4.76it/s] 57%|█████▋    | 17/30 [00:04<00:02,  4.96it/s]                                               {'loss': 0.2101, 'grad_norm': 2.7164359092712402, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:02,  4.96it/s] 60%|██████    | 18/30 [00:04<00:02,  5.42it/s]                                               {'loss': 0.1326, 'grad_norm': 1.7968308925628662, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  5.42it/s] 63%|██████▎   | 19/30 [00:04<00:02,  5.27it/s]                                               {'loss': 0.4337, 'grad_norm': 1.8288465738296509, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  5.27it/s] 67%|██████▋   | 20/30 [00:04<00:01,  5.16it/s]                                               {'loss': 0.3243, 'grad_norm': 0.7138944864273071, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  5.16it/s] 70%|███████   | 21/30 [00:04<00:01,  5.10it/s]                                               {'loss': 0.6799, 'grad_norm': 2.5598931312561035, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.10it/s] 73%|███████▎  | 22/30 [00:05<00:01,  5.23it/s]                                               {'loss': 0.3219, 'grad_norm': 0.8395752310752869, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:01,  5.23it/s] 77%|███████▋  | 23/30 [00:05<00:01,  5.03it/s]                                               {'loss': 0.4804, 'grad_norm': 2.1603012084960938, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  5.03it/s] 80%|████████  | 24/30 [00:05<00:01,  5.17it/s]                                               {'loss': 0.0851, 'grad_norm': 1.294354796409607, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.17it/s] 83%|████████▎ | 25/30 [00:05<00:01,  4.59it/s]                                               {'loss': 0.4159, 'grad_norm': 1.4864888191223145, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  4.59it/s] 87%|████████▋ | 26/30 [00:05<00:00,  4.69it/s]                                               {'loss': 0.4051, 'grad_norm': 1.9409725666046143, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  4.69it/s] 90%|█████████ | 27/30 [00:06<00:00,  5.15it/s]                                               {'loss': 0.2231, 'grad_norm': 0.8745415210723877, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  5.15it/s] 93%|█████████▎| 28/30 [00:06<00:00,  5.80it/s]                                               {'loss': 0.3908, 'grad_norm': 4.967918872833252, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  5.80it/s] 97%|█████████▋| 29/30 [00:06<00:00,  6.03it/s]                                               {'loss': 0.6028, 'grad_norm': 2.4952049255371094, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  6.03it/s]100%|██████████| 30/30 [00:06<00:00,  6.14it/s]                                               {'loss': 0.1354, 'grad_norm': 1.9595756530761719, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  6.14it/s]                                               {'train_runtime': 6.8636, 'train_samples_per_second': 61.921, 'train_steps_per_second': 4.371, 'train_loss': 0.4072056191662947, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  6.14it/s]100%|██████████| 30/30 [00:06<00:00,  4.37it/s]
CLIENT:67
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.47it/s]                                              {'loss': 0.5523, 'grad_norm': 0.5992297530174255, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.47it/s]                                              {'loss': 0.7363, 'grad_norm': 0.6217247247695923, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.47it/s] 10%|█         | 3/30 [00:00<00:02, 10.48it/s]                                              {'loss': 0.7414, 'grad_norm': 0.497346967458725, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.48it/s]                                              {'loss': 0.7124, 'grad_norm': 0.629636287689209, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.48it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.52it/s]                                              {'loss': 0.645, 'grad_norm': 0.2866480350494385, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.52it/s]                                              {'loss': 0.5802, 'grad_norm': 0.7694790363311768, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.52it/s] 23%|██▎       | 7/30 [00:00<00:02, 11.31it/s]                                              {'loss': 0.7116, 'grad_norm': 0.4310752749443054, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02, 11.31it/s]                                              {'loss': 0.6421, 'grad_norm': 0.23204979300498962, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 11.31it/s] 30%|███       | 9/30 [00:00<00:01, 11.66it/s]                                              {'loss': 0.6647, 'grad_norm': 0.3724561929702759, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 11.66it/s]                                              {'loss': 0.6812, 'grad_norm': 0.26125359535217285, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.66it/s] 37%|███▋      | 11/30 [00:00<00:01, 11.65it/s]                                               {'loss': 0.669, 'grad_norm': 0.9733017683029175, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 11.65it/s]                                               {'loss': 0.6681, 'grad_norm': 1.2892104387283325, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 11.65it/s] 43%|████▎     | 13/30 [00:01<00:01, 11.38it/s]                                               {'loss': 0.6546, 'grad_norm': 0.33580830693244934, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 11.38it/s]                                               {'loss': 0.6624, 'grad_norm': 0.5522804856300354, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.38it/s] 50%|█████     | 15/30 [00:01<00:01,  8.82it/s]                                               {'loss': 0.5914, 'grad_norm': 0.6754072308540344, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  8.82it/s] 53%|█████▎    | 16/30 [00:01<00:01,  8.74it/s]                                               {'loss': 0.6379, 'grad_norm': 0.720045268535614, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  8.74it/s]                                               {'loss': 0.6849, 'grad_norm': 0.48988041281700134, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01,  8.74it/s] 60%|██████    | 18/30 [00:01<00:01,  9.99it/s]                                               {'loss': 0.6834, 'grad_norm': 0.6318628787994385, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01,  9.99it/s]                                               {'loss': 0.6912, 'grad_norm': 0.7784565091133118, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:01,  9.99it/s] 67%|██████▋   | 20/30 [00:01<00:01,  9.69it/s]                                               {'loss': 0.6397, 'grad_norm': 0.2521120309829712, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:01,  9.69it/s]                                               {'loss': 0.6721, 'grad_norm': 0.2335461527109146, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00,  9.69it/s] 73%|███████▎  | 22/30 [00:02<00:00, 10.37it/s]                                               {'loss': 0.6187, 'grad_norm': 0.3777388036251068, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 10.37it/s]                                               {'loss': 0.6095, 'grad_norm': 0.5968958735466003, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 10.37it/s] 80%|████████  | 24/30 [00:02<00:00, 12.09it/s]                                               {'loss': 0.6364, 'grad_norm': 0.49497365951538086, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 12.09it/s]                                               {'loss': 0.7193, 'grad_norm': 0.5848148465156555, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.09it/s] 87%|████████▋ | 26/30 [00:02<00:00, 11.55it/s]                                               {'loss': 0.6426, 'grad_norm': 0.4233022630214691, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 11.55it/s]                                               {'loss': 0.6443, 'grad_norm': 0.5821385979652405, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 11.55it/s] 93%|█████████▎| 28/30 [00:02<00:00, 11.01it/s]                                               {'loss': 0.6354, 'grad_norm': 0.38834819197654724, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 11.01it/s]                                               {'loss': 0.5309, 'grad_norm': 0.43121960759162903, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 11.01it/s]100%|██████████| 30/30 [00:02<00:00, 10.14it/s]                                               {'loss': 0.5861, 'grad_norm': 0.6219854354858398, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 10.14it/s]                                               {'train_runtime': 3.1832, 'train_samples_per_second': 133.514, 'train_steps_per_second': 9.425, 'train_loss': 0.6515120685100555, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 10.14it/s]100%|██████████| 30/30 [00:03<00:00,  9.43it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  3%|▎         | 2/66 [00:00<00:03, 17.13it/s]  6%|▌         | 4/66 [00:00<00:04, 13.69it/s]  9%|▉         | 6/66 [00:00<00:04, 14.36it/s] 12%|█▏        | 8/66 [00:00<00:04, 13.33it/s] 15%|█▌        | 10/66 [00:00<00:04, 12.78it/s] 18%|█▊        | 12/66 [00:00<00:04, 13.41it/s] 21%|██        | 14/66 [00:01<00:04, 12.24it/s] 24%|██▍       | 16/66 [00:01<00:04, 11.53it/s] 27%|██▋       | 18/66 [00:01<00:04, 10.63it/s] 30%|███       | 20/66 [00:01<00:04, 10.43it/s] 33%|███▎      | 22/66 [00:01<00:03, 11.06it/s] 36%|███▋      | 24/66 [00:02<00:03, 10.86it/s] 39%|███▉      | 26/66 [00:02<00:03, 11.82it/s] 42%|████▏     | 28/66 [00:02<00:03, 11.74it/s] 45%|████▌     | 30/66 [00:02<00:03, 11.37it/s] 48%|████▊     | 32/66 [00:02<00:02, 11.43it/s] 52%|█████▏    | 34/66 [00:02<00:02, 12.26it/s] 55%|█████▍    | 36/66 [00:03<00:02, 10.95it/s] 58%|█████▊    | 38/66 [00:03<00:02, 10.68it/s] 61%|██████    | 40/66 [00:03<00:02, 10.49it/s] 64%|██████▎   | 42/66 [00:03<00:02, 10.43it/s] 67%|██████▋   | 44/66 [00:03<00:01, 12.01it/s] 70%|██████▉   | 46/66 [00:03<00:01, 11.94it/s] 73%|███████▎  | 48/66 [00:04<00:01, 11.33it/s] 76%|███████▌  | 50/66 [00:04<00:01, 10.19it/s] 79%|███████▉  | 52/66 [00:04<00:01,  9.60it/s] 82%|████████▏ | 54/66 [00:04<00:01, 10.38it/s] 85%|████████▍ | 56/66 [00:04<00:00, 11.53it/s] 88%|████████▊ | 58/66 [00:05<00:00, 11.71it/s] 91%|█████████ | 60/66 [00:05<00:00, 11.63it/s] 94%|█████████▍| 62/66 [00:05<00:00, 12.03it/s] 97%|█████████▋| 64/66 [00:05<00:00, 11.69it/s]100%|██████████| 66/66 [00:05<00:00, 11.86it/s]100%|██████████| 66/66 [00:05<00:00, 11.48it/s]
{'eval_loss': 0.6575864553451538, 'eval_model_preparation_time': 0.0064, 'eval_acc': 0.6663470757430489, 'eval_runtime': 5.9015, 'eval_samples_per_second': 176.736, 'eval_steps_per_second': 11.184}
ROUND:4
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:10,  2.90it/s]                                              {'loss': 0.5953, 'grad_norm': 1.411903977394104, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:10,  2.90it/s]  7%|▋         | 2/30 [00:00<00:07,  3.83it/s]                                              {'loss': 0.4989, 'grad_norm': 1.2161080837249756, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.83it/s] 10%|█         | 3/30 [00:00<00:06,  4.37it/s]                                              {'loss': 0.4034, 'grad_norm': 1.07857346534729, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.37it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.54it/s]                                              {'loss': 0.3378, 'grad_norm': 0.8165107369422913, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.54it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.80it/s]                                              {'loss': 0.7649, 'grad_norm': 2.4351069927215576, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.80it/s] 20%|██        | 6/30 [00:01<00:06,  3.60it/s]                                              {'loss': 0.0392, 'grad_norm': 0.7813573479652405, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.60it/s]                                              {'loss': 0.1451, 'grad_norm': 0.47341659665107727, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:06,  3.60it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.46it/s]                                              {'loss': 0.672, 'grad_norm': 2.107416868209839, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.46it/s]                                              {'loss': 0.7862, 'grad_norm': 4.150177001953125, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.46it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.13it/s]                                               {'loss': 0.8271, 'grad_norm': 3.1411356925964355, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.13it/s]                                               {'loss': 0.2883, 'grad_norm': 1.3911855220794678, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.13it/s] 40%|████      | 12/30 [00:02<00:02,  8.03it/s]                                               {'loss': 0.6824, 'grad_norm': 17.565494537353516, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  8.03it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.66it/s]                                               {'loss': 0.4005, 'grad_norm': 3.0234456062316895, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.66it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.76it/s]                                               {'loss': 0.4244, 'grad_norm': 3.044168710708618, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.76it/s] 50%|█████     | 15/30 [00:02<00:02,  7.41it/s]                                               {'loss': 0.5189, 'grad_norm': 1.7137184143066406, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.41it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.10it/s]                                               {'loss': 0.4716, 'grad_norm': 1.6107759475708008, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.10it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.52it/s]                                               {'loss': 0.2215, 'grad_norm': 3.2322475910186768, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.52it/s] 60%|██████    | 18/30 [00:02<00:01,  7.68it/s]                                               {'loss': 0.5997, 'grad_norm': 0.6212025284767151, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.68it/s] 63%|██████▎   | 19/30 [00:03<00:01,  7.11it/s]                                               {'loss': 0.4164, 'grad_norm': 1.4115766286849976, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  7.11it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.67it/s]                                               {'loss': 0.5023, 'grad_norm': 0.3228476345539093, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.67it/s] 70%|███████   | 21/30 [00:03<00:01,  5.32it/s]                                               {'loss': 0.476, 'grad_norm': 1.6901286840438843, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.32it/s]                                               {'loss': 0.4974, 'grad_norm': 0.8905880451202393, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  5.32it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.45it/s]                                               {'loss': 0.2054, 'grad_norm': 3.15156888961792, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.45it/s] 80%|████████  | 24/30 [00:03<00:00,  6.87it/s]                                               {'loss': 0.4652, 'grad_norm': 0.4717448055744171, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.87it/s] 83%|████████▎ | 25/30 [00:04<00:00,  6.75it/s]                                               {'loss': 0.3946, 'grad_norm': 1.2525231838226318, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  6.75it/s] 87%|████████▋ | 26/30 [00:04<00:00,  6.91it/s]                                               {'loss': 0.4863, 'grad_norm': 0.48143723607063293, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  6.91it/s] 90%|█████████ | 27/30 [00:04<00:00,  6.38it/s]                                               {'loss': 0.4957, 'grad_norm': 0.5067383646965027, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  6.38it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.83it/s]                                               {'loss': 0.2047, 'grad_norm': 3.254047155380249, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.83it/s] 97%|█████████▋| 29/30 [00:04<00:00,  6.98it/s]                                               {'loss': 0.379, 'grad_norm': 1.0781522989273071, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  6.98it/s]100%|██████████| 30/30 [00:04<00:00,  7.58it/s]                                               {'loss': 0.4218, 'grad_norm': 0.7904617190361023, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.58it/s]                                               {'train_runtime': 4.9217, 'train_samples_per_second': 86.352, 'train_steps_per_second': 6.095, 'train_loss': 0.4540703778465589, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.58it/s]100%|██████████| 30/30 [00:04<00:00,  6.10it/s]
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.03it/s]                                              {'loss': 0.7361, 'grad_norm': 1.2145634889602661, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.03it/s]  7%|▋         | 2/30 [00:00<00:05,  5.58it/s]                                              {'loss': 0.5394, 'grad_norm': 1.243333339691162, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.58it/s] 10%|█         | 3/30 [00:00<00:04,  6.00it/s]                                              {'loss': 0.3846, 'grad_norm': 1.0874524116516113, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.00it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.17it/s]                                              {'loss': 0.1226, 'grad_norm': 1.0686014890670776, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.17it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.29it/s]                                              {'loss': 0.3942, 'grad_norm': 1.3929896354675293, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.29it/s] 20%|██        | 6/30 [00:01<00:06,  3.95it/s]                                              {'loss': 0.0167, 'grad_norm': 0.3319653570652008, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.95it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.49it/s]                                              {'loss': 0.329, 'grad_norm': 0.9236692786216736, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.49it/s]                                              {'loss': 1.1775, 'grad_norm': 4.6211628913879395, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.49it/s] 30%|███       | 9/30 [00:01<00:03,  6.39it/s]                                              {'loss': 0.0358, 'grad_norm': 0.7544074654579163, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.39it/s]                                              {'loss': 0.31, 'grad_norm': 1.6915725469589233, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.39it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.73it/s]                                               {'loss': 0.0276, 'grad_norm': 0.5753328204154968, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.73it/s]                                               {'loss': 0.014, 'grad_norm': 0.5625724196434021, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.73it/s] 43%|████▎     | 13/30 [00:01<00:01,  8.83it/s]                                               {'loss': 0.5869, 'grad_norm': 3.09329891204834, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  8.83it/s] 47%|████▋     | 14/30 [00:02<00:01,  9.00it/s]                                               {'loss': 0.0258, 'grad_norm': 0.4765448570251465, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  9.00it/s] 50%|█████     | 15/30 [00:02<00:01,  9.05it/s]                                               {'loss': 0.611, 'grad_norm': 2.2821433544158936, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  9.05it/s]                                               {'loss': 0.2484, 'grad_norm': 1.873799204826355, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  9.05it/s] 57%|█████▋    | 17/30 [00:02<00:01, 10.09it/s]                                               {'loss': 0.0541, 'grad_norm': 0.7866363525390625, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01, 10.09it/s]                                               {'loss': 0.6038, 'grad_norm': 7.044445991516113, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 10.09it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.73it/s]                                               {'loss': 0.0293, 'grad_norm': 0.5892392992973328, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  9.73it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.03it/s]                                               {'loss': 0.6334, 'grad_norm': 4.741461277008057, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  9.03it/s] 70%|███████   | 21/30 [00:02<00:01,  8.53it/s]                                               {'loss': 0.2331, 'grad_norm': 1.249868392944336, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.53it/s] 73%|███████▎  | 22/30 [00:02<00:01,  7.94it/s]                                               {'loss': 0.1995, 'grad_norm': 1.260087013244629, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  7.94it/s] 77%|███████▋  | 23/30 [00:03<00:00,  8.03it/s]                                               {'loss': 0.2742, 'grad_norm': 1.0560506582260132, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  8.03it/s]                                               {'loss': 0.0454, 'grad_norm': 0.9002134203910828, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  8.03it/s] 83%|████████▎ | 25/30 [00:03<00:00,  7.98it/s]                                               {'loss': 0.3349, 'grad_norm': 1.4539531469345093, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  7.98it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.64it/s]                                               {'loss': 0.0515, 'grad_norm': 0.9102420210838318, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.64it/s] 90%|█████████ | 27/30 [00:03<00:00,  7.82it/s]                                               {'loss': 0.1754, 'grad_norm': 0.8651652336120605, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  7.82it/s] 93%|█████████▎| 28/30 [00:03<00:00,  7.38it/s]                                               {'loss': 0.3593, 'grad_norm': 2.2821404933929443, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  7.38it/s] 97%|█████████▋| 29/30 [00:03<00:00,  7.38it/s]                                               {'loss': 0.2161, 'grad_norm': 1.2616755962371826, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  7.38it/s]                                               {'loss': 0.5432, 'grad_norm': 2.876400947570801, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  7.38it/s]                                               {'train_runtime': 4.3459, 'train_samples_per_second': 97.794, 'train_steps_per_second': 6.903, 'train_loss': 0.31041951222966113, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.38it/s]100%|██████████| 30/30 [00:04<00:00,  6.91it/s]
CLIENT:79
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:09,  3.19it/s]                                              {'loss': 0.9071, 'grad_norm': 1.5224195718765259, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:09,  3.19it/s]  7%|▋         | 2/30 [00:00<00:06,  4.14it/s]                                              {'loss': 0.4713, 'grad_norm': 1.3940105438232422, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.14it/s] 10%|█         | 3/30 [00:00<00:06,  4.14it/s]                                              {'loss': 0.3747, 'grad_norm': 1.177888035774231, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.14it/s] 13%|█▎        | 4/30 [00:00<00:06,  4.07it/s]                                              {'loss': 0.1383, 'grad_norm': 1.4418789148330688, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:06,  4.07it/s] 17%|█▋        | 5/30 [00:01<00:06,  4.00it/s]                                              {'loss': 0.3081, 'grad_norm': 0.8885229229927063, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  4.00it/s] 20%|██        | 6/30 [00:02<00:10,  2.24it/s]                                              {'loss': 1.0212, 'grad_norm': 3.9277358055114746, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:10,  2.24it/s] 23%|██▎       | 7/30 [00:02<00:08,  2.72it/s]                                              {'loss': 0.1739, 'grad_norm': 0.4591997563838959, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:08,  2.72it/s] 27%|██▋       | 8/30 [00:02<00:07,  3.10it/s]                                              {'loss': 0.6081, 'grad_norm': 1.4473127126693726, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:07,  3.10it/s] 30%|███       | 9/30 [00:02<00:05,  3.59it/s]                                              {'loss': 0.487, 'grad_norm': 2.51672625541687, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  3.59it/s] 33%|███▎      | 10/30 [00:02<00:05,  3.71it/s]                                               {'loss': 0.1171, 'grad_norm': 1.9848737716674805, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  3.71it/s] 37%|███▋      | 11/30 [00:03<00:04,  3.98it/s]                                               {'loss': 0.0422, 'grad_norm': 0.5408474802970886, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:04,  3.98it/s] 40%|████      | 12/30 [00:03<00:04,  4.16it/s]                                               {'loss': 0.0112, 'grad_norm': 0.10414959490299225, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:04,  4.16it/s] 43%|████▎     | 13/30 [00:03<00:03,  4.57it/s]                                               {'loss': 0.5125, 'grad_norm': 2.6534953117370605, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  4.57it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.64it/s]                                               {'loss': 0.0329, 'grad_norm': 0.8665476441383362, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.64it/s] 50%|█████     | 15/30 [00:03<00:03,  4.73it/s]                                               {'loss': 0.9017, 'grad_norm': 2.123526096343994, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.73it/s] 53%|█████▎    | 16/30 [00:04<00:02,  4.67it/s]                                               {'loss': 0.2694, 'grad_norm': 1.8605438470840454, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:02,  4.67it/s] 57%|█████▋    | 17/30 [00:04<00:02,  4.61it/s]                                               {'loss': 0.0624, 'grad_norm': 1.8586194515228271, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:02,  4.61it/s]                                               {'loss': 0.0446, 'grad_norm': 0.682743489742279, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.61it/s] 63%|██████▎   | 19/30 [00:04<00:02,  5.19it/s]                                               {'loss': 0.3951, 'grad_norm': 2.724844455718994, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  5.19it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.93it/s]                                               {'loss': 0.2684, 'grad_norm': 2.30961275100708, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.93it/s] 70%|███████   | 21/30 [00:05<00:01,  4.67it/s]                                               {'loss': 0.0447, 'grad_norm': 0.7612557411193848, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:01,  4.67it/s] 73%|███████▎  | 22/30 [00:05<00:01,  4.67it/s]                                               {'loss': 0.4815, 'grad_norm': 8.504090309143066, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:01,  4.67it/s] 77%|███████▋  | 23/30 [00:05<00:01,  4.57it/s]                                               {'loss': 0.1766, 'grad_norm': 6.717102527618408, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  4.57it/s] 80%|████████  | 24/30 [00:05<00:01,  4.76it/s]                                               {'loss': 0.0669, 'grad_norm': 1.6288772821426392, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  4.76it/s] 83%|████████▎ | 25/30 [00:06<00:01,  3.73it/s]                                               {'loss': 0.0435, 'grad_norm': 0.8051137328147888, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:01,  3.73it/s] 87%|████████▋ | 26/30 [00:06<00:01,  3.24it/s]                                               {'loss': 0.361, 'grad_norm': 3.2780966758728027, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:01,  3.24it/s] 90%|█████████ | 27/30 [00:07<00:00,  3.03it/s]                                               {'loss': 0.0404, 'grad_norm': 0.3901486098766327, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:07<00:00,  3.03it/s] 93%|█████████▎| 28/30 [00:07<00:00,  2.97it/s]                                               {'loss': 0.2333, 'grad_norm': 1.8541046380996704, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  2.97it/s] 97%|█████████▋| 29/30 [00:07<00:00,  3.06it/s]                                               {'loss': 0.5095, 'grad_norm': 2.661025285720825, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  3.06it/s]100%|██████████| 30/30 [00:07<00:00,  3.78it/s]                                               {'loss': 0.0773, 'grad_norm': 1.2941840887069702, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  3.78it/s]                                               {'train_runtime': 7.9197, 'train_samples_per_second': 53.664, 'train_steps_per_second': 3.788, 'train_loss': 0.30606282434115806, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  3.78it/s]100%|██████████| 30/30 [00:07<00:00,  3.80it/s]
CLIENT:14
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.72it/s]                                              {'loss': 0.5708, 'grad_norm': 0.34942471981048584, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.72it/s]  7%|▋         | 2/30 [00:00<00:04,  5.67it/s]                                              {'loss': 0.6974, 'grad_norm': 1.095996618270874, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  5.67it/s] 10%|█         | 3/30 [00:00<00:04,  5.89it/s]                                              {'loss': 0.6687, 'grad_norm': 0.8437667489051819, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  5.89it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.94it/s]                                              {'loss': 0.6309, 'grad_norm': 0.6595926284790039, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.94it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.94it/s]                                              {'loss': 0.7732, 'grad_norm': 0.9403930902481079, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.94it/s] 20%|██        | 6/30 [00:01<00:07,  3.41it/s]                                              {'loss': 0.9875, 'grad_norm': 3.036259889602661, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:07,  3.41it/s] 23%|██▎       | 7/30 [00:01<00:05,  3.96it/s]                                              {'loss': 0.646, 'grad_norm': 0.6653681397438049, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  3.96it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.48it/s]                                              {'loss': 0.7352, 'grad_norm': 0.8880466818809509, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.48it/s] 30%|███       | 9/30 [00:01<00:04,  4.71it/s]                                              {'loss': 0.6319, 'grad_norm': 0.5627921223640442, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.71it/s] 33%|███▎      | 10/30 [00:02<00:04,  5.00it/s]                                               {'loss': 0.5849, 'grad_norm': 0.5765342116355896, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  5.00it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.42it/s]                                               {'loss': 0.6535, 'grad_norm': 0.40826672315597534, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.42it/s]                                               {'loss': 0.4923, 'grad_norm': 0.5711790323257446, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.42it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.59it/s]                                               {'loss': 0.575, 'grad_norm': 0.911592960357666, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.59it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.62it/s]                                               {'loss': 0.5334, 'grad_norm': 0.6560730338096619, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.62it/s] 50%|█████     | 15/30 [00:02<00:02,  6.59it/s]                                               {'loss': 0.697, 'grad_norm': 1.687433123588562, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.59it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.28it/s]                                               {'loss': 0.6392, 'grad_norm': 0.7730506062507629, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.28it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.88it/s]                                               {'loss': 0.5774, 'grad_norm': 0.599747896194458, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.88it/s] 60%|██████    | 18/30 [00:03<00:01,  6.55it/s]                                               {'loss': 0.7415, 'grad_norm': 1.7854615449905396, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  6.55it/s] 63%|██████▎   | 19/30 [00:03<00:01,  5.81it/s]                                               {'loss': 0.5255, 'grad_norm': 1.2138304710388184, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  5.81it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.67it/s]                                               {'loss': 0.6676, 'grad_norm': 0.9710431098937988, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.67it/s] 70%|███████   | 21/30 [00:03<00:01,  5.44it/s]                                               {'loss': 0.5236, 'grad_norm': 1.0874258279800415, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.44it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.07it/s]                                               {'loss': 0.6325, 'grad_norm': 0.7269673943519592, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.07it/s] 77%|███████▋  | 23/30 [00:04<00:01,  5.15it/s]                                               {'loss': 0.5714, 'grad_norm': 0.7260023951530457, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  5.15it/s]                                               {'loss': 0.5666, 'grad_norm': 0.79915851354599, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  5.15it/s] 83%|████████▎ | 25/30 [00:04<00:00,  5.86it/s]                                               {'loss': 0.6703, 'grad_norm': 1.0990333557128906, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  5.86it/s] 87%|████████▋ | 26/30 [00:04<00:00,  5.31it/s]                                               {'loss': 0.5606, 'grad_norm': 1.263444185256958, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  5.31it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.92it/s]                                               {'loss': 0.6131, 'grad_norm': 0.8435490727424622, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.92it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.73it/s]                                               {'loss': 0.5467, 'grad_norm': 0.7392396926879883, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.73it/s] 97%|█████████▋| 29/30 [00:05<00:00,  4.63it/s]                                               {'loss': 0.5595, 'grad_norm': 1.4882999658584595, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  4.63it/s]                                               {'loss': 0.4784, 'grad_norm': 0.7408834099769592, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.63it/s]                                               {'train_runtime': 5.6799, 'train_samples_per_second': 74.826, 'train_steps_per_second': 5.282, 'train_loss': 0.6250532746315003, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.63it/s]100%|██████████| 30/30 [00:05<00:00,  5.29it/s]
CLIENT:55
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.73it/s]                                              {'loss': 0.5974, 'grad_norm': 0.5312032103538513, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.73it/s]  7%|▋         | 2/30 [00:00<00:04,  6.84it/s]                                              {'loss': 0.7348, 'grad_norm': 0.687835693359375, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.84it/s] 10%|█         | 3/30 [00:00<00:04,  6.56it/s]                                              {'loss': 0.7006, 'grad_norm': 0.4337497353553772, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.56it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.36it/s]                                              {'loss': 0.6998, 'grad_norm': 0.8038622736930847, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.36it/s] 17%|█▋        | 5/30 [00:00<00:04,  6.18it/s]                                              {'loss': 0.6664, 'grad_norm': 0.453091561794281, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  6.18it/s] 20%|██        | 6/30 [00:00<00:03,  7.07it/s]                                              {'loss': 0.8083, 'grad_norm': 1.50031578540802, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.07it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.89it/s]                                              {'loss': 0.6874, 'grad_norm': 1.2113507986068726, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.89it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.77it/s]                                              {'loss': 0.6811, 'grad_norm': 0.4972015619277954, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.77it/s] 30%|███       | 9/30 [00:01<00:03,  6.71it/s]                                              {'loss': 0.6875, 'grad_norm': 0.3851849138736725, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.71it/s] 33%|███▎      | 10/30 [00:01<00:02,  6.71it/s]                                               {'loss': 0.6758, 'grad_norm': 0.27893128991127014, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  6.71it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.66it/s]                                               {'loss': 0.6751, 'grad_norm': 0.3441646099090576, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.66it/s]                                               {'loss': 0.6119, 'grad_norm': 0.7996770143508911, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.66it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.31it/s]                                               {'loss': 0.6435, 'grad_norm': 0.3607170283794403, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.31it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.89it/s]                                               {'loss': 0.6661, 'grad_norm': 0.5822793245315552, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.89it/s] 50%|█████     | 15/30 [00:02<00:02,  6.59it/s]                                               {'loss': 0.5916, 'grad_norm': 0.7493607401847839, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.59it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.13it/s]                                               {'loss': 0.6661, 'grad_norm': 0.8028269410133362, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.13it/s] 57%|█████▋    | 17/30 [00:02<00:02,  5.91it/s]                                               {'loss': 0.6723, 'grad_norm': 0.5025104284286499, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  5.91it/s] 60%|██████    | 18/30 [00:02<00:02,  5.95it/s]                                               {'loss': 0.6581, 'grad_norm': 0.6042606830596924, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:02,  5.95it/s] 63%|██████▎   | 19/30 [00:02<00:02,  5.48it/s]                                               {'loss': 0.7305, 'grad_norm': 1.4029927253723145, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  5.48it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.71it/s]                                               {'loss': 0.6618, 'grad_norm': 0.28108760714530945, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.71it/s] 70%|███████   | 21/30 [00:03<00:01,  5.83it/s]                                               {'loss': 0.6566, 'grad_norm': 0.34902188181877136, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.83it/s] 73%|███████▎  | 22/30 [00:03<00:01,  5.95it/s]                                               {'loss': 0.6576, 'grad_norm': 0.3590235710144043, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  5.95it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.87it/s]                                               {'loss': 0.6674, 'grad_norm': 0.9432269334793091, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.87it/s]                                               {'loss': 0.6208, 'grad_norm': 0.5214208960533142, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:01,  5.87it/s] 83%|████████▎ | 25/30 [00:03<00:00,  6.61it/s]                                               {'loss': 0.6788, 'grad_norm': 0.5467078685760498, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  6.61it/s] 87%|████████▋ | 26/30 [00:04<00:00,  6.61it/s]                                               {'loss': 0.6146, 'grad_norm': 0.4663458466529846, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  6.61it/s] 90%|█████████ | 27/30 [00:04<00:00,  6.45it/s]                                               {'loss': 0.6151, 'grad_norm': 0.5025740265846252, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  6.45it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.37it/s]                                               {'loss': 0.6543, 'grad_norm': 0.42296475172042847, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.37it/s] 97%|█████████▋| 29/30 [00:04<00:00,  6.41it/s]                                               {'loss': 0.6215, 'grad_norm': 0.6042658686637878, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  6.41it/s]                                               {'loss': 0.6115, 'grad_norm': 0.5365164279937744, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.41it/s]                                               {'train_runtime': 4.6846, 'train_samples_per_second': 90.723, 'train_steps_per_second': 6.404, 'train_loss': 0.663809863726298, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.41it/s]100%|██████████| 30/30 [00:04<00:00,  6.41it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.43it/s]                                              {'loss': 0.7001, 'grad_norm': 0.4945448637008667, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.43it/s]  7%|▋         | 2/30 [00:00<00:06,  4.66it/s]                                              {'loss': 0.7032, 'grad_norm': 0.5500035285949707, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.66it/s] 10%|█         | 3/30 [00:00<00:05,  4.65it/s]                                              {'loss': 0.6684, 'grad_norm': 0.7889145612716675, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.65it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.51it/s]                                              {'loss': 0.7061, 'grad_norm': 1.0964446067810059, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.51it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.88it/s]                                              {'loss': 0.7492, 'grad_norm': 1.1582818031311035, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.88it/s] 20%|██        | 6/30 [00:01<00:04,  5.70it/s]                                              {'loss': 0.7295, 'grad_norm': 1.1787551641464233, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.70it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.51it/s]                                              {'loss': 0.6332, 'grad_norm': 0.518770694732666, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.51it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.75it/s]                                              {'loss': 0.69, 'grad_norm': 0.36567676067352295, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.75it/s] 30%|███       | 9/30 [00:01<00:03,  6.14it/s]                                              {'loss': 0.696, 'grad_norm': 0.5177082419395447, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.14it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.28it/s]                                               {'loss': 0.6896, 'grad_norm': 0.44562238454818726, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.28it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.47it/s]                                               {'loss': 0.6151, 'grad_norm': 0.48998886346817017, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.47it/s]                                               {'loss': 0.4862, 'grad_norm': 1.8524723052978516, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.47it/s] 43%|████▎     | 13/30 [00:02<00:01,  9.15it/s]                                               {'loss': 0.6282, 'grad_norm': 0.41682830452919006, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:01,  9.15it/s]                                               {'loss': 0.6273, 'grad_norm': 0.5063970685005188, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  9.15it/s] 50%|█████     | 15/30 [00:02<00:01, 10.04it/s]                                               {'loss': 0.7017, 'grad_norm': 1.4913090467453003, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01, 10.04it/s]                                               {'loss': 0.6048, 'grad_norm': 0.7486864924430847, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01, 10.04it/s] 57%|█████▋    | 17/30 [00:02<00:01,  8.50it/s]                                               {'loss': 0.5181, 'grad_norm': 1.5500706434249878, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  8.50it/s]                                               {'loss': 0.5279, 'grad_norm': 1.5811609029769897, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  8.50it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.05it/s]                                               {'loss': 0.5392, 'grad_norm': 0.9952656030654907, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  9.05it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.33it/s]                                               {'loss': 0.6992, 'grad_norm': 1.530812382698059, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.33it/s] 70%|███████   | 21/30 [00:03<00:01,  7.94it/s]                                               {'loss': 0.6592, 'grad_norm': 1.386220932006836, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.94it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.52it/s]                                               {'loss': 0.5289, 'grad_norm': 0.8215277194976807, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.52it/s] 77%|███████▋  | 23/30 [00:03<00:00,  7.15it/s]                                               {'loss': 0.6209, 'grad_norm': 1.4222207069396973, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.15it/s]                                               {'loss': 0.6571, 'grad_norm': 2.4919373989105225, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.15it/s] 83%|████████▎ | 25/30 [00:03<00:00,  8.02it/s]                                               {'loss': 0.6423, 'grad_norm': 1.2338736057281494, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  8.02it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.62it/s]                                               {'loss': 0.6277, 'grad_norm': 1.046007513999939, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.62it/s] 90%|█████████ | 27/30 [00:03<00:00,  7.26it/s]                                               {'loss': 0.5151, 'grad_norm': 1.3272528648376465, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  7.26it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.28it/s]                                               {'loss': 0.5583, 'grad_norm': 1.1542768478393555, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.28it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.78it/s]                                               {'loss': 0.4875, 'grad_norm': 1.1458873748779297, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.78it/s]100%|██████████| 30/30 [00:04<00:00,  6.32it/s]                                               {'loss': 0.5226, 'grad_norm': 2.0861620903015137, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.32it/s]                                               {'train_runtime': 4.5232, 'train_samples_per_second': 93.96, 'train_steps_per_second': 6.632, 'train_loss': 0.6244207123915354, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.32it/s]100%|██████████| 30/30 [00:04<00:00,  6.63it/s]
CLIENT:19
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  4.92it/s]                                              {'loss': 0.6375, 'grad_norm': 0.5460548996925354, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  4.92it/s]  7%|▋         | 2/30 [00:00<00:12,  2.29it/s]                                              {'loss': 0.6926, 'grad_norm': 0.8509560823440552, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:12,  2.29it/s] 10%|█         | 3/30 [00:00<00:08,  3.15it/s]                                              {'loss': 0.7111, 'grad_norm': 0.5745477080345154, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:08,  3.15it/s] 13%|█▎        | 4/30 [00:01<00:06,  3.94it/s]                                              {'loss': 0.6479, 'grad_norm': 0.5767210721969604, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:06,  3.94it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.51it/s]                                              {'loss': 0.6673, 'grad_norm': 0.5690529346466064, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.51it/s] 20%|██        | 6/30 [00:01<00:04,  4.82it/s]                                              {'loss': 0.836, 'grad_norm': 1.9793869256973267, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.82it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.00it/s]                                              {'loss': 0.642, 'grad_norm': 0.7326561212539673, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.00it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.66it/s]                                              {'loss': 0.7281, 'grad_norm': 0.49541932344436646, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.66it/s] 30%|███       | 9/30 [00:02<00:04,  5.19it/s]                                              {'loss': 0.657, 'grad_norm': 0.5189927816390991, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  5.19it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.91it/s]                                               {'loss': 0.6471, 'grad_norm': 0.32441893219947815, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.91it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.19it/s]                                               {'loss': 0.6826, 'grad_norm': 0.24674098193645477, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.19it/s]                                               {'loss': 0.6246, 'grad_norm': 1.4791568517684937, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.19it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.91it/s]                                               {'loss': 0.5839, 'grad_norm': 0.4301099479198456, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.91it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.31it/s]                                               {'loss': 0.6661, 'grad_norm': 0.45214420557022095, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.31it/s] 50%|█████     | 15/30 [00:02<00:02,  7.31it/s]                                               {'loss': 0.745, 'grad_norm': 0.995089054107666, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.31it/s] 53%|█████▎    | 16/30 [00:03<00:02,  6.31it/s]                                               {'loss': 0.6501, 'grad_norm': 0.4050444960594177, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  6.31it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.86it/s]                                               {'loss': 0.6346, 'grad_norm': 0.5776314735412598, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.86it/s] 60%|██████    | 18/30 [00:03<00:01,  6.08it/s]                                               {'loss': 0.6503, 'grad_norm': 0.9716335535049438, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  6.08it/s] 63%|██████▎   | 19/30 [00:03<00:01,  6.55it/s]                                               {'loss': 0.6738, 'grad_norm': 2.272006034851074, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  6.55it/s]                                               {'loss': 0.6616, 'grad_norm': 0.3804534673690796, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  6.55it/s] 70%|███████   | 21/30 [00:03<00:01,  7.82it/s]                                               {'loss': 0.6215, 'grad_norm': 0.7570191621780396, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.82it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.78it/s]                                               {'loss': 0.6186, 'grad_norm': 0.4379771053791046, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.78it/s]                                               {'loss': 0.6416, 'grad_norm': 0.7297269701957703, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.78it/s] 80%|████████  | 24/30 [00:03<00:00,  9.96it/s]                                               {'loss': 0.5353, 'grad_norm': 1.0127882957458496, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  9.96it/s]                                               {'loss': 0.6051, 'grad_norm': 0.6109654903411865, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  9.96it/s] 87%|████████▋ | 26/30 [00:04<00:00, 10.19it/s]                                               {'loss': 0.5836, 'grad_norm': 0.4325232207775116, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00, 10.19it/s]                                               {'loss': 0.57, 'grad_norm': 1.8196370601654053, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00, 10.19it/s] 93%|█████████▎| 28/30 [00:04<00:00,  9.57it/s]                                               {'loss': 0.658, 'grad_norm': 0.6752060651779175, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  9.57it/s]                                               {'loss': 0.6287, 'grad_norm': 0.9593490362167358, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  9.57it/s]100%|██████████| 30/30 [00:04<00:00,  9.85it/s]                                               {'loss': 0.4931, 'grad_norm': 0.9104861617088318, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  9.85it/s]                                               {'train_runtime': 4.7475, 'train_samples_per_second': 89.52, 'train_steps_per_second': 6.319, 'train_loss': 0.6464935561021169, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  9.85it/s]100%|██████████| 30/30 [00:04<00:00,  6.33it/s]
CLIENT:28
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:02,  9.83it/s]                                              {'loss': 0.5933, 'grad_norm': 0.7512792944908142, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02,  9.83it/s]                                              {'loss': 0.713, 'grad_norm': 0.6779643297195435, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02,  9.83it/s] 10%|█         | 3/30 [00:00<00:02, 11.69it/s]                                              {'loss': 0.7156, 'grad_norm': 0.5172649025917053, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.69it/s]                                              {'loss': 0.7404, 'grad_norm': 0.6654236912727356, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.69it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.85it/s]                                              {'loss': 0.6804, 'grad_norm': 0.574277937412262, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.85it/s]                                              {'loss': 0.6619, 'grad_norm': 0.828079104423523, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:02, 11.85it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.34it/s]                                              {'loss': 0.658, 'grad_norm': 0.3488028943538666, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.34it/s]                                              {'loss': 0.6653, 'grad_norm': 0.5528801679611206, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  4.34it/s] 30%|███       | 9/30 [00:01<00:03,  5.63it/s]                                              {'loss': 0.6511, 'grad_norm': 0.32024237513542175, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.63it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.17it/s]                                               {'loss': 0.6704, 'grad_norm': 0.210720494389534, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.17it/s]                                               {'loss': 0.6942, 'grad_norm': 0.18187658488750458, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  6.17it/s] 40%|████      | 12/30 [00:01<00:02,  7.91it/s]                                               {'loss': 0.6952, 'grad_norm': 1.161970615386963, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.91it/s]                                               {'loss': 0.6506, 'grad_norm': 0.4114469587802887, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.91it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.24it/s]                                               {'loss': 0.6638, 'grad_norm': 0.6810073256492615, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.24it/s]                                               {'loss': 0.5295, 'grad_norm': 1.0363683700561523, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.24it/s] 53%|█████▎    | 16/30 [00:02<00:01,  8.98it/s]                                               {'loss': 0.6315, 'grad_norm': 0.4482575058937073, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.98it/s]                                               {'loss': 0.6865, 'grad_norm': 0.5699224472045898, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  8.98it/s] 60%|██████    | 18/30 [00:02<00:01, 10.65it/s]                                               {'loss': 0.6128, 'grad_norm': 0.38052940368652344, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 10.65it/s]                                               {'loss': 0.772, 'grad_norm': 0.9741370677947998, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01, 10.65it/s] 67%|██████▋   | 20/30 [00:02<00:00, 10.28it/s]                                               {'loss': 0.6341, 'grad_norm': 0.2110191434621811, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 10.28it/s]                                               {'loss': 0.6575, 'grad_norm': 0.3725183606147766, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 10.28it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.89it/s]                                               {'loss': 0.6103, 'grad_norm': 0.39215198159217834, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  9.89it/s]                                               {'loss': 0.568, 'grad_norm': 0.5068211555480957, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  9.89it/s] 80%|████████  | 24/30 [00:02<00:00, 11.24it/s]                                               {'loss': 0.6041, 'grad_norm': 0.7012143731117249, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 11.24it/s]                                               {'loss': 0.6883, 'grad_norm': 0.5028628706932068, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 11.24it/s] 87%|████████▋ | 26/30 [00:02<00:00, 10.74it/s]                                               {'loss': 0.6113, 'grad_norm': 0.510724663734436, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 10.74it/s]                                               {'loss': 0.6999, 'grad_norm': 0.7452929019927979, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 10.74it/s] 93%|█████████▎| 28/30 [00:03<00:00, 10.46it/s]                                               {'loss': 0.6124, 'grad_norm': 0.4297800064086914, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 10.46it/s]                                               {'loss': 0.5316, 'grad_norm': 0.7977305054664612, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 10.46it/s]100%|██████████| 30/30 [00:03<00:00, 11.91it/s]                                               {'loss': 0.5425, 'grad_norm': 0.6498456001281738, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.91it/s]                                               {'train_runtime': 3.4215, 'train_samples_per_second': 124.213, 'train_steps_per_second': 8.768, 'train_loss': 0.6481804947058359, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 11.91it/s]100%|██████████| 30/30 [00:03<00:00,  8.77it/s]
CLIENT:94
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:01<00:33,  1.17s/it]                                              {'loss': 0.5924, 'grad_norm': 0.6356278657913208, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:01<00:33,  1.17s/it]  7%|▋         | 2/30 [00:01<00:18,  1.53it/s]                                              {'loss': 0.7229, 'grad_norm': 0.6042304039001465, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:18,  1.53it/s] 10%|█         | 3/30 [00:01<00:12,  2.11it/s]                                              {'loss': 0.7842, 'grad_norm': 0.7605900168418884, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:12,  2.11it/s] 13%|█▎        | 4/30 [00:02<00:10,  2.47it/s]                                              {'loss': 0.6827, 'grad_norm': 0.8610709309577942, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:02<00:10,  2.47it/s] 17%|█▋        | 5/30 [00:02<00:09,  2.73it/s]                                              {'loss': 0.6915, 'grad_norm': 0.30976319313049316, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:02<00:09,  2.73it/s] 20%|██        | 6/30 [00:02<00:07,  3.37it/s]                                              {'loss': 0.739, 'grad_norm': 1.2750210762023926, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:07,  3.37it/s] 23%|██▎       | 7/30 [00:02<00:06,  3.35it/s]                                              {'loss': 0.6903, 'grad_norm': 0.2711837589740753, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:06,  3.35it/s] 27%|██▋       | 8/30 [00:03<00:06,  3.48it/s]                                              {'loss': 0.6902, 'grad_norm': 0.3467932939529419, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:03<00:06,  3.48it/s] 30%|███       | 9/30 [00:03<00:05,  3.70it/s]                                              {'loss': 0.675, 'grad_norm': 0.34734582901000977, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:03<00:05,  3.70it/s] 33%|███▎      | 10/30 [00:03<00:05,  3.93it/s]                                               {'loss': 0.6552, 'grad_norm': 0.25412508845329285, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:05,  3.93it/s] 37%|███▋      | 11/30 [00:03<00:04,  4.09it/s]                                               {'loss': 0.6641, 'grad_norm': 0.144762322306633, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:04,  4.09it/s]                                               {'loss': 0.6068, 'grad_norm': 0.925788164138794, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:04,  4.09it/s] 43%|████▎     | 13/30 [00:04<00:03,  4.93it/s]                                               {'loss': 0.6523, 'grad_norm': 0.3619542419910431, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:04<00:03,  4.93it/s] 47%|████▋     | 14/30 [00:04<00:03,  4.80it/s]                                               {'loss': 0.6816, 'grad_norm': 0.35541480779647827, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:03,  4.80it/s] 50%|█████     | 15/30 [00:04<00:03,  4.30it/s]                                               {'loss': 0.678, 'grad_norm': 0.735412061214447, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:03,  4.30it/s] 53%|█████▎    | 16/30 [00:04<00:03,  4.08it/s]                                               {'loss': 0.6474, 'grad_norm': 0.3989764153957367, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:03,  4.08it/s] 57%|█████▋    | 17/30 [00:05<00:03,  3.89it/s]                                               {'loss': 0.668, 'grad_norm': 0.2365485280752182, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:05<00:03,  3.89it/s] 60%|██████    | 18/30 [00:05<00:02,  4.70it/s]                                               {'loss': 0.6796, 'grad_norm': 0.600734531879425, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:02,  4.70it/s] 63%|██████▎   | 19/30 [00:05<00:02,  4.29it/s]                                               {'loss': 0.6861, 'grad_norm': 0.635313093662262, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  4.29it/s] 67%|██████▋   | 20/30 [00:05<00:02,  4.14it/s]                                               {'loss': 0.6809, 'grad_norm': 0.18981194496154785, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  4.14it/s] 70%|███████   | 21/30 [00:06<00:02,  3.85it/s]                                               {'loss': 0.6655, 'grad_norm': 0.2639816403388977, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:06<00:02,  3.85it/s] 73%|███████▎  | 22/30 [00:06<00:02,  3.64it/s]                                               {'loss': 0.6343, 'grad_norm': 0.2547481060028076, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:06<00:02,  3.64it/s] 77%|███████▋  | 23/30 [00:06<00:01,  3.64it/s]                                               {'loss': 0.6767, 'grad_norm': 0.6698832511901855, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  3.64it/s]                                               {'loss': 0.635, 'grad_norm': 0.31913721561431885, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  3.64it/s] 83%|████████▎ | 25/30 [00:06<00:01,  4.86it/s]                                               {'loss': 0.696, 'grad_norm': 0.9443559646606445, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:01,  4.86it/s] 87%|████████▋ | 26/30 [00:07<00:00,  4.80it/s]                                               {'loss': 0.669, 'grad_norm': 0.8516114950180054, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:07<00:00,  4.80it/s] 90%|█████████ | 27/30 [00:07<00:00,  4.81it/s]                                               {'loss': 0.6266, 'grad_norm': 0.43869855999946594, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:07<00:00,  4.81it/s] 93%|█████████▎| 28/30 [00:07<00:00,  4.56it/s]                                               {'loss': 0.6587, 'grad_norm': 0.4248269498348236, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  4.56it/s] 97%|█████████▋| 29/30 [00:07<00:00,  4.48it/s]                                               {'loss': 0.6449, 'grad_norm': 0.46857017278671265, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  4.48it/s]                                               {'loss': 0.632, 'grad_norm': 0.36831802129745483, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  4.48it/s]                                               {'train_runtime': 8.0451, 'train_samples_per_second': 52.827, 'train_steps_per_second': 3.729, 'train_loss': 0.6702259560426076, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.48it/s]100%|██████████| 30/30 [00:08<00:00,  3.73it/s]
CLIENT:25
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7137, 'grad_norm': 0.7609427571296692, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 10.53it/s]  7%|▋         | 2/30 [00:00<00:02, 11.76it/s]                                              {'loss': 0.5681, 'grad_norm': 0.9292551279067993, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 11.76it/s]                                              {'loss': 0.5604, 'grad_norm': 0.9401795268058777, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.76it/s] 13%|█▎        | 4/30 [00:00<00:02, 11.88it/s]                                              {'loss': 0.765, 'grad_norm': 3.0615298748016357, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.88it/s]                                              {'loss': 0.9411, 'grad_norm': 2.6691393852233887, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.88it/s] 20%|██        | 6/30 [00:01<00:04,  5.06it/s]                                              {'loss': 1.0135, 'grad_norm': 3.360729455947876, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.06it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.73it/s]                                              {'loss': 0.4819, 'grad_norm': 0.9581491947174072, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.73it/s]                                              {'loss': 0.8114, 'grad_norm': 1.5608280897140503, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.73it/s] 30%|███       | 9/30 [00:01<00:03,  6.88it/s]                                              {'loss': 0.631, 'grad_norm': 0.7286351323127747, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.88it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.13it/s]                                               {'loss': 0.6112, 'grad_norm': 0.6612798571586609, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.13it/s]                                               {'loss': 0.551, 'grad_norm': 0.7533462643623352, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.13it/s] 40%|████      | 12/30 [00:01<00:01,  9.01it/s]                                               {'loss': 0.3679, 'grad_norm': 2.0581939220428467, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01,  9.01it/s]                                               {'loss': 0.5882, 'grad_norm': 0.6652134656906128, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  9.01it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.55it/s]                                               {'loss': 0.5377, 'grad_norm': 0.9450171589851379, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.55it/s] 50%|█████     | 15/30 [00:02<00:01,  7.55it/s]                                               {'loss': 0.9078, 'grad_norm': 3.160231351852417, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.55it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.50it/s]                                               {'loss': 0.5271, 'grad_norm': 0.8862390518188477, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.50it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.38it/s]                                               {'loss': 0.3935, 'grad_norm': 1.212534785270691, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.38it/s]                                               {'loss': 0.2374, 'grad_norm': 1.7023708820343018, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.38it/s] 63%|██████▎   | 19/30 [00:02<00:01,  7.66it/s]                                               {'loss': 0.4555, 'grad_norm': 0.9137860536575317, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.66it/s]                                               {'loss': 0.6882, 'grad_norm': 1.496259331703186, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  7.66it/s] 70%|███████   | 21/30 [00:02<00:01,  8.00it/s]                                               {'loss': 0.5952, 'grad_norm': 1.547293782234192, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.00it/s] 73%|███████▎  | 22/30 [00:02<00:01,  7.61it/s]                                               {'loss': 0.4119, 'grad_norm': 0.7494816780090332, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  7.61it/s] 77%|███████▋  | 23/30 [00:03<00:00,  7.34it/s]                                               {'loss': 0.5865, 'grad_norm': 1.738285779953003, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.34it/s]                                               {'loss': 0.5293, 'grad_norm': 2.171032667160034, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.34it/s] 83%|████████▎ | 25/30 [00:03<00:00,  8.34it/s]                                               {'loss': 0.6578, 'grad_norm': 1.7858551740646362, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  8.34it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.82it/s]                                               {'loss': 0.4596, 'grad_norm': 0.9430344104766846, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.82it/s] 90%|█████████ | 27/30 [00:03<00:00,  7.50it/s]                                               {'loss': 0.4302, 'grad_norm': 0.9936445951461792, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  7.50it/s] 93%|█████████▎| 28/30 [00:03<00:00,  7.55it/s]                                               {'loss': 0.5168, 'grad_norm': 1.2193571329116821, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  7.55it/s] 97%|█████████▋| 29/30 [00:03<00:00,  7.35it/s]                                               {'loss': 0.565, 'grad_norm': 1.9873725175857544, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  7.35it/s]                                               {'loss': 0.3942, 'grad_norm': 1.3247427940368652, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  7.35it/s]                                               {'train_runtime': 4.1136, 'train_samples_per_second': 103.315, 'train_steps_per_second': 7.293, 'train_loss': 0.5832697570323944, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.35it/s]100%|██████████| 30/30 [00:04<00:00,  7.29it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  8%|▊         | 5/66 [00:00<00:01, 34.70it/s] 14%|█▎        | 9/66 [00:00<00:01, 30.98it/s] 20%|█▉        | 13/66 [00:00<00:01, 28.71it/s] 24%|██▍       | 16/66 [00:00<00:01, 27.22it/s] 29%|██▉       | 19/66 [00:00<00:01, 26.56it/s] 33%|███▎      | 22/66 [00:00<00:01, 27.53it/s] 39%|███▉      | 26/66 [00:00<00:01, 28.33it/s] 45%|████▌     | 30/66 [00:01<00:01, 28.98it/s] 50%|█████     | 33/66 [00:01<00:01, 28.23it/s] 55%|█████▍    | 36/66 [00:01<00:01, 25.67it/s] 59%|█████▉    | 39/66 [00:01<00:01, 23.35it/s] 64%|██████▎   | 42/66 [00:01<00:01, 21.65it/s] 68%|██████▊   | 45/66 [00:01<00:00, 21.12it/s] 73%|███████▎  | 48/66 [00:01<00:00, 20.42it/s] 77%|███████▋  | 51/66 [00:02<00:00, 21.68it/s] 82%|████████▏ | 54/66 [00:02<00:00, 22.40it/s] 86%|████████▋ | 57/66 [00:02<00:00, 24.00it/s] 91%|█████████ | 60/66 [00:02<00:00, 24.42it/s] 95%|█████████▌| 63/66 [00:02<00:00, 24.69it/s]100%|██████████| 66/66 [00:02<00:00, 25.32it/s]
{'eval_loss': 0.6540561318397522, 'eval_model_preparation_time': 0.006, 'eval_acc': 0.6673058485139022, 'eval_runtime': 2.6441, 'eval_samples_per_second': 394.456, 'eval_steps_per_second': 24.961}
ROUND:5
CLIENT:79
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  9.01it/s]                                              {'loss': 0.8943, 'grad_norm': 1.5486785173416138, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  9.01it/s]                                              {'loss': 0.4582, 'grad_norm': 1.4375431537628174, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  9.01it/s] 10%|█         | 3/30 [00:00<00:02, 11.94it/s]                                              {'loss': 0.369, 'grad_norm': 1.1772534847259521, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.94it/s]                                              {'loss': 0.1247, 'grad_norm': 2.992622137069702, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.94it/s] 17%|█▋        | 5/30 [00:00<00:01, 12.63it/s]                                              {'loss': 0.3026, 'grad_norm': 0.7965102791786194, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:01, 12.63it/s]                                              {'loss': 0.9739, 'grad_norm': 3.785456657409668, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 12.63it/s] 23%|██▎       | 7/30 [00:00<00:01, 14.28it/s]                                              {'loss': 0.1761, 'grad_norm': 0.43096715211868286, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.28it/s]                                              {'loss': 0.5979, 'grad_norm': 1.6519993543624878, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 14.28it/s] 30%|███       | 9/30 [00:00<00:01, 13.57it/s]                                              {'loss': 0.4356, 'grad_norm': 1.9862751960754395, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.57it/s]                                              {'loss': 0.145, 'grad_norm': 2.2817656993865967, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.57it/s] 37%|███▋      | 11/30 [00:00<00:01, 13.27it/s]                                               {'loss': 0.0532, 'grad_norm': 0.646925687789917, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.27it/s]                                               {'loss': 0.0104, 'grad_norm': 0.1017518937587738, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 13.27it/s] 43%|████▎     | 13/30 [00:00<00:01, 14.70it/s]                                               {'loss': 0.4876, 'grad_norm': 2.9324543476104736, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 14.70it/s]                                               {'loss': 0.0392, 'grad_norm': 0.5738324522972107, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 14.70it/s] 50%|█████     | 15/30 [00:01<00:01, 13.97it/s]                                               {'loss': 0.8786, 'grad_norm': 3.4500303268432617, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.97it/s]                                               {'loss': 0.2423, 'grad_norm': 1.0102224349975586, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.97it/s] 57%|█████▋    | 17/30 [00:01<00:00, 13.57it/s]                                               {'loss': 0.0654, 'grad_norm': 0.7909868359565735, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.57it/s]                                               {'loss': 0.063, 'grad_norm': 0.8508497476577759, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.57it/s] 63%|██████▎   | 19/30 [00:01<00:00, 14.44it/s]                                               {'loss': 0.286, 'grad_norm': 1.0629955530166626, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 14.44it/s]                                               {'loss': 0.2796, 'grad_norm': 0.9924654960632324, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 14.44it/s] 70%|███████   | 21/30 [00:01<00:00, 13.83it/s]                                               {'loss': 0.0448, 'grad_norm': 0.5944356322288513, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.83it/s]                                               {'loss': 0.4533, 'grad_norm': 2.777387857437134, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.83it/s] 77%|███████▋  | 23/30 [00:01<00:00, 13.52it/s]                                               {'loss': 0.14, 'grad_norm': 0.9799680709838867, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.52it/s]                                               {'loss': 0.0802, 'grad_norm': 0.910698413848877, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 13.52it/s] 83%|████████▎ | 25/30 [00:01<00:00, 14.93it/s]                                               {'loss': 0.0513, 'grad_norm': 0.6648843288421631, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.93it/s]                                               {'loss': 0.2923, 'grad_norm': 2.1949522495269775, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 14.93it/s] 90%|█████████ | 27/30 [00:01<00:00, 14.07it/s]                                               {'loss': 0.0402, 'grad_norm': 0.5555469989776611, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 14.07it/s]                                               {'loss': 0.2138, 'grad_norm': 1.3691468238830566, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 14.07it/s] 97%|█████████▋| 29/30 [00:02<00:00, 13.79it/s]                                               {'loss': 0.5363, 'grad_norm': 1.477486252784729, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.79it/s]                                               {'loss': 0.1263, 'grad_norm': 1.4812369346618652, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.79it/s]                                               {'train_runtime': 2.2685, 'train_samples_per_second': 187.35, 'train_steps_per_second': 13.225, 'train_loss': 0.2953706591700514, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.79it/s]100%|██████████| 30/30 [00:02<00:00, 13.23it/s]
CLIENT:75
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:09,  2.90it/s]                                              {'loss': 0.7835, 'grad_norm': 1.245715856552124, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:09,  2.90it/s]  7%|▋         | 2/30 [00:00<00:09,  2.94it/s]                                              {'loss': 0.547, 'grad_norm': 0.9935932159423828, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:09,  2.94it/s] 10%|█         | 3/30 [00:01<00:09,  2.98it/s]                                              {'loss': 0.5364, 'grad_norm': 0.684587836265564, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:09,  2.98it/s] 13%|█▎        | 4/30 [00:01<00:08,  2.95it/s]                                              {'loss': 0.7696, 'grad_norm': 4.4581499099731445, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  2.95it/s] 17%|█▋        | 5/30 [00:01<00:08,  3.08it/s]                                              {'loss': 1.0697, 'grad_norm': 13.730369567871094, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:08,  3.08it/s] 20%|██        | 6/30 [00:01<00:07,  3.11it/s]                                              {'loss': 1.033, 'grad_norm': 8.018491744995117, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:07,  3.11it/s] 23%|██▎       | 7/30 [00:02<00:07,  3.24it/s]                                              {'loss': 0.4107, 'grad_norm': 1.1748768091201782, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:07,  3.24it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.51it/s]                                              {'loss': 0.7319, 'grad_norm': 2.28551983833313, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.51it/s] 30%|███       | 9/30 [00:02<00:05,  4.14it/s]                                              {'loss': 0.7509, 'grad_norm': 1.6067537069320679, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  4.14it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.55it/s]                                               {'loss': 0.6528, 'grad_norm': 0.605290412902832, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.55it/s] 37%|███▋      | 11/30 [00:03<00:04,  4.56it/s]                                               {'loss': 0.662, 'grad_norm': 0.5065528154373169, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:04,  4.56it/s]                                               {'loss': 0.5567, 'grad_norm': 0.8139111995697021, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:03,  4.56it/s] 43%|████▎     | 13/30 [00:03<00:03,  5.39it/s]                                               {'loss': 0.5713, 'grad_norm': 0.6754409074783325, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  5.39it/s] 47%|████▋     | 14/30 [00:03<00:02,  5.39it/s]                                               {'loss': 0.6759, 'grad_norm': 0.5406901836395264, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:02,  5.39it/s] 50%|█████     | 15/30 [00:03<00:02,  5.02it/s]                                               {'loss': 0.8863, 'grad_norm': 2.765150308609009, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.02it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.08it/s]                                               {'loss': 0.5417, 'grad_norm': 1.340111255645752, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.08it/s] 57%|█████▋    | 17/30 [00:04<00:02,  5.23it/s]                                               {'loss': 0.6283, 'grad_norm': 0.6682121753692627, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:02,  5.23it/s]                                               {'loss': 0.7262, 'grad_norm': 0.5124842524528503, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  5.23it/s] 63%|██████▎   | 19/30 [00:04<00:01,  6.20it/s]                                               {'loss': 0.6581, 'grad_norm': 0.23443880677223206, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:01,  6.20it/s] 67%|██████▋   | 20/30 [00:04<00:01,  5.78it/s]                                               {'loss': 0.6516, 'grad_norm': 0.24833989143371582, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  5.78it/s] 70%|███████   | 21/30 [00:04<00:01,  5.46it/s]                                               {'loss': 0.7204, 'grad_norm': 0.42128288745880127, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.46it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.75it/s]                                               {'loss': 0.658, 'grad_norm': 0.39612501859664917, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.75it/s] 77%|███████▋  | 23/30 [00:05<00:01,  5.81it/s]                                               {'loss': 0.6573, 'grad_norm': 0.7132623791694641, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  5.81it/s]                                               {'loss': 0.679, 'grad_norm': 0.3414607346057892, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.81it/s] 83%|████████▎ | 25/30 [00:05<00:00,  7.46it/s]                                               {'loss': 0.6748, 'grad_norm': 0.1982644945383072, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  7.46it/s] 87%|████████▋ | 26/30 [00:05<00:00,  7.17it/s]                                               {'loss': 0.5633, 'grad_norm': 1.133859395980835, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  7.17it/s]                                               {'loss': 0.7224, 'grad_norm': 0.5647923350334167, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  7.17it/s] 93%|█████████▎| 28/30 [00:05<00:00,  7.61it/s]                                               {'loss': 0.6845, 'grad_norm': 0.13780276477336884, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  7.61it/s]                                               {'loss': 0.7188, 'grad_norm': 0.7300184965133667, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  7.61it/s]100%|██████████| 30/30 [00:05<00:00,  9.45it/s]                                               {'loss': 0.6299, 'grad_norm': 0.647860586643219, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  9.45it/s]                                               {'train_runtime': 5.9129, 'train_samples_per_second': 71.877, 'train_steps_per_second': 5.074, 'train_loss': 0.6850656867027283, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  9.45it/s]100%|██████████| 30/30 [00:05<00:00,  5.08it/s]
CLIENT:63
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.74it/s]                                              {'loss': 0.5859, 'grad_norm': 0.2902335524559021, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.74it/s]  7%|▋         | 2/30 [00:00<00:04,  6.96it/s]                                              {'loss': 0.6642, 'grad_norm': 0.6718466281890869, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.96it/s] 10%|█         | 3/30 [00:00<00:03,  6.81it/s]                                              {'loss': 0.7416, 'grad_norm': 0.5575317740440369, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  6.81it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.90it/s]                                              {'loss': 0.6685, 'grad_norm': 1.0160367488861084, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.90it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.40it/s]                                              {'loss': 0.6659, 'grad_norm': 0.4961831867694855, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.40it/s] 20%|██        | 6/30 [00:00<00:03,  7.07it/s]                                              {'loss': 0.7768, 'grad_norm': 1.7683274745941162, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.07it/s] 23%|██▎       | 7/30 [00:01<00:03,  7.16it/s]                                              {'loss': 0.6094, 'grad_norm': 0.568589985370636, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  7.16it/s] 27%|██▋       | 8/30 [00:01<00:03,  7.05it/s]                                              {'loss': 0.7261, 'grad_norm': 0.9374101161956787, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  7.05it/s] 30%|███       | 9/30 [00:01<00:03,  5.54it/s]                                              {'loss': 0.6589, 'grad_norm': 0.5752143263816833, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.54it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.15it/s]                                               {'loss': 0.5938, 'grad_norm': 0.7898997664451599, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.15it/s] 37%|███▋      | 11/30 [00:01<00:03,  5.00it/s]                                               {'loss': 0.6298, 'grad_norm': 0.4167849123477936, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  5.00it/s]                                               {'loss': 0.4833, 'grad_norm': 1.0305324792861938, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:03,  5.00it/s] 43%|████▎     | 13/30 [00:02<00:02,  5.85it/s]                                               {'loss': 0.5242, 'grad_norm': 0.9283031821250916, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  5.85it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.37it/s]                                               {'loss': 0.5302, 'grad_norm': 0.5065986514091492, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.37it/s] 50%|█████     | 15/30 [00:02<00:02,  5.03it/s]                                               {'loss': 0.8548, 'grad_norm': 3.09237003326416, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.03it/s] 53%|█████▎    | 16/30 [00:02<00:02,  4.98it/s]                                               {'loss': 0.5851, 'grad_norm': 1.0687110424041748, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  4.98it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.87it/s]                                               {'loss': 0.4478, 'grad_norm': 1.3246451616287231, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.87it/s] 60%|██████    | 18/30 [00:03<00:02,  5.25it/s]                                               {'loss': 0.6732, 'grad_norm': 1.4797124862670898, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.25it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.77it/s]                                               {'loss': 0.4703, 'grad_norm': 0.7957791686058044, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.77it/s] 67%|██████▋   | 20/30 [00:03<00:02,  4.97it/s]                                               {'loss': 0.6198, 'grad_norm': 1.0054893493652344, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:02,  4.97it/s] 70%|███████   | 21/30 [00:03<00:01,  4.55it/s]                                               {'loss': 0.5396, 'grad_norm': 0.8145922422409058, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  4.55it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.14it/s]                                               {'loss': 0.4713, 'grad_norm': 1.946246862411499, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.14it/s] 77%|███████▋  | 23/30 [00:04<00:01,  3.88it/s]                                               {'loss': 0.8794, 'grad_norm': 3.550821304321289, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  3.88it/s] 80%|████████  | 24/30 [00:04<00:01,  3.85it/s]                                               {'loss': 0.3692, 'grad_norm': 1.6959140300750732, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  3.85it/s] 83%|████████▎ | 25/30 [00:05<00:01,  3.65it/s]                                               {'loss': 0.5118, 'grad_norm': 0.7732475996017456, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  3.65it/s] 87%|████████▋ | 26/30 [00:05<00:01,  3.76it/s]                                               {'loss': 0.6121, 'grad_norm': 1.219899296760559, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:01,  3.76it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.00it/s]                                               {'loss': 0.4829, 'grad_norm': 1.727966547012329, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.00it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.41it/s]                                               {'loss': 0.6435, 'grad_norm': 1.419548749923706, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.41it/s] 97%|█████████▋| 29/30 [00:05<00:00,  4.80it/s]                                               {'loss': 0.4839, 'grad_norm': 1.0836668014526367, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  4.80it/s]100%|██████████| 30/30 [00:06<00:00,  4.83it/s]                                               {'loss': 0.3688, 'grad_norm': 1.5827606916427612, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.83it/s]                                               {'train_runtime': 6.475, 'train_samples_per_second': 65.637, 'train_steps_per_second': 4.633, 'train_loss': 0.5957347730795542, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.83it/s]100%|██████████| 30/30 [00:06<00:00,  4.63it/s]
CLIENT:15
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.09it/s]                                              {'loss': 0.9086, 'grad_norm': 1.8538854122161865, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.09it/s]  7%|▋         | 2/30 [00:00<00:04,  5.97it/s]                                              {'loss': 0.4356, 'grad_norm': 1.4833624362945557, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  5.97it/s] 10%|█         | 3/30 [00:00<00:04,  6.07it/s]                                              {'loss': 0.29, 'grad_norm': 1.4935688972473145, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.07it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.37it/s]                                              {'loss': 0.1324, 'grad_norm': 1.17951238155365, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.37it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.14it/s]                                              {'loss': 0.0809, 'grad_norm': 1.095353364944458, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.14it/s]                                              {'loss': 0.0108, 'grad_norm': 1.8963050842285156, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.14it/s] 23%|██▎       | 7/30 [00:01<00:03,  7.24it/s]                                              {'loss': 0.0075, 'grad_norm': 0.10774838179349899, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  7.24it/s] 27%|██▋       | 8/30 [00:01<00:03,  6.56it/s]                                              {'loss': 0.0027, 'grad_norm': 0.030497580766677856, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.56it/s] 30%|███       | 9/30 [00:01<00:03,  6.81it/s]                                              {'loss': 0.0029, 'grad_norm': 0.03795325383543968, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.81it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.53it/s]                                               {'loss': 0.0022, 'grad_norm': 0.056812968105077744, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.53it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.35it/s]                                               {'loss': 0.0014, 'grad_norm': 0.020677665248513222, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.35it/s] 40%|████      | 12/30 [00:01<00:02,  6.94it/s]                                               {'loss': 0.0009, 'grad_norm': 0.04728519916534424, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.94it/s] 43%|████▎     | 13/30 [00:01<00:02,  6.22it/s]                                               {'loss': 0.001, 'grad_norm': 0.025899279862642288, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  6.22it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.49it/s]                                               {'loss': 0.0005, 'grad_norm': 0.01064165960997343, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.49it/s] 50%|█████     | 15/30 [00:02<00:02,  5.99it/s]                                               {'loss': 0.0005, 'grad_norm': 0.00898471474647522, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.99it/s] 53%|█████▎    | 16/30 [00:02<00:02,  4.90it/s]                                               {'loss': 0.0004, 'grad_norm': 0.004879351705312729, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  4.90it/s] 57%|█████▋    | 17/30 [00:02<00:02,  5.09it/s]                                               {'loss': 0.0004, 'grad_norm': 0.009520831517875195, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  5.09it/s] 60%|██████    | 18/30 [00:02<00:02,  5.11it/s]                                               {'loss': 0.0004, 'grad_norm': 0.007277493365108967, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:02,  5.11it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.72it/s]                                               {'loss': 0.0004, 'grad_norm': 0.0051316688768565655, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.72it/s] 67%|██████▋   | 20/30 [00:03<00:02,  4.56it/s]                                               {'loss': 0.0003, 'grad_norm': 0.00537573779001832, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:02,  4.56it/s] 70%|███████   | 21/30 [00:03<00:02,  4.41it/s]                                               {'loss': 0.0004, 'grad_norm': 0.008422051556408405, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:02,  4.41it/s] 73%|███████▎  | 22/30 [00:03<00:01,  4.22it/s]                                               {'loss': 0.0003, 'grad_norm': 0.00658781174570322, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  4.22it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.57it/s]                                               {'loss': 0.0003, 'grad_norm': 0.004151803441345692, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.57it/s] 80%|████████  | 24/30 [00:04<00:01,  5.11it/s]                                               {'loss': 0.0003, 'grad_norm': 0.005155595485121012, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  5.11it/s] 83%|████████▎ | 25/30 [00:04<00:01,  4.78it/s]                                               {'loss': 0.0003, 'grad_norm': 0.005640554241836071, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:01,  4.78it/s] 87%|████████▋ | 26/30 [00:04<00:00,  5.10it/s]                                               {'loss': 0.0003, 'grad_norm': 0.005635615438222885, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  5.10it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.21it/s]                                               {'loss': 0.0003, 'grad_norm': 0.003038486000150442, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.21it/s] 93%|█████████▎| 28/30 [00:05<00:00,  5.21it/s]                                               {'loss': 0.0003, 'grad_norm': 0.004637300968170166, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  5.21it/s] 97%|█████████▋| 29/30 [00:05<00:00,  5.24it/s]                                               {'loss': 0.0003, 'grad_norm': 0.003825231222435832, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  5.24it/s]100%|██████████| 30/30 [00:05<00:00,  5.29it/s]                                               {'loss': 0.0003, 'grad_norm': 0.0034360710997134447, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.29it/s]                                               {'train_runtime': 5.9826, 'train_samples_per_second': 71.04, 'train_steps_per_second': 5.015, 'train_loss': 0.06275093561368218, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.29it/s]100%|██████████| 30/30 [00:05<00:00,  5.02it/s]
CLIENT:38
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.91it/s]                                              {'loss': 0.6148, 'grad_norm': 0.3325861692428589, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.91it/s]                                              {'loss': 0.6768, 'grad_norm': 0.9992474913597107, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.91it/s] 10%|█         | 3/30 [00:00<00:02, 10.32it/s]                                              {'loss': 0.6609, 'grad_norm': 0.8979732990264893, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.32it/s]                                              {'loss': 0.753, 'grad_norm': 1.06397545337677, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.32it/s] 17%|█▋        | 5/30 [00:00<00:02, 10.17it/s]                                              {'loss': 0.7696, 'grad_norm': 0.9612230658531189, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.17it/s]                                              {'loss': 0.8503, 'grad_norm': 1.1695988178253174, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 10.17it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.22it/s]                                              {'loss': 0.6168, 'grad_norm': 0.5170565247535706, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  9.22it/s] 27%|██▋       | 8/30 [00:00<00:02,  8.66it/s]                                              {'loss': 0.6876, 'grad_norm': 0.5595642924308777, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  8.66it/s] 30%|███       | 9/30 [00:01<00:02,  8.04it/s]                                              {'loss': 0.6512, 'grad_norm': 0.43262243270874023, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  8.04it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.68it/s]                                               {'loss': 0.6014, 'grad_norm': 0.8552547693252563, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.68it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.00it/s]                                               {'loss': 0.5588, 'grad_norm': 0.9693560600280762, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.00it/s]                                               {'loss': 0.4972, 'grad_norm': 0.7304992079734802, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.00it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.72it/s]                                               {'loss': 0.6053, 'grad_norm': 0.4759295880794525, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.72it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.66it/s]                                               {'loss': 0.5914, 'grad_norm': 0.4982450008392334, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.66it/s] 50%|█████     | 15/30 [00:01<00:02,  6.77it/s]                                               {'loss': 0.5399, 'grad_norm': 0.550544023513794, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:02,  6.77it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.38it/s]                                               {'loss': 0.5743, 'grad_norm': 0.622249960899353, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.38it/s] 57%|█████▋    | 17/30 [00:02<00:02,  6.03it/s]                                               {'loss': 0.4964, 'grad_norm': 1.4339865446090698, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  6.03it/s] 60%|██████    | 18/30 [00:02<00:01,  6.09it/s]                                               {'loss': 0.6251, 'grad_norm': 1.802417278289795, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  6.09it/s] 63%|██████▎   | 19/30 [00:02<00:01,  5.82it/s]                                               {'loss': 0.5182, 'grad_norm': 0.6937751770019531, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  5.82it/s] 67%|██████▋   | 20/30 [00:02<00:01,  5.89it/s]                                               {'loss': 0.5543, 'grad_norm': 0.8924703001976013, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  5.89it/s] 70%|███████   | 21/30 [00:02<00:01,  6.07it/s]                                               {'loss': 0.411, 'grad_norm': 1.5538480281829834, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  6.07it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.21it/s]                                               {'loss': 0.5806, 'grad_norm': 0.9908949136734009, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.21it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.48it/s]                                               {'loss': 0.6803, 'grad_norm': 1.532725214958191, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.48it/s]                                               {'loss': 0.4609, 'grad_norm': 1.7885318994522095, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.48it/s] 83%|████████▎ | 25/30 [00:03<00:00,  7.60it/s]                                               {'loss': 0.4887, 'grad_norm': 1.3064014911651611, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  7.60it/s] 87%|████████▋ | 26/30 [00:03<00:00,  6.76it/s]                                               {'loss': 0.4832, 'grad_norm': 0.9686342477798462, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  6.76it/s] 90%|█████████ | 27/30 [00:03<00:00,  6.17it/s]                                               {'loss': 0.5176, 'grad_norm': 0.9339649677276611, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  6.17it/s] 93%|█████████▎| 28/30 [00:04<00:00,  5.53it/s]                                               {'loss': 0.5474, 'grad_norm': 1.981020212173462, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  5.53it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.24it/s]                                               {'loss': 0.4816, 'grad_norm': 5.302975654602051, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.24it/s]                                               {'loss': 0.4143, 'grad_norm': 2.48163104057312, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.24it/s]                                               {'train_runtime': 4.5771, 'train_samples_per_second': 92.853, 'train_steps_per_second': 6.554, 'train_loss': 0.5836325089136759, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.24it/s]100%|██████████| 30/30 [00:04<00:00,  6.57it/s]
CLIENT:11
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  4.06it/s]                                              {'loss': 0.5959, 'grad_norm': 0.7832778692245483, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  4.06it/s]  7%|▋         | 2/30 [00:00<00:06,  4.56it/s]                                              {'loss': 0.7275, 'grad_norm': 0.7143428921699524, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.56it/s] 10%|█         | 3/30 [00:00<00:05,  4.64it/s]                                              {'loss': 0.7503, 'grad_norm': 0.6023699045181274, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.64it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.73it/s]                                              {'loss': 0.7008, 'grad_norm': 0.5394377708435059, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.73it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.85it/s]                                              {'loss': 0.7133, 'grad_norm': 0.3810088336467743, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.85it/s] 20%|██        | 6/30 [00:01<00:04,  5.71it/s]                                              {'loss': 0.7519, 'grad_norm': 1.1369984149932861, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.71it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.74it/s]                                              {'loss': 0.6747, 'grad_norm': 0.3651502728462219, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.74it/s] 27%|██▋       | 8/30 [00:01<00:05,  4.26it/s]                                              {'loss': 0.6805, 'grad_norm': 0.1940675526857376, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  4.26it/s] 30%|███       | 9/30 [00:02<00:05,  4.00it/s]                                              {'loss': 0.6456, 'grad_norm': 0.30209752917289734, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  4.00it/s] 33%|███▎      | 10/30 [00:02<00:05,  3.83it/s]                                               {'loss': 0.6575, 'grad_norm': 0.1458442509174347, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  3.83it/s] 37%|███▋      | 11/30 [00:02<00:04,  3.90it/s]                                               {'loss': 0.6711, 'grad_norm': 0.16193406283855438, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  3.90it/s]                                               {'loss': 0.57, 'grad_norm': 0.9893282651901245, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:04,  3.90it/s] 43%|████▎     | 13/30 [00:02<00:03,  4.91it/s]                                               {'loss': 0.6412, 'grad_norm': 0.4276424050331116, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  4.91it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.86it/s]                                               {'loss': 0.6814, 'grad_norm': 0.3767177164554596, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.86it/s] 50%|█████     | 15/30 [00:03<00:03,  4.58it/s]                                               {'loss': 0.7151, 'grad_norm': 0.6605175137519836, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.58it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.21it/s]                                               {'loss': 0.6698, 'grad_norm': 0.5187897086143494, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.21it/s] 57%|█████▋    | 17/30 [00:03<00:03,  3.96it/s]                                               {'loss': 0.6639, 'grad_norm': 0.29291781783103943, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  3.96it/s] 60%|██████    | 18/30 [00:04<00:02,  4.64it/s]                                               {'loss': 0.6769, 'grad_norm': 0.7496474385261536, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.64it/s] 63%|██████▎   | 19/30 [00:04<00:02,  4.30it/s]                                               {'loss': 0.6986, 'grad_norm': 0.6768646240234375, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  4.30it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.21it/s]                                               {'loss': 0.6597, 'grad_norm': 0.2941122353076935, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.21it/s] 70%|███████   | 21/30 [00:04<00:02,  3.94it/s]                                               {'loss': 0.6582, 'grad_norm': 0.3526197671890259, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:02,  3.94it/s] 73%|███████▎  | 22/30 [00:05<00:02,  3.76it/s]                                               {'loss': 0.6433, 'grad_norm': 0.3883916437625885, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:02,  3.76it/s] 77%|███████▋  | 23/30 [00:05<00:01,  3.65it/s]                                               {'loss': 0.664, 'grad_norm': 0.3932397961616516, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  3.65it/s] 80%|████████  | 24/30 [00:05<00:01,  4.49it/s]                                               {'loss': 0.6307, 'grad_norm': 0.47916731238365173, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  4.49it/s] 83%|████████▎ | 25/30 [00:05<00:01,  4.14it/s]                                               {'loss': 0.6488, 'grad_norm': 0.6088582277297974, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  4.14it/s] 87%|████████▋ | 26/30 [00:05<00:00,  4.43it/s]                                               {'loss': 0.6114, 'grad_norm': 0.35014522075653076, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:00,  4.43it/s] 90%|█████████ | 27/30 [00:06<00:00,  4.46it/s]                                               {'loss': 0.6547, 'grad_norm': 0.41684696078300476, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  4.46it/s] 93%|█████████▎| 28/30 [00:06<00:00,  4.11it/s]                                               {'loss': 0.676, 'grad_norm': 0.3906010389328003, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  4.11it/s] 97%|█████████▋| 29/30 [00:06<00:00,  3.97it/s]                                               {'loss': 0.6502, 'grad_norm': 0.5732629895210266, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  3.97it/s]                                               {'loss': 0.6363, 'grad_norm': 0.5685831904411316, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  3.97it/s]                                               {'train_runtime': 7.0155, 'train_samples_per_second': 60.58, 'train_steps_per_second': 4.276, 'train_loss': 0.6673149764537811, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  3.97it/s]100%|██████████| 30/30 [00:07<00:00,  4.28it/s]
CLIENT:40
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.29it/s]                                              {'loss': 0.8535, 'grad_norm': 3.3751893043518066, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.29it/s]  7%|▋         | 2/30 [00:00<00:08,  3.40it/s]                                              {'loss': 0.4716, 'grad_norm': 1.4792759418487549, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:08,  3.40it/s] 10%|█         | 3/30 [00:00<00:08,  3.34it/s]                                              {'loss': 0.4549, 'grad_norm': 1.1500825881958008, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:08,  3.34it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.55it/s]                                              {'loss': 0.1972, 'grad_norm': 1.031914234161377, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.55it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.90it/s]                                              {'loss': 0.0888, 'grad_norm': 0.9284238219261169, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.90it/s] 20%|██        | 6/30 [00:01<00:05,  4.67it/s]                                              {'loss': 0.0249, 'grad_norm': 0.5663840174674988, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.67it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.10it/s]                                              {'loss': 0.365, 'grad_norm': 0.8846198320388794, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.10it/s]                                              {'loss': 0.0071, 'grad_norm': 0.08501233160495758, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.10it/s] 30%|███       | 9/30 [00:01<00:03,  6.78it/s]                                              {'loss': 0.3612, 'grad_norm': 2.629939317703247, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  6.78it/s]                                              {'loss': 0.3093, 'grad_norm': 0.9824293255805969, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  6.78it/s] 37%|███▋      | 11/30 [00:01<00:02,  8.16it/s]                                               {'loss': 0.0108, 'grad_norm': 0.15443718433380127, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.16it/s]                                               {'loss': 0.0134, 'grad_norm': 0.20335109531879425, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  8.16it/s] 43%|████▎     | 13/30 [00:02<00:01,  9.68it/s]                                               {'loss': 0.2894, 'grad_norm': 1.177411437034607, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:01,  9.68it/s]                                               {'loss': 0.0137, 'grad_norm': 0.1500721424818039, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  9.68it/s] 50%|█████     | 15/30 [00:02<00:01, 10.45it/s]                                               {'loss': 0.2453, 'grad_norm': 0.9376690983772278, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01, 10.45it/s]                                               {'loss': 0.274, 'grad_norm': 0.47658199071884155, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01, 10.45it/s] 57%|█████▋    | 17/30 [00:02<00:01,  9.41it/s]                                               {'loss': 0.027, 'grad_norm': 0.27669575810432434, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  9.41it/s]                                               {'loss': 0.0325, 'grad_norm': 0.5264580845832825, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  9.41it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.63it/s]                                               {'loss': 0.0321, 'grad_norm': 0.31382572650909424, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.63it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.26it/s]                                               {'loss': 0.0282, 'grad_norm': 0.3246505558490753, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.26it/s] 70%|███████   | 21/30 [00:03<00:01,  6.77it/s]                                               {'loss': 0.0358, 'grad_norm': 0.3991192579269409, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  6.77it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.41it/s]                                               {'loss': 0.0365, 'grad_norm': 0.360551118850708, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.41it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.88it/s]                                               {'loss': 0.4606, 'grad_norm': 1.694419503211975, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.88it/s] 80%|████████  | 24/30 [00:03<00:00,  6.49it/s]                                               {'loss': 1.001, 'grad_norm': 3.9475932121276855, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.49it/s] 83%|████████▎ | 25/30 [00:04<00:00,  5.37it/s]                                               {'loss': 0.029, 'grad_norm': 0.21647852659225464, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  5.37it/s] 87%|████████▋ | 26/30 [00:04<00:00,  5.13it/s]                                               {'loss': 0.4989, 'grad_norm': 1.4649600982666016, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  5.13it/s] 90%|█████████ | 27/30 [00:04<00:00,  4.80it/s]                                               {'loss': 0.028, 'grad_norm': 0.30664750933647156, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  4.80it/s] 93%|█████████▎| 28/30 [00:04<00:00,  4.33it/s]                                               {'loss': 0.2272, 'grad_norm': 0.5705053210258484, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  4.33it/s] 97%|█████████▋| 29/30 [00:04<00:00,  4.36it/s]                                               {'loss': 0.0253, 'grad_norm': 0.20742225646972656, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  4.36it/s]100%|██████████| 30/30 [00:05<00:00,  4.99it/s]                                               {'loss': 0.0213, 'grad_norm': 0.17611049115657806, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.99it/s]                                               {'train_runtime': 5.2567, 'train_samples_per_second': 80.85, 'train_steps_per_second': 5.707, 'train_loss': 0.21545290450255075, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  4.99it/s]100%|██████████| 30/30 [00:05<00:00,  5.71it/s]
CLIENT:45
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.62it/s]                                              {'loss': 0.5824, 'grad_norm': 0.6431879997253418, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.62it/s]  7%|▋         | 2/30 [00:00<00:05,  5.21it/s]                                              {'loss': 0.7145, 'grad_norm': 0.6316874623298645, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.21it/s] 10%|█         | 3/30 [00:00<00:05,  4.67it/s]                                              {'loss': 0.6914, 'grad_norm': 0.4806966483592987, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.67it/s] 13%|█▎        | 4/30 [00:00<00:05,  5.10it/s]                                              {'loss': 0.6761, 'grad_norm': 0.7688046097755432, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  5.10it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.97it/s]                                              {'loss': 0.6379, 'grad_norm': 0.39656561613082886, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.97it/s] 20%|██        | 6/30 [00:01<00:10,  2.20it/s]                                              {'loss': 0.803, 'grad_norm': 1.4873512983322144, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:10,  2.20it/s] 23%|██▎       | 7/30 [00:02<00:08,  2.56it/s]                                              {'loss': 0.6995, 'grad_norm': 0.3554445505142212, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:08,  2.56it/s] 27%|██▋       | 8/30 [00:02<00:07,  3.02it/s]                                              {'loss': 0.6588, 'grad_norm': 0.5134040117263794, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:07,  3.02it/s] 30%|███       | 9/30 [00:02<00:06,  3.38it/s]                                              {'loss': 0.6506, 'grad_norm': 0.38463225960731506, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:06,  3.38it/s] 33%|███▎      | 10/30 [00:02<00:05,  3.78it/s]                                               {'loss': 0.6593, 'grad_norm': 0.2710951566696167, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  3.78it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.17it/s]                                               {'loss': 0.6673, 'grad_norm': 0.17908161878585815, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.17it/s] 40%|████      | 12/30 [00:03<00:03,  5.06it/s]                                               {'loss': 0.7651, 'grad_norm': 1.4428226947784424, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:03,  5.06it/s] 43%|████▎     | 13/30 [00:03<00:03,  5.29it/s]                                               {'loss': 0.6333, 'grad_norm': 1.6917219161987305, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  5.29it/s] 47%|████▋     | 14/30 [00:03<00:02,  5.50it/s]                                               {'loss': 0.6683, 'grad_norm': 0.3574508726596832, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:02,  5.50it/s] 50%|█████     | 15/30 [00:03<00:02,  5.49it/s]                                               {'loss': 0.5455, 'grad_norm': 2.357163429260254, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:02,  5.49it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.37it/s]                                               {'loss': 0.6426, 'grad_norm': 0.6447442173957825, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.37it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.57it/s]                                               {'loss': 0.6846, 'grad_norm': 0.41910621523857117, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.57it/s]                                               {'loss': 0.6457, 'grad_norm': 0.6758777499198914, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  5.57it/s] 63%|██████▎   | 19/30 [00:04<00:01,  6.90it/s]                                               {'loss': 0.7244, 'grad_norm': 0.821988582611084, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:01,  6.90it/s] 67%|██████▋   | 20/30 [00:04<00:01,  6.31it/s]                                               {'loss': 0.6631, 'grad_norm': 0.309049129486084, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  6.31it/s] 70%|███████   | 21/30 [00:04<00:01,  5.93it/s]                                               {'loss': 0.645, 'grad_norm': 0.3692164123058319, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.93it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.82it/s]                                               {'loss': 0.6446, 'grad_norm': 1.477651834487915, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.82it/s] 77%|███████▋  | 23/30 [00:04<00:01,  6.02it/s]                                               {'loss': 0.5978, 'grad_norm': 0.7264083623886108, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  6.02it/s]                                               {'loss': 0.6146, 'grad_norm': 0.7804819345474243, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  6.02it/s] 83%|████████▎ | 25/30 [00:05<00:00,  7.11it/s]                                               {'loss': 0.6854, 'grad_norm': 0.5476121306419373, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  7.11it/s] 87%|████████▋ | 26/30 [00:05<00:00,  7.05it/s]                                               {'loss': 0.6276, 'grad_norm': 0.5399726033210754, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  7.05it/s] 90%|█████████ | 27/30 [00:05<00:00,  6.88it/s]                                               {'loss': 0.668, 'grad_norm': 0.5758041739463806, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  6.88it/s] 93%|█████████▎| 28/30 [00:05<00:00,  6.98it/s]                                               {'loss': 0.6072, 'grad_norm': 0.5011195540428162, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  6.98it/s] 97%|█████████▋| 29/30 [00:05<00:00,  6.89it/s]                                               {'loss': 0.5713, 'grad_norm': 0.5277910232543945, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  6.89it/s]100%|██████████| 30/30 [00:05<00:00,  6.99it/s]                                               {'loss': 0.5951, 'grad_norm': 0.9280516505241394, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.99it/s]                                               {'train_runtime': 6.1439, 'train_samples_per_second': 69.174, 'train_steps_per_second': 4.883, 'train_loss': 0.6556666394074758, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  6.99it/s]100%|██████████| 30/30 [00:06<00:00,  4.88it/s]
CLIENT:39
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.62it/s]                                              {'loss': 0.6435, 'grad_norm': 0.6127222180366516, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.62it/s]  7%|▋         | 2/30 [00:00<00:07,  3.53it/s]                                              {'loss': 0.6019, 'grad_norm': 0.9167711138725281, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.53it/s] 10%|█         | 3/30 [00:00<00:07,  3.46it/s]                                              {'loss': 0.6052, 'grad_norm': 0.8108090758323669, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.46it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.64it/s]                                              {'loss': 0.4026, 'grad_norm': 1.1197590827941895, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.64it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.92it/s]                                              {'loss': 0.6977, 'grad_norm': 1.7538537979125977, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.92it/s]                                              {'loss': 0.5314, 'grad_norm': 1.9138855934143066, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.92it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.76it/s]                                              {'loss': 0.5319, 'grad_norm': 1.1814910173416138, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.76it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.62it/s]                                              {'loss': 0.5708, 'grad_norm': 1.0088649988174438, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.62it/s] 30%|███       | 9/30 [00:02<00:04,  4.66it/s]                                              {'loss': 0.6039, 'grad_norm': 1.1312826871871948, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.66it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.66it/s]                                               {'loss': 0.4266, 'grad_norm': 1.1942524909973145, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.66it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.50it/s]                                               {'loss': 0.452, 'grad_norm': 1.018058180809021, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.50it/s] 40%|████      | 12/30 [00:02<00:03,  4.94it/s]                                               {'loss': 0.3544, 'grad_norm': 0.9299683570861816, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.94it/s] 43%|████▎     | 13/30 [00:02<00:03,  4.91it/s]                                               {'loss': 0.5152, 'grad_norm': 0.7665656805038452, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  4.91it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.93it/s]                                               {'loss': 0.4066, 'grad_norm': 0.9927264451980591, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.93it/s] 50%|█████     | 15/30 [00:03<00:03,  4.67it/s]                                               {'loss': 0.7815, 'grad_norm': 3.391331911087036, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.67it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.54it/s]                                               {'loss': 0.4157, 'grad_norm': 1.0530624389648438, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.54it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.36it/s]                                               {'loss': 0.4521, 'grad_norm': 1.228251576423645, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.36it/s] 60%|██████    | 18/30 [00:04<00:02,  4.69it/s]                                               {'loss': 0.2471, 'grad_norm': 2.523946762084961, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.69it/s] 63%|██████▎   | 19/30 [00:04<00:02,  4.17it/s]                                               {'loss': 0.3819, 'grad_norm': 1.357150912284851, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  4.17it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.45it/s]                                               {'loss': 0.5298, 'grad_norm': 1.4428294897079468, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.45it/s] 70%|███████   | 21/30 [00:04<00:01,  4.85it/s]                                               {'loss': 0.479, 'grad_norm': 1.4121977090835571, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  4.85it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.18it/s]                                               {'loss': 0.4371, 'grad_norm': 1.1004477739334106, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.18it/s] 77%|███████▋  | 23/30 [00:05<00:01,  5.50it/s]                                               {'loss': 0.5435, 'grad_norm': 1.6714203357696533, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  5.50it/s] 80%|████████  | 24/30 [00:05<00:01,  5.84it/s]                                               {'loss': 0.1833, 'grad_norm': 2.83365797996521, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.84it/s] 83%|████████▎ | 25/30 [00:05<00:00,  5.32it/s]                                               {'loss': 0.2163, 'grad_norm': 2.6198060512542725, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  5.32it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.66it/s]                                               {'loss': 0.5032, 'grad_norm': 1.8501898050308228, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.66it/s] 90%|█████████ | 27/30 [00:05<00:00,  5.83it/s]                                               {'loss': 0.3407, 'grad_norm': 1.3829693794250488, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  5.83it/s] 93%|█████████▎| 28/30 [00:05<00:00,  5.79it/s]                                               {'loss': 0.4834, 'grad_norm': 2.8996012210845947, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  5.79it/s] 97%|█████████▋| 29/30 [00:06<00:00,  5.98it/s]                                               {'loss': 0.5234, 'grad_norm': 2.727123737335205, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  5.98it/s]100%|██████████| 30/30 [00:06<00:00,  6.79it/s]                                               {'loss': 0.6367, 'grad_norm': 5.761328220367432, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  6.79it/s]                                               {'train_runtime': 6.4222, 'train_samples_per_second': 66.176, 'train_steps_per_second': 4.671, 'train_loss': 0.4832876498500506, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  6.79it/s]100%|██████████| 30/30 [00:06<00:00,  4.67it/s]
CLIENT:62
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.65it/s]                                              {'loss': 0.8895, 'grad_norm': 2.4288275241851807, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.65it/s]  7%|▋         | 2/30 [00:00<00:06,  4.58it/s]                                              {'loss': 0.5115, 'grad_norm': 0.9832966923713684, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.58it/s] 10%|█         | 3/30 [00:00<00:05,  4.59it/s]                                              {'loss': 0.3283, 'grad_norm': 1.4688019752502441, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.59it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.66it/s]                                              {'loss': 0.4345, 'grad_norm': 0.98773592710495, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.66it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.49it/s]                                              {'loss': 0.3911, 'grad_norm': 1.8567222356796265, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.49it/s]                                              {'loss': 0.5628, 'grad_norm': 1.6130561828613281, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.49it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.29it/s]                                              {'loss': 0.2744, 'grad_norm': 1.6267632246017456, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.29it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.91it/s]                                              {'loss': 0.1675, 'grad_norm': 0.8460090160369873, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.91it/s] 30%|███       | 9/30 [00:01<00:03,  5.33it/s]                                              {'loss': 0.609, 'grad_norm': 2.795588254928589, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.33it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.14it/s]                                               {'loss': 0.595, 'grad_norm': 4.436987400054932, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.14it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.37it/s]                                               {'loss': 0.2026, 'grad_norm': 1.9415409564971924, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.37it/s]                                               {'loss': 0.4753, 'grad_norm': 3.864616632461548, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.37it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.01it/s]                                               {'loss': 0.4452, 'grad_norm': 1.784643292427063, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.01it/s] 47%|████▋     | 14/30 [00:02<00:02,  5.60it/s]                                               {'loss': 0.4218, 'grad_norm': 1.659900188446045, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.60it/s] 50%|█████     | 15/30 [00:02<00:02,  5.71it/s]                                               {'loss': 0.189, 'grad_norm': 2.64288592338562, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.71it/s] 53%|█████▎    | 16/30 [00:02<00:02,  5.76it/s]                                               {'loss': 0.4929, 'grad_norm': 1.715762734413147, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  5.76it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.49it/s]                                               {'loss': 0.4098, 'grad_norm': 1.7240166664123535, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.49it/s]                                               {'loss': 0.4413, 'grad_norm': 2.273618221282959, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  5.49it/s] 63%|██████▎   | 19/30 [00:03<00:01,  6.10it/s]                                               {'loss': 0.26, 'grad_norm': 2.0061655044555664, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  6.10it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.72it/s]                                               {'loss': 0.2784, 'grad_norm': 1.666593074798584, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.72it/s] 70%|███████   | 21/30 [00:03<00:01,  5.63it/s]                                               {'loss': 0.4586, 'grad_norm': 19.868606567382812, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.63it/s] 73%|███████▎  | 22/30 [00:03<00:01,  5.52it/s]                                               {'loss': 0.7455, 'grad_norm': 4.076831340789795, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  5.52it/s] 77%|███████▋  | 23/30 [00:04<00:01,  5.31it/s]                                               {'loss': 0.3089, 'grad_norm': 1.3412559032440186, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  5.31it/s]                                               {'loss': 0.0997, 'grad_norm': 1.878298282623291, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  5.31it/s] 83%|████████▎ | 25/30 [00:04<00:00,  6.22it/s]                                               {'loss': 0.3149, 'grad_norm': 1.224845290184021, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  6.22it/s] 87%|████████▋ | 26/30 [00:04<00:00,  6.29it/s]                                               {'loss': 0.4586, 'grad_norm': 1.3318642377853394, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  6.29it/s] 90%|█████████ | 27/30 [00:04<00:00,  6.39it/s]                                               {'loss': 0.3221, 'grad_norm': 2.1821470260620117, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  6.39it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.37it/s]                                               {'loss': 0.2612, 'grad_norm': 1.0551611185073853, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.37it/s] 97%|█████████▋| 29/30 [00:05<00:00,  6.50it/s]                                               {'loss': 0.3788, 'grad_norm': 2.2562406063079834, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  6.50it/s]                                               {'loss': 0.4953, 'grad_norm': 3.1347713470458984, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.50it/s]                                               {'train_runtime': 5.1979, 'train_samples_per_second': 81.764, 'train_steps_per_second': 5.772, 'train_loss': 0.40745011319716773, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.50it/s]100%|██████████| 30/30 [00:05<00:00,  5.78it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  5%|▍         | 3/66 [00:00<00:04, 15.16it/s]  8%|▊         | 5/66 [00:00<00:04, 12.42it/s] 11%|█         | 7/66 [00:00<00:05, 11.21it/s] 14%|█▎        | 9/66 [00:00<00:06,  8.36it/s] 15%|█▌        | 10/66 [00:01<00:06,  8.10it/s] 17%|█▋        | 11/66 [00:01<00:07,  7.09it/s] 18%|█▊        | 12/66 [00:01<00:07,  7.19it/s] 20%|█▉        | 13/66 [00:01<00:08,  6.45it/s] 21%|██        | 14/66 [00:01<00:07,  6.77it/s] 23%|██▎       | 15/66 [00:01<00:07,  6.58it/s] 24%|██▍       | 16/66 [00:02<00:07,  6.44it/s] 26%|██▌       | 17/66 [00:02<00:07,  6.55it/s] 27%|██▋       | 18/66 [00:02<00:07,  6.53it/s] 29%|██▉       | 19/66 [00:02<00:07,  6.58it/s] 30%|███       | 20/66 [00:02<00:06,  6.90it/s] 32%|███▏      | 21/66 [00:02<00:07,  6.23it/s] 33%|███▎      | 22/66 [00:02<00:06,  6.50it/s] 35%|███▍      | 23/66 [00:03<00:07,  6.05it/s] 36%|███▋      | 24/66 [00:03<00:06,  6.22it/s] 38%|███▊      | 25/66 [00:03<00:06,  6.21it/s] 39%|███▉      | 26/66 [00:03<00:06,  6.17it/s] 41%|████      | 27/66 [00:03<00:06,  6.22it/s] 42%|████▏     | 28/66 [00:03<00:06,  6.09it/s] 44%|████▍     | 29/66 [00:04<00:05,  6.42it/s] 45%|████▌     | 30/66 [00:04<00:05,  6.22it/s] 47%|████▋     | 31/66 [00:04<00:05,  6.42it/s] 48%|████▊     | 32/66 [00:04<00:05,  6.62it/s] 50%|█████     | 33/66 [00:04<00:05,  6.16it/s] 52%|█████▏    | 34/66 [00:04<00:04,  6.46it/s] 53%|█████▎    | 35/66 [00:05<00:05,  6.16it/s] 55%|█████▍    | 36/66 [00:05<00:04,  6.33it/s] 56%|█████▌    | 37/66 [00:05<00:04,  6.15it/s] 58%|█████▊    | 38/66 [00:05<00:04,  6.13it/s] 59%|█████▉    | 39/66 [00:05<00:04,  6.15it/s] 61%|██████    | 40/66 [00:05<00:04,  6.06it/s] 62%|██████▏   | 41/66 [00:06<00:04,  6.22it/s] 64%|██████▎   | 42/66 [00:06<00:03,  6.36it/s] 65%|██████▌   | 43/66 [00:06<00:03,  7.02it/s] 67%|██████▋   | 44/66 [00:06<00:02,  7.35it/s] 68%|██████▊   | 45/66 [00:06<00:02,  7.17it/s] 71%|███████   | 47/66 [00:06<00:02,  7.57it/s] 73%|███████▎  | 48/66 [00:06<00:02,  7.61it/s] 74%|███████▍  | 49/66 [00:07<00:02,  7.80it/s] 76%|███████▌  | 50/66 [00:07<00:02,  7.55it/s] 77%|███████▋  | 51/66 [00:07<00:01,  8.03it/s] 79%|███████▉  | 52/66 [00:07<00:01,  7.53it/s] 80%|████████  | 53/66 [00:07<00:01,  7.93it/s] 82%|████████▏ | 54/66 [00:07<00:01,  7.43it/s] 83%|████████▎ | 55/66 [00:07<00:01,  7.89it/s] 85%|████████▍ | 56/66 [00:07<00:01,  8.30it/s] 86%|████████▋ | 57/66 [00:08<00:01,  7.89it/s] 88%|████████▊ | 58/66 [00:08<00:01,  7.78it/s] 89%|████████▉ | 59/66 [00:08<00:00,  7.78it/s] 91%|█████████ | 60/66 [00:08<00:00,  7.67it/s] 94%|█████████▍| 62/66 [00:08<00:00,  8.13it/s] 95%|█████████▌| 63/66 [00:08<00:00,  8.16it/s] 97%|█████████▋| 64/66 [00:08<00:00,  7.68it/s] 98%|█████████▊| 65/66 [00:09<00:00,  7.85it/s]100%|██████████| 66/66 [00:09<00:00,  7.23it/s]
{'eval_loss': 0.649459958076477, 'eval_model_preparation_time': 0.0061, 'eval_acc': 0.6682646212847555, 'eval_runtime': 9.2473, 'eval_samples_per_second': 112.79, 'eval_steps_per_second': 7.137}
ROUND:6
CLIENT:69
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.63it/s]                                              {'loss': 0.7474, 'grad_norm': 1.4727452993392944, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.63it/s]  7%|▋         | 2/30 [00:00<00:03,  7.34it/s]                                              {'loss': 0.6596, 'grad_norm': 0.7217279076576233, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.34it/s] 10%|█         | 3/30 [00:00<00:03,  7.17it/s]                                              {'loss': 0.637, 'grad_norm': 1.5028272867202759, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.17it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.79it/s]                                              {'loss': 0.6777, 'grad_norm': 0.8407248258590698, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.79it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.52it/s]                                              {'loss': 0.6381, 'grad_norm': 0.4123077690601349, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.52it/s] 20%|██        | 6/30 [00:01<00:10,  2.28it/s]                                              {'loss': 0.9, 'grad_norm': 2.4212706089019775, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:10,  2.28it/s] 23%|██▎       | 7/30 [00:01<00:08,  2.87it/s]                                              {'loss': 0.6416, 'grad_norm': 0.449628621339798, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:08,  2.87it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.65it/s]                                              {'loss': 0.6814, 'grad_norm': 0.5720756649971008, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.65it/s] 30%|███       | 9/30 [00:02<00:04,  4.35it/s]                                              {'loss': 0.6383, 'grad_norm': 0.5262017846107483, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.35it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.10it/s]                                               {'loss': 0.6119, 'grad_norm': 0.43817445635795593, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.10it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.89it/s]                                               {'loss': 0.6693, 'grad_norm': 0.48236992955207825, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.89it/s] 40%|████      | 12/30 [00:02<00:02,  6.68it/s]                                               {'loss': 0.6562, 'grad_norm': 1.6293877363204956, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  6.68it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.77it/s]                                               {'loss': 0.5959, 'grad_norm': 0.7535308003425598, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.77it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.05it/s]                                               {'loss': 0.57, 'grad_norm': 0.6437336802482605, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.05it/s] 50%|█████     | 15/30 [00:02<00:02,  7.02it/s]                                               {'loss': 0.8017, 'grad_norm': 2.3624119758605957, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.02it/s] 53%|█████▎    | 16/30 [00:03<00:01,  7.19it/s]                                               {'loss': 0.6538, 'grad_norm': 1.1631160974502563, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:01,  7.19it/s] 57%|█████▋    | 17/30 [00:03<00:01,  6.95it/s]                                               {'loss': 0.5116, 'grad_norm': 1.2588279247283936, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:01,  6.95it/s]                                               {'loss': 0.6383, 'grad_norm': 1.7032266855239868, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  6.95it/s] 63%|██████▎   | 19/30 [00:03<00:01,  9.58it/s]                                               {'loss': 0.4995, 'grad_norm': 0.8434939980506897, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  9.58it/s]                                               {'loss': 0.5608, 'grad_norm': 0.8049741387367249, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  9.58it/s] 70%|███████   | 21/30 [00:03<00:00, 10.69it/s]                                               {'loss': 0.4971, 'grad_norm': 1.612605094909668, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:00, 10.69it/s]                                               {'loss': 0.585, 'grad_norm': 0.9531431198120117, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00, 10.69it/s] 77%|███████▋  | 23/30 [00:03<00:00,  9.87it/s]                                               {'loss': 0.6071, 'grad_norm': 1.7987964153289795, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  9.87it/s]                                               {'loss': 0.5134, 'grad_norm': 1.6658750772476196, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  9.87it/s] 83%|████████▎ | 25/30 [00:03<00:00,  9.96it/s]                                               {'loss': 0.5258, 'grad_norm': 1.3288123607635498, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  9.96it/s]                                               {'loss': 0.6048, 'grad_norm': 1.239260196685791, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  9.96it/s] 90%|█████████ | 27/30 [00:04<00:00,  8.20it/s]                                               {'loss': 0.4494, 'grad_norm': 1.070090651512146, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  8.20it/s]                                               {'loss': 0.4443, 'grad_norm': 1.2684674263000488, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  8.20it/s] 97%|█████████▋| 29/30 [00:04<00:00,  9.02it/s]                                               {'loss': 0.4944, 'grad_norm': 2.490640163421631, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  9.02it/s]                                               {'loss': 0.3604, 'grad_norm': 3.895922899246216, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  9.02it/s]                                               {'train_runtime': 4.5453, 'train_samples_per_second': 93.503, 'train_steps_per_second': 6.6, 'train_loss': 0.6023908217748006, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  9.02it/s]100%|██████████| 30/30 [00:04<00:00,  6.60it/s]
CLIENT:74
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.40it/s]                                              {'loss': 0.6971, 'grad_norm': 1.3471918106079102, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.40it/s]  7%|▋         | 2/30 [00:00<00:03,  8.87it/s]                                              {'loss': 0.492, 'grad_norm': 1.1000641584396362, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.87it/s]                                              {'loss': 0.7037, 'grad_norm': 1.889803409576416, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.87it/s] 13%|█▎        | 4/30 [00:00<00:02, 10.52it/s]                                              {'loss': 0.5954, 'grad_norm': 1.2899080514907837, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.52it/s]                                              {'loss': 0.4827, 'grad_norm': 0.8152456283569336, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.52it/s] 20%|██        | 6/30 [00:00<00:01, 12.25it/s]                                              {'loss': 0.8648, 'grad_norm': 1.921436071395874, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 12.25it/s]                                              {'loss': 0.333, 'grad_norm': 1.5035138130187988, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 12.25it/s] 27%|██▋       | 8/30 [00:00<00:01, 11.61it/s]                                              {'loss': 0.6942, 'grad_norm': 1.489789366722107, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 11.61it/s]                                              {'loss': 0.4824, 'grad_norm': 0.6487300992012024, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 11.61it/s] 33%|███▎      | 10/30 [00:00<00:01, 11.71it/s]                                               {'loss': 0.492, 'grad_norm': 0.8420244455337524, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.71it/s]                                               {'loss': 0.4152, 'grad_norm': 1.2603610754013062, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 11.71it/s] 40%|████      | 12/30 [00:00<00:01, 13.54it/s]                                               {'loss': 0.1746, 'grad_norm': 1.7193118333816528, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 13.54it/s]                                               {'loss': 0.4393, 'grad_norm': 1.02590811252594, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 13.54it/s] 47%|████▋     | 14/30 [00:01<00:01, 10.42it/s]                                               {'loss': 0.4304, 'grad_norm': 0.9023261666297913, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 10.42it/s]                                               {'loss': 0.5232, 'grad_norm': 1.8891738653182983, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 10.42it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.54it/s]                                               {'loss': 0.4301, 'grad_norm': 1.3248484134674072, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  9.54it/s]                                               {'loss': 0.3136, 'grad_norm': 1.422408103942871, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01,  9.54it/s] 60%|██████    | 18/30 [00:01<00:01,  9.89it/s]                                               {'loss': 0.7085, 'grad_norm': 3.141470193862915, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01,  9.89it/s]                                               {'loss': 0.3373, 'grad_norm': 1.14589262008667, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:01,  9.89it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.11it/s]                                               {'loss': 0.4703, 'grad_norm': 1.5198321342468262, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.11it/s] 70%|███████   | 21/30 [00:02<00:01,  7.83it/s]                                               {'loss': 0.3996, 'grad_norm': 0.9789160490036011, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  7.83it/s] 73%|███████▎  | 22/30 [00:02<00:01,  7.50it/s]                                               {'loss': 0.3472, 'grad_norm': 0.9984312653541565, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  7.50it/s] 77%|███████▋  | 23/30 [00:02<00:00,  7.02it/s]                                               {'loss': 0.4659, 'grad_norm': 1.3737550973892212, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  7.02it/s]                                               {'loss': 0.1853, 'grad_norm': 2.9928855895996094, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  7.02it/s] 83%|████████▎ | 25/30 [00:02<00:00,  8.03it/s]                                               {'loss': 0.169, 'grad_norm': 1.9642151594161987, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00,  8.03it/s] 87%|████████▋ | 26/30 [00:02<00:00,  7.50it/s]                                               {'loss': 0.5354, 'grad_norm': 1.2703213691711426, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00,  7.50it/s] 90%|█████████ | 27/30 [00:03<00:00,  7.20it/s]                                               {'loss': 0.5195, 'grad_norm': 2.190688133239746, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  7.20it/s] 93%|█████████▎| 28/30 [00:03<00:00,  6.95it/s]                                               {'loss': 0.2953, 'grad_norm': 1.0492874383926392, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  6.95it/s] 97%|█████████▋| 29/30 [00:03<00:00,  6.62it/s]                                               {'loss': 0.3636, 'grad_norm': 1.595913052558899, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  6.62it/s]                                               {'loss': 0.5954, 'grad_norm': 5.116978645324707, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  6.62it/s]                                               {'train_runtime': 3.4861, 'train_samples_per_second': 121.914, 'train_steps_per_second': 8.606, 'train_loss': 0.4652002155780792, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  6.62it/s]100%|██████████| 30/30 [00:03<00:00,  8.61it/s]
CLIENT:34
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:10,  2.82it/s]                                              {'loss': 0.7658, 'grad_norm': 1.802660346031189, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:10,  2.82it/s]  7%|▋         | 2/30 [00:00<00:07,  3.99it/s]                                              {'loss': 0.5222, 'grad_norm': 1.058969497680664, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.99it/s] 10%|█         | 3/30 [00:00<00:06,  4.38it/s]                                              {'loss': 0.3667, 'grad_norm': 1.0562467575073242, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.38it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.56it/s]                                              {'loss': 0.5749, 'grad_norm': 2.482647657394409, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.56it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.81it/s]                                              {'loss': 0.4722, 'grad_norm': 1.6219698190689087, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.81it/s] 20%|██        | 6/30 [00:01<00:04,  4.88it/s]                                              {'loss': 1.3187, 'grad_norm': 4.142226696014404, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.88it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.60it/s]                                              {'loss': 0.3412, 'grad_norm': 1.5151398181915283, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.60it/s] 27%|██▋       | 8/30 [00:01<00:05,  4.35it/s]                                              {'loss': 0.3563, 'grad_norm': 1.0658001899719238, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  4.35it/s] 30%|███       | 9/30 [00:01<00:04,  4.77it/s]                                              {'loss': 0.785, 'grad_norm': 2.6002442836761475, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.77it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.29it/s]                                               {'loss': 0.4058, 'grad_norm': 1.2766811847686768, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.29it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.44it/s]                                               {'loss': 0.5934, 'grad_norm': 1.3052105903625488, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.44it/s] 40%|████      | 12/30 [00:02<00:03,  5.84it/s]                                               {'loss': 0.4957, 'grad_norm': 1.9932456016540527, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.84it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.33it/s]                                               {'loss': 0.2997, 'grad_norm': 1.7850919961929321, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.33it/s] 47%|████▋     | 14/30 [00:02<00:03,  5.26it/s]                                               {'loss': 0.4298, 'grad_norm': 1.3500065803527832, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  5.26it/s] 50%|█████     | 15/30 [00:03<00:03,  4.98it/s]                                               {'loss': 0.7493, 'grad_norm': 2.2941689491271973, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.98it/s] 53%|█████▎    | 16/30 [00:03<00:02,  5.11it/s]                                               {'loss': 0.4458, 'grad_norm': 2.921433448791504, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  5.11it/s] 57%|█████▋    | 17/30 [00:03<00:02,  5.72it/s]                                               {'loss': 0.4678, 'grad_norm': 1.0931024551391602, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  5.72it/s] 60%|██████    | 18/30 [00:03<00:01,  6.48it/s]                                               {'loss': 0.8051, 'grad_norm': 4.576558589935303, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  6.48it/s] 63%|██████▎   | 19/30 [00:03<00:01,  6.57it/s]                                               {'loss': 0.3547, 'grad_norm': 1.172690749168396, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  6.57it/s] 67%|██████▋   | 20/30 [00:03<00:01,  6.92it/s]                                               {'loss': 0.4807, 'grad_norm': 0.792834997177124, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  6.92it/s] 70%|███████   | 21/30 [00:03<00:01,  7.40it/s]                                               {'loss': 0.4151, 'grad_norm': 1.457926869392395, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.40it/s] 73%|███████▎  | 22/30 [00:04<00:01,  7.72it/s]                                               {'loss': 0.516, 'grad_norm': 0.9977747201919556, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  7.72it/s]                                               {'loss': 0.347, 'grad_norm': 1.500349998474121, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:00,  7.72it/s] 80%|████████  | 24/30 [00:04<00:00,  8.79it/s]                                               {'loss': 0.4375, 'grad_norm': 1.9313527345657349, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  8.79it/s] 83%|████████▎ | 25/30 [00:04<00:00,  8.19it/s]                                               {'loss': 0.6464, 'grad_norm': 2.4049758911132812, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  8.19it/s] 87%|████████▋ | 26/30 [00:04<00:00,  8.07it/s]                                               {'loss': 0.5076, 'grad_norm': 1.3949817419052124, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  8.07it/s] 90%|█████████ | 27/30 [00:04<00:00,  8.10it/s]                                               {'loss': 0.3297, 'grad_norm': 0.8778353333473206, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  8.10it/s] 93%|█████████▎| 28/30 [00:04<00:00,  8.17it/s]                                               {'loss': 0.2682, 'grad_norm': 2.333155870437622, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  8.17it/s] 97%|█████████▋| 29/30 [00:04<00:00,  8.16it/s]                                               {'loss': 0.7113, 'grad_norm': 1.9404873847961426, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  8.16it/s]                                               {'loss': 0.245, 'grad_norm': 2.1428489685058594, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  8.16it/s]                                               {'train_runtime': 5.0991, 'train_samples_per_second': 83.349, 'train_steps_per_second': 5.883, 'train_loss': 0.5151553988456726, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  8.16it/s]100%|██████████| 30/30 [00:05<00:00,  5.92it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.48it/s]                                              {'loss': 0.5279, 'grad_norm': 0.3162679374217987, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.48it/s]                                              {'loss': 0.6502, 'grad_norm': 1.1186989545822144, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.48it/s] 10%|█         | 3/30 [00:00<00:02, 10.71it/s]                                              {'loss': 0.6629, 'grad_norm': 0.6933643817901611, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.71it/s]                                              {'loss': 0.7624, 'grad_norm': 2.3839995861053467, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.71it/s] 17%|█▋        | 5/30 [00:00<00:02, 10.99it/s]                                              {'loss': 0.7067, 'grad_norm': 1.2929567098617554, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.99it/s]                                              {'loss': 0.8614, 'grad_norm': 2.0648739337921143, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 10.99it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.71it/s]                                              {'loss': 0.64, 'grad_norm': 1.3478409051895142, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  9.71it/s] 27%|██▋       | 8/30 [00:00<00:02,  8.75it/s]                                              {'loss': 0.6548, 'grad_norm': 0.6458936333656311, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  8.75it/s] 30%|███       | 9/30 [00:01<00:02,  7.52it/s]                                              {'loss': 0.7034, 'grad_norm': 0.8550568222999573, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.52it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.62it/s]                                               {'loss': 0.6301, 'grad_norm': 0.9416112899780273, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.62it/s] 37%|███▋      | 11/30 [00:01<00:03,  5.80it/s]                                               {'loss': 0.6646, 'grad_norm': 4.500645637512207, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  5.80it/s] 40%|████      | 12/30 [00:01<00:02,  6.32it/s]                                               {'loss': 0.667, 'grad_norm': 1.0285142660140991, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.32it/s] 43%|████▎     | 13/30 [00:01<00:03,  5.35it/s]                                               {'loss': 0.6036, 'grad_norm': 0.6072675585746765, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:03,  5.35it/s] 47%|████▋     | 14/30 [00:02<00:03,  5.16it/s]                                               {'loss': 0.6282, 'grad_norm': 0.8954855799674988, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  5.16it/s]                                               {'loss': 0.6503, 'grad_norm': 1.5447858572006226, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.16it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.92it/s]                                               {'loss': 0.6042, 'grad_norm': 0.8992353081703186, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.92it/s]                                               {'loss': 0.5975, 'grad_norm': 0.7176219820976257, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  6.92it/s] 60%|██████    | 18/30 [00:02<00:01,  8.95it/s]                                               {'loss': 0.6071, 'grad_norm': 2.160466194152832, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  8.95it/s]                                               {'loss': 0.6099, 'grad_norm': 1.3356174230575562, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.95it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.87it/s]                                               {'loss': 0.6221, 'grad_norm': 0.5907576084136963, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.87it/s] 70%|███████   | 21/30 [00:02<00:01,  8.99it/s]                                               {'loss': 0.6228, 'grad_norm': 0.7136989235877991, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.99it/s] 73%|███████▎  | 22/30 [00:02<00:00,  8.61it/s]                                               {'loss': 0.5788, 'grad_norm': 1.0305582284927368, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  8.61it/s]                                               {'loss': 0.6208, 'grad_norm': 1.3668848276138306, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  8.61it/s] 80%|████████  | 24/30 [00:02<00:00, 10.87it/s]                                               {'loss': 0.5444, 'grad_norm': 1.357554316520691, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 10.87it/s]                                               {'loss': 0.5731, 'grad_norm': 1.7146663665771484, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00, 10.87it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.47it/s]                                               {'loss': 0.5257, 'grad_norm': 1.2528905868530273, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.47it/s] 90%|█████████ | 27/30 [00:03<00:00,  6.11it/s]                                               {'loss': 0.5521, 'grad_norm': 1.1143364906311035, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  6.11it/s] 93%|█████████▎| 28/30 [00:03<00:00,  5.37it/s]                                               {'loss': 0.6119, 'grad_norm': 1.1867027282714844, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  5.37it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.01it/s]                                               {'loss': 0.4489, 'grad_norm': 0.9070277810096741, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.01it/s]100%|██████████| 30/30 [00:04<00:00,  5.55it/s]                                               {'loss': 0.4283, 'grad_norm': 2.3743598461151123, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.55it/s]                                               {'train_runtime': 4.4272, 'train_samples_per_second': 95.998, 'train_steps_per_second': 6.776, 'train_loss': 0.6187038669983546, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.55it/s]100%|██████████| 30/30 [00:04<00:00,  6.78it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  5.82it/s]                                              {'loss': 0.5586, 'grad_norm': 0.590558648109436, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  5.82it/s]  7%|▋         | 2/30 [00:00<00:03,  7.26it/s]                                              {'loss': 0.7271, 'grad_norm': 0.648463249206543, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.26it/s] 10%|█         | 3/30 [00:00<00:03,  7.45it/s]                                              {'loss': 0.728, 'grad_norm': 0.6034573316574097, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.45it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.98it/s]                                              {'loss': 0.7411, 'grad_norm': 0.9053838849067688, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.98it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.69it/s]                                              {'loss': 0.7054, 'grad_norm': 0.8442447781562805, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.69it/s]                                              {'loss': 0.7661, 'grad_norm': 1.2753559350967407, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  6.69it/s] 23%|██▎       | 7/30 [00:00<00:02,  7.86it/s]                                              {'loss': 0.6516, 'grad_norm': 1.1437872648239136, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  7.86it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.72it/s]                                              {'loss': 0.7622, 'grad_norm': 0.7848077416419983, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.72it/s] 30%|███       | 9/30 [00:01<00:02,  7.94it/s]                                              {'loss': 0.5998, 'grad_norm': 0.7755289077758789, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.94it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.77it/s]                                               {'loss': 0.6632, 'grad_norm': 0.2152591496706009, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.77it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.59it/s]                                               {'loss': 0.6328, 'grad_norm': 0.34699034690856934, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.59it/s]                                               {'loss': 0.6168, 'grad_norm': 1.3575608730316162, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.59it/s] 43%|████▎     | 13/30 [00:01<00:01,  8.84it/s]                                               {'loss': 0.5982, 'grad_norm': 0.40268799662590027, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  8.84it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.98it/s]                                               {'loss': 0.6156, 'grad_norm': 1.1228841543197632, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.98it/s] 50%|█████     | 15/30 [00:01<00:01,  8.81it/s]                                               {'loss': 0.7716, 'grad_norm': 1.7003796100616455, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  8.81it/s] 53%|█████▎    | 16/30 [00:02<00:01,  8.28it/s]                                               {'loss': 0.6818, 'grad_norm': 1.3237531185150146, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.28it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.89it/s]                                               {'loss': 0.6083, 'grad_norm': 0.5622656941413879, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.89it/s]                                               {'loss': 0.6856, 'grad_norm': 0.994615912437439, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.89it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.77it/s]                                               {'loss': 0.6608, 'grad_norm': 0.5964160561561584, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.77it/s] 67%|██████▋   | 20/30 [00:02<00:01,  8.64it/s]                                               {'loss': 0.7301, 'grad_norm': 1.2370434999465942, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.64it/s] 70%|███████   | 21/30 [00:02<00:01,  7.47it/s]                                               {'loss': 0.6548, 'grad_norm': 0.4411855638027191, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  7.47it/s] 73%|███████▎  | 22/30 [00:02<00:01,  6.88it/s]                                               {'loss': 0.6287, 'grad_norm': 0.4713764190673828, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  6.88it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.26it/s]                                               {'loss': 0.6274, 'grad_norm': 0.3939213454723358, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.26it/s]                                               {'loss': 0.5919, 'grad_norm': 0.8921741843223572, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.26it/s] 83%|████████▎ | 25/30 [00:03<00:00,  6.51it/s]                                               {'loss': 0.6495, 'grad_norm': 0.5936630368232727, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  6.51it/s] 87%|████████▋ | 26/30 [00:03<00:00,  5.99it/s]                                               {'loss': 0.6542, 'grad_norm': 0.5881407260894775, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  5.99it/s] 90%|█████████ | 27/30 [00:03<00:00,  5.61it/s]                                               {'loss': 0.6463, 'grad_norm': 0.6592796444892883, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  5.61it/s] 93%|█████████▎| 28/30 [00:03<00:00,  5.46it/s]                                               {'loss': 0.622, 'grad_norm': 0.6271317601203918, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  5.46it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.18it/s]                                               {'loss': 0.6402, 'grad_norm': 0.8069443702697754, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.18it/s]100%|██████████| 30/30 [00:04<00:00,  5.99it/s]                                               {'loss': 0.5829, 'grad_norm': 0.8818026185035706, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.99it/s]                                               {'train_runtime': 4.3505, 'train_samples_per_second': 97.689, 'train_steps_per_second': 6.896, 'train_loss': 0.6600842475891113, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.99it/s]100%|██████████| 30/30 [00:04<00:00,  6.90it/s]
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:09,  2.98it/s]                                              {'loss': 0.8339, 'grad_norm': 2.3495945930480957, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:09,  2.98it/s]  7%|▋         | 2/30 [00:00<00:09,  2.98it/s]                                              {'loss': 0.5102, 'grad_norm': 1.393067717552185, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:09,  2.98it/s] 10%|█         | 3/30 [00:01<00:09,  2.89it/s]                                              {'loss': 0.2465, 'grad_norm': 1.701135277748108, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:09,  2.89it/s] 13%|█▎        | 4/30 [00:01<00:09,  2.89it/s]                                              {'loss': 0.3103, 'grad_norm': 0.8636651635169983, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:09,  2.89it/s] 17%|█▋        | 5/30 [00:01<00:07,  3.24it/s]                                              {'loss': 0.2496, 'grad_norm': 0.7755855917930603, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:07,  3.24it/s] 20%|██        | 6/30 [00:01<00:06,  3.79it/s]                                              {'loss': 0.0522, 'grad_norm': 0.8880254626274109, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.79it/s] 23%|██▎       | 7/30 [00:02<00:06,  3.67it/s]                                              {'loss': 0.2797, 'grad_norm': 1.3513919115066528, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:06,  3.67it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.58it/s]                                              {'loss': 0.0178, 'grad_norm': 0.45478323101997375, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.58it/s] 30%|███       | 9/30 [00:02<00:05,  3.55it/s]                                              {'loss': 0.3474, 'grad_norm': 6.356745719909668, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  3.55it/s] 33%|███▎      | 10/30 [00:02<00:05,  3.51it/s]                                               {'loss': 0.0128, 'grad_norm': 0.3341203033924103, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  3.51it/s] 37%|███▋      | 11/30 [00:03<00:05,  3.51it/s]                                               {'loss': 0.0126, 'grad_norm': 0.4666590690612793, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:05,  3.51it/s]                                               {'loss': 0.0087, 'grad_norm': 0.33552220463752747, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:05,  3.51it/s] 43%|████▎     | 13/30 [00:03<00:04,  4.10it/s]                                               {'loss': 0.0043, 'grad_norm': 0.1418287307024002, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:04,  4.10it/s] 47%|████▋     | 14/30 [00:03<00:04,  3.68it/s]                                               {'loss': 0.0025, 'grad_norm': 0.0451684296131134, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:04,  3.68it/s] 50%|█████     | 15/30 [00:04<00:04,  3.54it/s]                                               {'loss': 0.3739, 'grad_norm': 1.2079516649246216, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:04,  3.54it/s] 53%|█████▎    | 16/30 [00:04<00:03,  3.68it/s]                                               {'loss': 0.0022, 'grad_norm': 0.028877338394522667, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:03,  3.68it/s] 57%|█████▋    | 17/30 [00:04<00:03,  3.82it/s]                                               {'loss': 0.3384, 'grad_norm': 0.8664397597312927, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:03,  3.82it/s] 60%|██████    | 18/30 [00:04<00:02,  4.48it/s]                                               {'loss': 0.0032, 'grad_norm': 0.041441455483436584, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.48it/s] 63%|██████▎   | 19/30 [00:05<00:02,  4.06it/s]                                               {'loss': 0.0033, 'grad_norm': 0.04082328826189041, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:02,  4.06it/s] 67%|██████▋   | 20/30 [00:05<00:02,  3.83it/s]                                               {'loss': 0.3235, 'grad_norm': 0.9841713309288025, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  3.83it/s] 70%|███████   | 21/30 [00:05<00:02,  3.76it/s]                                               {'loss': 0.0052, 'grad_norm': 0.06810566782951355, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:02,  3.76it/s] 73%|███████▎  | 22/30 [00:06<00:02,  3.69it/s]                                               {'loss': 0.0069, 'grad_norm': 0.0976339802145958, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:06<00:02,  3.69it/s] 77%|███████▋  | 23/30 [00:06<00:01,  3.66it/s]                                               {'loss': 0.2993, 'grad_norm': 0.8694985508918762, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:01,  3.66it/s] 80%|████████  | 24/30 [00:06<00:01,  4.41it/s]                                               {'loss': 0.0095, 'grad_norm': 0.16633427143096924, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  4.41it/s] 83%|████████▎ | 25/30 [00:06<00:01,  3.93it/s]                                               {'loss': 0.5941, 'grad_norm': 1.649495244026184, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:01,  3.93it/s] 87%|████████▋ | 26/30 [00:07<00:01,  3.80it/s]                                               {'loss': 0.0149, 'grad_norm': 0.27001723647117615, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:07<00:01,  3.80it/s] 90%|█████████ | 27/30 [00:07<00:00,  3.76it/s]                                               {'loss': 0.0183, 'grad_norm': 0.39021560549736023, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:07<00:00,  3.76it/s] 93%|█████████▎| 28/30 [00:07<00:00,  3.67it/s]                                               {'loss': 0.0192, 'grad_norm': 0.40068429708480835, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  3.67it/s] 97%|█████████▋| 29/30 [00:07<00:00,  3.67it/s]                                               {'loss': 0.0168, 'grad_norm': 0.29934895038604736, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  3.67it/s]                                               {'loss': 0.0212, 'grad_norm': 0.4671941101551056, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  3.67it/s]                                               {'train_runtime': 8.048, 'train_samples_per_second': 52.808, 'train_steps_per_second': 3.728, 'train_loss': 0.1646137279535954, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  3.67it/s]100%|██████████| 30/30 [00:08<00:00,  3.73it/s]
CLIENT:32
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.86it/s]                                              {'loss': 0.6588, 'grad_norm': 0.47874489426612854, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.86it/s]  7%|▋         | 2/30 [00:00<00:04,  5.75it/s]                                              {'loss': 0.6759, 'grad_norm': 0.5465999245643616, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  5.75it/s] 10%|█         | 3/30 [00:00<00:04,  6.12it/s]                                              {'loss': 0.6488, 'grad_norm': 0.8007910251617432, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.12it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.06it/s]                                              {'loss': 0.7204, 'grad_norm': 1.5329039096832275, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.06it/s] 17%|█▋        | 5/30 [00:00<00:04,  6.19it/s]                                              {'loss': 0.6818, 'grad_norm': 0.6535906195640564, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  6.19it/s] 20%|██        | 6/30 [00:01<00:06,  3.48it/s]                                              {'loss': 0.8817, 'grad_norm': 2.9611594676971436, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.48it/s]                                              {'loss': 0.6418, 'grad_norm': 0.3587303161621094, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:06,  3.48it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.03it/s]                                              {'loss': 0.757, 'grad_norm': 0.7235498428344727, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.03it/s]                                              {'loss': 0.6579, 'grad_norm': 0.3849341571331024, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  5.03it/s] 33%|███▎      | 10/30 [00:01<00:03,  6.37it/s]                                               {'loss': 0.6406, 'grad_norm': 0.25814953446388245, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  6.37it/s]                                               {'loss': 0.6564, 'grad_norm': 0.3300096094608307, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.37it/s] 40%|████      | 12/30 [00:01<00:02,  7.53it/s]                                               {'loss': 0.5646, 'grad_norm': 1.3646323680877686, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.53it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.01it/s]                                               {'loss': 0.6105, 'grad_norm': 0.5679264664649963, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.01it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.72it/s]                                               {'loss': 0.6222, 'grad_norm': 0.3929312229156494, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.72it/s] 50%|█████     | 15/30 [00:02<00:02,  6.44it/s]                                               {'loss': 0.7456, 'grad_norm': 0.8820051550865173, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.44it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.22it/s]                                               {'loss': 0.6745, 'grad_norm': 0.7322498559951782, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.22it/s] 57%|█████▋    | 17/30 [00:02<00:02,  6.20it/s]                                               {'loss': 0.6174, 'grad_norm': 0.5916341543197632, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  6.20it/s] 60%|██████    | 18/30 [00:02<00:01,  6.57it/s]                                               {'loss': 0.6694, 'grad_norm': 1.1806602478027344, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  6.57it/s] 63%|██████▎   | 19/30 [00:03<00:01,  6.10it/s]                                               {'loss': 0.612, 'grad_norm': 0.6548703908920288, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  6.10it/s] 67%|██████▋   | 20/30 [00:03<00:01,  5.68it/s]                                               {'loss': 0.6553, 'grad_norm': 0.41406121850013733, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  5.68it/s] 70%|███████   | 21/30 [00:03<00:01,  5.38it/s]                                               {'loss': 0.612, 'grad_norm': 0.6918057203292847, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.38it/s] 73%|███████▎  | 22/30 [00:03<00:01,  5.38it/s]                                               {'loss': 0.6104, 'grad_norm': 0.6876364946365356, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  5.38it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.51it/s]                                               {'loss': 0.6142, 'grad_norm': 0.7202821373939514, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.51it/s]                                               {'loss': 0.5976, 'grad_norm': 0.8709421753883362, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  5.51it/s] 83%|████████▎ | 25/30 [00:04<00:00,  6.84it/s]                                               {'loss': 0.6116, 'grad_norm': 0.7225616574287415, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  6.84it/s] 87%|████████▋ | 26/30 [00:04<00:00,  6.92it/s]                                               {'loss': 0.5966, 'grad_norm': 0.47551673650741577, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  6.92it/s] 90%|█████████ | 27/30 [00:04<00:00,  6.85it/s]                                               {'loss': 0.5976, 'grad_norm': 0.7307262420654297, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  6.85it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.86it/s]                                               {'loss': 0.601, 'grad_norm': 0.5419299602508545, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.86it/s] 97%|█████████▋| 29/30 [00:04<00:00,  7.01it/s]                                               {'loss': 0.5633, 'grad_norm': 0.6767464876174927, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  7.01it/s]                                               {'loss': 0.5843, 'grad_norm': 0.9638685584068298, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.01it/s]                                               {'train_runtime': 5.0305, 'train_samples_per_second': 84.485, 'train_steps_per_second': 5.964, 'train_loss': 0.6460411131381989, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  7.01it/s]100%|██████████| 30/30 [00:05<00:00,  5.96it/s]
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.21it/s]                                              {'loss': 0.5739, 'grad_norm': 1.1400604248046875, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.21it/s]  7%|▋         | 2/30 [00:00<00:05,  5.19it/s]                                              {'loss': 0.4709, 'grad_norm': 1.2836531400680542, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.19it/s] 10%|█         | 3/30 [00:00<00:05,  4.75it/s]                                              {'loss': 0.3841, 'grad_norm': 0.8877687454223633, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.75it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.25it/s]                                              {'loss': 0.3194, 'grad_norm': 0.7952584624290466, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.25it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.41it/s]                                              {'loss': 0.7883, 'grad_norm': 2.538039207458496, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.41it/s] 20%|██        | 6/30 [00:01<00:03,  6.29it/s]                                              {'loss': 0.03, 'grad_norm': 0.6175819039344788, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:03,  6.29it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.54it/s]                                              {'loss': 0.1381, 'grad_norm': 0.6673778295516968, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.54it/s] 27%|██▋       | 8/30 [00:01<00:04,  5.24it/s]                                              {'loss': 0.6406, 'grad_norm': 2.4130196571350098, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  5.24it/s] 30%|███       | 9/30 [00:01<00:03,  5.35it/s]                                              {'loss': 0.7118, 'grad_norm': 2.8462777137756348, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.35it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.09it/s]                                               {'loss': 0.8092, 'grad_norm': 2.0610415935516357, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.09it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.24it/s]                                               {'loss': 0.2504, 'grad_norm': 1.8644113540649414, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.24it/s]                                               {'loss': 0.7689, 'grad_norm': 6.7471394538879395, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.24it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.03it/s]                                               {'loss': 0.3792, 'grad_norm': 3.9798810482025146, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.03it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.29it/s]                                               {'loss': 0.3181, 'grad_norm': 2.178584575653076, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.29it/s] 50%|█████     | 15/30 [00:02<00:02,  7.26it/s]                                               {'loss': 0.4153, 'grad_norm': 2.1344096660614014, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.26it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.22it/s]                                               {'loss': 0.4215, 'grad_norm': 1.9540586471557617, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.22it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.76it/s]                                               {'loss': 0.231, 'grad_norm': 3.592865467071533, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.76it/s]                                               {'loss': 0.4612, 'grad_norm': 2.6254782676696777, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.76it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.20it/s]                                               {'loss': 0.3158, 'grad_norm': 2.294414520263672, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  9.20it/s] 67%|██████▋   | 20/30 [00:03<00:01,  8.15it/s]                                               {'loss': 0.4475, 'grad_norm': 3.6463358402252197, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  8.15it/s]                                               {'loss': 0.3218, 'grad_norm': 5.625994682312012, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  8.15it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.81it/s]                                               {'loss': 0.404, 'grad_norm': 1.5313284397125244, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.81it/s] 77%|███████▋  | 23/30 [00:03<00:00,  7.17it/s]                                               {'loss': 0.1291, 'grad_norm': 2.394893169403076, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.17it/s] 80%|████████  | 24/30 [00:03<00:00,  7.17it/s]                                               {'loss': 0.3753, 'grad_norm': 2.210073471069336, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.17it/s] 83%|████████▎ | 25/30 [00:03<00:00,  6.97it/s]                                               {'loss': 0.343, 'grad_norm': 1.021215796470642, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  6.97it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.39it/s]                                               {'loss': 0.3579, 'grad_norm': 1.5837035179138184, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.39it/s] 90%|█████████ | 27/30 [00:04<00:00,  6.93it/s]                                               {'loss': 0.4344, 'grad_norm': 1.5463581085205078, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  6.93it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.23it/s]                                               {'loss': 0.0905, 'grad_norm': 1.6431641578674316, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.23it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.97it/s]                                               {'loss': 0.2783, 'grad_norm': 0.8217787146568298, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.97it/s]100%|██████████| 30/30 [00:04<00:00,  6.16it/s]                                               {'loss': 0.4058, 'grad_norm': 4.239926338195801, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.16it/s]                                               {'train_runtime': 4.9422, 'train_samples_per_second': 85.994, 'train_steps_per_second': 6.07, 'train_loss': 0.4005077420423428, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.16it/s]100%|██████████| 30/30 [00:04<00:00,  6.08it/s]
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.76it/s]                                              {'loss': 0.9036, 'grad_norm': 1.9853464365005493, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.76it/s]  7%|▋         | 2/30 [00:00<00:03,  7.58it/s]                                              {'loss': 0.4862, 'grad_norm': 1.3953759670257568, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.58it/s] 10%|█         | 3/30 [00:00<00:04,  5.87it/s]                                              {'loss': 0.275, 'grad_norm': 1.7280770540237427, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  5.87it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.30it/s]                                              {'loss': 0.3064, 'grad_norm': 0.77549809217453, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.30it/s]                                              {'loss': 0.0537, 'grad_norm': 0.6499465107917786, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.30it/s] 20%|██        | 6/30 [00:01<00:05,  4.09it/s]                                              {'loss': 0.0135, 'grad_norm': 0.1655241698026657, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.09it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.26it/s]                                              {'loss': 0.0094, 'grad_norm': 0.1495409905910492, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.26it/s] 27%|██▋       | 8/30 [00:01<00:05,  4.39it/s]                                              {'loss': 0.0036, 'grad_norm': 0.04200256988406181, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:05,  4.39it/s] 30%|███       | 9/30 [00:01<00:04,  4.45it/s]                                              {'loss': 0.0082, 'grad_norm': 0.15046770870685577, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.45it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.54it/s]                                               {'loss': 0.2273, 'grad_norm': 3.5262110233306885, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.54it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.62it/s]                                               {'loss': 0.0042, 'grad_norm': 0.10399474948644638, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.62it/s]                                               {'loss': 0.0057, 'grad_norm': 0.1014297604560852, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.62it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.66it/s]                                               {'loss': 0.0039, 'grad_norm': 0.08874331414699554, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.66it/s] 47%|████▋     | 14/30 [00:02<00:03,  4.77it/s]                                               {'loss': 0.0045, 'grad_norm': 0.09627699851989746, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  4.77it/s] 50%|█████     | 15/30 [00:03<00:03,  4.05it/s]                                               {'loss': 0.3866, 'grad_norm': 0.6738329529762268, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.05it/s] 53%|█████▎    | 16/30 [00:03<00:03,  3.61it/s]                                               {'loss': 0.0037, 'grad_norm': 0.11339528858661652, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  3.61it/s] 57%|█████▋    | 17/30 [00:03<00:03,  3.43it/s]                                               {'loss': 0.0091, 'grad_norm': 0.18440276384353638, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  3.43it/s] 60%|██████    | 18/30 [00:04<00:02,  4.17it/s]                                               {'loss': 0.0035, 'grad_norm': 0.0529283806681633, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.17it/s] 63%|██████▎   | 19/30 [00:04<00:02,  3.78it/s]                                               {'loss': 0.0037, 'grad_norm': 0.04511570557951927, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  3.78it/s] 67%|██████▋   | 20/30 [00:04<00:02,  3.44it/s]                                               {'loss': 0.0042, 'grad_norm': 0.061226364225149155, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  3.44it/s] 70%|███████   | 21/30 [00:05<00:02,  3.24it/s]                                               {'loss': 0.004, 'grad_norm': 0.049837660044431686, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:02,  3.24it/s] 73%|███████▎  | 22/30 [00:05<00:02,  3.20it/s]                                               {'loss': 0.0044, 'grad_norm': 0.06145412102341652, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:02,  3.20it/s] 77%|███████▋  | 23/30 [00:05<00:02,  3.13it/s]                                               {'loss': 0.4301, 'grad_norm': 0.9777460694313049, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:02,  3.13it/s] 80%|████████  | 24/30 [00:05<00:01,  3.93it/s]                                               {'loss': 0.0044, 'grad_norm': 0.06164541468024254, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  3.93it/s] 83%|████████▎ | 25/30 [00:06<00:01,  3.55it/s]                                               {'loss': 0.0056, 'grad_norm': 0.07945103943347931, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:01,  3.55it/s] 87%|████████▋ | 26/30 [00:06<00:01,  3.28it/s]                                               {'loss': 0.0054, 'grad_norm': 0.0768648162484169, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:01,  3.28it/s] 90%|█████████ | 27/30 [00:06<00:00,  3.11it/s]                                               {'loss': 0.2966, 'grad_norm': 0.8598677515983582, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  3.11it/s] 93%|█████████▎| 28/30 [00:07<00:00,  3.01it/s]                                               {'loss': 0.0046, 'grad_norm': 0.06126179173588753, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:07<00:00,  3.01it/s] 97%|█████████▋| 29/30 [00:07<00:00,  2.95it/s]                                               {'loss': 0.0047, 'grad_norm': 0.07361600548028946, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  2.95it/s]100%|██████████| 30/30 [00:07<00:00,  3.69it/s]                                               {'loss': 0.0047, 'grad_norm': 0.07095327228307724, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  3.69it/s]                                               {'train_runtime': 7.8313, 'train_samples_per_second': 54.269, 'train_steps_per_second': 3.831, 'train_loss': 0.11601180701206128, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  3.69it/s]100%|██████████| 30/30 [00:07<00:00,  3.83it/s]
CLIENT:3
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.702, 'grad_norm': 0.5134966373443604, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 10.71it/s]  7%|▋         | 2/30 [00:00<00:02, 10.48it/s]                                              {'loss': 0.6969, 'grad_norm': 0.5607748627662659, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 10.48it/s]                                              {'loss': 0.6627, 'grad_norm': 0.8278804421424866, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.48it/s] 13%|█▎        | 4/30 [00:00<00:02, 10.34it/s]                                              {'loss': 0.7101, 'grad_norm': 1.1678158044815063, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.34it/s]                                              {'loss': 0.7301, 'grad_norm': 0.7280616760253906, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.34it/s] 20%|██        | 6/30 [00:00<00:01, 13.16it/s]                                              {'loss': 0.7289, 'grad_norm': 1.2456468343734741, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 13.16it/s]                                              {'loss': 0.6305, 'grad_norm': 0.551655113697052, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 13.16it/s] 27%|██▋       | 8/30 [00:00<00:01, 12.53it/s]                                              {'loss': 0.6849, 'grad_norm': 0.39686092734336853, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 12.53it/s]                                              {'loss': 0.694, 'grad_norm': 0.5437130928039551, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 12.53it/s] 33%|███▎      | 10/30 [00:00<00:01, 12.38it/s]                                               {'loss': 0.6813, 'grad_norm': 0.44203290343284607, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 12.38it/s]                                               {'loss': 0.5953, 'grad_norm': 0.6165112853050232, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.38it/s] 40%|████      | 12/30 [00:00<00:01, 13.20it/s]                                               {'loss': 0.4858, 'grad_norm': 2.351722240447998, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 13.20it/s]                                               {'loss': 0.62, 'grad_norm': 0.9038869738578796, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 13.20it/s] 47%|████▋     | 14/30 [00:01<00:01, 12.81it/s]                                               {'loss': 0.6202, 'grad_norm': 0.8648248314857483, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 12.81it/s]                                               {'loss': 0.6834, 'grad_norm': 1.8534109592437744, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 12.81it/s] 53%|█████▎    | 16/30 [00:01<00:01, 12.49it/s]                                               {'loss': 0.5844, 'grad_norm': 1.066529631614685, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 12.49it/s]                                               {'loss': 0.5113, 'grad_norm': 1.8040364980697632, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.49it/s] 60%|██████    | 18/30 [00:01<00:00, 13.25it/s]                                               {'loss': 0.5279, 'grad_norm': 1.7536656856536865, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.25it/s]                                               {'loss': 0.5046, 'grad_norm': 1.1737256050109863, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.25it/s] 67%|██████▋   | 20/30 [00:01<00:00, 12.74it/s]                                               {'loss': 0.6725, 'grad_norm': 1.485021948814392, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 12.74it/s]                                               {'loss': 0.5917, 'grad_norm': 1.1996673345565796, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 12.74it/s] 73%|███████▎  | 22/30 [00:01<00:00, 12.10it/s]                                               {'loss': 0.4824, 'grad_norm': 0.9958956241607666, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 12.10it/s]                                               {'loss': 0.5564, 'grad_norm': 1.2111893892288208, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 12.10it/s] 80%|████████  | 24/30 [00:01<00:00, 11.57it/s]                                               {'loss': 0.5884, 'grad_norm': 2.7533953189849854, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 11.57it/s]                                               {'loss': 0.6099, 'grad_norm': 1.3414204120635986, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 11.57it/s] 87%|████████▋ | 26/30 [00:02<00:00, 10.53it/s]                                               {'loss': 0.598, 'grad_norm': 1.1044174432754517, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 10.53it/s]                                               {'loss': 0.4603, 'grad_norm': 1.5205347537994385, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 10.53it/s] 93%|█████████▎| 28/30 [00:02<00:00, 10.66it/s]                                               {'loss': 0.4884, 'grad_norm': 1.3030802011489868, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 10.66it/s]                                               {'loss': 0.4538, 'grad_norm': 1.2582221031188965, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 10.66it/s]100%|██████████| 30/30 [00:02<00:00, 10.80it/s]                                               {'loss': 0.4448, 'grad_norm': 2.020125150680542, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 10.80it/s]                                               {'train_runtime': 2.7475, 'train_samples_per_second': 154.685, 'train_steps_per_second': 10.919, 'train_loss': 0.6000345190366109, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 10.80it/s]100%|██████████| 30/30 [00:02<00:00, 10.92it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  6%|▌         | 4/66 [00:00<00:01, 38.89it/s] 12%|█▏        | 8/66 [00:00<00:01, 32.32it/s] 18%|█▊        | 12/66 [00:00<00:01, 30.69it/s] 24%|██▍       | 16/66 [00:00<00:01, 28.78it/s] 29%|██▉       | 19/66 [00:00<00:01, 28.12it/s] 33%|███▎      | 22/66 [00:00<00:01, 28.10it/s] 38%|███▊      | 25/66 [00:00<00:01, 28.45it/s] 42%|████▏     | 28/66 [00:00<00:01, 28.55it/s] 47%|████▋     | 31/66 [00:01<00:01, 27.44it/s] 52%|█████▏    | 34/66 [00:01<00:01, 28.01it/s] 56%|█████▌    | 37/66 [00:01<00:01, 28.58it/s] 61%|██████    | 40/66 [00:01<00:00, 28.96it/s] 67%|██████▋   | 44/66 [00:01<00:00, 29.26it/s] 73%|███████▎  | 48/66 [00:01<00:00, 29.16it/s] 79%|███████▉  | 52/66 [00:01<00:00, 28.84it/s] 83%|████████▎ | 55/66 [00:01<00:00, 29.04it/s] 89%|████████▉ | 59/66 [00:02<00:00, 29.34it/s] 94%|█████████▍| 62/66 [00:02<00:00, 29.32it/s] 98%|█████████▊| 65/66 [00:02<00:00, 29.09it/s]100%|██████████| 66/66 [00:02<00:00, 29.17it/s]
{'eval_loss': 0.6450086236000061, 'eval_model_preparation_time': 0.0078, 'eval_acc': 0.6720997123681688, 'eval_runtime': 2.303, 'eval_samples_per_second': 452.885, 'eval_steps_per_second': 28.658}
ROUND:7
CLIENT:71
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.95it/s]                                              {'loss': 0.7567, 'grad_norm': 0.8892920613288879, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.95it/s]                                              {'loss': 0.5478, 'grad_norm': 0.9873895049095154, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.95it/s] 10%|█         | 3/30 [00:00<00:02, 10.00it/s]                                              {'loss': 0.4687, 'grad_norm': 0.6655024290084839, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.00it/s]                                              {'loss': 0.8492, 'grad_norm': 3.1312103271484375, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.00it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.22it/s]                                              {'loss': 0.7835, 'grad_norm': 2.389420747756958, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.22it/s]                                              {'loss': 1.3575, 'grad_norm': 5.348040580749512, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.22it/s] 23%|██▎       | 7/30 [00:00<00:01, 12.36it/s]                                              {'loss': 0.5416, 'grad_norm': 0.7217071056365967, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 12.36it/s]                                              {'loss': 0.6709, 'grad_norm': 0.8719311356544495, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 12.36it/s] 30%|███       | 9/30 [00:00<00:01, 12.41it/s]                                              {'loss': 0.6168, 'grad_norm': 0.8666857481002808, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 12.41it/s]                                              {'loss': 0.5908, 'grad_norm': 0.7003967761993408, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 12.41it/s] 37%|███▋      | 11/30 [00:00<00:01, 12.10it/s]                                               {'loss': 0.6375, 'grad_norm': 0.6103664636611938, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.10it/s]                                               {'loss': 0.534, 'grad_norm': 1.2709311246871948, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 12.10it/s] 43%|████▎     | 13/30 [00:01<00:01, 12.65it/s]                                               {'loss': 0.6155, 'grad_norm': 1.0634962320327759, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 12.65it/s]                                               {'loss': 0.5402, 'grad_norm': 0.9913470149040222, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 12.65it/s] 50%|█████     | 15/30 [00:01<00:01, 12.66it/s]                                               {'loss': 0.6863, 'grad_norm': 1.553782343864441, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 12.66it/s]                                               {'loss': 0.546, 'grad_norm': 1.1376674175262451, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 12.66it/s] 57%|█████▋    | 17/30 [00:01<00:01, 12.84it/s]                                               {'loss': 0.5753, 'grad_norm': 1.3097158670425415, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.84it/s]                                               {'loss': 0.6321, 'grad_norm': 3.4757354259490967, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 12.84it/s] 63%|██████▎   | 19/30 [00:01<00:00, 13.62it/s]                                               {'loss': 0.4974, 'grad_norm': 1.5330274105072021, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.62it/s]                                               {'loss': 0.518, 'grad_norm': 0.7599736452102661, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.62it/s] 70%|███████   | 21/30 [00:01<00:00, 13.56it/s]                                               {'loss': 0.5162, 'grad_norm': 1.0746259689331055, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.56it/s]                                               {'loss': 0.5291, 'grad_norm': 1.7519934177398682, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.56it/s] 77%|███████▋  | 23/30 [00:01<00:00, 13.33it/s]                                               {'loss': 0.4978, 'grad_norm': 0.9862262606620789, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 13.33it/s]                                               {'loss': 0.3819, 'grad_norm': 1.4254556894302368, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 13.33it/s] 83%|████████▎ | 25/30 [00:01<00:00, 14.53it/s]                                               {'loss': 0.5252, 'grad_norm': 1.3135682344436646, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.53it/s]                                               {'loss': 0.3699, 'grad_norm': 1.4700908660888672, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 14.53it/s] 90%|█████████ | 27/30 [00:02<00:00, 13.83it/s]                                               {'loss': 0.4938, 'grad_norm': 1.3137731552124023, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.83it/s]                                               {'loss': 0.4374, 'grad_norm': 1.126076579093933, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.83it/s] 97%|█████████▋| 29/30 [00:02<00:00, 13.26it/s]                                               {'loss': 0.5326, 'grad_norm': 1.9635076522827148, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.26it/s]                                               {'loss': 0.2759, 'grad_norm': 2.8738043308258057, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.26it/s]                                               {'train_runtime': 2.4084, 'train_samples_per_second': 176.466, 'train_steps_per_second': 12.456, 'train_loss': 0.5841910262902578, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.26it/s]100%|██████████| 30/30 [00:02<00:00, 12.46it/s]
CLIENT:33
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.35it/s]                                              {'loss': 0.7289, 'grad_norm': 1.5393239259719849, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.35it/s]  7%|▋         | 2/30 [00:00<00:04,  5.86it/s]                                              {'loss': 0.6631, 'grad_norm': 0.5768313407897949, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  5.86it/s] 10%|█         | 3/30 [00:00<00:04,  6.43it/s]                                              {'loss': 0.7004, 'grad_norm': 0.9698726534843445, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.43it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.71it/s]                                              {'loss': 0.6776, 'grad_norm': 1.2425148487091064, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.71it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.02it/s]                                              {'loss': 0.705, 'grad_norm': 1.0609848499298096, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.02it/s]                                              {'loss': 0.7602, 'grad_norm': 4.080905914306641, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.02it/s] 23%|██▎       | 7/30 [00:00<00:02,  7.76it/s]                                              {'loss': 0.6577, 'grad_norm': 0.46834301948547363, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  7.76it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.77it/s]                                              {'loss': 0.6438, 'grad_norm': 0.39758414030075073, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.77it/s]                                              {'loss': 0.6258, 'grad_norm': 0.6762087345123291, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.77it/s] 33%|███▎      | 10/30 [00:01<00:02,  8.98it/s]                                               {'loss': 0.6921, 'grad_norm': 0.46316465735435486, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.98it/s]                                               {'loss': 0.6905, 'grad_norm': 0.684354841709137, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.98it/s] 40%|████      | 12/30 [00:01<00:01, 10.22it/s]                                               {'loss': 0.747, 'grad_norm': 1.9862569570541382, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 10.22it/s]                                               {'loss': 0.714, 'grad_norm': 2.5886073112487793, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 10.22it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.58it/s]                                               {'loss': 0.6796, 'grad_norm': 0.6719893217086792, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  9.58it/s]                                               {'loss': 0.5181, 'grad_norm': 0.9805156588554382, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  9.58it/s] 53%|█████▎    | 16/30 [00:01<00:01, 10.14it/s]                                               {'loss': 0.6529, 'grad_norm': 0.7645447850227356, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 10.14it/s]                                               {'loss': 0.709, 'grad_norm': 0.9534792304039001, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 10.14it/s] 60%|██████    | 18/30 [00:01<00:01, 11.75it/s]                                               {'loss': 0.623, 'grad_norm': 1.1680108308792114, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 11.75it/s]                                               {'loss': 0.6626, 'grad_norm': 1.4496221542358398, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 11.75it/s] 67%|██████▋   | 20/30 [00:02<00:00, 12.36it/s]                                               {'loss': 0.6396, 'grad_norm': 0.4514544606208801, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 12.36it/s]                                               {'loss': 0.6568, 'grad_norm': 0.8301548361778259, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 12.36it/s] 73%|███████▎  | 22/30 [00:02<00:00, 12.34it/s]                                               {'loss': 0.6495, 'grad_norm': 0.8044606447219849, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 12.34it/s]                                               {'loss': 0.6036, 'grad_norm': 0.7652656435966492, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 12.34it/s] 80%|████████  | 24/30 [00:02<00:00, 13.19it/s]                                               {'loss': 0.6165, 'grad_norm': 0.9053496718406677, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 13.19it/s]                                               {'loss': 0.6601, 'grad_norm': 1.9537250995635986, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 13.19it/s] 87%|████████▋ | 26/30 [00:02<00:00, 12.64it/s]                                               {'loss': 0.6488, 'grad_norm': 0.7424166798591614, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 12.64it/s]                                               {'loss': 0.6479, 'grad_norm': 0.6089341044425964, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 12.64it/s] 93%|█████████▎| 28/30 [00:02<00:00, 12.32it/s]                                               {'loss': 0.624, 'grad_norm': 0.49021828174591064, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 12.32it/s]                                               {'loss': 0.5622, 'grad_norm': 0.6600763201713562, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 12.32it/s]100%|██████████| 30/30 [00:02<00:00, 13.57it/s]                                               {'loss': 0.6596, 'grad_norm': 1.1128253936767578, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.57it/s]                                               {'train_runtime': 2.9427, 'train_samples_per_second': 144.425, 'train_steps_per_second': 10.195, 'train_loss': 0.6606562236944834, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.57it/s]100%|██████████| 30/30 [00:02<00:00, 10.20it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  7.20it/s]                                              {'loss': 0.8491, 'grad_norm': 1.6788620948791504, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  7.20it/s]  7%|▋         | 2/30 [00:00<00:03,  8.53it/s]                                              {'loss': 0.4879, 'grad_norm': 1.2567113637924194, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.53it/s]                                              {'loss': 0.7155, 'grad_norm': 1.2902960777282715, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.53it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.57it/s]                                              {'loss': 0.6971, 'grad_norm': 1.54904043674469, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  9.57it/s]                                              {'loss': 0.6797, 'grad_norm': 1.952528715133667, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  9.57it/s] 20%|██        | 6/30 [00:01<00:05,  4.47it/s]                                              {'loss': 1.1495, 'grad_norm': 3.288022994995117, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.47it/s]                                              {'loss': 0.3372, 'grad_norm': 1.6601473093032837, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.47it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.87it/s]                                              {'loss': 0.5192, 'grad_norm': 1.840332269668579, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.87it/s]                                              {'loss': 0.6644, 'grad_norm': 1.910009741783142, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.87it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.15it/s]                                               {'loss': 0.5969, 'grad_norm': 1.372992992401123, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.15it/s]                                               {'loss': 0.6843, 'grad_norm': 2.581021547317505, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.15it/s] 40%|████      | 12/30 [00:01<00:02,  8.73it/s]                                               {'loss': 0.4367, 'grad_norm': 11.03538703918457, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.73it/s]                                               {'loss': 0.5404, 'grad_norm': 1.9909250736236572, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  8.73it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.12it/s]                                               {'loss': 0.6372, 'grad_norm': 2.3733298778533936, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.12it/s] 50%|█████     | 15/30 [00:02<00:02,  6.58it/s]                                               {'loss': 0.6582, 'grad_norm': 2.113752603530884, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.58it/s] 53%|█████▎    | 16/30 [00:02<00:02,  5.46it/s]                                               {'loss': 0.53, 'grad_norm': 2.5856566429138184, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  5.46it/s] 57%|█████▋    | 17/30 [00:02<00:02,  5.00it/s]                                               {'loss': 0.5514, 'grad_norm': 1.9007854461669922, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  5.00it/s] 60%|██████    | 18/30 [00:02<00:02,  5.46it/s]                                               {'loss': 0.6343, 'grad_norm': 4.143009185791016, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:02,  5.46it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.82it/s]                                               {'loss': 0.5704, 'grad_norm': 1.2986522912979126, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.82it/s] 67%|██████▋   | 20/30 [00:03<00:02,  4.35it/s]                                               {'loss': 0.3614, 'grad_norm': 6.386983871459961, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:02,  4.35it/s] 70%|███████   | 21/30 [00:03<00:02,  4.16it/s]                                               {'loss': 0.622, 'grad_norm': 1.8823927640914917, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:02,  4.16it/s] 73%|███████▎  | 22/30 [00:03<00:02,  3.98it/s]                                               {'loss': 0.5954, 'grad_norm': 1.7664117813110352, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:02,  3.98it/s] 77%|███████▋  | 23/30 [00:04<00:01,  3.77it/s]                                               {'loss': 0.7368, 'grad_norm': 2.1183993816375732, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  3.77it/s] 80%|████████  | 24/30 [00:04<00:01,  4.24it/s]                                               {'loss': 0.3917, 'grad_norm': 2.348839282989502, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  4.24it/s] 83%|████████▎ | 25/30 [00:04<00:01,  4.22it/s]                                               {'loss': 0.3904, 'grad_norm': 1.6227400302886963, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:01,  4.22it/s] 87%|████████▋ | 26/30 [00:04<00:00,  4.03it/s]                                               {'loss': 0.4855, 'grad_norm': 1.1938130855560303, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  4.03it/s] 90%|█████████ | 27/30 [00:05<00:00,  3.75it/s]                                               {'loss': 0.6366, 'grad_norm': 2.1319196224212646, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  3.75it/s] 93%|█████████▎| 28/30 [00:05<00:00,  3.55it/s]                                               {'loss': 0.4887, 'grad_norm': 1.7414597272872925, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  3.55it/s] 97%|█████████▋| 29/30 [00:05<00:00,  3.51it/s]                                               {'loss': 0.5295, 'grad_norm': 1.1974554061889648, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  3.51it/s]100%|██████████| 30/30 [00:06<00:00,  4.11it/s]                                               {'loss': 0.2483, 'grad_norm': 2.513578176498413, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.11it/s]                                               {'train_runtime': 6.1426, 'train_samples_per_second': 69.189, 'train_steps_per_second': 4.884, 'train_loss': 0.5808615818619728, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.11it/s]100%|██████████| 30/30 [00:06<00:00,  4.88it/s]
CLIENT:10
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.28it/s]                                              {'loss': 0.6051, 'grad_norm': 0.5892193913459778, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.28it/s]  7%|▋         | 2/30 [00:01<00:16,  1.65it/s]                                              {'loss': 0.7315, 'grad_norm': 0.9162550568580627, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:16,  1.65it/s]                                              {'loss': 0.7026, 'grad_norm': 0.41484659910202026, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:16,  1.65it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.65it/s]                                              {'loss': 0.6876, 'grad_norm': 0.7895814776420593, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.65it/s]                                              {'loss': 0.704, 'grad_norm': 0.41744494438171387, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.65it/s] 20%|██        | 6/30 [00:01<00:04,  5.84it/s]                                              {'loss': 0.561, 'grad_norm': 0.93440842628479, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.84it/s]                                              {'loss': 0.6484, 'grad_norm': 0.3077664077281952, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  5.84it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.46it/s]                                              {'loss': 0.6863, 'grad_norm': 0.3528665602207184, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.46it/s]                                              {'loss': 0.6141, 'grad_norm': 0.38985884189605713, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.46it/s] 33%|███▎      | 10/30 [00:01<00:02,  8.75it/s]                                               {'loss': 0.5835, 'grad_norm': 0.7784687876701355, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.75it/s]                                               {'loss': 0.6602, 'grad_norm': 0.41128724813461304, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.75it/s] 40%|████      | 12/30 [00:01<00:01, 10.74it/s]                                               {'loss': 0.5201, 'grad_norm': 0.47382569313049316, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01, 10.74it/s]                                               {'loss': 0.5696, 'grad_norm': 0.6000481843948364, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 10.74it/s] 47%|████▋     | 14/30 [00:01<00:01, 11.37it/s]                                               {'loss': 0.4934, 'grad_norm': 0.5548143982887268, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.37it/s]                                               {'loss': 0.9045, 'grad_norm': 2.5990467071533203, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01, 11.37it/s] 53%|█████▎    | 16/30 [00:02<00:01, 11.61it/s]                                               {'loss': 0.6729, 'grad_norm': 1.367121934890747, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01, 11.61it/s]                                               {'loss': 0.4716, 'grad_norm': 1.1878858804702759, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01, 11.61it/s] 60%|██████    | 18/30 [00:02<00:00, 12.64it/s]                                               {'loss': 0.6655, 'grad_norm': 1.0004866123199463, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:00, 12.64it/s]                                               {'loss': 0.6243, 'grad_norm': 0.7427121996879578, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 12.64it/s] 67%|██████▋   | 20/30 [00:02<00:00, 12.38it/s]                                               {'loss': 0.5871, 'grad_norm': 0.42873305082321167, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 12.38it/s]                                               {'loss': 0.5398, 'grad_norm': 0.7553567290306091, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 12.38it/s] 73%|███████▎  | 22/30 [00:02<00:00, 12.12it/s]                                               {'loss': 0.5143, 'grad_norm': 0.8206393122673035, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 12.12it/s]                                               {'loss': 0.6188, 'grad_norm': 0.8210903406143188, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 12.12it/s] 80%|████████  | 24/30 [00:02<00:00, 12.66it/s]                                               {'loss': 0.5461, 'grad_norm': 1.8563283681869507, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 12.66it/s]                                               {'loss': 0.7001, 'grad_norm': 2.768267869949341, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 12.66it/s] 87%|████████▋ | 26/30 [00:02<00:00, 12.05it/s]                                               {'loss': 0.5315, 'grad_norm': 1.0119259357452393, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 12.05it/s]                                               {'loss': 0.5602, 'grad_norm': 0.9123830199241638, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 12.05it/s] 93%|█████████▎| 28/30 [00:03<00:00, 11.94it/s]                                               {'loss': 0.5736, 'grad_norm': 2.0130574703216553, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 11.94it/s]                                               {'loss': 0.5809, 'grad_norm': 1.2730371952056885, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 11.94it/s]100%|██████████| 30/30 [00:03<00:00, 13.33it/s]                                               {'loss': 0.5899, 'grad_norm': 1.4255287647247314, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.33it/s]                                               {'train_runtime': 3.2567, 'train_samples_per_second': 130.501, 'train_steps_per_second': 9.212, 'train_loss': 0.614950234691302, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.33it/s]100%|██████████| 30/30 [00:03<00:00,  9.21it/s]
CLIENT:44
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  4.87it/s]                                              {'loss': 0.8685, 'grad_norm': 2.026620388031006, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  4.87it/s]  7%|▋         | 2/30 [00:00<00:05,  5.07it/s]                                              {'loss': 0.4854, 'grad_norm': 1.356307029724121, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  5.07it/s] 10%|█         | 3/30 [00:00<00:05,  5.15it/s]                                              {'loss': 0.3186, 'grad_norm': 1.3164042234420776, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  5.15it/s] 13%|█▎        | 4/30 [00:00<00:05,  5.09it/s]                                              {'loss': 0.1078, 'grad_norm': 1.0986886024475098, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  5.09it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.06it/s]                                              {'loss': 0.0306, 'grad_norm': 0.48248913884162903, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:04,  5.06it/s]                                              {'loss': 0.0048, 'grad_norm': 0.07978516072034836, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.06it/s] 23%|██▎       | 7/30 [00:01<00:03,  5.96it/s]                                              {'loss': 0.2056, 'grad_norm': 0.911605179309845, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  5.96it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.72it/s]                                              {'loss': 0.003, 'grad_norm': 0.03378855809569359, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.72it/s] 30%|███       | 9/30 [00:01<00:03,  5.30it/s]                                              {'loss': 0.3441, 'grad_norm': 1.0187009572982788, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.30it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.07it/s]                                               {'loss': 0.6613, 'grad_norm': 2.771705150604248, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.07it/s] 37%|███▋      | 11/30 [00:02<00:03,  4.84it/s]                                               {'loss': 0.3256, 'grad_norm': 2.2887508869171143, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  4.84it/s] 40%|████      | 12/30 [00:02<00:03,  5.46it/s]                                               {'loss': 0.0125, 'grad_norm': 0.1868794858455658, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.46it/s] 43%|████▎     | 13/30 [00:02<00:03,  4.49it/s]                                               {'loss': 0.2897, 'grad_norm': 1.1507521867752075, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  4.49it/s] 47%|████▋     | 14/30 [00:02<00:03,  4.64it/s]                                               {'loss': 0.0138, 'grad_norm': 0.18071486055850983, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  4.64it/s] 50%|█████     | 15/30 [00:03<00:03,  4.50it/s]                                               {'loss': 0.6336, 'grad_norm': 2.1632072925567627, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.50it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.39it/s]                                               {'loss': 0.0323, 'grad_norm': 0.38359856605529785, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.39it/s] 57%|█████▋    | 17/30 [00:03<00:03,  4.33it/s]                                               {'loss': 0.4564, 'grad_norm': 1.600723147392273, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  4.33it/s] 60%|██████    | 18/30 [00:03<00:02,  4.59it/s]                                               {'loss': 0.0439, 'grad_norm': 0.44670647382736206, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.59it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.54it/s]                                               {'loss': 0.3756, 'grad_norm': 1.3307067155838013, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.54it/s] 67%|██████▋   | 20/30 [00:04<00:01,  5.07it/s]                                               {'loss': 0.065, 'grad_norm': 0.5607157349586487, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  5.07it/s] 70%|███████   | 21/30 [00:04<00:01,  5.59it/s]                                               {'loss': 0.0762, 'grad_norm': 0.6238889098167419, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.59it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.71it/s]                                               {'loss': 0.5184, 'grad_norm': 1.3398542404174805, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.71it/s] 77%|███████▋  | 23/30 [00:04<00:01,  6.28it/s]                                               {'loss': 0.0721, 'grad_norm': 0.6078112125396729, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  6.28it/s]                                               {'loss': 0.0708, 'grad_norm': 0.9160988926887512, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  6.28it/s] 83%|████████▎ | 25/30 [00:04<00:00,  6.28it/s]                                               {'loss': 0.0564, 'grad_norm': 0.5027502775192261, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  6.28it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.42it/s]                                               {'loss': 0.2654, 'grad_norm': 1.6662663221359253, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.42it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.85it/s]                                               {'loss': 0.2677, 'grad_norm': 0.8872277140617371, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.85it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.59it/s]                                               {'loss': 0.2434, 'grad_norm': 0.40174442529678345, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.59it/s] 97%|█████████▋| 29/30 [00:05<00:00,  4.40it/s]                                               {'loss': 0.2111, 'grad_norm': 0.5531278848648071, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  4.40it/s]100%|██████████| 30/30 [00:05<00:00,  5.02it/s]                                               {'loss': 0.7255, 'grad_norm': 3.5880963802337646, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.02it/s]                                               {'train_runtime': 6.1776, 'train_samples_per_second': 68.797, 'train_steps_per_second': 4.856, 'train_loss': 0.2595042722687746, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.02it/s]100%|██████████| 30/30 [00:06<00:00,  4.86it/s]
CLIENT:34
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.78it/s]                                              {'loss': 0.7487, 'grad_norm': 1.8290424346923828, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.78it/s]  7%|▋         | 2/30 [00:00<00:08,  3.27it/s]                                              {'loss': 0.5091, 'grad_norm': 1.1166177988052368, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:08,  3.27it/s] 10%|█         | 3/30 [00:00<00:08,  3.12it/s]                                              {'loss': 0.3533, 'grad_norm': 1.2567028999328613, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:08,  3.12it/s] 13%|█▎        | 4/30 [00:01<00:08,  3.05it/s]                                              {'loss': 0.5951, 'grad_norm': 2.7143993377685547, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  3.05it/s] 17%|█▋        | 5/30 [00:01<00:08,  3.01it/s]                                              {'loss': 0.4758, 'grad_norm': 1.960128903388977, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:08,  3.01it/s] 20%|██        | 6/30 [00:02<00:13,  1.72it/s]                                              {'loss': 1.3173, 'grad_norm': 4.495180606842041, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:02<00:13,  1.72it/s] 23%|██▎       | 7/30 [00:03<00:11,  1.95it/s]                                              {'loss': 0.3197, 'grad_norm': 0.6479321718215942, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:03<00:11,  1.95it/s] 27%|██▋       | 8/30 [00:03<00:10,  2.14it/s]                                              {'loss': 0.3649, 'grad_norm': 1.088322401046753, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:03<00:10,  2.14it/s] 30%|███       | 9/30 [00:03<00:09,  2.30it/s]                                              {'loss': 0.7263, 'grad_norm': 2.013073444366455, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:03<00:09,  2.30it/s] 33%|███▎      | 10/30 [00:04<00:08,  2.44it/s]                                               {'loss': 0.3926, 'grad_norm': 0.9889942407608032, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:04<00:08,  2.44it/s] 37%|███▋      | 11/30 [00:04<00:07,  2.52it/s]                                               {'loss': 0.5959, 'grad_norm': 1.3310908079147339, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:04<00:07,  2.52it/s] 40%|████      | 12/30 [00:04<00:05,  3.18it/s]                                               {'loss': 0.4436, 'grad_norm': 1.530633807182312, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:04<00:05,  3.18it/s] 43%|████▎     | 13/30 [00:04<00:05,  3.05it/s]                                               {'loss': 0.312, 'grad_norm': 1.830767035484314, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:04<00:05,  3.05it/s] 47%|████▋     | 14/30 [00:05<00:05,  2.91it/s]                                               {'loss': 0.4208, 'grad_norm': 1.1585079431533813, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:05<00:05,  2.91it/s] 50%|█████     | 15/30 [00:05<00:05,  2.85it/s]                                               {'loss': 0.6943, 'grad_norm': 2.0664074420928955, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:05<00:05,  2.85it/s] 53%|█████▎    | 16/30 [00:06<00:04,  2.84it/s]                                               {'loss': 0.4556, 'grad_norm': 1.592552900314331, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:06<00:04,  2.84it/s] 57%|█████▋    | 17/30 [00:06<00:04,  2.80it/s]                                               {'loss': 0.4561, 'grad_norm': 1.3707343339920044, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:06<00:04,  2.80it/s] 60%|██████    | 18/30 [00:06<00:03,  3.44it/s]                                               {'loss': 0.7241, 'grad_norm': 2.951890230178833, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:06<00:03,  3.44it/s] 63%|██████▎   | 19/30 [00:06<00:03,  3.21it/s]                                               {'loss': 0.3764, 'grad_norm': 1.0869477987289429, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:06<00:03,  3.21it/s] 67%|██████▋   | 20/30 [00:07<00:03,  3.04it/s]                                               {'loss': 0.4465, 'grad_norm': 0.8039198517799377, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:07<00:03,  3.04it/s] 70%|███████   | 21/30 [00:07<00:03,  2.95it/s]                                               {'loss': 0.3787, 'grad_norm': 1.296889305114746, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:07<00:03,  2.95it/s] 73%|███████▎  | 22/30 [00:08<00:02,  2.88it/s]                                               {'loss': 0.493, 'grad_norm': 0.9274976253509521, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:08<00:02,  2.88it/s] 77%|███████▋  | 23/30 [00:08<00:02,  2.84it/s]                                               {'loss': 0.3404, 'grad_norm': 2.0092742443084717, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:08<00:02,  2.84it/s] 80%|████████  | 24/30 [00:08<00:01,  3.47it/s]                                               {'loss': 0.3698, 'grad_norm': 1.4232951402664185, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:08<00:01,  3.47it/s] 83%|████████▎ | 25/30 [00:08<00:01,  3.21it/s]                                               {'loss': 0.6378, 'grad_norm': 2.415464162826538, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:08<00:01,  3.21it/s] 87%|████████▋ | 26/30 [00:09<00:01,  3.07it/s]                                               {'loss': 0.4651, 'grad_norm': 1.5341970920562744, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:09<00:01,  3.07it/s] 90%|█████████ | 27/30 [00:09<00:01,  2.96it/s]                                               {'loss': 0.3047, 'grad_norm': 1.0019400119781494, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:09<00:01,  2.96it/s] 93%|█████████▎| 28/30 [00:10<00:00,  2.92it/s]                                               {'loss': 0.2573, 'grad_norm': 1.9343667030334473, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:10<00:00,  2.92it/s] 97%|█████████▋| 29/30 [00:10<00:00,  2.91it/s]                                               {'loss': 0.5949, 'grad_norm': 2.402637481689453, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:10<00:00,  2.91it/s]100%|██████████| 30/30 [00:10<00:00,  3.65it/s]                                               {'loss': 0.2344, 'grad_norm': 2.1817593574523926, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:10<00:00,  3.65it/s]                                               {'train_runtime': 10.5411, 'train_samples_per_second': 40.318, 'train_steps_per_second': 2.846, 'train_loss': 0.4934681644042333, 'epoch': 5.0}
100%|██████████| 30/30 [00:10<00:00,  3.65it/s]100%|██████████| 30/30 [00:10<00:00,  2.85it/s]
CLIENT:24
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  8.10it/s]                                              {'loss': 0.8601, 'grad_norm': 1.93332040309906, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  8.10it/s]                                              {'loss': 0.4903, 'grad_norm': 1.204235315322876, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  8.10it/s] 10%|█         | 3/30 [00:00<00:02,  9.33it/s]                                              {'loss': 0.4676, 'grad_norm': 0.8516800403594971, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02,  9.33it/s]                                              {'loss': 0.1892, 'grad_norm': 0.7231975793838501, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  9.33it/s] 17%|█▋        | 5/30 [00:00<00:02, 10.20it/s]                                              {'loss': 0.6877, 'grad_norm': 3.2851204872131348, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.20it/s]                                              {'loss': 2.2544, 'grad_norm': 9.760324478149414, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:02, 10.20it/s] 23%|██▎       | 7/30 [00:01<00:06,  3.54it/s]                                              {'loss': 0.0697, 'grad_norm': 0.8446158170700073, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:06,  3.54it/s]                                              {'loss': 0.8366, 'grad_norm': 6.577883243560791, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:06,  3.54it/s] 30%|███       | 9/30 [00:01<00:04,  4.86it/s]                                              {'loss': 0.3261, 'grad_norm': 6.963874340057373, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.86it/s]                                              {'loss': 0.2194, 'grad_norm': 1.4581838846206665, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:04,  4.86it/s] 37%|███▋      | 11/30 [00:01<00:03,  6.18it/s]                                               {'loss': 1.0016, 'grad_norm': 8.037464141845703, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  6.18it/s]                                               {'loss': 0.078, 'grad_norm': 1.5089677572250366, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.18it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.82it/s]                                               {'loss': 0.462, 'grad_norm': 2.731553792953491, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.82it/s]                                               {'loss': 0.1951, 'grad_norm': 0.8109754323959351, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.82it/s] 50%|█████     | 15/30 [00:02<00:01,  8.81it/s]                                               {'loss': 0.6756, 'grad_norm': 10.623111724853516, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.81it/s]                                               {'loss': 0.5386, 'grad_norm': 3.7845237255096436, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.81it/s] 57%|█████▋    | 17/30 [00:02<00:01,  9.76it/s]                                               {'loss': 0.4422, 'grad_norm': 2.1536388397216797, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  9.76it/s]                                               {'loss': 0.785, 'grad_norm': 5.616934299468994, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  9.76it/s] 63%|██████▎   | 19/30 [00:02<00:00, 11.11it/s]                                               {'loss': 0.3122, 'grad_norm': 1.1590521335601807, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 11.11it/s]                                               {'loss': 0.3244, 'grad_norm': 2.7709908485412598, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 11.11it/s] 70%|███████   | 21/30 [00:02<00:01,  8.96it/s]                                               {'loss': 0.4732, 'grad_norm': 1.9185587167739868, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.96it/s]                                               {'loss': 0.4236, 'grad_norm': 1.357264518737793, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  8.96it/s] 77%|███████▋  | 23/30 [00:03<00:00,  8.08it/s]                                               {'loss': 0.5252, 'grad_norm': 3.812521457672119, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  8.08it/s] 80%|████████  | 24/30 [00:03<00:00,  8.19it/s]                                               {'loss': 0.3946, 'grad_norm': 2.656245231628418, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  8.19it/s] 83%|████████▎ | 25/30 [00:03<00:00,  6.87it/s]                                               {'loss': 0.3157, 'grad_norm': 1.5099544525146484, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  6.87it/s] 87%|████████▋ | 26/30 [00:03<00:00,  6.11it/s]                                               {'loss': 0.456, 'grad_norm': 1.683451533317566, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  6.11it/s] 90%|█████████ | 27/30 [00:03<00:00,  5.91it/s]                                               {'loss': 0.3015, 'grad_norm': 1.2744060754776, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  5.91it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.03it/s]                                               {'loss': 0.4585, 'grad_norm': 1.59773850440979, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.03it/s]                                               {'loss': 0.4157, 'grad_norm': 3.8940091133117676, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  6.03it/s]100%|██████████| 30/30 [00:04<00:00,  8.42it/s]                                               {'loss': 0.1692, 'grad_norm': 2.3613436222076416, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  8.42it/s]                                               {'train_runtime': 4.1975, 'train_samples_per_second': 101.252, 'train_steps_per_second': 7.147, 'train_loss': 0.504957540333271, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  8.42it/s]100%|██████████| 30/30 [00:04<00:00,  7.15it/s]
CLIENT:98
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]                                      {'loss': 0.7464, 'grad_norm': 1.7761139869689941, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:02, 10.94it/s]  7%|▋         | 2/30 [00:00<00:02, 10.92it/s]                                              {'loss': 0.5694, 'grad_norm': 1.212786078453064, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02, 10.92it/s]                                              {'loss': 0.4241, 'grad_norm': 0.7948358654975891, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.92it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.65it/s]                                              {'loss': 0.2872, 'grad_norm': 0.8841602802276611, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02,  9.65it/s] 17%|█▋        | 5/30 [00:00<00:02,  8.88it/s]                                              {'loss': 0.3537, 'grad_norm': 0.8559445142745972, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  8.88it/s] 20%|██        | 6/30 [00:00<00:03,  7.51it/s]                                              {'loss': 1.5257, 'grad_norm': 4.9763641357421875, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.51it/s] 23%|██▎       | 7/30 [00:00<00:03,  6.83it/s]                                              {'loss': 0.2453, 'grad_norm': 0.7724483609199524, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:03,  6.83it/s]                                              {'loss': 0.6531, 'grad_norm': 2.864891290664673, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:03,  6.83it/s] 30%|███       | 9/30 [00:01<00:02,  8.08it/s]                                              {'loss': 0.3714, 'grad_norm': 0.7246496081352234, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  8.08it/s] 33%|███▎      | 10/30 [00:01<00:02,  8.39it/s]                                               {'loss': 0.3199, 'grad_norm': 0.5919309258460999, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.39it/s] 37%|███▋      | 11/30 [00:01<00:02,  8.58it/s]                                               {'loss': 0.3899, 'grad_norm': 0.72648024559021, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.58it/s] 40%|████      | 12/30 [00:01<00:02,  8.12it/s]                                               {'loss': 0.4495, 'grad_norm': 1.5184310674667358, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.12it/s] 43%|████▎     | 13/30 [00:01<00:02,  6.34it/s]                                               {'loss': 0.1627, 'grad_norm': 1.432302474975586, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  6.34it/s] 47%|████▋     | 14/30 [00:01<00:02,  5.93it/s]                                               {'loss': 0.4889, 'grad_norm': 1.0202608108520508, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  5.93it/s] 50%|█████     | 15/30 [00:02<00:02,  5.71it/s]                                               {'loss': 0.478, 'grad_norm': 0.9140147566795349, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  5.71it/s] 53%|█████▎    | 16/30 [00:02<00:02,  5.54it/s]                                               {'loss': 0.3946, 'grad_norm': 0.7468343377113342, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  5.54it/s] 57%|█████▋    | 17/30 [00:02<00:02,  5.63it/s]                                               {'loss': 0.3068, 'grad_norm': 1.3444534540176392, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  5.63it/s] 60%|██████    | 18/30 [00:02<00:02,  5.67it/s]                                               {'loss': 0.6403, 'grad_norm': 2.2699685096740723, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:02,  5.67it/s] 63%|██████▎   | 19/30 [00:02<00:02,  5.09it/s]                                               {'loss': 0.3739, 'grad_norm': 1.2202320098876953, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:02,  5.09it/s] 67%|██████▋   | 20/30 [00:03<00:02,  5.00it/s]                                               {'loss': 0.3331, 'grad_norm': 0.9732720851898193, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:02,  5.00it/s] 70%|███████   | 21/30 [00:03<00:01,  5.18it/s]                                               {'loss': 0.34, 'grad_norm': 0.916427731513977, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.18it/s] 73%|███████▎  | 22/30 [00:03<00:01,  5.30it/s]                                               {'loss': 0.4128, 'grad_norm': 0.694084644317627, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  5.30it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.97it/s]                                               {'loss': 0.3155, 'grad_norm': 0.7614656686782837, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.97it/s] 80%|████████  | 24/30 [00:03<00:00,  6.29it/s]                                               {'loss': 0.4189, 'grad_norm': 1.4354792833328247, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.29it/s] 83%|████████▎ | 25/30 [00:03<00:00,  5.82it/s]                                               {'loss': 0.2134, 'grad_norm': 0.8536338210105896, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  5.82it/s] 87%|████████▋ | 26/30 [00:04<00:00,  5.88it/s]                                               {'loss': 0.4771, 'grad_norm': 1.397056221961975, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  5.88it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.79it/s]                                               {'loss': 0.4658, 'grad_norm': 1.5513243675231934, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.79it/s] 93%|█████████▎| 28/30 [00:04<00:00,  5.32it/s]                                               {'loss': 0.1933, 'grad_norm': 1.0769505500793457, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  5.32it/s] 97%|█████████▋| 29/30 [00:04<00:00,  5.36it/s]                                               {'loss': 0.334, 'grad_norm': 1.5723854303359985, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  5.36it/s]100%|██████████| 30/30 [00:04<00:00,  5.30it/s]                                               {'loss': 0.6708, 'grad_norm': 3.362687826156616, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  5.30it/s]                                               {'train_runtime': 5.4156, 'train_samples_per_second': 78.477, 'train_steps_per_second': 5.54, 'train_loss': 0.4451852758725484, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  5.30it/s]100%|██████████| 30/30 [00:05<00:00,  5.54it/s]
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.70it/s]                                              {'loss': 0.5783, 'grad_norm': 0.6181938648223877, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.70it/s]  7%|▋         | 2/30 [00:00<00:06,  4.54it/s]                                              {'loss': 0.6961, 'grad_norm': 0.7436906695365906, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.54it/s] 10%|█         | 3/30 [00:00<00:05,  4.72it/s]                                              {'loss': 0.684, 'grad_norm': 0.615078330039978, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.72it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.66it/s]                                              {'loss': 0.7083, 'grad_norm': 0.722313404083252, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.66it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.96it/s]                                              {'loss': 0.688, 'grad_norm': 0.7184293866157532, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.96it/s] 20%|██        | 6/30 [00:01<00:08,  2.75it/s]                                              {'loss': 0.9653, 'grad_norm': 1.8344838619232178, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:08,  2.75it/s] 23%|██▎       | 7/30 [00:01<00:06,  3.31it/s]                                              {'loss': 0.6237, 'grad_norm': 0.5893360376358032, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:06,  3.31it/s]                                              {'loss': 0.747, 'grad_norm': 0.7877139449119568, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.31it/s] 30%|███       | 9/30 [00:02<00:04,  4.66it/s]                                              {'loss': 0.6393, 'grad_norm': 0.5024961233139038, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:04,  4.66it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.17it/s]                                               {'loss': 0.6393, 'grad_norm': 0.4405952990055084, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.17it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.57it/s]                                               {'loss': 0.6791, 'grad_norm': 0.3186298608779907, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.57it/s]                                               {'loss': 0.628, 'grad_norm': 1.2661515474319458, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.57it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.19it/s]                                               {'loss': 0.631, 'grad_norm': 0.5280448198318481, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.19it/s] 47%|████▋     | 14/30 [00:02<00:03,  5.21it/s]                                               {'loss': 0.625, 'grad_norm': 0.46802055835723877, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  5.21it/s] 50%|█████     | 15/30 [00:03<00:03,  4.61it/s]                                               {'loss': 0.6091, 'grad_norm': 0.8941900134086609, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.61it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.24it/s]                                               {'loss': 0.6405, 'grad_norm': 0.4220544695854187, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.24it/s] 57%|█████▋    | 17/30 [00:03<00:03,  4.05it/s]                                               {'loss': 0.6281, 'grad_norm': 0.6732879281044006, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  4.05it/s]                                               {'loss': 0.643, 'grad_norm': 1.0591596364974976, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.05it/s] 63%|██████▎   | 19/30 [00:04<00:02,  5.06it/s]                                               {'loss': 0.6212, 'grad_norm': 0.7811176180839539, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  5.06it/s] 67%|██████▋   | 20/30 [00:04<00:01,  5.32it/s]                                               {'loss': 0.6598, 'grad_norm': 0.5545303225517273, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  5.32it/s] 70%|███████   | 21/30 [00:04<00:01,  5.64it/s]                                               {'loss': 0.6016, 'grad_norm': 0.8174314498901367, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.64it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.93it/s]                                               {'loss': 0.625, 'grad_norm': 0.7070910334587097, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.93it/s] 77%|███████▋  | 23/30 [00:04<00:01,  6.19it/s]                                               {'loss': 0.5793, 'grad_norm': 0.7984609603881836, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  6.19it/s]                                               {'loss': 0.6169, 'grad_norm': 1.0453156232833862, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  6.19it/s] 83%|████████▎ | 25/30 [00:05<00:00,  5.72it/s]                                               {'loss': 0.5234, 'grad_norm': 0.8115234375, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  5.72it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.00it/s]                                               {'loss': 0.574, 'grad_norm': 0.7676396369934082, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.00it/s] 90%|█████████ | 27/30 [00:05<00:00,  4.53it/s]                                               {'loss': 0.5481, 'grad_norm': 1.5261842012405396, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  4.53it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.31it/s]                                               {'loss': 0.5752, 'grad_norm': 0.7355989813804626, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.31it/s] 97%|█████████▋| 29/30 [00:06<00:00,  4.17it/s]                                               {'loss': 0.5386, 'grad_norm': 1.06722092628479, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  4.17it/s]100%|██████████| 30/30 [00:06<00:00,  4.92it/s]                                               {'loss': 0.5519, 'grad_norm': 2.1574180126190186, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.92it/s]                                               {'train_runtime': 6.3697, 'train_samples_per_second': 66.722, 'train_steps_per_second': 4.71, 'train_loss': 0.635597272713979, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.92it/s]100%|██████████| 30/30 [00:06<00:00,  4.71it/s]
CLIENT:95
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  9.22it/s]                                              {'loss': 0.5835, 'grad_norm': 0.7788146734237671, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  9.22it/s]                                              {'loss': 0.7382, 'grad_norm': 0.8311075568199158, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  9.22it/s] 10%|█         | 3/30 [00:00<00:02, 10.23it/s]                                              {'loss': 0.7408, 'grad_norm': 0.6867741942405701, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.23it/s]                                              {'loss': 0.6723, 'grad_norm': 0.8879172801971436, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.23it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.42it/s]                                              {'loss': 0.6665, 'grad_norm': 0.3310062289237976, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.42it/s]                                              {'loss': 0.6887, 'grad_norm': 0.7550037503242493, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:02, 11.42it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.70it/s]                                              {'loss': 0.6695, 'grad_norm': 0.41105231642723083, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.70it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.99it/s]                                              {'loss': 0.6946, 'grad_norm': 0.3296746015548706, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.99it/s] 30%|███       | 9/30 [00:01<00:03,  5.37it/s]                                              {'loss': 0.622, 'grad_norm': 0.35528579354286194, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.37it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.82it/s]                                               {'loss': 0.6522, 'grad_norm': 0.6819009780883789, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.82it/s]                                               {'loss': 0.6729, 'grad_norm': 0.2972026765346527, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:03,  5.82it/s] 40%|████      | 12/30 [00:01<00:02,  8.27it/s]                                               {'loss': 0.4414, 'grad_norm': 0.5294874906539917, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.27it/s]                                               {'loss': 0.684, 'grad_norm': 0.7117765545845032, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.27it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.39it/s]                                               {'loss': 0.6349, 'grad_norm': 0.4255790710449219, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.39it/s]                                               {'loss': 0.6671, 'grad_norm': 1.373944878578186, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.39it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.88it/s]                                               {'loss': 0.6109, 'grad_norm': 0.6631155610084534, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.88it/s] 57%|█████▋    | 17/30 [00:02<00:01,  6.51it/s]                                               {'loss': 0.6215, 'grad_norm': 0.4558205306529999, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  6.51it/s] 60%|██████    | 18/30 [00:02<00:01,  6.87it/s]                                               {'loss': 0.6445, 'grad_norm': 0.485033243894577, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  6.87it/s] 63%|██████▎   | 19/30 [00:02<00:01,  6.40it/s]                                               {'loss': 0.6266, 'grad_norm': 0.9573853015899658, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  6.40it/s] 67%|██████▋   | 20/30 [00:03<00:01,  6.58it/s]                                               {'loss': 0.6465, 'grad_norm': 0.2796885669231415, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  6.58it/s] 70%|███████   | 21/30 [00:03<00:01,  7.09it/s]                                               {'loss': 0.6224, 'grad_norm': 0.5396556258201599, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.09it/s] 73%|███████▎  | 22/30 [00:03<00:01,  6.83it/s]                                               {'loss': 0.6042, 'grad_norm': 0.7651206851005554, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  6.83it/s] 77%|███████▋  | 23/30 [00:03<00:01,  6.14it/s]                                               {'loss': 0.6683, 'grad_norm': 0.6519577503204346, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  6.14it/s] 80%|████████  | 24/30 [00:03<00:00,  6.25it/s]                                               {'loss': 0.619, 'grad_norm': 1.759101390838623, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  6.25it/s] 83%|████████▎ | 25/30 [00:03<00:00,  5.43it/s]                                               {'loss': 0.7456, 'grad_norm': 1.0540610551834106, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  5.43it/s] 87%|████████▋ | 26/30 [00:03<00:00,  6.13it/s]                                               {'loss': 0.5648, 'grad_norm': 0.9360617399215698, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  6.13it/s] 90%|█████████ | 27/30 [00:04<00:00,  6.37it/s]                                               {'loss': 0.5852, 'grad_norm': 1.7338447570800781, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  6.37it/s]                                               {'loss': 0.5965, 'grad_norm': 0.782076895236969, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.37it/s] 97%|█████████▋| 29/30 [00:04<00:00,  6.30it/s]                                               {'loss': 0.5331, 'grad_norm': 0.8623128533363342, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  6.30it/s]                                               {'loss': 0.5578, 'grad_norm': 1.5048942565917969, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.30it/s]                                               {'train_runtime': 4.6117, 'train_samples_per_second': 92.158, 'train_steps_per_second': 6.505, 'train_loss': 0.6358423113822937, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.30it/s]100%|██████████| 30/30 [00:04<00:00,  6.51it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  5%|▍         | 3/66 [00:00<00:03, 18.11it/s]  8%|▊         | 5/66 [00:00<00:03, 17.10it/s] 11%|█         | 7/66 [00:00<00:03, 15.97it/s] 14%|█▎        | 9/66 [00:00<00:03, 15.94it/s] 17%|█▋        | 11/66 [00:00<00:03, 15.09it/s] 20%|█▉        | 13/66 [00:00<00:03, 14.53it/s] 23%|██▎       | 15/66 [00:00<00:03, 14.69it/s] 26%|██▌       | 17/66 [00:01<00:03, 14.86it/s] 29%|██▉       | 19/66 [00:01<00:03, 14.71it/s] 32%|███▏      | 21/66 [00:01<00:03, 13.50it/s] 35%|███▍      | 23/66 [00:01<00:03, 13.70it/s] 38%|███▊      | 25/66 [00:01<00:03, 13.23it/s] 41%|████      | 27/66 [00:01<00:03, 12.91it/s] 44%|████▍     | 29/66 [00:02<00:02, 12.82it/s] 47%|████▋     | 31/66 [00:02<00:02, 12.70it/s] 50%|█████     | 33/66 [00:02<00:02, 12.88it/s] 53%|█████▎    | 35/66 [00:02<00:02, 12.47it/s] 56%|█████▌    | 37/66 [00:02<00:02, 11.58it/s] 59%|█████▉    | 39/66 [00:02<00:02, 11.65it/s] 62%|██████▏   | 41/66 [00:03<00:02, 11.95it/s] 65%|██████▌   | 43/66 [00:03<00:02, 11.41it/s] 68%|██████▊   | 45/66 [00:03<00:01, 11.36it/s] 71%|███████   | 47/66 [00:03<00:01, 11.58it/s] 74%|███████▍  | 49/66 [00:03<00:01, 11.44it/s] 77%|███████▋  | 51/66 [00:03<00:01, 11.65it/s] 80%|████████  | 53/66 [00:04<00:01, 12.47it/s] 85%|████████▍ | 56/66 [00:04<00:00, 15.09it/s] 89%|████████▉ | 59/66 [00:04<00:00, 16.51it/s] 92%|█████████▏| 61/66 [00:04<00:00, 15.50it/s] 95%|█████████▌| 63/66 [00:04<00:00, 15.41it/s]100%|██████████| 66/66 [00:04<00:00, 13.93it/s]
{'eval_loss': 0.6408883333206177, 'eval_model_preparation_time': 0.0334, 'eval_acc': 0.6778523489932886, 'eval_runtime': 4.8692, 'eval_samples_per_second': 214.201, 'eval_steps_per_second': 13.554}
ROUND:8
CLIENT:2
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.71it/s]                                              {'loss': 0.6325, 'grad_norm': 0.6429534554481506, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.71it/s]  7%|▋         | 2/30 [00:00<00:04,  6.00it/s]                                              {'loss': 0.7197, 'grad_norm': 0.6945229172706604, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.00it/s] 10%|█         | 3/30 [00:00<00:04,  5.44it/s]                                              {'loss': 0.6928, 'grad_norm': 0.48827889561653137, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  5.44it/s] 13%|█▎        | 4/30 [00:00<00:05,  5.15it/s]                                              {'loss': 0.6645, 'grad_norm': 0.5498296022415161, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  5.15it/s] 17%|█▋        | 5/30 [00:00<00:05,  4.99it/s]                                              {'loss': 0.6544, 'grad_norm': 0.30915889143943787, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:05,  4.99it/s] 20%|██        | 6/30 [00:01<00:04,  4.85it/s]                                              {'loss': 0.8201, 'grad_norm': 1.7656806707382202, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.85it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.83it/s]                                              {'loss': 0.7097, 'grad_norm': 0.5840747952461243, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.83it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.85it/s]                                              {'loss': 0.7006, 'grad_norm': 0.4579617977142334, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.85it/s] 30%|███       | 9/30 [00:01<00:04,  5.10it/s]                                              {'loss': 0.7355, 'grad_norm': 0.5955049991607666, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  5.10it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.07it/s]                                               {'loss': 0.6761, 'grad_norm': 0.22949892282485962, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.07it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.24it/s]                                               {'loss': 0.6668, 'grad_norm': 0.35726282000541687, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.24it/s]                                               {'loss': 0.6955, 'grad_norm': 1.167076826095581, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.24it/s] 43%|████▎     | 13/30 [00:02<00:03,  5.65it/s]                                               {'loss': 0.671, 'grad_norm': 0.42781567573547363, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:03,  5.65it/s] 47%|████▋     | 14/30 [00:02<00:03,  4.98it/s]                                               {'loss': 0.621, 'grad_norm': 0.8297215104103088, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:03,  4.98it/s] 50%|█████     | 15/30 [00:02<00:03,  4.62it/s]                                               {'loss': 0.6701, 'grad_norm': 1.0058611631393433, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.62it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.24it/s]                                               {'loss': 0.631, 'grad_norm': 0.47818639874458313, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.24it/s] 57%|█████▋    | 17/30 [00:03<00:03,  4.00it/s]                                               {'loss': 0.617, 'grad_norm': 0.7073224782943726, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:03,  4.00it/s]                                               {'loss': 0.6951, 'grad_norm': 1.1367619037628174, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.00it/s] 63%|██████▎   | 19/30 [00:03<00:02,  4.66it/s]                                               {'loss': 0.5977, 'grad_norm': 1.4575376510620117, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:02,  4.66it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.41it/s]                                               {'loss': 0.6221, 'grad_norm': 0.2917932868003845, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.41it/s] 70%|███████   | 21/30 [00:04<00:01,  4.73it/s]                                               {'loss': 0.6494, 'grad_norm': 0.4018884599208832, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  4.73it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.55it/s]                                               {'loss': 0.6643, 'grad_norm': 0.5399525165557861, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.55it/s] 77%|███████▋  | 23/30 [00:04<00:01,  4.46it/s]                                               {'loss': 0.6463, 'grad_norm': 1.064682960510254, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  4.46it/s] 80%|████████  | 24/30 [00:04<00:01,  5.22it/s]                                               {'loss': 0.7031, 'grad_norm': 1.157979965209961, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:01,  5.22it/s] 83%|████████▎ | 25/30 [00:05<00:01,  4.60it/s]                                               {'loss': 0.5771, 'grad_norm': 0.7108099460601807, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  4.60it/s] 87%|████████▋ | 26/30 [00:05<00:00,  4.37it/s]                                               {'loss': 0.6135, 'grad_norm': 0.8854830265045166, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  4.37it/s] 90%|█████████ | 27/30 [00:05<00:00,  3.70it/s]                                               {'loss': 0.632, 'grad_norm': 0.5290932059288025, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  3.70it/s] 93%|█████████▎| 28/30 [00:06<00:00,  3.77it/s]                                               {'loss': 0.6041, 'grad_norm': 0.6498128771781921, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  3.77it/s] 97%|█████████▋| 29/30 [00:06<00:00,  3.66it/s]                                               {'loss': 0.5132, 'grad_norm': 0.6462967991828918, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  3.66it/s]                                               {'loss': 0.566, 'grad_norm': 1.418469786643982, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  3.66it/s]                                               {'train_runtime': 6.6192, 'train_samples_per_second': 64.207, 'train_steps_per_second': 4.532, 'train_loss': 0.6553975303967794, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  3.66it/s]100%|██████████| 30/30 [00:06<00:00,  4.53it/s]
CLIENT:49
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.70it/s]                                              {'loss': 0.7009, 'grad_norm': 1.2520793676376343, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.70it/s]  7%|▋         | 2/30 [00:01<00:15,  1.80it/s]                                              {'loss': 0.4354, 'grad_norm': 1.4845439195632935, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:15,  1.80it/s] 10%|█         | 3/30 [00:01<00:10,  2.48it/s]                                              {'loss': 0.275, 'grad_norm': 0.9286206364631653, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:10,  2.48it/s] 13%|█▎        | 4/30 [00:01<00:08,  3.09it/s]                                              {'loss': 0.4241, 'grad_norm': 1.7952834367752075, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  3.09it/s] 17%|█▋        | 5/30 [00:01<00:06,  3.65it/s]                                              {'loss': 0.32, 'grad_norm': 0.6611668467521667, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:06,  3.65it/s] 20%|██        | 6/30 [00:01<00:05,  4.52it/s]                                              {'loss': 0.0424, 'grad_norm': 0.7253206968307495, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.52it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.78it/s]                                              {'loss': 0.5235, 'grad_norm': 2.109800100326538, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.78it/s] 27%|██▋       | 8/30 [00:02<00:03,  5.68it/s]                                              {'loss': 0.2727, 'grad_norm': 0.9624077677726746, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:03,  5.68it/s] 30%|███       | 9/30 [00:02<00:03,  6.42it/s]                                              {'loss': 0.5392, 'grad_norm': 2.2033534049987793, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:03,  6.42it/s] 33%|███▎      | 10/30 [00:02<00:03,  6.54it/s]                                               {'loss': 0.4714, 'grad_norm': 4.593205451965332, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  6.54it/s] 37%|███▋      | 11/30 [00:02<00:02,  7.19it/s]                                               {'loss': 0.0981, 'grad_norm': 1.4717851877212524, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:02,  7.19it/s]                                               {'loss': 0.0427, 'grad_norm': 1.0394046306610107, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  7.19it/s] 43%|████▎     | 13/30 [00:02<00:02,  8.45it/s]                                               {'loss': 0.282, 'grad_norm': 2.886449098587036, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  8.45it/s] 47%|████▋     | 14/30 [00:02<00:01,  8.28it/s]                                               {'loss': 0.2415, 'grad_norm': 2.336364507675171, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  8.28it/s]                                               {'loss': 0.7031, 'grad_norm': 4.462600231170654, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  8.28it/s] 53%|█████▎    | 16/30 [00:02<00:01,  8.29it/s]                                               {'loss': 0.2663, 'grad_norm': 0.9493700861930847, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  8.29it/s]                                               {'loss': 0.1975, 'grad_norm': 1.2648382186889648, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:01,  8.29it/s] 60%|██████    | 18/30 [00:03<00:01,  9.54it/s]                                               {'loss': 0.0556, 'grad_norm': 0.8726858496665955, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  9.54it/s] 63%|██████▎   | 19/30 [00:03<00:01,  9.27it/s]                                               {'loss': 0.0734, 'grad_norm': 1.0320481061935425, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  9.27it/s] 67%|██████▋   | 20/30 [00:03<00:01,  9.31it/s]                                               {'loss': 0.6233, 'grad_norm': 3.6137170791625977, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  9.31it/s]                                               {'loss': 0.3067, 'grad_norm': 2.7702581882476807, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:00,  9.31it/s] 73%|███████▎  | 22/30 [00:03<00:00,  8.81it/s]                                               {'loss': 0.0511, 'grad_norm': 0.9349410533905029, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00,  8.81it/s] 77%|███████▋  | 23/30 [00:03<00:00,  8.75it/s]                                               {'loss': 0.3559, 'grad_norm': 3.2104275226593018, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  8.75it/s]                                               {'loss': 0.0414, 'grad_norm': 1.6511629819869995, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  8.75it/s] 83%|████████▎ | 25/30 [00:03<00:00,  9.27it/s]                                               {'loss': 0.4844, 'grad_norm': 1.864145278930664, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  9.27it/s] 87%|████████▋ | 26/30 [00:04<00:00,  9.10it/s]                                               {'loss': 0.0449, 'grad_norm': 0.7243102788925171, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  9.10it/s] 90%|█████████ | 27/30 [00:04<00:00,  9.13it/s]                                               {'loss': 0.1941, 'grad_norm': 0.8731130361557007, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  9.13it/s] 93%|█████████▎| 28/30 [00:04<00:00,  8.35it/s]                                               {'loss': 0.1611, 'grad_norm': 0.8657190203666687, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  8.35it/s] 97%|█████████▋| 29/30 [00:04<00:00,  8.35it/s]                                               {'loss': 0.3425, 'grad_norm': 1.5223560333251953, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  8.35it/s]                                               {'loss': 0.412, 'grad_norm': 3.651256561279297, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  8.35it/s]                                               {'train_runtime': 4.563, 'train_samples_per_second': 93.141, 'train_steps_per_second': 6.575, 'train_loss': 0.29940332323312757, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  8.35it/s]100%|██████████| 30/30 [00:04<00:00,  6.58it/s]
CLIENT:82
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.71it/s]                                              {'loss': 0.801, 'grad_norm': 1.1618882417678833, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.71it/s]  7%|▋         | 2/30 [00:00<00:04,  6.68it/s]                                              {'loss': 0.6047, 'grad_norm': 0.637275755405426, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.68it/s]                                              {'loss': 0.522, 'grad_norm': 0.5047767162322998, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.68it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.85it/s]                                              {'loss': 0.5542, 'grad_norm': 1.0649864673614502, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.85it/s]                                              {'loss': 0.8304, 'grad_norm': 3.6661667823791504, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.85it/s] 20%|██        | 6/30 [00:01<00:08,  2.95it/s]                                              {'loss': 0.9535, 'grad_norm': 1.8924674987792969, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:08,  2.95it/s] 23%|██▎       | 7/30 [00:01<00:07,  3.22it/s]                                              {'loss': 0.5173, 'grad_norm': 0.6165696978569031, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:07,  3.22it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.64it/s]                                              {'loss': 0.7094, 'grad_norm': 1.2831170558929443, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.64it/s] 30%|███       | 9/30 [00:02<00:05,  4.14it/s]                                              {'loss': 0.5156, 'grad_norm': 1.086188793182373, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  4.14it/s] 33%|███▎      | 10/30 [00:02<00:04,  4.53it/s]                                               {'loss': 0.5587, 'grad_norm': 1.1486319303512573, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:04,  4.53it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.72it/s]                                               {'loss': 0.7151, 'grad_norm': 1.0497792959213257, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.72it/s] 40%|████      | 12/30 [00:02<00:03,  5.24it/s]                                               {'loss': 0.5177, 'grad_norm': 1.9531153440475464, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.24it/s] 43%|████▎     | 13/30 [00:03<00:03,  4.71it/s]                                               {'loss': 0.5823, 'grad_norm': 1.3871945142745972, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  4.71it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.77it/s]                                               {'loss': 0.5333, 'grad_norm': 0.9957026243209839, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.77it/s] 50%|█████     | 15/30 [00:03<00:03,  4.93it/s]                                               {'loss': 0.6605, 'grad_norm': 1.5853521823883057, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.93it/s] 53%|█████▎    | 16/30 [00:03<00:02,  4.81it/s]                                               {'loss': 0.6248, 'grad_norm': 1.7285871505737305, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:02,  4.81it/s] 57%|█████▋    | 17/30 [00:03<00:02,  4.93it/s]                                               {'loss': 0.5716, 'grad_norm': 1.446269154548645, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:02,  4.93it/s]                                               {'loss': 0.6013, 'grad_norm': 3.044891595840454, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:02,  4.93it/s] 63%|██████▎   | 19/30 [00:04<00:01,  5.83it/s]                                               {'loss': 0.5619, 'grad_norm': 1.6726703643798828, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:01,  5.83it/s] 67%|██████▋   | 20/30 [00:04<00:01,  5.61it/s]                                               {'loss': 0.4504, 'grad_norm': 2.3056721687316895, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  5.61it/s] 70%|███████   | 21/30 [00:04<00:01,  5.10it/s]                                               {'loss': 0.5599, 'grad_norm': 1.403125524520874, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.10it/s] 73%|███████▎  | 22/30 [00:04<00:01,  4.85it/s]                                               {'loss': 0.6145, 'grad_norm': 1.6495802402496338, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  4.85it/s] 77%|███████▋  | 23/30 [00:04<00:01,  5.12it/s]                                               {'loss': 0.5484, 'grad_norm': 1.5263185501098633, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:01,  5.12it/s] 80%|████████  | 24/30 [00:05<00:01,  5.73it/s]                                               {'loss': 0.4443, 'grad_norm': 3.329970121383667, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.73it/s] 83%|████████▎ | 25/30 [00:05<00:00,  5.90it/s]                                               {'loss': 0.5094, 'grad_norm': 1.7271140813827515, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  5.90it/s] 87%|████████▋ | 26/30 [00:05<00:00,  5.43it/s]                                               {'loss': 0.5895, 'grad_norm': 1.3499610424041748, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  5.43it/s] 90%|█████████ | 27/30 [00:05<00:00,  5.01it/s]                                               {'loss': 0.4004, 'grad_norm': 1.1291487216949463, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  5.01it/s] 93%|█████████▎| 28/30 [00:05<00:00,  4.94it/s]                                               {'loss': 0.3644, 'grad_norm': 2.8386456966400146, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  4.94it/s] 97%|█████████▋| 29/30 [00:06<00:00,  5.01it/s]                                               {'loss': 0.6094, 'grad_norm': 2.442948579788208, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  5.01it/s]                                               {'loss': 0.4499, 'grad_norm': 3.6070475578308105, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.01it/s]                                               {'train_runtime': 6.5882, 'train_samples_per_second': 64.51, 'train_steps_per_second': 4.554, 'train_loss': 0.5825238277514776, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  5.01it/s]100%|██████████| 30/30 [00:06<00:00,  4.55it/s]
CLIENT:31
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  7.10it/s]                                              {'loss': 0.7508, 'grad_norm': 1.4710278511047363, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  7.10it/s]                                              {'loss': 0.5327, 'grad_norm': 1.0450074672698975, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.10it/s] 10%|█         | 3/30 [00:00<00:03,  7.38it/s]                                              {'loss': 0.5041, 'grad_norm': 0.5717182755470276, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.38it/s] 13%|█▎        | 4/30 [00:00<00:03,  6.63it/s]                                              {'loss': 0.4595, 'grad_norm': 1.3367459774017334, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  6.63it/s] 17%|█▋        | 5/30 [00:00<00:03,  6.64it/s]                                              {'loss': 0.6558, 'grad_norm': 2.5606021881103516, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  6.64it/s] 20%|██        | 6/30 [00:01<00:10,  2.24it/s]                                              {'loss': 1.2792, 'grad_norm': 4.396912097930908, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:10,  2.24it/s] 23%|██▎       | 7/30 [00:01<00:08,  2.74it/s]                                              {'loss': 0.4565, 'grad_norm': 1.8079633712768555, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:08,  2.74it/s] 27%|██▋       | 8/30 [00:02<00:07,  3.04it/s]                                              {'loss': 0.5168, 'grad_norm': 1.0355560779571533, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:07,  3.04it/s] 30%|███       | 9/30 [00:02<00:06,  3.36it/s]                                              {'loss': 0.6156, 'grad_norm': 0.9819130897521973, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:06,  3.36it/s] 33%|███▎      | 10/30 [00:02<00:05,  3.62it/s]                                               {'loss': 0.5583, 'grad_norm': 2.2089107036590576, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  3.62it/s] 37%|███▋      | 11/30 [00:02<00:05,  3.78it/s]                                               {'loss': 0.7551, 'grad_norm': 2.4617502689361572, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:05,  3.78it/s] 40%|████      | 12/30 [00:03<00:04,  4.34it/s]                                               {'loss': 0.4174, 'grad_norm': 1.996617078781128, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:04,  4.34it/s] 43%|████▎     | 13/30 [00:03<00:04,  4.21it/s]                                               {'loss': 0.5965, 'grad_norm': 3.245407819747925, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:04,  4.21it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.35it/s]                                               {'loss': 0.3711, 'grad_norm': 2.040163040161133, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.35it/s] 50%|█████     | 15/30 [00:03<00:03,  4.52it/s]                                               {'loss': 0.5492, 'grad_norm': 1.4126336574554443, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.52it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.60it/s]                                               {'loss': 0.5882, 'grad_norm': 1.5996928215026855, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.60it/s] 57%|█████▋    | 17/30 [00:04<00:02,  4.71it/s]                                               {'loss': 0.4494, 'grad_norm': 1.3104071617126465, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:02,  4.71it/s]                                               {'loss': 0.8951, 'grad_norm': 4.142340660095215, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.71it/s] 63%|██████▎   | 19/30 [00:04<00:02,  5.44it/s]                                               {'loss': 0.4501, 'grad_norm': 1.4268049001693726, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  5.44it/s] 67%|██████▋   | 20/30 [00:04<00:01,  5.20it/s]                                               {'loss': 0.4632, 'grad_norm': 1.1220998764038086, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:01,  5.20it/s] 70%|███████   | 21/30 [00:04<00:01,  5.29it/s]                                               {'loss': 0.4952, 'grad_norm': 1.8742215633392334, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.29it/s] 73%|███████▎  | 22/30 [00:05<00:01,  5.02it/s]                                               {'loss': 0.6299, 'grad_norm': 2.137479782104492, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:01,  5.02it/s] 77%|███████▋  | 23/30 [00:05<00:01,  4.86it/s]                                               {'loss': 0.3877, 'grad_norm': 1.5042845010757446, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  4.86it/s] 80%|████████  | 24/30 [00:05<00:01,  5.55it/s]                                               {'loss': 0.5324, 'grad_norm': 9.115403175354004, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.55it/s] 83%|████████▎ | 25/30 [00:05<00:01,  4.87it/s]                                               {'loss': 0.4393, 'grad_norm': 1.270631194114685, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:01,  4.87it/s] 87%|████████▋ | 26/30 [00:05<00:00,  4.48it/s]                                               {'loss': 0.6168, 'grad_norm': 1.868843674659729, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  4.48it/s] 90%|█████████ | 27/30 [00:06<00:00,  4.24it/s]                                               {'loss': 0.3966, 'grad_norm': 0.9742340445518494, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  4.24it/s] 93%|█████████▎| 28/30 [00:06<00:00,  3.98it/s]                                               {'loss': 0.4003, 'grad_norm': 1.0434335470199585, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  3.98it/s] 97%|█████████▋| 29/30 [00:06<00:00,  3.95it/s]                                               {'loss': 0.5715, 'grad_norm': 2.097566604614258, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:06<00:00,  3.95it/s]100%|██████████| 30/30 [00:06<00:00,  4.75it/s]                                               {'loss': 0.2993, 'grad_norm': 3.5964643955230713, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.75it/s]                                               {'train_runtime': 6.9876, 'train_samples_per_second': 60.822, 'train_steps_per_second': 4.293, 'train_loss': 0.5544555097818374, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  4.75it/s]100%|██████████| 30/30 [00:06<00:00,  4.30it/s]
CLIENT:37
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:28,  1.02it/s]                                              {'loss': 0.8294, 'grad_norm': 1.7212409973144531, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:28,  1.02it/s]                                              {'loss': 0.4738, 'grad_norm': 1.2991188764572144, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:27,  1.02it/s] 10%|█         | 3/30 [00:01<00:08,  3.12it/s]                                              {'loss': 0.716, 'grad_norm': 1.4124281406402588, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:08,  3.12it/s]                                              {'loss': 0.6984, 'grad_norm': 1.5397478342056274, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:08,  3.12it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.99it/s]                                              {'loss': 0.6764, 'grad_norm': 2.001862049102783, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.99it/s]                                              {'loss': 1.1359, 'grad_norm': 3.786123037338257, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  4.99it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.89it/s]                                              {'loss': 0.3386, 'grad_norm': 1.6499366760253906, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.89it/s]                                              {'loss': 0.5109, 'grad_norm': 1.8021373748779297, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  6.89it/s] 30%|███       | 9/30 [00:01<00:02,  8.39it/s]                                              {'loss': 0.6661, 'grad_norm': 1.9685707092285156, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  8.39it/s]                                              {'loss': 0.5987, 'grad_norm': 1.445056438446045, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.39it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.32it/s]                                               {'loss': 0.6845, 'grad_norm': 1.3529863357543945, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  9.32it/s]                                               {'loss': 0.3783, 'grad_norm': 1.864445686340332, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:01,  9.32it/s] 43%|████▎     | 13/30 [00:01<00:01, 10.58it/s]                                               {'loss': 0.4409, 'grad_norm': 1.4035149812698364, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 10.58it/s]                                               {'loss': 0.5231, 'grad_norm': 1.3929003477096558, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01, 10.58it/s] 50%|█████     | 15/30 [00:02<00:01, 11.22it/s]                                               {'loss': 0.5672, 'grad_norm': 1.7775822877883911, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01, 11.22it/s]                                               {'loss': 0.4556, 'grad_norm': 1.7371656894683838, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01, 11.22it/s] 57%|█████▋    | 17/30 [00:02<00:01, 11.23it/s]                                               {'loss': 0.4875, 'grad_norm': 1.6331439018249512, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01, 11.23it/s]                                               {'loss': 0.622, 'grad_norm': 2.483454704284668, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 11.23it/s] 63%|██████▎   | 19/30 [00:02<00:00, 11.71it/s]                                               {'loss': 0.5098, 'grad_norm': 1.125561237335205, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 11.71it/s]                                               {'loss': 0.3616, 'grad_norm': 1.3645600080490112, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 11.71it/s] 70%|███████   | 21/30 [00:02<00:00, 11.79it/s]                                               {'loss': 0.5187, 'grad_norm': 1.3519765138626099, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 11.79it/s]                                               {'loss': 0.4911, 'grad_norm': 1.5378305912017822, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 11.79it/s] 77%|███████▋  | 23/30 [00:02<00:00, 12.08it/s]                                               {'loss': 0.6704, 'grad_norm': 1.9638524055480957, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 12.08it/s]                                               {'loss': 0.2296, 'grad_norm': 1.6634825468063354, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 12.08it/s] 83%|████████▎ | 25/30 [00:02<00:00, 13.24it/s]                                               {'loss': 0.3351, 'grad_norm': 1.5795255899429321, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 13.24it/s]                                               {'loss': 0.3852, 'grad_norm': 1.0392709970474243, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.24it/s] 90%|█████████ | 27/30 [00:03<00:00, 12.08it/s]                                               {'loss': 0.5489, 'grad_norm': 2.482084035873413, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 12.08it/s]                                               {'loss': 0.4119, 'grad_norm': 1.8984601497650146, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 12.08it/s] 97%|█████████▋| 29/30 [00:03<00:00, 12.24it/s]                                               {'loss': 0.4366, 'grad_norm': 1.3254456520080566, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 12.24it/s]                                               {'loss': 0.2191, 'grad_norm': 2.59097957611084, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.24it/s]                                               {'train_runtime': 3.3326, 'train_samples_per_second': 127.529, 'train_steps_per_second': 9.002, 'train_loss': 0.5307111710309982, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 12.24it/s]100%|██████████| 30/30 [00:03<00:00,  9.00it/s]
CLIENT:12
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  3.89it/s]                                              {'loss': 0.561, 'grad_norm': 0.5362620949745178, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:07,  3.89it/s]  7%|▋         | 2/30 [00:00<00:07,  3.85it/s]                                              {'loss': 0.6966, 'grad_norm': 0.5843642950057983, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.85it/s] 10%|█         | 3/30 [00:00<00:06,  4.15it/s]                                              {'loss': 0.6949, 'grad_norm': 0.5436450839042664, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.15it/s] 13%|█▎        | 4/30 [00:00<00:06,  4.26it/s]                                              {'loss': 0.6237, 'grad_norm': 0.572945237159729, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:06,  4.26it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.64it/s]                                              {'loss': 0.6639, 'grad_norm': 0.4725661873817444, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.64it/s]                                              {'loss': 0.7339, 'grad_norm': 1.5957436561584473, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.64it/s] 23%|██▎       | 7/30 [00:01<00:03,  6.25it/s]                                              {'loss': 0.6409, 'grad_norm': 0.7382438778877258, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:03,  6.25it/s] 27%|██▋       | 8/30 [00:01<00:03,  5.84it/s]                                              {'loss': 0.6525, 'grad_norm': 0.5397220849990845, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:03,  5.84it/s] 30%|███       | 9/30 [00:01<00:03,  5.48it/s]                                              {'loss': 0.6009, 'grad_norm': 0.45899462699890137, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:03,  5.48it/s] 33%|███▎      | 10/30 [00:02<00:03,  5.10it/s]                                               {'loss': 0.6372, 'grad_norm': 0.3316784203052521, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  5.10it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.02it/s]                                               {'loss': 0.6472, 'grad_norm': 0.4253382980823517, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.02it/s]                                               {'loss': 0.5576, 'grad_norm': 1.4798226356506348, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.02it/s] 43%|████▎     | 13/30 [00:02<00:02,  5.77it/s]                                               {'loss': 0.6227, 'grad_norm': 1.3466522693634033, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  5.77it/s]                                               {'loss': 0.5856, 'grad_norm': 0.7793508768081665, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  5.77it/s] 50%|█████     | 15/30 [00:02<00:02,  7.24it/s]                                               {'loss': 0.6601, 'grad_norm': 1.595863699913025, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.24it/s]                                               {'loss': 0.5292, 'grad_norm': 1.1284277439117432, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.24it/s] 57%|█████▋    | 17/30 [00:02<00:01,  8.65it/s]                                               {'loss': 0.5623, 'grad_norm': 0.867914617061615, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  8.65it/s]                                               {'loss': 0.6415, 'grad_norm': 1.5884668827056885, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  8.65it/s] 63%|██████▎   | 19/30 [00:02<00:01, 10.45it/s]                                               {'loss': 0.4577, 'grad_norm': 1.3024396896362305, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01, 10.45it/s]                                               {'loss': 0.5603, 'grad_norm': 0.9762865304946899, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:00, 10.45it/s] 70%|███████   | 21/30 [00:03<00:00, 10.90it/s]                                               {'loss': 0.467, 'grad_norm': 1.2104662656784058, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:00, 10.90it/s]                                               {'loss': 0.4958, 'grad_norm': 1.1938649415969849, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:00, 10.90it/s] 77%|███████▋  | 23/30 [00:03<00:00,  9.86it/s]                                               {'loss': 0.4707, 'grad_norm': 1.0993146896362305, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  9.86it/s]                                               {'loss': 0.3657, 'grad_norm': 1.74069082736969, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  9.86it/s] 83%|████████▎ | 25/30 [00:03<00:00,  9.17it/s]                                               {'loss': 0.5649, 'grad_norm': 2.035191535949707, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  9.17it/s] 87%|████████▋ | 26/30 [00:03<00:00,  8.32it/s]                                               {'loss': 0.377, 'grad_norm': 1.1091398000717163, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  8.32it/s] 90%|█████████ | 27/30 [00:03<00:00,  7.83it/s]                                               {'loss': 0.4749, 'grad_norm': 1.6103487014770508, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  7.83it/s] 93%|█████████▎| 28/30 [00:04<00:00,  7.18it/s]                                               {'loss': 0.3685, 'grad_norm': 1.4026403427124023, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  7.18it/s] 97%|█████████▋| 29/30 [00:04<00:00,  6.10it/s]                                               {'loss': 0.3005, 'grad_norm': 1.555242657661438, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  6.10it/s]100%|██████████| 30/30 [00:04<00:00,  6.48it/s]                                               {'loss': 0.3382, 'grad_norm': 2.8670811653137207, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.48it/s]                                               {'train_runtime': 4.5666, 'train_samples_per_second': 93.068, 'train_steps_per_second': 6.569, 'train_loss': 0.5517614930868149, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  6.48it/s]100%|██████████| 30/30 [00:04<00:00,  6.58it/s]
CLIENT:87
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:08,  3.39it/s]                                              {'loss': 0.5639, 'grad_norm': 0.6106883883476257, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:08,  3.39it/s]  7%|▋         | 2/30 [00:00<00:06,  4.21it/s]                                              {'loss': 0.6926, 'grad_norm': 0.7999261617660522, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.21it/s] 10%|█         | 3/30 [00:00<00:05,  4.64it/s]                                              {'loss': 0.7452, 'grad_norm': 0.5427402853965759, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  4.64it/s] 13%|█▎        | 4/30 [00:00<00:05,  4.75it/s]                                              {'loss': 0.6507, 'grad_norm': 0.5819423198699951, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:05,  4.75it/s] 17%|█▋        | 5/30 [00:01<00:04,  5.43it/s]                                              {'loss': 0.62, 'grad_norm': 0.33095023036003113, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:04,  5.43it/s] 20%|██        | 6/30 [00:01<00:08,  2.76it/s]                                              {'loss': 0.7747, 'grad_norm': 1.3506298065185547, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:08,  2.76it/s] 23%|██▎       | 7/30 [00:02<00:08,  2.80it/s]                                              {'loss': 0.6489, 'grad_norm': 0.35206565260887146, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:02<00:08,  2.80it/s] 27%|██▋       | 8/30 [00:02<00:07,  2.87it/s]                                              {'loss': 0.6588, 'grad_norm': 0.3827538788318634, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:07,  2.87it/s] 30%|███       | 9/30 [00:02<00:07,  2.70it/s]                                              {'loss': 0.6593, 'grad_norm': 0.5020217895507812, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:07,  2.70it/s] 33%|███▎      | 10/30 [00:03<00:07,  2.76it/s]                                               {'loss': 0.6503, 'grad_norm': 0.42064985632896423, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:03<00:07,  2.76it/s] 37%|███▋      | 11/30 [00:03<00:06,  2.85it/s]                                               {'loss': 0.618, 'grad_norm': 0.3838176131248474, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:03<00:06,  2.85it/s] 40%|████      | 12/30 [00:03<00:05,  3.45it/s]                                               {'loss': 0.5895, 'grad_norm': 1.2108665704727173, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:03<00:05,  3.45it/s] 43%|████▎     | 13/30 [00:03<00:04,  3.41it/s]                                               {'loss': 0.594, 'grad_norm': 0.7832193970680237, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:04,  3.41it/s] 47%|████▋     | 14/30 [00:04<00:04,  3.25it/s]                                               {'loss': 0.5937, 'grad_norm': 0.49547886848449707, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:04<00:04,  3.25it/s] 50%|█████     | 15/30 [00:04<00:04,  3.42it/s]                                               {'loss': 0.7339, 'grad_norm': 1.2267987728118896, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:04<00:04,  3.42it/s] 53%|█████▎    | 16/30 [00:04<00:04,  3.45it/s]                                               {'loss': 0.5599, 'grad_norm': 0.6792933940887451, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:04,  3.45it/s] 57%|█████▋    | 17/30 [00:05<00:03,  3.45it/s]                                               {'loss': 0.5626, 'grad_norm': 0.9923691153526306, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:05<00:03,  3.45it/s] 60%|██████    | 18/30 [00:05<00:03,  3.62it/s]                                               {'loss': 0.8893, 'grad_norm': 2.5149896144866943, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:05<00:03,  3.62it/s] 63%|██████▎   | 19/30 [00:05<00:03,  3.34it/s]                                               {'loss': 0.6509, 'grad_norm': 1.4158083200454712, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:05<00:03,  3.34it/s] 67%|██████▋   | 20/30 [00:06<00:03,  3.09it/s]                                               {'loss': 0.6456, 'grad_norm': 0.8079260587692261, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:06<00:03,  3.09it/s] 70%|███████   | 21/30 [00:06<00:02,  3.02it/s]                                               {'loss': 0.5567, 'grad_norm': 1.0591129064559937, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:06<00:02,  3.02it/s] 73%|███████▎  | 22/30 [00:06<00:02,  3.21it/s]                                               {'loss': 0.5642, 'grad_norm': 1.4220409393310547, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:06<00:02,  3.21it/s] 77%|███████▋  | 23/30 [00:06<00:02,  3.41it/s]                                               {'loss': 0.582, 'grad_norm': 1.4813838005065918, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:06<00:02,  3.41it/s] 80%|████████  | 24/30 [00:07<00:01,  3.93it/s]                                               {'loss': 0.4897, 'grad_norm': 2.5636160373687744, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:07<00:01,  3.93it/s] 83%|████████▎ | 25/30 [00:07<00:01,  4.19it/s]                                               {'loss': 0.579, 'grad_norm': 1.4105151891708374, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:07<00:01,  4.19it/s] 87%|████████▋ | 26/30 [00:07<00:00,  4.42it/s]                                               {'loss': 0.5655, 'grad_norm': 1.0802735090255737, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:07<00:00,  4.42it/s] 90%|█████████ | 27/30 [00:07<00:00,  4.41it/s]                                               {'loss': 0.5458, 'grad_norm': 1.0731929540634155, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:07<00:00,  4.41it/s] 93%|█████████▎| 28/30 [00:08<00:00,  4.18it/s]                                               {'loss': 0.5135, 'grad_norm': 0.9035323262214661, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:08<00:00,  4.18it/s] 97%|█████████▋| 29/30 [00:08<00:00,  4.14it/s]                                               {'loss': 0.4797, 'grad_norm': 1.1967228651046753, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:08<00:00,  4.14it/s]100%|██████████| 30/30 [00:08<00:00,  4.27it/s]                                               {'loss': 0.5613, 'grad_norm': 2.3596913814544678, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.27it/s]                                               {'train_runtime': 8.9003, 'train_samples_per_second': 47.751, 'train_steps_per_second': 3.371, 'train_loss': 0.6179759333531062, 'epoch': 5.0}
100%|██████████| 30/30 [00:08<00:00,  4.27it/s]100%|██████████| 30/30 [00:08<00:00,  3.37it/s]
CLIENT:42
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.17it/s]                                              {'loss': 0.5707, 'grad_norm': 3.606851100921631, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.17it/s]  7%|▋         | 2/30 [00:00<00:07,  3.65it/s]                                              {'loss': 0.7145, 'grad_norm': 1.2917026281356812, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:07,  3.65it/s] 10%|█         | 3/30 [00:00<00:07,  3.53it/s]                                              {'loss': 0.6974, 'grad_norm': 0.5763922333717346, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:07,  3.53it/s] 13%|█▎        | 4/30 [00:01<00:07,  3.42it/s]                                              {'loss': 0.6833, 'grad_norm': 1.8901342153549194, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:07,  3.42it/s] 17%|█▋        | 5/30 [00:01<00:07,  3.43it/s]                                              {'loss': 0.7164, 'grad_norm': 1.6297650337219238, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:07,  3.43it/s] 20%|██        | 6/30 [00:01<00:05,  4.32it/s]                                              {'loss': 0.6087, 'grad_norm': 1.5923296213150024, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:05,  4.32it/s] 23%|██▎       | 7/30 [00:01<00:05,  4.04it/s]                                              {'loss': 0.6541, 'grad_norm': 0.3495120704174042, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  4.04it/s] 27%|██▋       | 8/30 [00:02<00:05,  3.82it/s]                                              {'loss': 0.8218, 'grad_norm': 1.388797640800476, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:05,  3.82it/s] 30%|███       | 9/30 [00:02<00:05,  3.92it/s]                                              {'loss': 0.5681, 'grad_norm': 0.6754912734031677, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  3.92it/s] 33%|███▎      | 10/30 [00:02<00:05,  3.84it/s]                                               {'loss': 0.6687, 'grad_norm': 0.38597550988197327, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  3.84it/s] 37%|███▋      | 11/30 [00:02<00:04,  4.08it/s]                                               {'loss': 0.628, 'grad_norm': 0.3321484625339508, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  4.08it/s] 40%|████      | 12/30 [00:02<00:03,  4.81it/s]                                               {'loss': 0.5571, 'grad_norm': 1.1103899478912354, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  4.81it/s] 43%|████▎     | 13/30 [00:03<00:03,  4.34it/s]                                               {'loss': 0.5954, 'grad_norm': 0.5055339932441711, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  4.34it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.16it/s]                                               {'loss': 0.5873, 'grad_norm': 0.506986677646637, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.16it/s] 50%|█████     | 15/30 [00:03<00:03,  3.94it/s]                                               {'loss': 0.743, 'grad_norm': 1.8212617635726929, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  3.94it/s] 53%|█████▎    | 16/30 [00:04<00:03,  3.98it/s]                                               {'loss': 0.648, 'grad_norm': 1.3623301982879639, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:04<00:03,  3.98it/s] 57%|█████▋    | 17/30 [00:04<00:03,  4.11it/s]                                               {'loss': 0.5983, 'grad_norm': 0.7180923819541931, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:03,  4.11it/s] 60%|██████    | 18/30 [00:04<00:02,  4.34it/s]                                               {'loss': 0.6339, 'grad_norm': 0.810474157333374, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.34it/s] 63%|██████▎   | 19/30 [00:04<00:02,  4.01it/s]                                               {'loss': 0.6319, 'grad_norm': 0.8314372897148132, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  4.01it/s] 67%|██████▋   | 20/30 [00:05<00:02,  3.78it/s]                                               {'loss': 0.6766, 'grad_norm': 1.103066086769104, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:05<00:02,  3.78it/s] 70%|███████   | 21/30 [00:05<00:02,  3.86it/s]                                               {'loss': 0.6378, 'grad_norm': 0.5624703764915466, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:05<00:02,  3.86it/s] 73%|███████▎  | 22/30 [00:05<00:02,  3.91it/s]                                               {'loss': 0.6355, 'grad_norm': 0.7071887850761414, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:05<00:02,  3.91it/s] 77%|███████▋  | 23/30 [00:05<00:01,  4.09it/s]                                               {'loss': 0.6196, 'grad_norm': 0.6075615286827087, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  4.09it/s] 80%|████████  | 24/30 [00:06<00:01,  4.15it/s]                                               {'loss': 0.5582, 'grad_norm': 1.2910984754562378, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:06<00:01,  4.15it/s] 83%|████████▎ | 25/30 [00:06<00:01,  3.88it/s]                                               {'loss': 0.6124, 'grad_norm': 0.7562441229820251, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:06<00:01,  3.88it/s] 87%|████████▋ | 26/30 [00:06<00:00,  4.27it/s]                                               {'loss': 0.6404, 'grad_norm': 0.8458137512207031, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:06<00:00,  4.27it/s] 90%|█████████ | 27/30 [00:06<00:00,  4.95it/s]                                               {'loss': 0.6207, 'grad_norm': 0.8233312368392944, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:06<00:00,  4.95it/s] 93%|█████████▎| 28/30 [00:06<00:00,  4.96it/s]                                               {'loss': 0.5861, 'grad_norm': 0.9132325053215027, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:06<00:00,  4.96it/s] 97%|█████████▋| 29/30 [00:07<00:00,  4.99it/s]                                               {'loss': 0.5859, 'grad_norm': 0.8745977282524109, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:07<00:00,  4.99it/s]100%|██████████| 30/30 [00:07<00:00,  5.30it/s]                                               {'loss': 0.5538, 'grad_norm': 1.1130257844924927, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  5.30it/s]                                               {'train_runtime': 7.5713, 'train_samples_per_second': 56.133, 'train_steps_per_second': 3.962, 'train_loss': 0.635109297434489, 'epoch': 5.0}
100%|██████████| 30/30 [00:07<00:00,  5.30it/s]100%|██████████| 30/30 [00:07<00:00,  3.96it/s]
CLIENT:99
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:13,  3.34it/s]                                              {'loss': 0.6469, 'grad_norm': 0.8457005023956299, 'learning_rate': 0.001, 'epoch': 0.11}
  2%|▏         | 1/45 [00:00<00:13,  3.34it/s]  4%|▍         | 2/45 [00:00<00:10,  4.03it/s]                                              {'loss': 0.5681, 'grad_norm': 1.013800024986267, 'learning_rate': 0.0009777777777777777, 'epoch': 0.22}
  4%|▍         | 2/45 [00:00<00:10,  4.03it/s]  7%|▋         | 3/45 [00:00<00:10,  4.18it/s]                                              {'loss': 0.3852, 'grad_norm': 1.2381093502044678, 'learning_rate': 0.0009555555555555556, 'epoch': 0.33}
  7%|▋         | 3/45 [00:00<00:10,  4.18it/s]  9%|▉         | 4/45 [00:01<00:10,  3.75it/s]                                              {'loss': 0.6438, 'grad_norm': 1.3113117218017578, 'learning_rate': 0.0009333333333333333, 'epoch': 0.44}
  9%|▉         | 4/45 [00:01<00:10,  3.75it/s] 11%|█         | 5/45 [00:01<00:10,  3.80it/s]                                              {'loss': 0.3062, 'grad_norm': 0.927491307258606, 'learning_rate': 0.0009111111111111111, 'epoch': 0.56}
 11%|█         | 5/45 [00:01<00:10,  3.80it/s] 13%|█▎        | 6/45 [00:01<00:09,  3.94it/s]                                              {'loss': 0.6658, 'grad_norm': 3.4338738918304443, 'learning_rate': 0.0008888888888888888, 'epoch': 0.67}
 13%|█▎        | 6/45 [00:01<00:09,  3.94it/s] 16%|█▌        | 7/45 [00:01<00:08,  4.48it/s]                                              {'loss': 0.431, 'grad_norm': 1.9652771949768066, 'learning_rate': 0.0008666666666666667, 'epoch': 0.78}
 16%|█▌        | 7/45 [00:01<00:08,  4.48it/s] 18%|█▊        | 8/45 [00:01<00:06,  5.41it/s]                                              {'loss': 0.8167, 'grad_norm': 3.891589641571045, 'learning_rate': 0.0008444444444444444, 'epoch': 0.89}
 18%|█▊        | 8/45 [00:01<00:06,  5.41it/s] 20%|██        | 9/45 [00:01<00:06,  5.46it/s]                                              {'loss': 0.4653, 'grad_norm': 2.0683584213256836, 'learning_rate': 0.0008222222222222222, 'epoch': 1.0}
 20%|██        | 9/45 [00:01<00:06,  5.46it/s] 22%|██▏       | 10/45 [00:02<00:06,  5.17it/s]                                               {'loss': 0.4028, 'grad_norm': 1.105933427810669, 'learning_rate': 0.0008, 'epoch': 1.11}
 22%|██▏       | 10/45 [00:02<00:06,  5.17it/s] 24%|██▍       | 11/45 [00:02<00:06,  5.20it/s]                                               {'loss': 0.4149, 'grad_norm': 1.1853877305984497, 'learning_rate': 0.0007777777777777778, 'epoch': 1.22}
 24%|██▍       | 11/45 [00:02<00:06,  5.20it/s] 27%|██▋       | 12/45 [00:02<00:06,  4.76it/s]                                               {'loss': 0.4278, 'grad_norm': 0.9643444418907166, 'learning_rate': 0.0007555555555555555, 'epoch': 1.33}
 27%|██▋       | 12/45 [00:02<00:06,  4.76it/s] 29%|██▉       | 13/45 [00:02<00:07,  4.56it/s]                                               {'loss': 0.3551, 'grad_norm': 3.458010673522949, 'learning_rate': 0.0007333333333333333, 'epoch': 1.44}
 29%|██▉       | 13/45 [00:02<00:07,  4.56it/s] 31%|███       | 14/45 [00:03<00:07,  4.25it/s]                                               {'loss': 0.5257, 'grad_norm': 1.6144307851791382, 'learning_rate': 0.0007111111111111111, 'epoch': 1.56}
 31%|███       | 14/45 [00:03<00:07,  4.25it/s] 33%|███▎      | 15/45 [00:03<00:06,  4.32it/s]                                               {'loss': 0.5853, 'grad_norm': 1.023783564567566, 'learning_rate': 0.000688888888888889, 'epoch': 1.67}
 33%|███▎      | 15/45 [00:03<00:06,  4.32it/s] 36%|███▌      | 16/45 [00:03<00:06,  4.50it/s]                                               {'loss': 0.4685, 'grad_norm': 0.9263727068901062, 'learning_rate': 0.0006666666666666666, 'epoch': 1.78}
 36%|███▌      | 16/45 [00:03<00:06,  4.50it/s] 38%|███▊      | 17/45 [00:03<00:05,  4.72it/s]                                               {'loss': 0.5009, 'grad_norm': 1.3483421802520752, 'learning_rate': 0.0006444444444444444, 'epoch': 1.89}
 38%|███▊      | 17/45 [00:03<00:05,  4.72it/s] 40%|████      | 18/45 [00:03<00:05,  4.60it/s]                                               {'loss': 0.5896, 'grad_norm': 1.0590330362319946, 'learning_rate': 0.0006222222222222223, 'epoch': 2.0}
 40%|████      | 18/45 [00:04<00:05,  4.60it/s] 42%|████▏     | 19/45 [00:04<00:06,  4.15it/s]                                               {'loss': 0.5691, 'grad_norm': 1.1088594198226929, 'learning_rate': 0.0006, 'epoch': 2.11}
 42%|████▏     | 19/45 [00:04<00:06,  4.15it/s] 44%|████▍     | 20/45 [00:04<00:05,  4.34it/s]                                               {'loss': 0.4234, 'grad_norm': 0.8504427671432495, 'learning_rate': 0.0005777777777777778, 'epoch': 2.22}
 44%|████▍     | 20/45 [00:04<00:05,  4.34it/s] 47%|████▋     | 21/45 [00:04<00:05,  4.44it/s]                                               {'loss': 0.5122, 'grad_norm': 1.2048383951187134, 'learning_rate': 0.0005555555555555556, 'epoch': 2.33}
 47%|████▋     | 21/45 [00:04<00:05,  4.44it/s] 49%|████▉     | 22/45 [00:04<00:04,  4.70it/s]                                               {'loss': 0.3433, 'grad_norm': 3.2871298789978027, 'learning_rate': 0.0005333333333333334, 'epoch': 2.44}
 49%|████▉     | 22/45 [00:04<00:04,  4.70it/s] 51%|█████     | 23/45 [00:05<00:04,  5.02it/s]                                               {'loss': 0.3814, 'grad_norm': 1.4392544031143188, 'learning_rate': 0.0005111111111111111, 'epoch': 2.56}
 51%|█████     | 23/45 [00:05<00:04,  5.02it/s] 53%|█████▎    | 24/45 [00:05<00:04,  5.12it/s]                                               {'loss': 0.3914, 'grad_norm': 0.9310158491134644, 'learning_rate': 0.0004888888888888889, 'epoch': 2.67}
 53%|█████▎    | 24/45 [00:05<00:04,  5.12it/s] 56%|█████▌    | 25/45 [00:05<00:03,  5.22it/s]                                               {'loss': 0.273, 'grad_norm': 2.5181798934936523, 'learning_rate': 0.00046666666666666666, 'epoch': 2.78}
 56%|█████▌    | 25/45 [00:05<00:03,  5.22it/s] 58%|█████▊    | 26/45 [00:05<00:03,  5.39it/s]                                               {'loss': 0.5017, 'grad_norm': 1.9883365631103516, 'learning_rate': 0.0004444444444444444, 'epoch': 2.89}
 58%|█████▊    | 26/45 [00:05<00:03,  5.39it/s] 60%|██████    | 27/45 [00:05<00:03,  5.70it/s]                                               {'loss': 0.4206, 'grad_norm': 2.513434886932373, 'learning_rate': 0.0004222222222222222, 'epoch': 3.0}
 60%|██████    | 27/45 [00:05<00:03,  5.70it/s] 62%|██████▏   | 28/45 [00:05<00:02,  6.38it/s]                                               {'loss': 0.1283, 'grad_norm': 1.7008641958236694, 'learning_rate': 0.0004, 'epoch': 3.11}
 62%|██████▏   | 28/45 [00:05<00:02,  6.38it/s]                                               {'loss': 0.4828, 'grad_norm': 2.517284631729126, 'learning_rate': 0.00037777777777777777, 'epoch': 3.22}
 64%|██████▍   | 29/45 [00:05<00:02,  6.38it/s] 67%|██████▋   | 30/45 [00:06<00:01,  7.93it/s]                                               {'loss': 0.7002, 'grad_norm': 3.9735617637634277, 'learning_rate': 0.00035555555555555557, 'epoch': 3.33}
 67%|██████▋   | 30/45 [00:06<00:01,  7.93it/s] 69%|██████▉   | 31/45 [00:06<00:01,  7.93it/s]                                               {'loss': 0.2121, 'grad_norm': 1.0267285108566284, 'learning_rate': 0.0003333333333333333, 'epoch': 3.44}
 69%|██████▉   | 31/45 [00:06<00:01,  7.93it/s] 71%|███████   | 32/45 [00:06<00:01,  7.22it/s]                                               {'loss': 0.3513, 'grad_norm': 1.5087523460388184, 'learning_rate': 0.0003111111111111111, 'epoch': 3.56}
 71%|███████   | 32/45 [00:06<00:01,  7.22it/s] 73%|███████▎  | 33/45 [00:06<00:01,  6.40it/s]                                               {'loss': 0.3226, 'grad_norm': 1.9520983695983887, 'learning_rate': 0.0002888888888888889, 'epoch': 3.67}
 73%|███████▎  | 33/45 [00:06<00:01,  6.40it/s] 76%|███████▌  | 34/45 [00:06<00:01,  5.94it/s]                                               {'loss': 0.6175, 'grad_norm': 2.7631545066833496, 'learning_rate': 0.0002666666666666667, 'epoch': 3.78}
 76%|███████▌  | 34/45 [00:06<00:01,  5.94it/s] 78%|███████▊  | 35/45 [00:06<00:01,  5.46it/s]                                               {'loss': 0.5301, 'grad_norm': 2.4552907943725586, 'learning_rate': 0.00024444444444444443, 'epoch': 3.89}
 78%|███████▊  | 35/45 [00:06<00:01,  5.46it/s] 80%|████████  | 36/45 [00:07<00:01,  5.67it/s]                                               {'loss': 0.1362, 'grad_norm': 2.047471284866333, 'learning_rate': 0.0002222222222222222, 'epoch': 4.0}
 80%|████████  | 36/45 [00:07<00:01,  5.67it/s] 82%|████████▏ | 37/45 [00:07<00:01,  4.90it/s]                                               {'loss': 0.3371, 'grad_norm': 1.3240044116973877, 'learning_rate': 0.0002, 'epoch': 4.11}
 82%|████████▏ | 37/45 [00:07<00:01,  4.90it/s] 84%|████████▍ | 38/45 [00:07<00:01,  5.13it/s]                                               {'loss': 0.2414, 'grad_norm': 1.913265585899353, 'learning_rate': 0.00017777777777777779, 'epoch': 4.22}
 84%|████████▍ | 38/45 [00:07<00:01,  5.13it/s] 87%|████████▋ | 39/45 [00:07<00:01,  5.43it/s]                                               {'loss': 0.4602, 'grad_norm': 1.8590437173843384, 'learning_rate': 0.00015555555555555556, 'epoch': 4.33}
 87%|████████▋ | 39/45 [00:07<00:01,  5.43it/s] 89%|████████▉ | 40/45 [00:07<00:00,  5.44it/s]                                               {'loss': 0.2148, 'grad_norm': 1.754955768585205, 'learning_rate': 0.00013333333333333334, 'epoch': 4.44}
 89%|████████▉ | 40/45 [00:07<00:00,  5.44it/s] 91%|█████████ | 41/45 [00:08<00:00,  5.03it/s]                                               {'loss': 0.255, 'grad_norm': 4.078823566436768, 'learning_rate': 0.0001111111111111111, 'epoch': 4.56}
 91%|█████████ | 41/45 [00:08<00:00,  5.03it/s] 93%|█████████▎| 42/45 [00:08<00:00,  4.94it/s]                                               {'loss': 0.4363, 'grad_norm': 1.8750112056732178, 'learning_rate': 8.888888888888889e-05, 'epoch': 4.67}
 93%|█████████▎| 42/45 [00:08<00:00,  4.94it/s] 96%|█████████▌| 43/45 [00:08<00:00,  4.83it/s]                                               {'loss': 0.4011, 'grad_norm': 2.322046995162964, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.78}
 96%|█████████▌| 43/45 [00:08<00:00,  4.83it/s] 98%|█████████▊| 44/45 [00:08<00:00,  4.88it/s]                                               {'loss': 0.4449, 'grad_norm': 1.932307243347168, 'learning_rate': 4.4444444444444447e-05, 'epoch': 4.89}
 98%|█████████▊| 44/45 [00:08<00:00,  4.88it/s]                                               {'loss': 0.3543, 'grad_norm': 1.8716416358947754, 'learning_rate': 2.2222222222222223e-05, 'epoch': 5.0}
100%|██████████| 45/45 [00:08<00:00,  4.88it/s]                                               {'train_runtime': 8.9746, 'train_samples_per_second': 75.77, 'train_steps_per_second': 5.014, 'train_loss': 0.43648688859409757, 'epoch': 5.0}
100%|██████████| 45/45 [00:08<00:00,  4.88it/s]100%|██████████| 45/45 [00:08<00:00,  5.02it/s]
CLIENT:85
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.28it/s]                                              {'loss': 0.5191, 'grad_norm': 0.8207016587257385, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.28it/s]  7%|▋         | 2/30 [00:00<00:05,  4.73it/s]                                              {'loss': 0.6911, 'grad_norm': 0.6380177140235901, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:05,  4.73it/s] 10%|█         | 3/30 [00:00<00:05,  5.12it/s]                                              {'loss': 0.7269, 'grad_norm': 0.8916196227073669, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:05,  5.12it/s] 13%|█▎        | 4/30 [00:00<00:04,  5.21it/s]                                              {'loss': 0.6537, 'grad_norm': 0.6043010354042053, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  5.21it/s] 17%|█▋        | 5/30 [00:00<00:04,  5.15it/s]                                              {'loss': 0.627, 'grad_norm': 0.5297669172286987, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:04,  5.15it/s] 20%|██        | 6/30 [00:01<00:04,  5.57it/s]                                              {'loss': 0.67, 'grad_norm': 0.7429602146148682, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.57it/s] 23%|██▎       | 7/30 [00:01<00:04,  4.70it/s]                                              {'loss': 0.6169, 'grad_norm': 0.3616582155227661, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  4.70it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.84it/s]                                              {'loss': 0.6467, 'grad_norm': 0.5573788285255432, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.84it/s] 30%|███       | 9/30 [00:01<00:04,  5.08it/s]                                              {'loss': 0.6025, 'grad_norm': 0.5264062285423279, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  5.08it/s] 33%|███▎      | 10/30 [00:01<00:03,  5.44it/s]                                               {'loss': 0.6194, 'grad_norm': 0.47815802693367004, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:03,  5.44it/s] 37%|███▋      | 11/30 [00:02<00:03,  5.94it/s]                                               {'loss': 0.6364, 'grad_norm': 0.4780087172985077, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:03,  5.94it/s]                                               {'loss': 0.6356, 'grad_norm': 1.377246618270874, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:03,  5.94it/s] 43%|████▎     | 13/30 [00:02<00:02,  7.36it/s]                                               {'loss': 0.6119, 'grad_norm': 0.6529519557952881, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  7.36it/s] 47%|████▋     | 14/30 [00:02<00:02,  6.79it/s]                                               {'loss': 0.5789, 'grad_norm': 2.581922769546509, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  6.79it/s] 50%|█████     | 15/30 [00:02<00:02,  6.79it/s]                                               {'loss': 0.6442, 'grad_norm': 1.64015793800354, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  6.79it/s] 53%|█████▎    | 16/30 [00:02<00:02,  6.74it/s]                                               {'loss': 0.562, 'grad_norm': 0.748924732208252, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:02,  6.74it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.00it/s]                                               {'loss': 0.616, 'grad_norm': 0.7592248916625977, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.00it/s]                                               {'loss': 0.5364, 'grad_norm': 1.2670074701309204, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.00it/s] 63%|██████▎   | 19/30 [00:03<00:01,  8.52it/s]                                               {'loss': 0.4739, 'grad_norm': 1.0462459325790405, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  8.52it/s] 67%|██████▋   | 20/30 [00:03<00:01,  8.28it/s]                                               {'loss': 0.5941, 'grad_norm': 0.8383859395980835, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  8.28it/s] 70%|███████   | 21/30 [00:03<00:01,  8.05it/s]                                               {'loss': 0.6054, 'grad_norm': 1.1033858060836792, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  8.05it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.82it/s]                                               {'loss': 0.4928, 'grad_norm': 2.048358201980591, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.82it/s] 77%|███████▋  | 23/30 [00:03<00:00,  7.59it/s]                                               {'loss': 0.5933, 'grad_norm': 2.757446050643921, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.59it/s]                                               {'loss': 0.4643, 'grad_norm': 3.068899154663086, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.59it/s] 83%|████████▎ | 25/30 [00:03<00:00,  8.42it/s]                                               {'loss': 0.4928, 'grad_norm': 1.993280053138733, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  8.42it/s] 87%|████████▋ | 26/30 [00:03<00:00,  8.09it/s]                                               {'loss': 0.4451, 'grad_norm': 1.7479876279830933, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  8.09it/s] 90%|█████████ | 27/30 [00:04<00:00,  7.84it/s]                                               {'loss': 0.5032, 'grad_norm': 1.8594473600387573, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  7.84it/s] 93%|█████████▎| 28/30 [00:04<00:00,  7.89it/s]                                               {'loss': 0.4657, 'grad_norm': 2.530291795730591, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  7.89it/s] 97%|█████████▋| 29/30 [00:04<00:00,  7.82it/s]                                               {'loss': 0.4394, 'grad_norm': 2.037355661392212, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  7.82it/s]                                               {'loss': 0.4929, 'grad_norm': 2.1624696254730225, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.82it/s]                                               {'train_runtime': 4.4667, 'train_samples_per_second': 95.149, 'train_steps_per_second': 6.716, 'train_loss': 0.5752551277478536, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  7.82it/s]100%|██████████| 30/30 [00:04<00:00,  6.73it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  3%|▎         | 2/66 [00:00<00:04, 13.71it/s]  6%|▌         | 4/66 [00:00<00:06,  9.02it/s]  8%|▊         | 5/66 [00:00<00:06,  8.87it/s]  9%|▉         | 6/66 [00:00<00:07,  7.65it/s] 11%|█         | 7/66 [00:00<00:07,  7.46it/s] 12%|█▏        | 8/66 [00:00<00:07,  7.37it/s] 14%|█▎        | 9/66 [00:01<00:07,  7.48it/s] 15%|█▌        | 10/66 [00:01<00:07,  7.26it/s] 17%|█▋        | 11/66 [00:01<00:07,  7.44it/s] 18%|█▊        | 12/66 [00:01<00:06,  7.96it/s] 20%|█▉        | 13/66 [00:01<00:06,  7.97it/s] 21%|██        | 14/66 [00:01<00:06,  8.10it/s] 24%|██▍       | 16/66 [00:01<00:05,  8.63it/s] 26%|██▌       | 17/66 [00:02<00:05,  8.93it/s] 29%|██▉       | 19/66 [00:02<00:04,  9.97it/s] 32%|███▏      | 21/66 [00:02<00:04,  9.80it/s] 33%|███▎      | 22/66 [00:02<00:04,  9.24it/s] 35%|███▍      | 23/66 [00:02<00:04,  9.19it/s] 38%|███▊      | 25/66 [00:02<00:04,  9.87it/s] 41%|████      | 27/66 [00:03<00:03, 11.02it/s] 44%|████▍     | 29/66 [00:03<00:03, 11.08it/s] 47%|████▋     | 31/66 [00:03<00:03, 11.40it/s] 50%|█████     | 33/66 [00:03<00:02, 12.33it/s] 53%|█████▎    | 35/66 [00:03<00:02, 12.60it/s] 56%|█████▌    | 37/66 [00:03<00:02, 13.00it/s] 59%|█████▉    | 39/66 [00:03<00:01, 13.56it/s] 62%|██████▏   | 41/66 [00:04<00:01, 13.48it/s] 65%|██████▌   | 43/66 [00:04<00:01, 14.66it/s] 68%|██████▊   | 45/66 [00:04<00:01, 14.94it/s] 71%|███████   | 47/66 [00:04<00:01, 14.55it/s] 74%|███████▍  | 49/66 [00:04<00:01, 13.38it/s] 77%|███████▋  | 51/66 [00:04<00:01, 12.60it/s] 80%|████████  | 53/66 [00:04<00:00, 13.23it/s] 85%|████████▍ | 56/66 [00:05<00:00, 14.77it/s] 89%|████████▉ | 59/66 [00:05<00:00, 16.49it/s] 92%|█████████▏| 61/66 [00:05<00:00, 16.29it/s] 95%|█████████▌| 63/66 [00:05<00:00, 16.82it/s] 98%|█████████▊| 65/66 [00:05<00:00, 17.34it/s]100%|██████████| 66/66 [00:05<00:00, 11.78it/s]
{'eval_loss': 0.6365790367126465, 'eval_model_preparation_time': 0.0506, 'eval_acc': 0.6816874400767018, 'eval_runtime': 5.7596, 'eval_samples_per_second': 181.089, 'eval_steps_per_second': 11.459}
ROUND:9
CLIENT:8
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:06,  4.38it/s]                                              {'loss': 0.8473, 'grad_norm': 2.1839544773101807, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:06,  4.38it/s]  7%|▋         | 2/30 [00:00<00:06,  4.24it/s]                                              {'loss': 0.4396, 'grad_norm': 1.5811564922332764, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:06,  4.24it/s] 10%|█         | 3/30 [00:00<00:06,  4.30it/s]                                              {'loss': 0.2191, 'grad_norm': 1.6296082735061646, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:06,  4.30it/s] 13%|█▎        | 4/30 [00:00<00:06,  4.32it/s]                                              {'loss': 0.2797, 'grad_norm': 0.6945785284042358, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:06,  4.32it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.47it/s]                                              {'loss': 0.0321, 'grad_norm': 0.4606752395629883, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.47it/s] 20%|██        | 6/30 [00:01<00:06,  3.55it/s]                                              {'loss': 0.0124, 'grad_norm': 0.1307697594165802, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:06,  3.55it/s] 23%|██▎       | 7/30 [00:01<00:06,  3.56it/s]                                              {'loss': 0.0123, 'grad_norm': 0.26258939504623413, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:06,  3.56it/s] 27%|██▋       | 8/30 [00:02<00:06,  3.53it/s]                                              {'loss': 0.0043, 'grad_norm': 0.054534781724214554, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:06,  3.53it/s] 30%|███       | 9/30 [00:02<00:05,  3.51it/s]                                              {'loss': 0.0048, 'grad_norm': 0.07730185985565186, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:05,  3.51it/s] 33%|███▎      | 10/30 [00:02<00:05,  3.64it/s]                                               {'loss': 0.3093, 'grad_norm': 4.402478218078613, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:05,  3.64it/s] 37%|███▋      | 11/30 [00:02<00:04,  3.91it/s]                                               {'loss': 0.0029, 'grad_norm': 0.0530797503888607, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:04,  3.91it/s]                                               {'loss': 0.0058, 'grad_norm': 0.09897610545158386, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:04,  3.91it/s] 43%|████▎     | 13/30 [00:03<00:03,  4.83it/s]                                               {'loss': 0.0029, 'grad_norm': 0.04695908725261688, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:03<00:03,  4.83it/s] 47%|████▋     | 14/30 [00:03<00:03,  4.70it/s]                                               {'loss': 0.004, 'grad_norm': 0.0927947536110878, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:03<00:03,  4.70it/s] 50%|█████     | 15/30 [00:03<00:03,  4.60it/s]                                               {'loss': 0.3772, 'grad_norm': 0.7665241360664368, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:03<00:03,  4.60it/s] 53%|█████▎    | 16/30 [00:03<00:03,  4.60it/s]                                               {'loss': 0.004, 'grad_norm': 0.0776224434375763, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:03,  4.60it/s] 57%|█████▋    | 17/30 [00:04<00:02,  4.39it/s]                                               {'loss': 0.0101, 'grad_norm': 0.3357681930065155, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:04<00:02,  4.39it/s]                                               {'loss': 0.0048, 'grad_norm': 0.08959822356700897, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:04<00:02,  4.39it/s] 63%|██████▎   | 19/30 [00:04<00:02,  5.01it/s]                                               {'loss': 0.0035, 'grad_norm': 0.04935389384627342, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:04<00:02,  5.01it/s] 67%|██████▋   | 20/30 [00:04<00:02,  4.94it/s]                                               {'loss': 0.0038, 'grad_norm': 0.06814095377922058, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:04<00:02,  4.94it/s] 70%|███████   | 21/30 [00:04<00:01,  5.08it/s]                                               {'loss': 0.0036, 'grad_norm': 0.04953111335635185, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:04<00:01,  5.08it/s] 73%|███████▎  | 22/30 [00:04<00:01,  5.57it/s]                                               {'loss': 0.004, 'grad_norm': 0.059768449515104294, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:04<00:01,  5.57it/s] 77%|███████▋  | 23/30 [00:05<00:01,  5.68it/s]                                               {'loss': 0.4364, 'grad_norm': 1.7294692993164062, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:05<00:01,  5.68it/s]                                               {'loss': 0.0032, 'grad_norm': 0.05716165900230408, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:05<00:01,  5.68it/s] 83%|████████▎ | 25/30 [00:05<00:00,  6.43it/s]                                               {'loss': 0.0065, 'grad_norm': 0.13687309622764587, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:05<00:00,  6.43it/s] 87%|████████▋ | 26/30 [00:05<00:00,  6.28it/s]                                               {'loss': 0.0046, 'grad_norm': 0.07062943279743195, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:05<00:00,  6.28it/s] 90%|█████████ | 27/30 [00:05<00:00,  6.20it/s]                                               {'loss': 0.318, 'grad_norm': 0.8416839241981506, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:05<00:00,  6.20it/s] 93%|█████████▎| 28/30 [00:05<00:00,  6.35it/s]                                               {'loss': 0.0045, 'grad_norm': 0.05296194925904274, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:05<00:00,  6.35it/s] 97%|█████████▋| 29/30 [00:05<00:00,  6.57it/s]                                               {'loss': 0.0046, 'grad_norm': 0.081695057451725, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:05<00:00,  6.57it/s]                                               {'loss': 0.0054, 'grad_norm': 0.10043582320213318, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  6.57it/s]                                               {'train_runtime': 6.3105, 'train_samples_per_second': 67.348, 'train_steps_per_second': 4.754, 'train_loss': 0.11235521979009112, 'epoch': 5.0}
100%|██████████| 30/30 [00:06<00:00,  6.57it/s]100%|██████████| 30/30 [00:06<00:00,  4.76it/s]
CLIENT:93
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:26,  1.08it/s]                                              {'loss': 0.5981, 'grad_norm': 0.7988227009773254, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:26,  1.08it/s]  7%|▋         | 2/30 [00:01<00:13,  2.13it/s]                                              {'loss': 0.7032, 'grad_norm': 0.7294105291366577, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:01<00:13,  2.13it/s] 10%|█         | 3/30 [00:01<00:08,  3.28it/s]                                              {'loss': 0.7235, 'grad_norm': 0.7624643445014954, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:01<00:08,  3.28it/s] 13%|█▎        | 4/30 [00:01<00:06,  4.05it/s]                                              {'loss': 0.6551, 'grad_norm': 0.7158843278884888, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:01<00:06,  4.05it/s] 17%|█▋        | 5/30 [00:01<00:05,  4.66it/s]                                              {'loss': 0.6248, 'grad_norm': 0.4594082534313202, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:01<00:05,  4.66it/s] 20%|██        | 6/30 [00:01<00:04,  5.21it/s]                                              {'loss': 0.6157, 'grad_norm': 0.8072322607040405, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:04,  5.21it/s] 23%|██▎       | 7/30 [00:01<00:04,  5.12it/s]                                              {'loss': 0.6566, 'grad_norm': 0.4542616605758667, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:04,  5.12it/s] 27%|██▋       | 8/30 [00:02<00:03,  5.52it/s]                                              {'loss': 0.6371, 'grad_norm': 0.5208149552345276, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:02<00:03,  5.52it/s] 30%|███       | 9/30 [00:02<00:03,  5.68it/s]                                              {'loss': 0.6444, 'grad_norm': 0.47142302989959717, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:02<00:03,  5.68it/s] 33%|███▎      | 10/30 [00:02<00:03,  6.15it/s]                                               {'loss': 0.6897, 'grad_norm': 0.6772860884666443, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:02<00:03,  6.15it/s] 37%|███▋      | 11/30 [00:02<00:02,  6.50it/s]                                               {'loss': 0.673, 'grad_norm': 1.0091732740402222, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:02<00:02,  6.50it/s] 40%|████      | 12/30 [00:02<00:02,  7.13it/s]                                               {'loss': 0.6094, 'grad_norm': 1.0376296043395996, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:02<00:02,  7.13it/s] 43%|████▎     | 13/30 [00:02<00:02,  6.97it/s]                                               {'loss': 0.6644, 'grad_norm': 0.6788879036903381, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:02,  6.97it/s] 47%|████▋     | 14/30 [00:02<00:02,  7.10it/s]                                               {'loss': 0.6865, 'grad_norm': 0.3988487422466278, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:02,  7.10it/s] 50%|█████     | 15/30 [00:02<00:02,  7.30it/s]                                               {'loss': 0.5623, 'grad_norm': 1.1996195316314697, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:02,  7.30it/s] 53%|█████▎    | 16/30 [00:03<00:01,  7.18it/s]                                               {'loss': 0.6099, 'grad_norm': 0.5184435248374939, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:03<00:01,  7.18it/s] 57%|█████▋    | 17/30 [00:03<00:01,  7.54it/s]                                               {'loss': 0.6575, 'grad_norm': 0.7860995531082153, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:03<00:01,  7.54it/s] 60%|██████    | 18/30 [00:03<00:01,  7.36it/s]                                               {'loss': 0.5839, 'grad_norm': 0.8582444787025452, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:03<00:01,  7.36it/s] 63%|██████▎   | 19/30 [00:03<00:01,  7.18it/s]                                               {'loss': 0.5825, 'grad_norm': 0.8527827262878418, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:03<00:01,  7.18it/s] 67%|██████▋   | 20/30 [00:03<00:01,  7.51it/s]                                               {'loss': 0.5839, 'grad_norm': 0.5709700584411621, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:03<00:01,  7.51it/s] 70%|███████   | 21/30 [00:03<00:01,  7.72it/s]                                               {'loss': 0.5898, 'grad_norm': 0.6651932597160339, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  7.72it/s] 73%|███████▎  | 22/30 [00:03<00:01,  7.03it/s]                                               {'loss': 0.5868, 'grad_norm': 0.4535714387893677, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  7.03it/s] 77%|███████▋  | 23/30 [00:04<00:00,  7.22it/s]                                               {'loss': 0.6248, 'grad_norm': 1.2736310958862305, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:04<00:00,  7.22it/s]                                               {'loss': 0.4681, 'grad_norm': 1.1015702486038208, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:04<00:00,  7.22it/s] 83%|████████▎ | 25/30 [00:04<00:00,  7.14it/s]                                               {'loss': 0.6231, 'grad_norm': 1.0863161087036133, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:04<00:00,  7.14it/s] 87%|████████▋ | 26/30 [00:04<00:00,  6.83it/s]                                               {'loss': 0.5531, 'grad_norm': 0.7494221925735474, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:04<00:00,  6.83it/s] 90%|█████████ | 27/30 [00:04<00:00,  6.53it/s]                                               {'loss': 0.5424, 'grad_norm': 0.8633792996406555, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  6.53it/s] 93%|█████████▎| 28/30 [00:04<00:00,  6.26it/s]                                               {'loss': 0.5959, 'grad_norm': 0.886559009552002, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  6.26it/s] 97%|█████████▋| 29/30 [00:04<00:00,  6.63it/s]                                               {'loss': 0.4495, 'grad_norm': 0.803185760974884, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  6.63it/s]                                               {'loss': 0.5937, 'grad_norm': 2.0422019958496094, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.63it/s]                                               {'train_runtime': 5.1674, 'train_samples_per_second': 82.247, 'train_steps_per_second': 5.806, 'train_loss': 0.6129606525103252, 'epoch': 5.0}
100%|██████████| 30/30 [00:05<00:00,  6.63it/s]100%|██████████| 30/30 [00:05<00:00,  5.81it/s]
CLIENT:4
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  9.18it/s]                                              {'loss': 0.6857, 'grad_norm': 2.0915944576263428, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  9.18it/s]                                              {'loss': 0.4791, 'grad_norm': 1.5421258211135864, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  9.18it/s] 10%|█         | 3/30 [00:00<00:02, 11.72it/s]                                              {'loss': 0.3412, 'grad_norm': 0.8977545499801636, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.72it/s]                                              {'loss': 0.0818, 'grad_norm': 0.8929908275604248, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.72it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.39it/s]                                              {'loss': 0.4211, 'grad_norm': 1.6719518899917603, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.39it/s]                                              {'loss': 0.0071, 'grad_norm': 0.117586649954319, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.39it/s] 23%|██▎       | 7/30 [00:00<00:01, 13.16it/s]                                              {'loss': 0.3382, 'grad_norm': 0.63968425989151, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 13.16it/s]                                              {'loss': 1.3198, 'grad_norm': 4.433598041534424, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.16it/s] 30%|███       | 9/30 [00:00<00:01, 13.13it/s]                                              {'loss': 0.0166, 'grad_norm': 0.21633650362491608, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.13it/s]                                              {'loss': 0.3281, 'grad_norm': 0.7309718132019043, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.13it/s] 37%|███▋      | 11/30 [00:00<00:01, 12.77it/s]                                               {'loss': 0.0123, 'grad_norm': 0.16176757216453552, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 12.77it/s]                                               {'loss': 0.0108, 'grad_norm': 1.1162232160568237, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 12.77it/s] 43%|████▎     | 13/30 [00:00<00:01, 13.94it/s]                                               {'loss': 0.5923, 'grad_norm': 1.21415376663208, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 13.94it/s]                                               {'loss': 0.0199, 'grad_norm': 0.2840370237827301, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.94it/s] 50%|█████     | 15/30 [00:01<00:01, 13.11it/s]                                               {'loss': 0.5897, 'grad_norm': 1.8813602924346924, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.11it/s]                                               {'loss': 0.2498, 'grad_norm': 2.517683982849121, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.11it/s] 57%|█████▋    | 17/30 [00:01<00:01, 12.48it/s]                                               {'loss': 0.0447, 'grad_norm': 0.6134483218193054, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 12.48it/s]                                               {'loss': 0.7034, 'grad_norm': 3.094200849533081, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 12.48it/s] 63%|██████▎   | 19/30 [00:01<00:00, 12.99it/s]                                               {'loss': 0.0356, 'grad_norm': 0.5658823847770691, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 12.99it/s]                                               {'loss': 0.7808, 'grad_norm': 2.418027639389038, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 12.99it/s] 70%|███████   | 21/30 [00:01<00:00, 12.34it/s]                                               {'loss': 0.2438, 'grad_norm': 1.0297828912734985, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 12.34it/s]                                               {'loss': 0.2579, 'grad_norm': 0.9151545166969299, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 12.34it/s] 77%|███████▋  | 23/30 [00:01<00:00, 12.24it/s]                                               {'loss': 0.2469, 'grad_norm': 1.212063193321228, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 12.24it/s]                                               {'loss': 0.0525, 'grad_norm': 1.2139374017715454, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 12.24it/s] 83%|████████▎ | 25/30 [00:01<00:00, 13.44it/s]                                               {'loss': 0.2258, 'grad_norm': 1.271047830581665, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 13.44it/s]                                               {'loss': 0.0596, 'grad_norm': 0.5151515603065491, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.44it/s] 90%|█████████ | 27/30 [00:02<00:00, 13.37it/s]                                               {'loss': 0.2044, 'grad_norm': 0.9901092052459717, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.37it/s]                                               {'loss': 0.2902, 'grad_norm': 1.7728732824325562, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.37it/s] 97%|█████████▋| 29/30 [00:02<00:00, 13.25it/s]                                               {'loss': 0.2234, 'grad_norm': 0.8810327649116516, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.25it/s]                                               {'loss': 0.594, 'grad_norm': 1.7521815299987793, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.25it/s]                                               {'train_runtime': 2.3628, 'train_samples_per_second': 179.871, 'train_steps_per_second': 12.697, 'train_loss': 0.3152167186141014, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.25it/s]100%|██████████| 30/30 [00:02<00:00, 12.70it/s]
CLIENT:5
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  7.03it/s]                                              {'loss': 0.6682, 'grad_norm': 0.7671301960945129, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  7.03it/s]  7%|▋         | 2/30 [00:00<00:04,  6.97it/s]                                              {'loss': 0.708, 'grad_norm': 0.5622619986534119, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.97it/s] 10%|█         | 3/30 [00:00<00:03,  7.06it/s]                                              {'loss': 0.7135, 'grad_norm': 0.439504474401474, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  7.06it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.01it/s]                                              {'loss': 0.6518, 'grad_norm': 0.5844203233718872, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.01it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.01it/s]                                              {'loss': 0.6544, 'grad_norm': 0.5586001873016357, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.01it/s]                                              {'loss': 0.6805, 'grad_norm': 1.6196682453155518, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.01it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.16it/s]                                              {'loss': 0.6595, 'grad_norm': 0.5826426148414612, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  8.16it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.62it/s]                                              {'loss': 0.6242, 'grad_norm': 0.4681539535522461, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.62it/s] 30%|███       | 9/30 [00:01<00:02,  7.27it/s]                                              {'loss': 0.6153, 'grad_norm': 0.5258998274803162, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.27it/s] 33%|███▎      | 10/30 [00:01<00:02,  7.03it/s]                                               {'loss': 0.6847, 'grad_norm': 0.33305519819259644, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  7.03it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.05it/s]                                               {'loss': 0.6652, 'grad_norm': 0.4128182530403137, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.05it/s]                                               {'loss': 0.8258, 'grad_norm': 3.345170021057129, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.05it/s] 43%|████▎     | 13/30 [00:01<00:02,  7.84it/s]                                               {'loss': 0.6537, 'grad_norm': 0.8981963992118835, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  7.84it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.58it/s]                                               {'loss': 0.7001, 'grad_norm': 3.7683095932006836, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.58it/s] 50%|█████     | 15/30 [00:01<00:01,  7.96it/s]                                               {'loss': 0.5022, 'grad_norm': 0.7166800498962402, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.96it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.31it/s]                                               {'loss': 0.6379, 'grad_norm': 0.6942279934883118, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.31it/s] 57%|█████▋    | 17/30 [00:02<00:02,  6.33it/s]                                               {'loss': 0.6889, 'grad_norm': 1.014493465423584, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:02,  6.33it/s]                                               {'loss': 0.6219, 'grad_norm': 0.6457275748252869, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  6.33it/s] 63%|██████▎   | 19/30 [00:02<00:01,  6.58it/s]                                               {'loss': 0.7597, 'grad_norm': 1.374253749847412, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  6.58it/s] 67%|██████▋   | 20/30 [00:02<00:01,  6.10it/s]                                               {'loss': 0.6072, 'grad_norm': 0.3415035307407379, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  6.10it/s] 70%|███████   | 21/30 [00:03<00:01,  5.80it/s]                                               {'loss': 0.6464, 'grad_norm': 0.4235062897205353, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:03<00:01,  5.80it/s] 73%|███████▎  | 22/30 [00:03<00:01,  5.46it/s]                                               {'loss': 0.6203, 'grad_norm': 0.4919801354408264, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:03<00:01,  5.46it/s] 77%|███████▋  | 23/30 [00:03<00:01,  5.96it/s]                                               {'loss': 0.5761, 'grad_norm': 0.5118640065193176, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:01,  5.96it/s]                                               {'loss': 0.618, 'grad_norm': 0.6604870557785034, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:01,  5.96it/s] 83%|████████▎ | 25/30 [00:03<00:00,  6.00it/s]                                               {'loss': 0.6781, 'grad_norm': 0.8668572902679443, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  6.00it/s] 87%|████████▋ | 26/30 [00:03<00:00,  5.52it/s]                                               {'loss': 0.579, 'grad_norm': 0.5012356042861938, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  5.52it/s] 90%|█████████ | 27/30 [00:04<00:00,  5.07it/s]                                               {'loss': 0.5805, 'grad_norm': 0.5523825883865356, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:04<00:00,  5.07it/s] 93%|█████████▎| 28/30 [00:04<00:00,  4.92it/s]                                               {'loss': 0.6008, 'grad_norm': 0.5164860486984253, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:04<00:00,  4.92it/s] 97%|█████████▋| 29/30 [00:04<00:00,  4.78it/s]                                               {'loss': 0.5611, 'grad_norm': 0.8970231413841248, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:04<00:00,  4.78it/s]                                               {'loss': 0.5781, 'grad_norm': 0.7937338352203369, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  4.78it/s]                                               {'train_runtime': 4.8272, 'train_samples_per_second': 88.043, 'train_steps_per_second': 6.215, 'train_loss': 0.6453649719556173, 'epoch': 5.0}
100%|██████████| 30/30 [00:04<00:00,  4.78it/s]100%|██████████| 30/30 [00:04<00:00,  6.22it/s]
CLIENT:52
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  7.39it/s]                                              {'loss': 0.6024, 'grad_norm': 0.6855142712593079, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  7.39it/s]                                              {'loss': 0.678, 'grad_norm': 0.6117490530014038, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.39it/s] 10%|█         | 3/30 [00:00<00:02, 10.25it/s]                                              {'loss': 0.7157, 'grad_norm': 0.423320472240448, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.25it/s]                                              {'loss': 0.6313, 'grad_norm': 0.7137925624847412, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.25it/s] 17%|█▋        | 5/30 [00:00<00:02, 11.37it/s]                                              {'loss': 0.6349, 'grad_norm': 0.49221566319465637, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 11.37it/s]                                              {'loss': 0.7376, 'grad_norm': 1.6966674327850342, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 11.37it/s] 23%|██▎       | 7/30 [00:00<00:01, 11.70it/s]                                              {'loss': 0.6861, 'grad_norm': 0.40530675649642944, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 11.70it/s]                                              {'loss': 0.6324, 'grad_norm': 0.5701519846916199, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 11.70it/s] 30%|███       | 9/30 [00:00<00:01, 11.61it/s]                                              {'loss': 0.6295, 'grad_norm': 0.45938074588775635, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 11.61it/s]                                              {'loss': 0.6837, 'grad_norm': 0.5644283294677734, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 11.61it/s] 37%|███▋      | 11/30 [00:00<00:01, 11.80it/s]                                               {'loss': 0.6625, 'grad_norm': 0.3214167058467865, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 11.80it/s]                                               {'loss': 0.7021, 'grad_norm': 2.088773488998413, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 11.80it/s] 43%|████▎     | 13/30 [00:01<00:01, 13.40it/s]                                               {'loss': 0.6352, 'grad_norm': 0.6979838609695435, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 13.40it/s]                                               {'loss': 0.6607, 'grad_norm': 0.4645397663116455, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 13.40it/s] 50%|█████     | 15/30 [00:01<00:01, 13.32it/s]                                               {'loss': 0.5262, 'grad_norm': 0.9745901823043823, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.32it/s]                                               {'loss': 0.6158, 'grad_norm': 0.6066295504570007, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.32it/s] 57%|█████▋    | 17/30 [00:01<00:00, 13.21it/s]                                               {'loss': 0.6752, 'grad_norm': 0.6002062559127808, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.21it/s]                                               {'loss': 0.6064, 'grad_norm': 0.6442901492118835, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.21it/s] 63%|██████▎   | 19/30 [00:01<00:00, 13.66it/s]                                               {'loss': 0.6949, 'grad_norm': 1.8825372457504272, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 13.66it/s]                                               {'loss': 0.6037, 'grad_norm': 0.4420344829559326, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 13.66it/s] 70%|███████   | 21/30 [00:01<00:00, 13.29it/s]                                               {'loss': 0.6199, 'grad_norm': 0.4302736818790436, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 13.29it/s]                                               {'loss': 0.5914, 'grad_norm': 0.4061301052570343, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 13.29it/s] 77%|███████▋  | 23/30 [00:01<00:00, 12.80it/s]                                               {'loss': 0.5589, 'grad_norm': 0.6641152501106262, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 12.80it/s]                                               {'loss': 0.544, 'grad_norm': 0.8133305311203003, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 12.80it/s] 83%|████████▎ | 25/30 [00:01<00:00, 14.02it/s]                                               {'loss': 0.5973, 'grad_norm': 0.7704229354858398, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 14.02it/s]                                               {'loss': 0.5803, 'grad_norm': 0.610444188117981, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 14.02it/s] 90%|█████████ | 27/30 [00:02<00:00, 13.32it/s]                                               {'loss': 0.5811, 'grad_norm': 0.6806496977806091, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.32it/s]                                               {'loss': 0.5804, 'grad_norm': 0.9818109273910522, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.32it/s] 97%|█████████▋| 29/30 [00:02<00:00, 13.10it/s]                                               {'loss': 0.4951, 'grad_norm': 0.819583535194397, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.10it/s]                                               {'loss': 0.599, 'grad_norm': 1.0163835287094116, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.10it/s]                                               {'train_runtime': 2.3935, 'train_samples_per_second': 177.565, 'train_steps_per_second': 12.534, 'train_loss': 0.6253904650608698, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.10it/s]100%|██████████| 30/30 [00:02<00:00, 12.54it/s]
CLIENT:41
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  7.05it/s]                                              {'loss': 0.6588, 'grad_norm': 0.8449341058731079, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  7.05it/s]  7%|▋         | 2/30 [00:00<00:03,  7.17it/s]                                              {'loss': 0.5229, 'grad_norm': 0.960914134979248, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:03,  7.17it/s] 10%|█         | 3/30 [00:00<00:03,  8.13it/s]                                              {'loss': 0.515, 'grad_norm': 0.860992968082428, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  8.13it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.58it/s]                                              {'loss': 0.7633, 'grad_norm': 23.04839515686035, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:03,  7.58it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.32it/s]                                              {'loss': 0.9019, 'grad_norm': 4.4275712966918945, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.32it/s] 20%|██        | 6/30 [00:01<00:07,  3.35it/s]                                              {'loss': 0.9328, 'grad_norm': 3.792701244354248, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:01<00:07,  3.35it/s] 23%|██▎       | 7/30 [00:01<00:05,  3.96it/s]                                              {'loss': 0.5659, 'grad_norm': 1.1011155843734741, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:01<00:05,  3.96it/s] 27%|██▋       | 8/30 [00:01<00:04,  4.41it/s]                                              {'loss': 0.9558, 'grad_norm': 3.270908832550049, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:04,  4.41it/s] 30%|███       | 9/30 [00:01<00:04,  4.93it/s]                                              {'loss': 0.5103, 'grad_norm': 0.7729361057281494, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:04,  4.93it/s]                                              {'loss': 0.4822, 'grad_norm': 1.174411416053772, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:04,  4.93it/s] 37%|███▋      | 11/30 [00:01<00:02,  7.09it/s]                                               {'loss': 0.5414, 'grad_norm': 1.2675021886825562, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  7.09it/s]                                               {'loss': 0.3692, 'grad_norm': 1.4349445104599, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  7.09it/s] 43%|████▎     | 13/30 [00:02<00:01,  9.07it/s]                                               {'loss': 0.4936, 'grad_norm': 1.4410191774368286, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:02<00:01,  9.07it/s]                                               {'loss': 0.5525, 'grad_norm': 0.7435518503189087, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:02<00:01,  9.07it/s] 50%|█████     | 15/30 [00:02<00:01, 10.16it/s]                                               {'loss': 1.0, 'grad_norm': 4.3758625984191895, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01, 10.16it/s]                                               {'loss': 0.5188, 'grad_norm': 5.661783695220947, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01, 10.16it/s] 57%|█████▋    | 17/30 [00:02<00:01, 10.90it/s]                                               {'loss': 0.4857, 'grad_norm': 1.1768252849578857, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01, 10.90it/s]                                               {'loss': 0.4514, 'grad_norm': 2.253432273864746, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01, 10.90it/s] 63%|██████▎   | 19/30 [00:02<00:00, 12.64it/s]                                               {'loss': 0.496, 'grad_norm': 2.0181732177734375, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:00, 12.64it/s]                                               {'loss': 0.5464, 'grad_norm': 0.974231481552124, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:00, 12.64it/s] 70%|███████   | 21/30 [00:02<00:00, 12.58it/s]                                               {'loss': 0.5494, 'grad_norm': 1.2450863122940063, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:00, 12.58it/s]                                               {'loss': 0.6275, 'grad_norm': 1.7259334325790405, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00, 12.58it/s] 77%|███████▋  | 23/30 [00:02<00:00, 12.55it/s]                                               {'loss': 0.5888, 'grad_norm': 1.5158843994140625, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00, 12.55it/s]                                               {'loss': 0.3512, 'grad_norm': 3.3453855514526367, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00, 12.55it/s] 83%|████████▎ | 25/30 [00:02<00:00, 13.94it/s]                                               {'loss': 0.3189, 'grad_norm': 2.0945370197296143, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 13.94it/s]                                               {'loss': 0.5014, 'grad_norm': 1.0070463418960571, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.94it/s] 90%|█████████ | 27/30 [00:03<00:00, 13.48it/s]                                               {'loss': 0.5858, 'grad_norm': 1.804883599281311, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00, 13.48it/s]                                               {'loss': 0.6481, 'grad_norm': 1.9968301057815552, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00, 13.48it/s] 97%|█████████▋| 29/30 [00:03<00:00, 13.29it/s]                                               {'loss': 0.4444, 'grad_norm': 1.9798208475112915, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00, 13.29it/s]                                               {'loss': 0.6562, 'grad_norm': 2.6906516551971436, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.29it/s]                                               {'train_runtime': 3.3186, 'train_samples_per_second': 128.064, 'train_steps_per_second': 9.04, 'train_loss': 0.5845162590344747, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00, 13.29it/s]100%|██████████| 30/30 [00:03<00:00,  9.04it/s]
CLIENT:0
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:05,  5.60it/s]                                              {'loss': 0.7718, 'grad_norm': 2.4734256267547607, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:05,  5.60it/s]  7%|▋         | 2/30 [00:00<00:04,  5.92it/s]                                              {'loss': 0.4568, 'grad_norm': 1.5986515283584595, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  5.92it/s] 10%|█         | 3/30 [00:00<00:04,  6.17it/s]                                              {'loss': 0.1807, 'grad_norm': 1.595715880393982, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:04,  6.17it/s]                                              {'loss': 0.3205, 'grad_norm': 0.8866592049598694, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.17it/s] 17%|█▋        | 5/30 [00:00<00:02,  8.72it/s]                                              {'loss': 0.2498, 'grad_norm': 1.5406478643417358, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02,  8.72it/s]                                              {'loss': 0.0301, 'grad_norm': 0.6644089818000793, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02,  8.72it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.45it/s]                                              {'loss': 0.1698, 'grad_norm': 1.2749838829040527, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  9.45it/s] 27%|██▋       | 8/30 [00:00<00:02,  8.74it/s]                                              {'loss': 0.0626, 'grad_norm': 2.2635650634765625, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:02,  8.74it/s] 30%|███       | 9/30 [00:01<00:02,  8.66it/s]                                              {'loss': 0.3214, 'grad_norm': 1.3611351251602173, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  8.66it/s] 33%|███▎      | 10/30 [00:01<00:02,  8.68it/s]                                               {'loss': 0.0103, 'grad_norm': 0.3648149371147156, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  8.68it/s] 37%|███▋      | 11/30 [00:01<00:02,  8.17it/s]                                               {'loss': 0.0093, 'grad_norm': 0.26384237408638, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  8.17it/s]                                               {'loss': 0.0101, 'grad_norm': 0.17535042762756348, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  8.17it/s] 43%|████▎     | 13/30 [00:01<00:01,  8.98it/s]                                               {'loss': 0.0066, 'grad_norm': 0.1156177744269371, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01,  8.98it/s] 47%|████▋     | 14/30 [00:01<00:01,  8.45it/s]                                               {'loss': 0.0052, 'grad_norm': 0.08103308081626892, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01,  8.45it/s] 50%|█████     | 15/30 [00:01<00:01,  7.90it/s]                                               {'loss': 0.3839, 'grad_norm': 1.5598167181015015, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01,  7.90it/s] 53%|█████▎    | 16/30 [00:01<00:01,  7.73it/s]                                               {'loss': 0.0046, 'grad_norm': 0.05979674682021141, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01,  7.73it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.35it/s]                                               {'loss': 0.3272, 'grad_norm': 0.8670811653137207, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.35it/s]                                               {'loss': 0.0055, 'grad_norm': 0.0972721055150032, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.35it/s] 63%|██████▎   | 19/30 [00:02<00:01,  8.95it/s]                                               {'loss': 0.0059, 'grad_norm': 0.0863063633441925, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  8.95it/s]                                               {'loss': 0.3093, 'grad_norm': 1.5506227016448975, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  8.95it/s] 70%|███████   | 21/30 [00:02<00:01,  8.96it/s]                                               {'loss': 0.0068, 'grad_norm': 0.08305831998586655, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  8.96it/s] 73%|███████▎  | 22/30 [00:02<00:00,  8.62it/s]                                               {'loss': 0.0076, 'grad_norm': 0.08419602364301682, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:00,  8.62it/s] 77%|███████▋  | 23/30 [00:02<00:00,  8.75it/s]                                               {'loss': 0.2898, 'grad_norm': 0.8545989990234375, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:02<00:00,  8.75it/s]                                               {'loss': 0.0071, 'grad_norm': 0.11991085857152939, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:02<00:00,  8.75it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.18it/s]                                               {'loss': 0.6001, 'grad_norm': 2.5777716636657715, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00,  9.18it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.81it/s]                                               {'loss': 0.0091, 'grad_norm': 0.1176084354519844, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.81it/s] 90%|█████████ | 27/30 [00:03<00:00,  8.07it/s]                                               {'loss': 0.0084, 'grad_norm': 0.10896115005016327, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  8.07it/s] 93%|█████████▎| 28/30 [00:03<00:00,  7.30it/s]                                               {'loss': 0.0099, 'grad_norm': 0.12049805372953415, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  7.30it/s] 97%|█████████▋| 29/30 [00:03<00:00,  6.97it/s]                                               {'loss': 0.0096, 'grad_norm': 0.1200704276561737, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  6.97it/s]                                               {'loss': 0.0096, 'grad_norm': 0.1496260017156601, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  6.97it/s]                                               {'train_runtime': 3.8632, 'train_samples_per_second': 110.011, 'train_steps_per_second': 7.766, 'train_loss': 0.153310925901557, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  6.97it/s]100%|██████████| 30/30 [00:03<00:00,  7.78it/s]
CLIENT:73
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:04,  6.32it/s]                                              {'loss': 0.5826, 'grad_norm': 0.5973016619682312, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:04,  6.32it/s]  7%|▋         | 2/30 [00:00<00:04,  6.73it/s]                                              {'loss': 0.6869, 'grad_norm': 0.7835764288902283, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:04,  6.73it/s] 10%|█         | 3/30 [00:00<00:03,  6.81it/s]                                              {'loss': 0.6755, 'grad_norm': 0.6204802989959717, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:03,  6.81it/s] 13%|█▎        | 4/30 [00:00<00:04,  6.44it/s]                                              {'loss': 0.6985, 'grad_norm': 0.7373731732368469, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:04,  6.44it/s] 17%|█▋        | 5/30 [00:00<00:03,  7.29it/s]                                              {'loss': 0.6826, 'grad_norm': 0.7286970615386963, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:03,  7.29it/s]                                              {'loss': 0.9741, 'grad_norm': 2.0418312549591064, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:03,  7.29it/s] 23%|██▎       | 7/30 [00:00<00:02,  7.94it/s]                                              {'loss': 0.6121, 'grad_norm': 0.6558997631072998, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:02,  7.94it/s] 27%|██▋       | 8/30 [00:01<00:02,  7.47it/s]                                              {'loss': 0.7476, 'grad_norm': 0.9394941329956055, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:01<00:02,  7.47it/s] 30%|███       | 9/30 [00:01<00:02,  7.15it/s]                                              {'loss': 0.6264, 'grad_norm': 0.5604484677314758, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:01<00:02,  7.15it/s] 33%|███▎      | 10/30 [00:01<00:02,  6.90it/s]                                               {'loss': 0.626, 'grad_norm': 0.5569165945053101, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:01<00:02,  6.90it/s] 37%|███▋      | 11/30 [00:01<00:02,  6.84it/s]                                               {'loss': 0.667, 'grad_norm': 0.42801982164382935, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:01<00:02,  6.84it/s]                                               {'loss': 0.6213, 'grad_norm': 1.5007953643798828, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:01<00:02,  6.84it/s] 43%|████▎     | 13/30 [00:01<00:02,  8.15it/s]                                               {'loss': 0.6103, 'grad_norm': 0.6729414463043213, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:02,  8.15it/s] 47%|████▋     | 14/30 [00:01<00:02,  7.97it/s]                                               {'loss': 0.6062, 'grad_norm': 0.5975625514984131, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:02,  7.97it/s] 50%|█████     | 15/30 [00:02<00:01,  7.58it/s]                                               {'loss': 0.5884, 'grad_norm': 1.0658208131790161, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:02<00:01,  7.58it/s] 53%|█████▎    | 16/30 [00:02<00:01,  7.54it/s]                                               {'loss': 0.6242, 'grad_norm': 0.4444253146648407, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:02<00:01,  7.54it/s] 57%|█████▋    | 17/30 [00:02<00:01,  7.43it/s]                                               {'loss': 0.5904, 'grad_norm': 0.872673511505127, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:02<00:01,  7.43it/s]                                               {'loss': 0.634, 'grad_norm': 1.070995807647705, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:02<00:01,  7.43it/s] 63%|██████▎   | 19/30 [00:02<00:01,  7.69it/s]                                               {'loss': 0.5762, 'grad_norm': 0.9972718358039856, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:02<00:01,  7.69it/s] 67%|██████▋   | 20/30 [00:02<00:01,  7.36it/s]                                               {'loss': 0.6262, 'grad_norm': 0.7704617977142334, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:02<00:01,  7.36it/s] 70%|███████   | 21/30 [00:02<00:01,  7.30it/s]                                               {'loss': 0.5528, 'grad_norm': 1.1989097595214844, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:02<00:01,  7.30it/s] 73%|███████▎  | 22/30 [00:02<00:01,  7.28it/s]                                               {'loss': 0.5878, 'grad_norm': 0.9654400944709778, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:02<00:01,  7.28it/s] 77%|███████▋  | 23/30 [00:03<00:00,  7.30it/s]                                               {'loss': 0.5553, 'grad_norm': 0.9237344264984131, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:03<00:00,  7.30it/s]                                               {'loss': 0.5718, 'grad_norm': 1.4846067428588867, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:03<00:00,  7.30it/s] 83%|████████▎ | 25/30 [00:03<00:00,  8.28it/s]                                               {'loss': 0.4543, 'grad_norm': 1.0325819253921509, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:03<00:00,  8.28it/s] 87%|████████▋ | 26/30 [00:03<00:00,  7.76it/s]                                               {'loss': 0.5088, 'grad_norm': 1.2080663442611694, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:03<00:00,  7.76it/s] 90%|█████████ | 27/30 [00:03<00:00,  7.86it/s]                                               {'loss': 0.4689, 'grad_norm': 1.394718050956726, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:03<00:00,  7.86it/s] 93%|█████████▎| 28/30 [00:03<00:00,  7.85it/s]                                               {'loss': 0.4861, 'grad_norm': 1.08072829246521, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:03<00:00,  7.85it/s] 97%|█████████▋| 29/30 [00:03<00:00,  8.07it/s]                                               {'loss': 0.4796, 'grad_norm': 1.5316482782363892, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:03<00:00,  8.07it/s]                                               {'loss': 0.4388, 'grad_norm': 2.631131410598755, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.07it/s]                                               {'train_runtime': 3.9708, 'train_samples_per_second': 107.032, 'train_steps_per_second': 7.555, 'train_loss': 0.6053505877653758, 'epoch': 5.0}
100%|██████████| 30/30 [00:03<00:00,  8.07it/s]100%|██████████| 30/30 [00:03<00:00,  7.56it/s]
CLIENT:88
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  9.58it/s]                                              {'loss': 0.5451, 'grad_norm': 1.0334609746932983, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  9.58it/s]                                              {'loss': 0.4285, 'grad_norm': 1.3852062225341797, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02,  9.58it/s] 10%|█         | 3/30 [00:00<00:02, 11.36it/s]                                              {'loss': 0.3736, 'grad_norm': 0.685752809047699, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 11.36it/s]                                              {'loss': 0.3032, 'grad_norm': 0.7641302943229675, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 11.36it/s] 17%|█▋        | 5/30 [00:00<00:02, 12.24it/s]                                              {'loss': 0.8024, 'grad_norm': 2.30709171295166, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 12.24it/s]                                              {'loss': 0.0287, 'grad_norm': 0.47189074754714966, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:01, 12.24it/s] 23%|██▎       | 7/30 [00:00<00:01, 14.18it/s]                                              {'loss': 0.1403, 'grad_norm': 0.5707830786705017, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 14.18it/s]                                              {'loss': 0.5775, 'grad_norm': 1.7739235162734985, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 14.18it/s] 30%|███       | 9/30 [00:00<00:01, 13.67it/s]                                              {'loss': 0.6321, 'grad_norm': 2.4875142574310303, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 13.67it/s]                                              {'loss': 0.7351, 'grad_norm': 2.573763847351074, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 13.67it/s] 37%|███▋      | 11/30 [00:00<00:01, 13.11it/s]                                               {'loss': 0.2851, 'grad_norm': 1.304966688156128, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 13.11it/s]                                               {'loss': 0.4832, 'grad_norm': 7.322360992431641, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 13.11it/s] 43%|████▎     | 13/30 [00:00<00:01, 14.52it/s]                                               {'loss': 0.346, 'grad_norm': 1.8260999917984009, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:00<00:01, 14.52it/s]                                               {'loss': 0.3513, 'grad_norm': 2.23602557182312, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 14.52it/s] 50%|█████     | 15/30 [00:01<00:01, 13.96it/s]                                               {'loss': 0.3832, 'grad_norm': 3.862114191055298, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 13.96it/s]                                               {'loss': 0.4207, 'grad_norm': 2.2117066383361816, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 13.96it/s] 57%|█████▋    | 17/30 [00:01<00:00, 13.70it/s]                                               {'loss': 0.2288, 'grad_norm': 3.119842290878296, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:00, 13.70it/s]                                               {'loss': 0.4635, 'grad_norm': 4.459206581115723, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:00, 13.70it/s] 63%|██████▎   | 19/30 [00:01<00:00, 15.14it/s]                                               {'loss': 0.3536, 'grad_norm': 3.5336132049560547, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 15.14it/s]                                               {'loss': 0.5045, 'grad_norm': 4.109306812286377, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 15.14it/s] 70%|███████   | 21/30 [00:01<00:00, 14.59it/s]                                               {'loss': 0.3679, 'grad_norm': 2.1106903553009033, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 14.59it/s]                                               {'loss': 0.4739, 'grad_norm': 3.963913917541504, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 14.59it/s] 77%|███████▋  | 23/30 [00:01<00:00, 14.14it/s]                                               {'loss': 0.13, 'grad_norm': 2.29986310005188, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 14.14it/s]                                               {'loss': 0.321, 'grad_norm': 2.2684829235076904, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 14.14it/s] 83%|████████▎ | 25/30 [00:01<00:00, 15.23it/s]                                               {'loss': 0.3461, 'grad_norm': 2.8736274242401123, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:01<00:00, 15.23it/s]                                               {'loss': 0.4489, 'grad_norm': 4.689624786376953, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:01<00:00, 15.23it/s] 90%|█████████ | 27/30 [00:01<00:00, 14.56it/s]                                               {'loss': 0.4088, 'grad_norm': 1.82943594455719, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:01<00:00, 14.56it/s]                                               {'loss': 0.1026, 'grad_norm': 1.8074740171432495, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 14.56it/s] 97%|█████████▋| 29/30 [00:02<00:00, 13.75it/s]                                               {'loss': 0.2832, 'grad_norm': 1.124337911605835, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.75it/s]                                               {'loss': 0.3887, 'grad_norm': 5.187190532684326, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.75it/s]                                               {'train_runtime': 2.2115, 'train_samples_per_second': 192.18, 'train_steps_per_second': 13.566, 'train_loss': 0.388581283390522, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.75it/s]100%|██████████| 30/30 [00:02<00:00, 13.57it/s]
CLIENT:68
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:334: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:03,  9.43it/s]                                              {'loss': 0.5265, 'grad_norm': 0.3846413791179657, 'learning_rate': 0.001, 'epoch': 0.17}
  3%|▎         | 1/30 [00:00<00:03,  9.43it/s]                                              {'loss': 0.6402, 'grad_norm': 1.2295838594436646, 'learning_rate': 0.0009666666666666667, 'epoch': 0.33}
  7%|▋         | 2/30 [00:00<00:02,  9.43it/s] 10%|█         | 3/30 [00:00<00:02, 10.06it/s]                                              {'loss': 0.6529, 'grad_norm': 0.7580355405807495, 'learning_rate': 0.0009333333333333333, 'epoch': 0.5}
 10%|█         | 3/30 [00:00<00:02, 10.06it/s]                                              {'loss': 0.7353, 'grad_norm': 1.5363949537277222, 'learning_rate': 0.0009000000000000001, 'epoch': 0.67}
 13%|█▎        | 4/30 [00:00<00:02, 10.06it/s] 17%|█▋        | 5/30 [00:00<00:02, 10.99it/s]                                              {'loss': 0.6827, 'grad_norm': 1.8851794004440308, 'learning_rate': 0.0008666666666666667, 'epoch': 0.83}
 17%|█▋        | 5/30 [00:00<00:02, 10.99it/s]                                              {'loss': 0.824, 'grad_norm': 2.250617265701294, 'learning_rate': 0.0008333333333333334, 'epoch': 1.0}
 20%|██        | 6/30 [00:00<00:02, 10.99it/s] 23%|██▎       | 7/30 [00:00<00:01, 13.27it/s]                                              {'loss': 0.6929, 'grad_norm': 1.104607343673706, 'learning_rate': 0.0008, 'epoch': 1.17}
 23%|██▎       | 7/30 [00:00<00:01, 13.27it/s]                                              {'loss': 0.6352, 'grad_norm': 0.7466691136360168, 'learning_rate': 0.0007666666666666667, 'epoch': 1.33}
 27%|██▋       | 8/30 [00:00<00:01, 13.27it/s] 30%|███       | 9/30 [00:00<00:01, 12.71it/s]                                              {'loss': 0.6495, 'grad_norm': 0.9758611917495728, 'learning_rate': 0.0007333333333333333, 'epoch': 1.5}
 30%|███       | 9/30 [00:00<00:01, 12.71it/s]                                              {'loss': 0.6409, 'grad_norm': 1.2497097253799438, 'learning_rate': 0.0007, 'epoch': 1.67}
 33%|███▎      | 10/30 [00:00<00:01, 12.71it/s] 37%|███▋      | 11/30 [00:00<00:01, 11.98it/s]                                               {'loss': 0.6176, 'grad_norm': 1.016400694847107, 'learning_rate': 0.0006666666666666666, 'epoch': 1.83}
 37%|███▋      | 11/30 [00:00<00:01, 11.98it/s]                                               {'loss': 0.5773, 'grad_norm': 2.591850757598877, 'learning_rate': 0.0006333333333333333, 'epoch': 2.0}
 40%|████      | 12/30 [00:00<00:01, 11.98it/s] 43%|████▎     | 13/30 [00:01<00:01, 11.71it/s]                                               {'loss': 0.5336, 'grad_norm': 1.300840139389038, 'learning_rate': 0.0006, 'epoch': 2.17}
 43%|████▎     | 13/30 [00:01<00:01, 11.71it/s]                                               {'loss': 0.5835, 'grad_norm': 2.1111910343170166, 'learning_rate': 0.0005666666666666667, 'epoch': 2.33}
 47%|████▋     | 14/30 [00:01<00:01, 11.71it/s] 50%|█████     | 15/30 [00:01<00:01, 11.81it/s]                                               {'loss': 0.9177, 'grad_norm': 6.984413146972656, 'learning_rate': 0.0005333333333333334, 'epoch': 2.5}
 50%|█████     | 15/30 [00:01<00:01, 11.81it/s]                                               {'loss': 0.5912, 'grad_norm': 4.236138820648193, 'learning_rate': 0.0005, 'epoch': 2.67}
 53%|█████▎    | 16/30 [00:01<00:01, 11.81it/s] 57%|█████▋    | 17/30 [00:01<00:01, 11.98it/s]                                               {'loss': 0.5184, 'grad_norm': 2.4060540199279785, 'learning_rate': 0.00046666666666666666, 'epoch': 2.83}
 57%|█████▋    | 17/30 [00:01<00:01, 11.98it/s]                                               {'loss': 0.4972, 'grad_norm': 4.935009479522705, 'learning_rate': 0.00043333333333333337, 'epoch': 3.0}
 60%|██████    | 18/30 [00:01<00:01, 11.98it/s] 63%|██████▎   | 19/30 [00:01<00:00, 12.96it/s]                                               {'loss': 0.4714, 'grad_norm': 1.660699486732483, 'learning_rate': 0.0004, 'epoch': 3.17}
 63%|██████▎   | 19/30 [00:01<00:00, 12.96it/s]                                               {'loss': 0.5047, 'grad_norm': 1.641291856765747, 'learning_rate': 0.00036666666666666667, 'epoch': 3.33}
 67%|██████▋   | 20/30 [00:01<00:00, 12.96it/s] 70%|███████   | 21/30 [00:01<00:00, 12.72it/s]                                               {'loss': 0.4714, 'grad_norm': 1.6307824850082397, 'learning_rate': 0.0003333333333333333, 'epoch': 3.5}
 70%|███████   | 21/30 [00:01<00:00, 12.72it/s]                                               {'loss': 0.5277, 'grad_norm': 2.226687431335449, 'learning_rate': 0.0003, 'epoch': 3.67}
 73%|███████▎  | 22/30 [00:01<00:00, 12.72it/s] 77%|███████▋  | 23/30 [00:01<00:00, 12.61it/s]                                               {'loss': 0.5506, 'grad_norm': 1.5602467060089111, 'learning_rate': 0.0002666666666666667, 'epoch': 3.83}
 77%|███████▋  | 23/30 [00:01<00:00, 12.61it/s]                                               {'loss': 0.4481, 'grad_norm': 3.6573753356933594, 'learning_rate': 0.00023333333333333333, 'epoch': 4.0}
 80%|████████  | 24/30 [00:01<00:00, 12.61it/s] 83%|████████▎ | 25/30 [00:02<00:00, 13.72it/s]                                               {'loss': 0.42, 'grad_norm': 1.3208009004592896, 'learning_rate': 0.0002, 'epoch': 4.17}
 83%|████████▎ | 25/30 [00:02<00:00, 13.72it/s]                                               {'loss': 0.5441, 'grad_norm': 3.2047970294952393, 'learning_rate': 0.00016666666666666666, 'epoch': 4.33}
 87%|████████▋ | 26/30 [00:02<00:00, 13.72it/s] 90%|█████████ | 27/30 [00:02<00:00, 13.48it/s]                                               {'loss': 0.4308, 'grad_norm': 1.9312686920166016, 'learning_rate': 0.00013333333333333334, 'epoch': 4.5}
 90%|█████████ | 27/30 [00:02<00:00, 13.48it/s]                                               {'loss': 0.4789, 'grad_norm': 2.8261842727661133, 'learning_rate': 0.0001, 'epoch': 4.67}
 93%|█████████▎| 28/30 [00:02<00:00, 13.48it/s] 97%|█████████▋| 29/30 [00:02<00:00, 13.28it/s]                                               {'loss': 0.4273, 'grad_norm': 1.4018621444702148, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.83}
 97%|█████████▋| 29/30 [00:02<00:00, 13.28it/s]                                               {'loss': 0.3761, 'grad_norm': 2.6464662551879883, 'learning_rate': 3.3333333333333335e-05, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.28it/s]                                               {'train_runtime': 2.4149, 'train_samples_per_second': 175.993, 'train_steps_per_second': 12.423, 'train_loss': 0.5722492774327596, 'epoch': 5.0}
100%|██████████| 30/30 [00:02<00:00, 13.28it/s]100%|██████████| 30/30 [00:02<00:00, 12.43it/s]
/home/suxiaoxin/Paper/Fed_LLM/fed_trainer.py:367: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|          | 0/66 [00:00<?, ?it/s]  5%|▍         | 3/66 [00:00<00:02, 23.38it/s]  9%|▉         | 6/66 [00:00<00:03, 18.96it/s] 12%|█▏        | 8/66 [00:00<00:03, 17.13it/s] 15%|█▌        | 10/66 [00:00<00:03, 16.43it/s] 18%|█▊        | 12/66 [00:00<00:03, 15.73it/s] 21%|██        | 14/66 [00:00<00:03, 14.74it/s] 24%|██▍       | 16/66 [00:00<00:03, 15.54it/s] 27%|██▋       | 18/66 [00:01<00:02, 16.18it/s] 30%|███       | 20/66 [00:01<00:02, 15.36it/s] 33%|███▎      | 22/66 [00:01<00:02, 15.06it/s] 36%|███▋      | 24/66 [00:01<00:02, 15.15it/s] 39%|███▉      | 26/66 [00:01<00:02, 15.25it/s] 42%|████▏     | 28/66 [00:01<00:02, 15.35it/s] 45%|████▌     | 30/66 [00:01<00:02, 15.51it/s] 48%|████▊     | 32/66 [00:02<00:02, 15.43it/s] 52%|█████▏    | 34/66 [00:02<00:01, 16.08it/s] 55%|█████▍    | 36/66 [00:02<00:01, 15.61it/s] 58%|█████▊    | 38/66 [00:02<00:01, 15.24it/s] 61%|██████    | 40/66 [00:02<00:01, 15.06it/s] 64%|██████▎   | 42/66 [00:02<00:01, 15.03it/s] 67%|██████▋   | 44/66 [00:02<00:01, 15.48it/s] 70%|██████▉   | 46/66 [00:02<00:01, 15.82it/s] 73%|███████▎  | 48/66 [00:03<00:01, 15.68it/s] 76%|███████▌  | 50/66 [00:03<00:01, 15.83it/s] 80%|████████  | 53/66 [00:03<00:00, 19.02it/s] 86%|████████▋ | 57/66 [00:03<00:00, 22.57it/s] 91%|█████████ | 60/66 [00:03<00:00, 23.81it/s] 95%|█████████▌| 63/66 [00:03<00:00, 23.19it/s]100%|██████████| 66/66 [00:03<00:00, 17.56it/s]
{'eval_loss': 0.6322367787361145, 'eval_model_preparation_time': 0.0064, 'eval_acc': 0.6912751677852349, 'eval_runtime': 3.8045, 'eval_samples_per_second': 274.149, 'eval_steps_per_second': 17.348}
